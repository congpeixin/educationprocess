2017-08-09 15:28:52,167 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 154.0 with 2 tasks
2017-08-09 15:28:52,176 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 154.0 (TID 308, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,178 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 154.0 (TID 309, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,178 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 154.0 (TID 308)
2017-08-09 15:28:52,178 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 154.0 (TID 309)
2017-08-09 15:28:52,180 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,180 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,182 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 154.0 (TID 308). 714 bytes result sent to driver
2017-08-09 15:28:52,182 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 154.0 (TID 309). 714 bytes result sent to driver
2017-08-09 15:28:52,183 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 154.0 (TID 308) in 7 ms on localhost (1/2)
2017-08-09 15:28:52,183 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 154.0 (TID 309) in 6 ms on localhost (2/2)
2017-08-09 15:28:52,183 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 154.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,183 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 154 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,183 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 154 finished: foreachPartition at streamingProcess.scala:50, took 0.021861 s
2017-08-09 15:28:52,184 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263655000 ms.0 from job set of time 1502263655000 ms
2017-08-09 15:28:52,184 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 77.184 s for time 1502263655000 ms (execution: 0.032 s)
2017-08-09 15:28:52,184 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 307 from persistence list
2017-08-09 15:28:52,184 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263660000 ms.0 from job set of time 1502263660000 ms
2017-08-09 15:28:52,184 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 307
2017-08-09 15:28:52,184 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 306 from persistence list
2017-08-09 15:28:52,184 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 306
2017-08-09 15:28:52,184 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,185 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263645000 ms
2017-08-09 15:28:52,195 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,195 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 155 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,195 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 155 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,196 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,196 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,196 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 155 (MapPartitionsRDD[311] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_155 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-09 15:28:52,200 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_155_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-09 15:28:52,200 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_155_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:28:52,200 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 155 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,200 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 155 (MapPartitionsRDD[311] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,200 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 155.0 with 2 tasks
2017-08-09 15:28:52,201 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 155.0 (TID 310, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,201 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 155.0 (TID 311, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,201 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 155.0 (TID 310)
2017-08-09 15:28:52,201 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 155.0 (TID 311)
2017-08-09 15:28:52,203 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,203 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,206 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 155.0 (TID 310). 714 bytes result sent to driver
2017-08-09 15:28:52,206 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 155.0 (TID 311). 714 bytes result sent to driver
2017-08-09 15:28:52,207 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 155.0 (TID 311) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,207 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 155.0 (TID 310) in 6 ms on localhost (2/2)
2017-08-09 15:28:52,207 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 155.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,207 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 155 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-09 15:28:52,207 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 155 finished: foreachPartition at streamingProcess.scala:50, took 0.011885 s
2017-08-09 15:28:52,207 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263660000 ms.0 from job set of time 1502263660000 ms
2017-08-09 15:28:52,207 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 72.207 s for time 1502263660000 ms (execution: 0.023 s)
2017-08-09 15:28:52,207 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 309 from persistence list
2017-08-09 15:28:52,207 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263665000 ms.0 from job set of time 1502263665000 ms
2017-08-09 15:28:52,208 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 308 from persistence list
2017-08-09 15:28:52,208 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 309
2017-08-09 15:28:52,208 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 308
2017-08-09 15:28:52,208 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,208 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263650000 ms
2017-08-09 15:28:52,217 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,217 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 156 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,217 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 156 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,217 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,217 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,217 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 156 (MapPartitionsRDD[313] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,221 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_156 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-09 15:28:52,224 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_156_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-09 15:28:52,224 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_156_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:28:52,225 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 156 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,225 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 156 (MapPartitionsRDD[313] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,225 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 156.0 with 2 tasks
2017-08-09 15:28:52,226 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 156.0 (TID 312, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,226 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 156.0 (TID 313, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,226 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 156.0 (TID 312)
2017-08-09 15:28:52,226 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 156.0 (TID 313)
2017-08-09 15:28:52,236 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_141_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:28:52,236 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,236 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,238 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_142_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,239 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_143_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,240 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 156.0 (TID 313). 787 bytes result sent to driver
2017-08-09 15:28:52,240 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 156.0 (TID 312). 787 bytes result sent to driver
2017-08-09 15:28:52,242 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 156.0 (TID 312) in 17 ms on localhost (1/2)
2017-08-09 15:28:52,242 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_144_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,242 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 156.0 (TID 313) in 16 ms on localhost (2/2)
2017-08-09 15:28:52,242 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 156.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 156 (foreachPartition at streamingProcess.scala:50) finished in 0.017 s
2017-08-09 15:28:52,242 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 156 finished: foreachPartition at streamingProcess.scala:50, took 0.025630 s
2017-08-09 15:28:52,243 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263665000 ms.0 from job set of time 1502263665000 ms
2017-08-09 15:28:52,243 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 67.243 s for time 1502263665000 ms (execution: 0.036 s)
2017-08-09 15:28:52,243 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263670000 ms.0 from job set of time 1502263670000 ms
2017-08-09 15:28:52,243 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 311 from persistence list
2017-08-09 15:28:52,245 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_145_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,245 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 311
2017-08-09 15:28:52,245 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 310 from persistence list
2017-08-09 15:28:52,245 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 310
2017-08-09 15:28:52,245 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,245 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263655000 ms
2017-08-09 15:28:52,246 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_146_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,246 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_147_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,247 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_148_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,248 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_149_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,249 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_150_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,250 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_151_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,250 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_152_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,251 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_153_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,251 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_154_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:28:52,252 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_155_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:28:52,256 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,257 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 157 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,257 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 157 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,257 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,257 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,257 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 157 (MapPartitionsRDD[315] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_157 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-09 15:28:52,262 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_157_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-09 15:28:52,262 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_157_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:28:52,263 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 157 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,263 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 157 (MapPartitionsRDD[315] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,263 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 157.0 with 2 tasks
2017-08-09 15:28:52,263 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 157.0 (TID 314, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,263 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 157.0 (TID 315, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,264 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 157.0 (TID 314)
2017-08-09 15:28:52,264 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 157.0 (TID 315)
2017-08-09 15:28:52,266 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,266 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,269 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 157.0 (TID 315). 714 bytes result sent to driver
2017-08-09 15:28:52,269 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 157.0 (TID 314). 714 bytes result sent to driver
2017-08-09 15:28:52,272 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 157.0 (TID 314) in 9 ms on localhost (1/2)
2017-08-09 15:28:52,272 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 157.0 (TID 315) in 9 ms on localhost (2/2)
2017-08-09 15:28:52,272 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 157.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,272 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 157 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-09 15:28:52,272 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 157 finished: foreachPartition at streamingProcess.scala:50, took 0.016333 s
2017-08-09 15:28:52,273 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263670000 ms.0 from job set of time 1502263670000 ms
2017-08-09 15:28:52,273 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 62.273 s for time 1502263670000 ms (execution: 0.030 s)
2017-08-09 15:28:52,273 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 313 from persistence list
2017-08-09 15:28:52,273 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263675000 ms.0 from job set of time 1502263675000 ms
2017-08-09 15:28:52,273 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 313
2017-08-09 15:28:52,273 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 312 from persistence list
2017-08-09 15:28:52,273 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 312
2017-08-09 15:28:52,273 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,273 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263660000 ms
2017-08-09 15:28:52,285 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 158 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 158 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 158 (MapPartitionsRDD[317] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,292 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_158 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-09 15:28:52,294 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_158_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-09 15:28:52,295 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_158_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,296 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 158 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,296 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 158 (MapPartitionsRDD[317] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,296 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 158.0 with 2 tasks
2017-08-09 15:28:52,297 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 158.0 (TID 316, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,297 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 158.0 (TID 317, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,297 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 158.0 (TID 317)
2017-08-09 15:28:52,297 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 158.0 (TID 316)
2017-08-09 15:28:52,299 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,299 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,302 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 158.0 (TID 317). 714 bytes result sent to driver
2017-08-09 15:28:52,302 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 158.0 (TID 316). 714 bytes result sent to driver
2017-08-09 15:28:52,303 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 158.0 (TID 317) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,303 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 158.0 (TID 316) in 7 ms on localhost (2/2)
2017-08-09 15:28:52,303 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 158.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,304 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 158 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,304 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 158 finished: foreachPartition at streamingProcess.scala:50, took 0.018555 s
2017-08-09 15:28:52,305 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263675000 ms.0 from job set of time 1502263675000 ms
2017-08-09 15:28:52,306 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 57.305 s for time 1502263675000 ms (execution: 0.032 s)
2017-08-09 15:28:52,306 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263680000 ms.0 from job set of time 1502263680000 ms
2017-08-09 15:28:52,306 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 315 from persistence list
2017-08-09 15:28:52,307 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 315
2017-08-09 15:28:52,307 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 314 from persistence list
2017-08-09 15:28:52,308 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 314
2017-08-09 15:28:52,308 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,308 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263665000 ms
2017-08-09 15:28:52,320 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,320 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 159 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,320 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 159 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,320 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,321 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,321 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 159 (MapPartitionsRDD[319] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,324 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_159 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-09 15:28:52,327 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_159_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-09 15:28:52,328 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_159_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,328 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 159 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,328 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 159 (MapPartitionsRDD[319] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,328 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 159.0 with 2 tasks
2017-08-09 15:28:52,329 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 159.0 (TID 318, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,329 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 159.0 (TID 319, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,329 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 159.0 (TID 319)
2017-08-09 15:28:52,329 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 159.0 (TID 318)
2017-08-09 15:28:52,331 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,331 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,334 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 159.0 (TID 319). 714 bytes result sent to driver
2017-08-09 15:28:52,334 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 159.0 (TID 318). 714 bytes result sent to driver
2017-08-09 15:28:52,335 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 159.0 (TID 319) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,335 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 159.0 (TID 318) in 7 ms on localhost (2/2)
2017-08-09 15:28:52,335 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 159.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 159 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,335 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 159 finished: foreachPartition at streamingProcess.scala:50, took 0.015267 s
2017-08-09 15:28:52,335 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263680000 ms.0 from job set of time 1502263680000 ms
2017-08-09 15:28:52,336 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 52.335 s for time 1502263680000 ms (execution: 0.029 s)
2017-08-09 15:28:52,336 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263685000 ms.0 from job set of time 1502263685000 ms
2017-08-09 15:28:52,336 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 317 from persistence list
2017-08-09 15:28:52,336 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 317
2017-08-09 15:28:52,336 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 316 from persistence list
2017-08-09 15:28:52,336 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 316
2017-08-09 15:28:52,336 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,336 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263670000 ms
2017-08-09 15:28:52,347 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 160 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 160 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 160 (MapPartitionsRDD[321] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,350 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_160 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-09 15:28:52,352 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_160_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-09 15:28:52,352 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_160_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,352 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 160 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,352 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 160 (MapPartitionsRDD[321] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,352 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 160.0 with 2 tasks
2017-08-09 15:28:52,353 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 160.0 (TID 320, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,353 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 160.0 (TID 321, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,353 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 160.0 (TID 320)
2017-08-09 15:28:52,353 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 160.0 (TID 321)
2017-08-09 15:28:52,355 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,355 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,359 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 160.0 (TID 321). 714 bytes result sent to driver
2017-08-09 15:28:52,359 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 160.0 (TID 320). 714 bytes result sent to driver
2017-08-09 15:28:52,360 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 160.0 (TID 321) in 7 ms on localhost (1/2)
2017-08-09 15:28:52,360 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 160.0 (TID 320) in 8 ms on localhost (2/2)
2017-08-09 15:28:52,360 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 160.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,360 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 160 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-09 15:28:52,361 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 160 finished: foreachPartition at streamingProcess.scala:50, took 0.013593 s
2017-08-09 15:28:52,361 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263685000 ms.0 from job set of time 1502263685000 ms
2017-08-09 15:28:52,361 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 47.361 s for time 1502263685000 ms (execution: 0.025 s)
2017-08-09 15:28:52,361 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263690000 ms.0 from job set of time 1502263690000 ms
2017-08-09 15:28:52,361 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 319 from persistence list
2017-08-09 15:28:52,361 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 319
2017-08-09 15:28:52,361 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 318 from persistence list
2017-08-09 15:28:52,361 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 318
2017-08-09 15:28:52,361 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,361 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263675000 ms
2017-08-09 15:28:52,370 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,370 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 161 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,370 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 161 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,370 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,370 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,371 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 161 (MapPartitionsRDD[323] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,373 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_161 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-09 15:28:52,375 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_161_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-09 15:28:52,375 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_161_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,375 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 161 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,376 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 161 (MapPartitionsRDD[323] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,376 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 161.0 with 2 tasks
2017-08-09 15:28:52,376 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 161.0 (TID 322, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,377 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 161.0 (TID 323, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,377 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 161.0 (TID 323)
2017-08-09 15:28:52,377 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 161.0 (TID 322)
2017-08-09 15:28:52,378 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,378 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,381 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 161.0 (TID 322). 714 bytes result sent to driver
2017-08-09 15:28:52,381 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 161.0 (TID 323). 714 bytes result sent to driver
2017-08-09 15:28:52,382 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 161.0 (TID 323) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,383 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 161.0 (TID 322) in 7 ms on localhost (2/2)
2017-08-09 15:28:52,383 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 161.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,383 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 161 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,383 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 161 finished: foreachPartition at streamingProcess.scala:50, took 0.013000 s
2017-08-09 15:28:52,383 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263690000 ms.0 from job set of time 1502263690000 ms
2017-08-09 15:28:52,383 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 42.383 s for time 1502263690000 ms (execution: 0.022 s)
2017-08-09 15:28:52,383 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 321 from persistence list
2017-08-09 15:28:52,383 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263695000 ms.0 from job set of time 1502263695000 ms
2017-08-09 15:28:52,384 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 321
2017-08-09 15:28:52,384 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 320 from persistence list
2017-08-09 15:28:52,384 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 320
2017-08-09 15:28:52,384 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,384 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263680000 ms
2017-08-09 15:28:52,393 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,393 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 162 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 162 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 162 (MapPartitionsRDD[325] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,396 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_162 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-09 15:28:52,398 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_162_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-09 15:28:52,399 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_162_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 162 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,400 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 162 (MapPartitionsRDD[325] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,400 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 162.0 with 2 tasks
2017-08-09 15:28:52,400 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 162.0 (TID 324, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,400 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 162.0 (TID 325, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,401 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 162.0 (TID 324)
2017-08-09 15:28:52,401 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 162.0 (TID 325)
2017-08-09 15:28:52,402 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,402 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,405 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 162.0 (TID 324). 714 bytes result sent to driver
2017-08-09 15:28:52,405 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 162.0 (TID 325). 714 bytes result sent to driver
2017-08-09 15:28:52,406 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 162.0 (TID 325) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,406 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 162.0 (TID 324) in 6 ms on localhost (2/2)
2017-08-09 15:28:52,407 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 162.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 162 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,407 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 162 finished: foreachPartition at streamingProcess.scala:50, took 0.013489 s
2017-08-09 15:28:52,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263695000 ms.0 from job set of time 1502263695000 ms
2017-08-09 15:28:52,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 37.407 s for time 1502263695000 ms (execution: 0.024 s)
2017-08-09 15:28:52,407 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 323 from persistence list
2017-08-09 15:28:52,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263700000 ms.0 from job set of time 1502263700000 ms
2017-08-09 15:28:52,407 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 323
2017-08-09 15:28:52,407 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 322 from persistence list
2017-08-09 15:28:52,408 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 322
2017-08-09 15:28:52,408 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,408 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263685000 ms
2017-08-09 15:28:52,416 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 163 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 163 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 163 (MapPartitionsRDD[327] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_163 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-09 15:28:52,422 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_163_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-09 15:28:52,422 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_163_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,423 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 163 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,423 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 163 (MapPartitionsRDD[327] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,423 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 163.0 with 2 tasks
2017-08-09 15:28:52,423 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 163.0 (TID 326, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,424 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 163.0 (TID 327, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,424 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 163.0 (TID 327)
2017-08-09 15:28:52,424 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 163.0 (TID 326)
2017-08-09 15:28:52,426 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,426 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,429 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 163.0 (TID 326). 714 bytes result sent to driver
2017-08-09 15:28:52,429 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 163.0 (TID 327). 714 bytes result sent to driver
2017-08-09 15:28:52,430 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 163.0 (TID 326) in 7 ms on localhost (1/2)
2017-08-09 15:28:52,431 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 163.0 (TID 327) in 7 ms on localhost (2/2)
2017-08-09 15:28:52,431 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 163.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,431 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 163 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-09 15:28:52,431 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 163 finished: foreachPartition at streamingProcess.scala:50, took 0.014389 s
2017-08-09 15:28:52,431 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263700000 ms.0 from job set of time 1502263700000 ms
2017-08-09 15:28:52,431 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 32.431 s for time 1502263700000 ms (execution: 0.024 s)
2017-08-09 15:28:52,431 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 325 from persistence list
2017-08-09 15:28:52,431 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263705000 ms.0 from job set of time 1502263705000 ms
2017-08-09 15:28:52,432 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 325
2017-08-09 15:28:52,432 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 324 from persistence list
2017-08-09 15:28:52,432 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 324
2017-08-09 15:28:52,432 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,432 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263690000 ms
2017-08-09 15:28:52,442 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,442 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 164 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,443 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 164 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,443 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,443 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,443 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 164 (MapPartitionsRDD[329] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,446 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_164 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-09 15:28:52,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_164_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-09 15:28:52,448 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_164_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,449 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 164 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,449 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 164 (MapPartitionsRDD[329] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,449 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 164.0 with 2 tasks
2017-08-09 15:28:52,449 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 164.0 (TID 328, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,450 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 164.0 (TID 329, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,450 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 164.0 (TID 329)
2017-08-09 15:28:52,450 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 164.0 (TID 328)
2017-08-09 15:28:52,452 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,452 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,454 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 164.0 (TID 328). 714 bytes result sent to driver
2017-08-09 15:28:52,454 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 164.0 (TID 329). 714 bytes result sent to driver
2017-08-09 15:28:52,456 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 164.0 (TID 328) in 7 ms on localhost (1/2)
2017-08-09 15:28:52,456 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 164.0 (TID 329) in 7 ms on localhost (2/2)
2017-08-09 15:28:52,456 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 164.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,457 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 164 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,457 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 164 finished: foreachPartition at streamingProcess.scala:50, took 0.014502 s
2017-08-09 15:28:52,457 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263705000 ms.0 from job set of time 1502263705000 ms
2017-08-09 15:28:52,457 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 27.457 s for time 1502263705000 ms (execution: 0.026 s)
2017-08-09 15:28:52,457 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 327 from persistence list
2017-08-09 15:28:52,457 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263710000 ms.0 from job set of time 1502263710000 ms
2017-08-09 15:28:52,457 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 327
2017-08-09 15:28:52,458 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 326 from persistence list
2017-08-09 15:28:52,458 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 326
2017-08-09 15:28:52,458 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,458 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263695000 ms
2017-08-09 15:28:52,468 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 165 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 165 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 165 (MapPartitionsRDD[331] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,472 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_165 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-09 15:28:52,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_165_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-09 15:28:52,476 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_165_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,476 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 165 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,476 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 165 (MapPartitionsRDD[331] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,476 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 165.0 with 2 tasks
2017-08-09 15:28:52,477 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 165.0 (TID 330, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,477 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 165.0 (TID 331, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,477 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 165.0 (TID 330)
2017-08-09 15:28:52,477 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 165.0 (TID 331)
2017-08-09 15:28:52,479 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,479 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,481 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 165.0 (TID 330). 714 bytes result sent to driver
2017-08-09 15:28:52,481 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 165.0 (TID 331). 714 bytes result sent to driver
2017-08-09 15:28:52,483 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 165.0 (TID 330) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,483 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 165.0 (TID 331) in 6 ms on localhost (2/2)
2017-08-09 15:28:52,483 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 165.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,483 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 165 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,483 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 165 finished: foreachPartition at streamingProcess.scala:50, took 0.015302 s
2017-08-09 15:28:52,484 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263710000 ms.0 from job set of time 1502263710000 ms
2017-08-09 15:28:52,484 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 22.484 s for time 1502263710000 ms (execution: 0.027 s)
2017-08-09 15:28:52,484 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 329 from persistence list
2017-08-09 15:28:52,484 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 328 from persistence list
2017-08-09 15:28:52,484 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263715000 ms.0 from job set of time 1502263715000 ms
2017-08-09 15:28:52,484 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 329
2017-08-09 15:28:52,485 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 328
2017-08-09 15:28:52,485 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,485 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263700000 ms
2017-08-09 15:28:52,496 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 166 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 166 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 166 (MapPartitionsRDD[333] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,500 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_166 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-09 15:28:52,502 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_166_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-09 15:28:52,502 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_166_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,502 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 166 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,503 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 166 (MapPartitionsRDD[333] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,503 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 166.0 with 2 tasks
2017-08-09 15:28:52,503 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 166.0 (TID 332, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,504 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 166.0 (TID 333, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,504 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 166.0 (TID 332)
2017-08-09 15:28:52,504 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 166.0 (TID 333)
2017-08-09 15:28:52,506 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,506 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,509 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 166.0 (TID 332). 714 bytes result sent to driver
2017-08-09 15:28:52,509 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 166.0 (TID 333). 714 bytes result sent to driver
2017-08-09 15:28:52,511 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 166.0 (TID 332) in 8 ms on localhost (1/2)
2017-08-09 15:28:52,511 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 166.0 (TID 333) in 8 ms on localhost (2/2)
2017-08-09 15:28:52,511 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 166.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,511 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 166 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-09 15:28:52,511 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 166 finished: foreachPartition at streamingProcess.scala:50, took 0.015525 s
2017-08-09 15:28:52,512 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263715000 ms.0 from job set of time 1502263715000 ms
2017-08-09 15:28:52,512 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 17.512 s for time 1502263715000 ms (execution: 0.028 s)
2017-08-09 15:28:52,512 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 331 from persistence list
2017-08-09 15:28:52,512 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263720000 ms.0 from job set of time 1502263720000 ms
2017-08-09 15:28:52,512 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 331
2017-08-09 15:28:52,512 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 330 from persistence list
2017-08-09 15:28:52,512 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 330
2017-08-09 15:28:52,512 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,513 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263705000 ms
2017-08-09 15:28:52,522 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,523 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 167 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,523 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 167 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,523 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,523 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,523 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 167 (MapPartitionsRDD[335] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,525 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_167 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-09 15:28:52,528 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_167_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-09 15:28:52,528 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_167_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,528 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 167 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,528 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 167 (MapPartitionsRDD[335] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,528 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 167.0 with 2 tasks
2017-08-09 15:28:52,529 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 167.0 (TID 334, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,529 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 167.0 (TID 335, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,529 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 167.0 (TID 334)
2017-08-09 15:28:52,529 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 167.0 (TID 335)
2017-08-09 15:28:52,531 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,531 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,534 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 167.0 (TID 335). 801 bytes result sent to driver
2017-08-09 15:28:52,534 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 167.0 (TID 334). 801 bytes result sent to driver
2017-08-09 15:28:52,535 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 167.0 (TID 335) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,535 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 167.0 (TID 334) in 6 ms on localhost (2/2)
2017-08-09 15:28:52,535 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 167.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,535 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 167 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,535 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 167 finished: foreachPartition at streamingProcess.scala:50, took 0.012776 s
2017-08-09 15:28:52,536 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263720000 ms.0 from job set of time 1502263720000 ms
2017-08-09 15:28:52,536 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 12.536 s for time 1502263720000 ms (execution: 0.024 s)
2017-08-09 15:28:52,536 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 333 from persistence list
2017-08-09 15:28:52,536 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263725000 ms.0 from job set of time 1502263725000 ms
2017-08-09 15:28:52,536 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 333
2017-08-09 15:28:52,536 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 332 from persistence list
2017-08-09 15:28:52,536 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 332
2017-08-09 15:28:52,536 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,536 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263710000 ms
2017-08-09 15:28:52,546 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,546 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 168 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,546 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 168 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,546 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,546 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,546 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 168 (MapPartitionsRDD[337] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,548 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_168 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-09 15:28:52,550 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_168_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-09 15:28:52,550 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_168_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,551 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 168 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,551 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 168 (MapPartitionsRDD[337] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,551 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 168.0 with 2 tasks
2017-08-09 15:28:52,551 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 168.0 (TID 336, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,552 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 168.0 (TID 337, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,552 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 168.0 (TID 337)
2017-08-09 15:28:52,552 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 168.0 (TID 336)
2017-08-09 15:28:52,554 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,554 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,556 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 168.0 (TID 336). 714 bytes result sent to driver
2017-08-09 15:28:52,556 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 168.0 (TID 337). 714 bytes result sent to driver
2017-08-09 15:28:52,557 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 168.0 (TID 336) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,558 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 168.0 (TID 337) in 7 ms on localhost (2/2)
2017-08-09 15:28:52,558 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 168.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,558 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 168 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,558 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 168 finished: foreachPartition at streamingProcess.scala:50, took 0.012137 s
2017-08-09 15:28:52,558 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263725000 ms.0 from job set of time 1502263725000 ms
2017-08-09 15:28:52,558 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 7.558 s for time 1502263725000 ms (execution: 0.022 s)
2017-08-09 15:28:52,558 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 335 from persistence list
2017-08-09 15:28:52,558 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263730000 ms.0 from job set of time 1502263730000 ms
2017-08-09 15:28:52,559 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 335
2017-08-09 15:28:52,559 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 334 from persistence list
2017-08-09 15:28:52,559 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 334
2017-08-09 15:28:52,559 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,559 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263715000 ms
2017-08-09 15:28:52,568 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 169 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 169 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 169 (MapPartitionsRDD[339] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,571 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_169 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-09 15:28:52,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_169_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-09 15:28:52,573 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_169_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,574 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 169 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,574 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 169 (MapPartitionsRDD[339] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,574 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 169.0 with 2 tasks
2017-08-09 15:28:52,575 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 169.0 (TID 338, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,575 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 169.0 (TID 339, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,575 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 169.0 (TID 338)
2017-08-09 15:28:52,575 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 169.0 (TID 339)
2017-08-09 15:28:52,577 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,577 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,579 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 169.0 (TID 339). 714 bytes result sent to driver
2017-08-09 15:28:52,579 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 169.0 (TID 338). 714 bytes result sent to driver
2017-08-09 15:28:52,580 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 169.0 (TID 339) in 5 ms on localhost (1/2)
2017-08-09 15:28:52,580 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 169.0 (TID 338) in 6 ms on localhost (2/2)
2017-08-09 15:28:52,580 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 169.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,580 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 169 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-09 15:28:52,580 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 169 finished: foreachPartition at streamingProcess.scala:50, took 0.011870 s
2017-08-09 15:28:52,581 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263730000 ms.0 from job set of time 1502263730000 ms
2017-08-09 15:28:52,581 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.580 s for time 1502263730000 ms (execution: 0.022 s)
2017-08-09 15:28:52,581 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 337 from persistence list
2017-08-09 15:28:52,581 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 337
2017-08-09 15:28:52,581 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 336 from persistence list
2017-08-09 15:28:52,581 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 336
2017-08-09 15:28:52,581 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,581 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263720000 ms
2017-08-09 15:28:55,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263735000 ms
2017-08-09 15:28:55,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263735000 ms.0 from job set of time 1502263735000 ms
2017-08-09 15:28:55,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 170 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 170 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 170 (MapPartitionsRDD[341] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:55,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_170 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-09 15:28:55,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_170_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-09 15:28:55,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_170_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:28:55,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 170 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:55,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 170 (MapPartitionsRDD[341] at map at streamingProcess.scala:48)
2017-08-09 15:28:55,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 170.0 with 2 tasks
2017-08-09 15:28:55,037 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 170.0 (TID 340, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:55,037 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 170.0 (TID 341, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:55,038 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 170.0 (TID 340)
2017-08-09 15:28:55,038 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 170.0 (TID 341)
2017-08-09 15:28:55,040 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:55,040 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:55,043 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 170.0 (TID 341). 714 bytes result sent to driver
2017-08-09 15:28:55,043 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 170.0 (TID 340). 714 bytes result sent to driver
2017-08-09 15:28:55,045 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 170.0 (TID 341) in 8 ms on localhost (1/2)
2017-08-09 15:28:55,045 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 170.0 (TID 340) in 9 ms on localhost (2/2)
2017-08-09 15:28:55,045 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 170.0, whose tasks have all completed, from pool 
2017-08-09 15:28:55,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 170 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-09 15:28:55,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 170 finished: foreachPartition at streamingProcess.scala:50, took 0.016644 s
2017-08-09 15:28:55,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263735000 ms.0 from job set of time 1502263735000 ms
2017-08-09 15:28:55,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.046 s for time 1502263735000 ms (execution: 0.030 s)
2017-08-09 15:28:55,046 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 339 from persistence list
2017-08-09 15:28:55,046 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 339
2017-08-09 15:28:55,046 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 338 from persistence list
2017-08-09 15:28:55,046 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 338
2017-08-09 15:28:55,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:55,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263725000 ms
2017-08-09 15:29:00,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263740000 ms
2017-08-09 15:29:00,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263740000 ms.0 from job set of time 1502263740000 ms
2017-08-09 15:29:00,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 171 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 171 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:00,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:00,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 171 (MapPartitionsRDD[343] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_171 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-09 15:29:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_171_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-09 15:29:00,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_171_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:29:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 171 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 171 (MapPartitionsRDD[343] at map at streamingProcess.scala:48)
2017-08-09 15:29:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 171.0 with 2 tasks
2017-08-09 15:29:00,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 171.0 (TID 342, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:00,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 171.0 (TID 343, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:00,056 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 171.0 (TID 343)
2017-08-09 15:29:00,056 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 171.0 (TID 342)
2017-08-09 15:29:00,058 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:00,058 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:00,061 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 171.0 (TID 342). 714 bytes result sent to driver
2017-08-09 15:29:00,061 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 171.0 (TID 343). 714 bytes result sent to driver
2017-08-09 15:29:00,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 171.0 (TID 342) in 8 ms on localhost (1/2)
2017-08-09 15:29:00,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 171.0 (TID 343) in 8 ms on localhost (2/2)
2017-08-09 15:29:00,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 171.0, whose tasks have all completed, from pool 
2017-08-09 15:29:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 171 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-09 15:29:00,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 171 finished: foreachPartition at streamingProcess.scala:50, took 0.021332 s
2017-08-09 15:29:00,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263740000 ms.0 from job set of time 1502263740000 ms
2017-08-09 15:29:00,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1502263740000 ms (execution: 0.045 s)
2017-08-09 15:29:00,064 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 341 from persistence list
2017-08-09 15:29:00,064 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 341
2017-08-09 15:29:00,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 340 from persistence list
2017-08-09 15:29:00,064 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 340
2017-08-09 15:29:00,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:00,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263730000 ms
2017-08-09 15:29:05,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263745000 ms
2017-08-09 15:29:05,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263745000 ms.0 from job set of time 1502263745000 ms
2017-08-09 15:29:05,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:05,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 172 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:05,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 172 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:05,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:05,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:05,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 172 (MapPartitionsRDD[345] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:05,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_172 stored as values in memory (estimated size 34.5 KB, free 412.9 MB)
2017-08-09 15:29:05,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_172_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-09 15:29:05,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_172_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:29:05,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_156_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:29:05,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 172 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:05,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 172 (MapPartitionsRDD[345] at map at streamingProcess.scala:48)
2017-08-09 15:29:05,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 172.0 with 2 tasks
2017-08-09 15:29:05,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_157_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:29:05,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 172.0 (TID 344, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:05,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 172.0 (TID 345, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:05,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_158_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:29:05,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 172.0 (TID 344)
2017-08-09 15:29:05,072 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 172.0 (TID 345)
2017-08-09 15:29:05,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_159_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:29:05,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_160_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:29:05,074 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:05,074 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:05,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_161_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:29:05,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_162_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:05,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_163_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:05,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_164_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:05,077 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 172.0 (TID 345). 714 bytes result sent to driver
2017-08-09 15:29:05,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 172.0 (TID 344). 714 bytes result sent to driver
2017-08-09 15:29:05,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_165_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:05,078 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_166_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:05,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 172.0 (TID 344) in 7 ms on localhost (1/2)
2017-08-09 15:29:05,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 172.0 (TID 345) in 8 ms on localhost (2/2)
2017-08-09 15:29:05,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 172.0, whose tasks have all completed, from pool 
2017-08-09 15:29:05,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 172 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-09 15:29:05,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 172 finished: foreachPartition at streamingProcess.scala:50, took 0.033358 s
2017-08-09 15:29:05,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_167_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:05,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263745000 ms.0 from job set of time 1502263745000 ms
2017-08-09 15:29:05,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1502263745000 ms (execution: 0.060 s)
2017-08-09 15:29:05,080 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 343 from persistence list
2017-08-09 15:29:05,080 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 343
2017-08-09 15:29:05,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 342 from persistence list
2017-08-09 15:29:05,080 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_168_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:05,080 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 342
2017-08-09 15:29:05,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:05,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263735000 ms
2017-08-09 15:29:05,081 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_169_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:05,082 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_170_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:29:05,083 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_171_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:29:10,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263750000 ms
2017-08-09 15:29:10,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263750000 ms.0 from job set of time 1502263750000 ms
2017-08-09 15:29:10,035 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:10,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 173 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:10,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 173 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:10,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:10,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:10,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 173 (MapPartitionsRDD[347] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_173 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-09 15:29:10,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_173_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-09 15:29:10,044 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_173_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:29:10,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 173 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:10,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 173 (MapPartitionsRDD[347] at map at streamingProcess.scala:48)
2017-08-09 15:29:10,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 173.0 with 2 tasks
2017-08-09 15:29:10,046 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 173.0 (TID 346, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:10,046 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 173.0 (TID 347, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:10,046 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 173.0 (TID 347)
2017-08-09 15:29:10,046 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 173.0 (TID 346)
2017-08-09 15:29:10,049 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:10,049 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:10,053 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 173.0 (TID 346). 714 bytes result sent to driver
2017-08-09 15:29:10,053 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 173.0 (TID 347). 714 bytes result sent to driver
2017-08-09 15:29:10,055 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 173.0 (TID 346) in 10 ms on localhost (1/2)
2017-08-09 15:29:10,055 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 173.0 (TID 347) in 9 ms on localhost (2/2)
2017-08-09 15:29:10,055 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 173.0, whose tasks have all completed, from pool 
2017-08-09 15:29:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 173 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-09 15:29:10,056 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 173 finished: foreachPartition at streamingProcess.scala:50, took 0.020808 s
2017-08-09 15:29:10,056 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263750000 ms.0 from job set of time 1502263750000 ms
2017-08-09 15:29:10,056 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.056 s for time 1502263750000 ms (execution: 0.038 s)
2017-08-09 15:29:10,056 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 345 from persistence list
2017-08-09 15:29:10,057 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 345
2017-08-09 15:29:10,057 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 344 from persistence list
2017-08-09 15:29:10,057 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 344
2017-08-09 15:29:10,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:10,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263740000 ms
2017-08-09 15:29:15,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263755000 ms
2017-08-09 15:29:15,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263755000 ms.0 from job set of time 1502263755000 ms
2017-08-09 15:29:15,056 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 174 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 174 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:15,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 174 (MapPartitionsRDD[349] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_174 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-09 15:29:15,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_174_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-09 15:29:15,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_174_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 174 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 174 (MapPartitionsRDD[349] at map at streamingProcess.scala:48)
2017-08-09 15:29:15,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 174.0 with 2 tasks
2017-08-09 15:29:15,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 174.0 (TID 348, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:15,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 174.0 (TID 349, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:15,073 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 174.0 (TID 348)
2017-08-09 15:29:15,073 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 174.0 (TID 349)
2017-08-09 15:29:15,077 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:15,077 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:15,082 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 174.0 (TID 348). 714 bytes result sent to driver
2017-08-09 15:29:15,082 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 174.0 (TID 349). 714 bytes result sent to driver
2017-08-09 15:29:15,085 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 174.0 (TID 348) in 15 ms on localhost (1/2)
2017-08-09 15:29:15,085 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 174.0 (TID 349) in 13 ms on localhost (2/2)
2017-08-09 15:29:15,085 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 174.0, whose tasks have all completed, from pool 
2017-08-09 15:29:15,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 174 (foreachPartition at streamingProcess.scala:50) finished in 0.015 s
2017-08-09 15:29:15,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 174 finished: foreachPartition at streamingProcess.scala:50, took 0.029234 s
2017-08-09 15:29:15,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263755000 ms.0 from job set of time 1502263755000 ms
2017-08-09 15:29:15,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.086 s for time 1502263755000 ms (execution: 0.064 s)
2017-08-09 15:29:15,087 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 347 from persistence list
2017-08-09 15:29:15,087 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 347
2017-08-09 15:29:15,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 346 from persistence list
2017-08-09 15:29:15,088 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 346
2017-08-09 15:29:15,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:15,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263745000 ms
2017-08-09 15:29:20,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263760000 ms
2017-08-09 15:29:20,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263760000 ms.0 from job set of time 1502263760000 ms
2017-08-09 15:29:20,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 175 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 175 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 175 (MapPartitionsRDD[351] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:20,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_175 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-09 15:29:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_175_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-09 15:29:20,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_175_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 175 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 175 (MapPartitionsRDD[351] at map at streamingProcess.scala:48)
2017-08-09 15:29:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 175.0 with 2 tasks
2017-08-09 15:29:20,037 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 175.0 (TID 350, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:20,037 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 175.0 (TID 351, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:20,037 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 175.0 (TID 350)
2017-08-09 15:29:20,039 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 175.0 (TID 351)
2017-08-09 15:29:20,041 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:20,041 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:20,044 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 175.0 (TID 351). 714 bytes result sent to driver
2017-08-09 15:29:20,044 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 175.0 (TID 350). 714 bytes result sent to driver
2017-08-09 15:29:20,046 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 175.0 (TID 350) in 10 ms on localhost (1/2)
2017-08-09 15:29:20,046 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 175.0 (TID 351) in 9 ms on localhost (2/2)
2017-08-09 15:29:20,046 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 175.0, whose tasks have all completed, from pool 
2017-08-09 15:29:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 175 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-09 15:29:20,046 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 175 finished: foreachPartition at streamingProcess.scala:50, took 0.016840 s
2017-08-09 15:29:20,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263760000 ms.0 from job set of time 1502263760000 ms
2017-08-09 15:29:20,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.047 s for time 1502263760000 ms (execution: 0.033 s)
2017-08-09 15:29:20,047 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 349 from persistence list
2017-08-09 15:29:20,047 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 349
2017-08-09 15:29:20,047 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 348 from persistence list
2017-08-09 15:29:20,048 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 348
2017-08-09 15:29:20,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:20,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263750000 ms
2017-08-09 15:29:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263765000 ms
2017-08-09 15:29:25,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263765000 ms.0 from job set of time 1502263765000 ms
2017-08-09 15:29:25,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:25,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 176 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:25,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 176 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:25,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:25,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:25,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 176 (MapPartitionsRDD[353] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:25,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_176 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-09 15:29:25,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_176_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-09 15:29:25,033 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_176_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:25,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 176 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:25,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 176 (MapPartitionsRDD[353] at map at streamingProcess.scala:48)
2017-08-09 15:29:25,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 176.0 with 2 tasks
2017-08-09 15:29:25,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 176.0 (TID 352, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:25,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 176.0 (TID 353, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:25,035 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 176.0 (TID 352)
2017-08-09 15:29:25,035 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 176.0 (TID 353)
2017-08-09 15:29:25,037 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:25,037 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:25,042 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 176.0 (TID 352). 714 bytes result sent to driver
2017-08-09 15:29:25,042 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 176.0 (TID 353). 714 bytes result sent to driver
2017-08-09 15:29:25,043 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 176.0 (TID 353) in 8 ms on localhost (1/2)
2017-08-09 15:29:25,044 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 176.0 (TID 352) in 10 ms on localhost (2/2)
2017-08-09 15:29:25,044 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 176.0, whose tasks have all completed, from pool 
2017-08-09 15:29:25,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 176 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-09 15:29:25,044 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 176 finished: foreachPartition at streamingProcess.scala:50, took 0.018065 s
2017-08-09 15:29:25,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263765000 ms.0 from job set of time 1502263765000 ms
2017-08-09 15:29:25,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.044 s for time 1502263765000 ms (execution: 0.031 s)
2017-08-09 15:29:25,045 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 351 from persistence list
2017-08-09 15:29:25,045 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 350 from persistence list
2017-08-09 15:29:25,045 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 351
2017-08-09 15:29:25,045 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 350
2017-08-09 15:29:25,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:25,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263755000 ms
2017-08-09 15:29:30,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263770000 ms
2017-08-09 15:29:30,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263770000 ms.0 from job set of time 1502263770000 ms
2017-08-09 15:29:30,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 177 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 177 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 177 (MapPartitionsRDD[355] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_177 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-09 15:29:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_177_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-09 15:29:30,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_177_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 177 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 177 (MapPartitionsRDD[355] at map at streamingProcess.scala:48)
2017-08-09 15:29:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 177.0 with 2 tasks
2017-08-09 15:29:30,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 177.0 (TID 354, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:30,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 177.0 (TID 355, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:30,067 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 177.0 (TID 355)
2017-08-09 15:29:30,067 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 177.0 (TID 354)
2017-08-09 15:29:30,070 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:30,070 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:30,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 177.0 (TID 355). 714 bytes result sent to driver
2017-08-09 15:29:30,074 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 177.0 (TID 354). 714 bytes result sent to driver
2017-08-09 15:29:30,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 177.0 (TID 355) in 12 ms on localhost (1/2)
2017-08-09 15:29:30,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 177.0 (TID 354) in 13 ms on localhost (2/2)
2017-08-09 15:29:30,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 177.0, whose tasks have all completed, from pool 
2017-08-09 15:29:30,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 177 (foreachPartition at streamingProcess.scala:50) finished in 0.014 s
2017-08-09 15:29:30,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 177 finished: foreachPartition at streamingProcess.scala:50, took 0.027696 s
2017-08-09 15:29:30,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263770000 ms.0 from job set of time 1502263770000 ms
2017-08-09 15:29:30,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1502263770000 ms (execution: 0.062 s)
2017-08-09 15:29:30,080 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 353 from persistence list
2017-08-09 15:29:30,081 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 353
2017-08-09 15:29:30,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 352 from persistence list
2017-08-09 15:29:30,081 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 352
2017-08-09 15:29:30,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:30,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263760000 ms
2017-08-09 15:29:35,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263775000 ms
2017-08-09 15:29:35,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263775000 ms.0 from job set of time 1502263775000 ms
2017-08-09 15:29:35,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 178 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 178 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 178 (MapPartitionsRDD[357] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:35,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_178 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-09 15:29:35,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_178_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-09 15:29:35,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_178_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:35,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 178 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:35,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 178 (MapPartitionsRDD[357] at map at streamingProcess.scala:48)
2017-08-09 15:29:35,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 178.0 with 2 tasks
2017-08-09 15:29:35,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 178.0 (TID 356, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:35,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 178.0 (TID 357, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:35,058 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 178.0 (TID 356)
2017-08-09 15:29:35,058 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 178.0 (TID 357)
2017-08-09 15:29:35,061 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:35,061 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:35,065 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 178.0 (TID 356). 714 bytes result sent to driver
2017-08-09 15:29:35,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 178.0 (TID 357). 714 bytes result sent to driver
2017-08-09 15:29:35,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 178.0 (TID 356) in 11 ms on localhost (1/2)
2017-08-09 15:29:35,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 178.0 (TID 357) in 10 ms on localhost (2/2)
2017-08-09 15:29:35,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 178.0, whose tasks have all completed, from pool 
2017-08-09 15:29:35,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 178 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-09 15:29:35,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 178 finished: foreachPartition at streamingProcess.scala:50, took 0.023330 s
2017-08-09 15:29:35,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263775000 ms.0 from job set of time 1502263775000 ms
2017-08-09 15:29:35,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1502263775000 ms (execution: 0.049 s)
2017-08-09 15:29:35,068 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 355 from persistence list
2017-08-09 15:29:35,068 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 355
2017-08-09 15:29:35,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 354 from persistence list
2017-08-09 15:29:35,069 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 354
2017-08-09 15:29:35,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:35,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263765000 ms
2017-08-09 15:29:40,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263780000 ms
2017-08-09 15:29:40,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263780000 ms.0 from job set of time 1502263780000 ms
2017-08-09 15:29:40,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 179 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 179 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 179 (MapPartitionsRDD[359] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_179 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-09 15:29:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_179_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-09 15:29:40,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_179_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 179 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 179 (MapPartitionsRDD[359] at map at streamingProcess.scala:48)
2017-08-09 15:29:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 179.0 with 2 tasks
2017-08-09 15:29:40,053 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 179.0 (TID 358, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:40,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 179.0 (TID 359, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:40,054 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 179.0 (TID 358)
2017-08-09 15:29:40,056 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 179.0 (TID 359)
2017-08-09 15:29:40,057 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:40,058 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:40,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 179.0 (TID 358). 714 bytes result sent to driver
2017-08-09 15:29:40,061 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 179.0 (TID 359). 714 bytes result sent to driver
2017-08-09 15:29:40,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 179.0 (TID 358) in 9 ms on localhost (1/2)
2017-08-09 15:29:40,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 179.0 (TID 359) in 10 ms on localhost (2/2)
2017-08-09 15:29:40,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 179.0, whose tasks have all completed, from pool 
2017-08-09 15:29:40,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 179 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-09 15:29:40,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 179 finished: foreachPartition at streamingProcess.scala:50, took 0.020867 s
2017-08-09 15:29:40,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263780000 ms.0 from job set of time 1502263780000 ms
2017-08-09 15:29:40,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1502263780000 ms (execution: 0.046 s)
2017-08-09 15:29:40,064 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 357 from persistence list
2017-08-09 15:29:40,064 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 357
2017-08-09 15:29:40,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 356 from persistence list
2017-08-09 15:29:40,064 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 356
2017-08-09 15:29:40,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:40,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263770000 ms
2017-08-09 15:29:45,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263785000 ms
2017-08-09 15:29:45,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263785000 ms.0 from job set of time 1502263785000 ms
2017-08-09 15:29:45,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 180 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 180 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 180 (MapPartitionsRDD[361] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:45,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_180 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-09 15:29:45,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_180_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-09 15:29:45,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_180_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:45,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 180 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:45,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 180 (MapPartitionsRDD[361] at map at streamingProcess.scala:48)
2017-08-09 15:29:45,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 180.0 with 2 tasks
2017-08-09 15:29:45,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 180.0 (TID 360, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:45,076 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 180.0 (TID 361, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:45,078 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 180.0 (TID 360)
2017-08-09 15:29:45,078 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 180.0 (TID 361)
2017-08-09 15:29:45,085 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:45,085 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:45,089 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 180.0 (TID 360). 714 bytes result sent to driver
2017-08-09 15:29:45,089 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 180.0 (TID 361). 714 bytes result sent to driver
2017-08-09 15:29:45,092 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 180.0 (TID 361) in 17 ms on localhost (1/2)
2017-08-09 15:29:45,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 180.0 (TID 360) in 18 ms on localhost (2/2)
2017-08-09 15:29:45,093 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 180.0, whose tasks have all completed, from pool 
2017-08-09 15:29:45,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 180 (foreachPartition at streamingProcess.scala:50) finished in 0.019 s
2017-08-09 15:29:45,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 180 finished: foreachPartition at streamingProcess.scala:50, took 0.039137 s
2017-08-09 15:29:45,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263785000 ms.0 from job set of time 1502263785000 ms
2017-08-09 15:29:45,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.094 s for time 1502263785000 ms (execution: 0.075 s)
2017-08-09 15:29:45,094 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 359 from persistence list
2017-08-09 15:29:45,094 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 359
2017-08-09 15:29:45,094 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 358 from persistence list
2017-08-09 15:29:45,095 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 358
2017-08-09 15:29:45,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:45,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263775000 ms
2017-08-09 15:29:50,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263790000 ms
2017-08-09 15:29:50,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263790000 ms.0 from job set of time 1502263790000 ms
2017-08-09 15:29:50,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 181 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 181 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 181 (MapPartitionsRDD[363] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:50,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_181 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-09 15:29:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_181_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-09 15:29:50,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_181_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 181 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:50,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 181 (MapPartitionsRDD[363] at map at streamingProcess.scala:48)
2017-08-09 15:29:50,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 181.0 with 2 tasks
2017-08-09 15:29:50,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 181.0 (TID 362, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:50,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 181.0 (TID 363, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:50,072 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 181.0 (TID 363)
2017-08-09 15:29:50,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 181.0 (TID 362)
2017-08-09 15:29:50,075 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:50,076 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:50,078 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 181.0 (TID 362). 714 bytes result sent to driver
2017-08-09 15:29:50,080 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 181.0 (TID 363). 714 bytes result sent to driver
2017-08-09 15:29:50,081 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 181.0 (TID 362) in 11 ms on localhost (1/2)
2017-08-09 15:29:50,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 181.0 (TID 363) in 11 ms on localhost (2/2)
2017-08-09 15:29:50,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 181.0, whose tasks have all completed, from pool 
2017-08-09 15:29:50,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 181 (foreachPartition at streamingProcess.scala:50) finished in 0.012 s
2017-08-09 15:29:50,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 181 finished: foreachPartition at streamingProcess.scala:50, took 0.029459 s
2017-08-09 15:29:50,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263790000 ms.0 from job set of time 1502263790000 ms
2017-08-09 15:29:50,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1502263790000 ms (execution: 0.064 s)
2017-08-09 15:29:50,083 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 361 from persistence list
2017-08-09 15:29:50,084 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 361
2017-08-09 15:29:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 360 from persistence list
2017-08-09 15:29:50,084 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 360
2017-08-09 15:29:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263780000 ms
2017-08-09 15:29:55,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263795000 ms
2017-08-09 15:29:55,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263795000 ms.0 from job set of time 1502263795000 ms
2017-08-09 15:29:55,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 182 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 182 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 182 (MapPartitionsRDD[365] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_182 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-09 15:29:55,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_182_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-09 15:29:55,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_182_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:29:55,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 182 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:55,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 182 (MapPartitionsRDD[365] at map at streamingProcess.scala:48)
2017-08-09 15:29:55,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 182.0 with 2 tasks
2017-08-09 15:29:55,075 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 182.0 (TID 364, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:55,076 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 182.0 (TID 365, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:55,076 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 182.0 (TID 365)
2017-08-09 15:29:55,076 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 182.0 (TID 364)
2017-08-09 15:29:55,080 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:55,080 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:55,084 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 182.0 (TID 365). 714 bytes result sent to driver
2017-08-09 15:29:55,084 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 182.0 (TID 364). 714 bytes result sent to driver
2017-08-09 15:29:55,086 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 182.0 (TID 365) in 11 ms on localhost (1/2)
2017-08-09 15:29:55,086 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 182.0 (TID 364) in 12 ms on localhost (2/2)
2017-08-09 15:29:55,086 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 182.0, whose tasks have all completed, from pool 
2017-08-09 15:29:55,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 182 (foreachPartition at streamingProcess.scala:50) finished in 0.012 s
2017-08-09 15:29:55,087 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 182 finished: foreachPartition at streamingProcess.scala:50, took 0.032846 s
2017-08-09 15:29:55,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263795000 ms.0 from job set of time 1502263795000 ms
2017-08-09 15:29:55,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1502263795000 ms (execution: 0.068 s)
2017-08-09 15:29:55,087 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 363 from persistence list
2017-08-09 15:29:55,088 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 363
2017-08-09 15:29:55,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 362 from persistence list
2017-08-09 15:29:55,088 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 362
2017-08-09 15:29:55,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:55,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263785000 ms
2017-08-09 15:30:00,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263800000 ms
2017-08-09 15:30:00,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263800000 ms.0 from job set of time 1502263800000 ms
2017-08-09 15:30:00,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:30:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 183 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:30:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 183 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:30:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:30:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:30:00,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 183 (MapPartitionsRDD[367] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:30:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_183 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-09 15:30:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_183_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-09 15:30:00,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_183_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:00,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 183 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:30:00,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 183 (MapPartitionsRDD[367] at map at streamingProcess.scala:48)
2017-08-09 15:30:00,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 183.0 with 2 tasks
2017-08-09 15:30:00,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 183.0 (TID 366, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:30:00,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 183.0 (TID 367, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:30:00,069 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 183.0 (TID 366)
2017-08-09 15:30:00,069 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 183.0 (TID 367)
2017-08-09 15:30:00,072 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:30:00,072 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:30:00,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 183.0 (TID 367). 714 bytes result sent to driver
2017-08-09 15:30:00,077 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 183.0 (TID 366). 714 bytes result sent to driver
2017-08-09 15:30:00,079 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 183.0 (TID 367) in 11 ms on localhost (1/2)
2017-08-09 15:30:00,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 183.0 (TID 366) in 11 ms on localhost (2/2)
2017-08-09 15:30:00,081 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 183.0, whose tasks have all completed, from pool 
2017-08-09 15:30:00,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 183 (foreachPartition at streamingProcess.scala:50) finished in 0.014 s
2017-08-09 15:30:00,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 183 finished: foreachPartition at streamingProcess.scala:50, took 0.027836 s
2017-08-09 15:30:00,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263800000 ms.0 from job set of time 1502263800000 ms
2017-08-09 15:30:00,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1502263800000 ms (execution: 0.062 s)
2017-08-09 15:30:00,082 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 365 from persistence list
2017-08-09 15:30:00,082 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 365
2017-08-09 15:30:00,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 364 from persistence list
2017-08-09 15:30:00,083 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 364
2017-08-09 15:30:00,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:30:00,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263790000 ms
2017-08-09 15:30:05,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263805000 ms
2017-08-09 15:30:05,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263805000 ms.0 from job set of time 1502263805000 ms
2017-08-09 15:30:05,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:30:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 184 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:30:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 184 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:30:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:30:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:30:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 184 (MapPartitionsRDD[369] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:30:05,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_184 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-09 15:30:05,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_184_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-09 15:30:05,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_184_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:05,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 184 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:30:05,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 184 (MapPartitionsRDD[369] at map at streamingProcess.scala:48)
2017-08-09 15:30:05,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 184.0 with 2 tasks
2017-08-09 15:30:05,053 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 184.0 (TID 368, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:30:05,053 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 184.0 (TID 369, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:30:05,054 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 184.0 (TID 369)
2017-08-09 15:30:05,054 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 184.0 (TID 368)
2017-08-09 15:30:05,056 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:30:05,056 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:30:05,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 184.0 (TID 369). 714 bytes result sent to driver
2017-08-09 15:30:05,060 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 184.0 (TID 368). 714 bytes result sent to driver
2017-08-09 15:30:05,061 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 184.0 (TID 369) in 8 ms on localhost (1/2)
2017-08-09 15:30:05,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 184.0 (TID 368) in 8 ms on localhost (2/2)
2017-08-09 15:30:05,062 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 184.0, whose tasks have all completed, from pool 
2017-08-09 15:30:05,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 184 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-09 15:30:05,062 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 184 finished: foreachPartition at streamingProcess.scala:50, took 0.018723 s
2017-08-09 15:30:05,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263805000 ms.0 from job set of time 1502263805000 ms
2017-08-09 15:30:05,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.062 s for time 1502263805000 ms (execution: 0.043 s)
2017-08-09 15:30:05,063 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 367 from persistence list
2017-08-09 15:30:05,063 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 367
2017-08-09 15:30:05,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 366 from persistence list
2017-08-09 15:30:05,063 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 366
2017-08-09 15:30:05,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:30:05,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263795000 ms
2017-08-09 15:30:10,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263810000 ms
2017-08-09 15:30:10,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263810000 ms.0 from job set of time 1502263810000 ms
2017-08-09 15:30:10,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:30:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 185 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:30:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 185 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:30:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:30:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:30:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 185 (MapPartitionsRDD[371] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:30:10,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_185 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-09 15:30:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_185_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-09 15:30:10,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_185_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 185 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:30:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 185 (MapPartitionsRDD[371] at map at streamingProcess.scala:48)
2017-08-09 15:30:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 185.0 with 2 tasks
2017-08-09 15:30:10,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 185.0 (TID 370, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:30:10,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 185.0 (TID 371, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:30:10,054 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 185.0 (TID 371)
2017-08-09 15:30:10,054 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 185.0 (TID 370)
2017-08-09 15:30:10,057 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:30:10,057 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:30:10,062 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 185.0 (TID 371). 714 bytes result sent to driver
2017-08-09 15:30:10,062 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 185.0 (TID 370). 714 bytes result sent to driver
2017-08-09 15:30:10,064 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 185.0 (TID 371) in 11 ms on localhost (1/2)
2017-08-09 15:30:10,064 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 185.0 (TID 370) in 12 ms on localhost (2/2)
2017-08-09 15:30:10,065 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 185.0, whose tasks have all completed, from pool 
2017-08-09 15:30:10,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 185 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-09 15:30:10,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 185 finished: foreachPartition at streamingProcess.scala:50, took 0.026123 s
2017-08-09 15:30:10,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263810000 ms.0 from job set of time 1502263810000 ms
2017-08-09 15:30:10,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1502263810000 ms (execution: 0.048 s)
2017-08-09 15:30:10,066 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 369 from persistence list
2017-08-09 15:30:10,066 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 369
2017-08-09 15:30:10,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 368 from persistence list
2017-08-09 15:30:10,067 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 368
2017-08-09 15:30:10,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:30:10,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263800000 ms
2017-08-09 15:30:15,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263815000 ms
2017-08-09 15:30:15,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263815000 ms.0 from job set of time 1502263815000 ms
2017-08-09 15:30:15,028 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:30:15,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 186 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:30:15,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 186 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:30:15,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:30:15,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:30:15,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 186 (MapPartitionsRDD[373] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:30:15,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_186 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-09 15:30:15,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_186_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-09 15:30:15,036 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_186_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:30:15,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 186 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:30:15,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 186 (MapPartitionsRDD[373] at map at streamingProcess.scala:48)
2017-08-09 15:30:15,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 186.0 with 2 tasks
2017-08-09 15:30:15,038 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 186.0 (TID 372, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:30:15,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 186.0 (TID 373, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:30:15,040 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 186.0 (TID 372)
2017-08-09 15:30:15,040 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 186.0 (TID 373)
2017-08-09 15:30:15,042 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:30:15,042 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:30:15,045 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 186.0 (TID 372). 714 bytes result sent to driver
2017-08-09 15:30:15,045 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 186.0 (TID 373). 714 bytes result sent to driver
2017-08-09 15:30:15,047 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 186.0 (TID 372) in 10 ms on localhost (1/2)
2017-08-09 15:30:15,047 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 186.0 (TID 373) in 9 ms on localhost (2/2)
2017-08-09 15:30:15,047 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 186.0, whose tasks have all completed, from pool 
2017-08-09 15:30:15,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 186 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-09 15:30:15,048 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 186 finished: foreachPartition at streamingProcess.scala:50, took 0.019406 s
2017-08-09 15:30:15,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263815000 ms.0 from job set of time 1502263815000 ms
2017-08-09 15:30:15,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1502263815000 ms (execution: 0.033 s)
2017-08-09 15:30:15,048 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 371 from persistence list
2017-08-09 15:30:15,049 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 371
2017-08-09 15:30:15,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 370 from persistence list
2017-08-09 15:30:15,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:30:15,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263805000 ms
2017-08-09 15:30:15,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_186_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:15,069 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 370
2017-08-09 15:30:15,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_172_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:15,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_173_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:15,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_174_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:15,079 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_175_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:30:15,080 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_176_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:30:15,081 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_177_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:30:15,082 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_178_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:30:15,083 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_179_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:30:15,083 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_180_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:30:15,085 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_181_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:30:15,087 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_182_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:30:15,089 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_183_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:30:15,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_184_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:30:15,093 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_185_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:01:46,166 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-10 09:01:46,795 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-10 09:01:47,767 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-10 09:01:47,768 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-10 09:01:47,769 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-10 09:01:47,770 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-10 09:01:47,771 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-10 09:01:48,700 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 49883.
2017-08-10 09:01:48,755 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-10 09:01:48,811 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-10 09:01:48,863 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-7037e1a9-a04b-4253-84e7-89ca66d541e3
2017-08-10 09:01:48,898 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-10 09:01:48,977 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-10 09:01:49,161 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @4525ms
2017-08-10 09:01:49,317 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-10 09:01:49,335 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e70ea3{/jobs,null,AVAILABLE}
2017-08-10 09:01:49,335 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fe7086{/jobs/json,null,AVAILABLE}
2017-08-10 09:01:49,335 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15d8c96{/jobs/job,null,AVAILABLE}
2017-08-10 09:01:49,336 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/jobs/job/json,null,AVAILABLE}
2017-08-10 09:01:49,336 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/stages,null,AVAILABLE}
2017-08-10 09:01:49,336 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/stages/json,null,AVAILABLE}
2017-08-10 09:01:49,336 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/stages/stage,null,AVAILABLE}
2017-08-10 09:01:49,337 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/stages/stage/json,null,AVAILABLE}
2017-08-10 09:01:49,337 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/stages/pool,null,AVAILABLE}
2017-08-10 09:01:49,337 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/stages/pool/json,null,AVAILABLE}
2017-08-10 09:01:49,337 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/storage,null,AVAILABLE}
2017-08-10 09:01:49,337 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/storage/json,null,AVAILABLE}
2017-08-10 09:01:49,337 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/storage/rdd,null,AVAILABLE}
2017-08-10 09:01:49,338 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/storage/rdd/json,null,AVAILABLE}
2017-08-10 09:01:49,338 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/environment,null,AVAILABLE}
2017-08-10 09:01:49,338 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/environment/json,null,AVAILABLE}
2017-08-10 09:01:49,338 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/executors,null,AVAILABLE}
2017-08-10 09:01:49,338 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/executors/json,null,AVAILABLE}
2017-08-10 09:01:49,338 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0fba8{/executors/threadDump,null,AVAILABLE}
2017-08-10 09:01:49,339 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5de5a4{/executors/threadDump/json,null,AVAILABLE}
2017-08-10 09:01:49,345 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1ef6856{/static,null,AVAILABLE}
2017-08-10 09:01:49,346 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@b7a938{/,null,AVAILABLE}
2017-08-10 09:01:49,346 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1af2e7d{/api,null,AVAILABLE}
2017-08-10 09:01:49,347 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@648ce9{/stages/stage/kill,null,AVAILABLE}
2017-08-10 09:01:49,354 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@73c5a4{HTTP/1.1}{0.0.0.0:4040}
2017-08-10 09:01:49,355 [main] INFO  [org.spark_project.jetty.server.Server] - Started @4718ms
2017-08-10 09:01:49,355 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-10 09:01:49,358 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-10 09:01:49,506 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-10 09:01:49,547 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49904.
2017-08-10 09:01:49,547 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:49904
2017-08-10 09:01:49,569 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 49904)
2017-08-10 09:01:49,573 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:49904 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 49904)
2017-08-10 09:01:49,578 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 49904)
2017-08-10 09:01:49,813 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15f4131{/metrics/json,null,AVAILABLE}
2017-08-10 09:01:50,698 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-08-10 09:01:50,699 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-10 09:01:50,699 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-10 09:01:50,700 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-08-10 09:01:50,700 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@1f545f2
2017-08-10 09:01:50,701 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-08-10 09:01:50,701 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-10 09:01:50,701 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-10 09:01:50,701 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-08-10 09:01:50,701 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@117304d
2017-08-10 09:01:50,702 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-08-10 09:01:50,702 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-10 09:01:50,702 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-10 09:01:50,702 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-08-10 09:01:50,702 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@17e6f9b
2017-08-10 09:01:50,783 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1502326915000
2017-08-10 09:01:50,785 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1502326915000 ms
2017-08-10 09:01:50,801 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-10 09:01:50,804 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@504e66{/streaming,null,AVAILABLE}
2017-08-10 09:01:50,805 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1c4d5e1{/streaming/json,null,AVAILABLE}
2017-08-10 09:01:50,805 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1835e65{/streaming/batch,null,AVAILABLE}
2017-08-10 09:01:50,806 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@231dfd{/streaming/batch/json,null,AVAILABLE}
2017-08-10 09:01:50,807 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@161a019{/static/streaming,null,AVAILABLE}
2017-08-10 09:01:50,808 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-10 09:01:55,307 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326915000 ms
2017-08-10 09:01:55,321 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326915000 ms.0 from job set of time 1502326915000 ms
2017-08-10 09:01:55,392 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:01:55,416 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:01:55,416 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:01:55,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:01:55,419 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:01:55,435 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:01:55,578 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-10 09:01:55,588 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 34.5 KB, free 413.9 MB)
2017-08-10 09:01:55,893 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.9 MB)
2017-08-10 09:01:55,895 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:01:55,899 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:01:55,904 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcess.scala:48)
2017-08-10 09:01:55,907 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-10 09:01:55,956 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-08-10 09:01:55,959 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-08-10 09:01:55,984 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-10 09:01:55,984 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-10 09:01:56,024 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 0 -> 25
2017-08-10 09:01:56,024 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 0 -> 25
2017-08-10 09:01:56,237 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-08-10 09:01:56,581 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-08-10 09:02:00,511 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326920000 ms
2017-08-10 09:02:05,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326925000 ms
2017-08-10 09:02:06,755 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-08-10 09:02:06,778 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 10818 ms on localhost (1/2)
2017-08-10 09:02:07,990 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-08-10 09:02:07,996 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 12059 ms on localhost (2/2)
2017-08-10 09:02:08,000 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcess.scala:50) finished in 12.077 s
2017-08-10 09:02:08,002 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-08-10 09:02:08,018 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcess.scala:50, took 12.624829 s
2017-08-10 09:02:08,029 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326915000 ms.0 from job set of time 1502326915000 ms
2017-08-10 09:02:08,030 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 13.025 s for time 1502326915000 ms (execution: 12.712 s)
2017-08-10 09:02:08,031 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326920000 ms.0 from job set of time 1502326920000 ms
2017-08-10 09:02:08,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:02:08,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:02:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:02:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:02:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:02:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:02:08,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:02:08,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:02:08,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-10 09:02:08,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:02:08,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:02:08,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:02:08,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcess.scala:48)
2017-08-10 09:02:08,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-08-10 09:02:08,073 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-08-10 09:02:08,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-08-10 09:02:08,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-08-10 09:02:08,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-08-10 09:02:08,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 25 -> 50
2017-08-10 09:02:08,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 25 -> 50
2017-08-10 09:02:10,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326930000 ms
2017-08-10 09:02:15,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326935000 ms
2017-08-10 09:02:19,423 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 787 bytes result sent to driver
2017-08-10 09:02:19,427 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 11353 ms on localhost (1/2)
2017-08-10 09:02:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326940000 ms
2017-08-10 09:02:25,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326945000 ms
2017-08-10 09:02:30,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326950000 ms
2017-08-10 09:02:31,847 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 787 bytes result sent to driver
2017-08-10 09:02:31,851 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 23779 ms on localhost (2/2)
2017-08-10 09:02:31,851 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-08-10 09:02:31,851 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcess.scala:50) finished in 23.780 s
2017-08-10 09:02:31,852 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: foreachPartition at streamingProcess.scala:50, took 23.797256 s
2017-08-10 09:02:31,852 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326920000 ms.0 from job set of time 1502326920000 ms
2017-08-10 09:02:31,853 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 31.852 s for time 1502326920000 ms (execution: 23.821 s)
2017-08-10 09:02:31,853 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326925000 ms.0 from job set of time 1502326925000 ms
2017-08-10 09:02:31,861 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-08-10 09:02:31,884 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-08-10 09:02:31,884 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:02:31,884 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-10 09:02:31,886 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-08-10 09:02:31,886 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:02:31,887 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-08-10 09:02:31,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:02:31,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:02:31,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:02:31,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:02:31,888 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:02:31,892 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:02:31,898 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:02:31,899 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:02:31,899 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:02:31,900 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcess.scala:48)
2017-08-10 09:02:31,900 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2017-08-10 09:02:31,906 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5655 bytes)
2017-08-10 09:02:31,908 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5655 bytes)
2017-08-10 09:02:31,908 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2017-08-10 09:02:31,908 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2017-08-10 09:02:31,912 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 50 -> 75
2017-08-10 09:02:31,912 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 50 -> 75
2017-08-10 09:02:33,113 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:02:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326955000 ms
2017-08-10 09:02:35,408 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 787 bytes result sent to driver
2017-08-10 09:02:35,411 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 3509 ms on localhost (1/2)
2017-08-10 09:02:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326960000 ms
2017-08-10 09:02:45,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326965000 ms
2017-08-10 09:02:50,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326970000 ms
2017-08-10 09:02:55,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326975000 ms
2017-08-10 09:02:57,683 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2.0 (TID 5). 787 bytes result sent to driver
2017-08-10 09:02:57,687 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2.0 (TID 5) in 25780 ms on localhost (2/2)
2017-08-10 09:02:57,687 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-08-10 09:02:57,687 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (foreachPartition at streamingProcess.scala:50) finished in 25.787 s
2017-08-10 09:02:57,688 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: foreachPartition at streamingProcess.scala:50, took 25.800129 s
2017-08-10 09:02:57,688 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326925000 ms.0 from job set of time 1502326925000 ms
2017-08-10 09:02:57,689 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 3 from persistence list
2017-08-10 09:02:57,689 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 52.688 s for time 1502326925000 ms (execution: 25.835 s)
2017-08-10 09:02:57,689 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326930000 ms.0 from job set of time 1502326930000 ms
2017-08-10 09:02:57,689 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 3
2017-08-10 09:02:57,689 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2 from persistence list
2017-08-10 09:02:57,690 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:02:57,690 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326915000 ms
2017-08-10 09:02:57,690 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2
2017-08-10 09:02:57,699 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:02:57,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:02:57,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 3 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:02:57,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:02:57,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:02:57,701 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:02:57,704 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:02:57,709 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:02:57,709 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:02:57,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:02:57,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcess.scala:48)
2017-08-10 09:02:57,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 3.0 with 2 tasks
2017-08-10 09:02:57,712 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 3.0 (TID 6, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:02:57,712 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 3.0 (TID 7, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:02:57,712 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 3.0 (TID 7)
2017-08-10 09:02:57,712 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 3.0 (TID 6)
2017-08-10 09:02:57,715 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 75 -> 100
2017-08-10 09:02:57,716 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 75 -> 100
2017-08-10 09:02:58,307 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:02:59,809 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 3.0 (TID 7). 787 bytes result sent to driver
2017-08-10 09:02:59,811 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 3.0 (TID 7) in 2099 ms on localhost (1/2)
2017-08-10 09:03:00,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326980000 ms
2017-08-10 09:03:05,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326985000 ms
2017-08-10 09:03:10,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326990000 ms
2017-08-10 09:03:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326995000 ms
2017-08-10 09:03:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327000000 ms
2017-08-10 09:03:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327005000 ms
2017-08-10 09:03:30,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327010000 ms
2017-08-10 09:03:34,637 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 3.0 (TID 6). 787 bytes result sent to driver
2017-08-10 09:03:34,641 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 3.0 (TID 6) in 36930 ms on localhost (2/2)
2017-08-10 09:03:34,642 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-08-10 09:03:34,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 3 (foreachPartition at streamingProcess.scala:50) finished in 36.932 s
2017-08-10 09:03:34,643 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: foreachPartition at streamingProcess.scala:50, took 36.941422 s
2017-08-10 09:03:34,643 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326930000 ms.0 from job set of time 1502326930000 ms
2017-08-10 09:03:34,645 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 84.643 s for time 1502326930000 ms (execution: 36.954 s)
2017-08-10 09:03:34,645 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326935000 ms.0 from job set of time 1502326935000 ms
2017-08-10 09:03:34,645 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 5 from persistence list
2017-08-10 09:03:34,646 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 5
2017-08-10 09:03:34,646 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 4 from persistence list
2017-08-10 09:03:34,646 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 4
2017-08-10 09:03:34,646 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:03:34,647 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326920000 ms
2017-08-10 09:03:34,665 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:03:34,666 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 4 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:03:34,667 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:03:34,667 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:03:34,667 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:03:34,667 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:03:34,673 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:03:34,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:03:34,681 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:03:34,682 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:03:34,682 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcess.scala:48)
2017-08-10 09:03:34,682 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 2 tasks
2017-08-10 09:03:34,684 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 8, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:03:34,686 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 4.0 (TID 9, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:03:34,687 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 4.0 (TID 9)
2017-08-10 09:03:34,687 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 8)
2017-08-10 09:03:34,693 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 100 -> 125
2017-08-10 09:03:34,693 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 100 -> 125
2017-08-10 09:03:35,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327015000 ms
2017-08-10 09:03:37,396 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_3_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:03:40,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327020000 ms
2017-08-10 09:03:40,279 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 4.0 (TID 9). 787 bytes result sent to driver
2017-08-10 09:03:40,282 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 4.0 (TID 9) in 5597 ms on localhost (1/2)
2017-08-10 09:03:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327025000 ms
2017-08-10 09:03:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327030000 ms
2017-08-10 09:03:55,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327035000 ms
2017-08-10 09:04:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327040000 ms
2017-08-10 09:04:05,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327045000 ms
2017-08-10 09:04:10,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327050000 ms
2017-08-10 09:04:15,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327055000 ms
2017-08-10 09:04:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327060000 ms
2017-08-10 09:04:25,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327065000 ms
2017-08-10 09:04:26,624 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 8). 787 bytes result sent to driver
2017-08-10 09:04:26,626 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 8) in 51943 ms on localhost (2/2)
2017-08-10 09:04:26,626 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-08-10 09:04:26,626 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (foreachPartition at streamingProcess.scala:50) finished in 51.943 s
2017-08-10 09:04:26,627 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 4 finished: foreachPartition at streamingProcess.scala:50, took 51.958454 s
2017-08-10 09:04:26,627 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326935000 ms.0 from job set of time 1502326935000 ms
2017-08-10 09:04:26,627 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 131.627 s for time 1502326935000 ms (execution: 51.982 s)
2017-08-10 09:04:26,628 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 7 from persistence list
2017-08-10 09:04:26,628 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326940000 ms.0 from job set of time 1502326940000 ms
2017-08-10 09:04:26,628 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 7
2017-08-10 09:04:26,628 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 6 from persistence list
2017-08-10 09:04:26,628 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 6
2017-08-10 09:04:26,628 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:04:26,629 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326925000 ms
2017-08-10 09:04:26,639 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:04:26,640 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 5 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:04:26,640 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 5 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:04:26,641 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:04:26,641 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:04:26,641 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:04:26,644 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:04:26,648 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:04:26,649 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:04:26,650 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:04:26,650 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcess.scala:48)
2017-08-10 09:04:26,650 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 5.0 with 2 tasks
2017-08-10 09:04:26,652 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 5.0 (TID 10, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:04:26,652 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 5.0 (TID 11, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:04:26,652 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 5.0 (TID 11)
2017-08-10 09:04:26,653 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 5.0 (TID 10)
2017-08-10 09:04:26,655 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 125 -> 150
2017-08-10 09:04:26,655 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 125 -> 150
2017-08-10 09:04:30,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327070000 ms
2017-08-10 09:04:35,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327075000 ms
2017-08-10 09:04:38,164 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 5.0 (TID 11). 714 bytes result sent to driver
2017-08-10 09:04:38,167 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 5.0 (TID 11) in 11515 ms on localhost (1/2)
2017-08-10 09:04:40,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327080000 ms
2017-08-10 09:04:45,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327085000 ms
2017-08-10 09:04:45,123 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_4_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:04:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327090000 ms
2017-08-10 09:04:51,714 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 5.0 (TID 10). 787 bytes result sent to driver
2017-08-10 09:04:51,716 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 5.0 (TID 10) in 25065 ms on localhost (2/2)
2017-08-10 09:04:51,716 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-08-10 09:04:51,716 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 5 (foreachPartition at streamingProcess.scala:50) finished in 25.065 s
2017-08-10 09:04:51,716 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 5 finished: foreachPartition at streamingProcess.scala:50, took 25.075325 s
2017-08-10 09:04:51,717 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326940000 ms.0 from job set of time 1502326940000 ms
2017-08-10 09:04:51,717 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 151.717 s for time 1502326940000 ms (execution: 25.089 s)
2017-08-10 09:04:51,717 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326945000 ms.0 from job set of time 1502326945000 ms
2017-08-10 09:04:51,717 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 9 from persistence list
2017-08-10 09:04:51,718 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 9
2017-08-10 09:04:51,718 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 8 from persistence list
2017-08-10 09:04:51,719 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 8
2017-08-10 09:04:51,719 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:04:51,719 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326930000 ms
2017-08-10 09:04:51,732 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:04:51,733 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 6 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:04:51,733 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 6 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:04:51,733 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:04:51,733 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:04:51,734 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:04:51,736 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:04:51,740 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:04:51,741 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_6_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:04:51,742 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:04:51,742 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcess.scala:48)
2017-08-10 09:04:51,743 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 6.0 with 2 tasks
2017-08-10 09:04:51,744 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 6.0 (TID 12, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:04:51,745 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 6.0 (TID 13, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:04:51,745 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 6.0 (TID 12)
2017-08-10 09:04:51,745 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 6.0 (TID 13)
2017-08-10 09:04:51,747 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 150 -> 175
2017-08-10 09:04:51,747 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 150 -> 175
2017-08-10 09:04:54,301 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_5_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:04:55,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327095000 ms
2017-08-10 09:05:00,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327100000 ms
2017-08-10 09:05:05,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327105000 ms
2017-08-10 09:05:05,684 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 6.0 (TID 13). 787 bytes result sent to driver
2017-08-10 09:05:05,687 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 6.0 (TID 13) in 13943 ms on localhost (1/2)
2017-08-10 09:05:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327110000 ms
2017-08-10 09:05:14,546 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 6.0 (TID 12). 787 bytes result sent to driver
2017-08-10 09:05:14,548 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 6.0 (TID 12) in 22805 ms on localhost (2/2)
2017-08-10 09:05:14,548 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2017-08-10 09:05:14,548 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 6 (foreachPartition at streamingProcess.scala:50) finished in 22.805 s
2017-08-10 09:05:14,549 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 6 finished: foreachPartition at streamingProcess.scala:50, took 22.815048 s
2017-08-10 09:05:14,549 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326945000 ms.0 from job set of time 1502326945000 ms
2017-08-10 09:05:14,549 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 169.549 s for time 1502326945000 ms (execution: 22.832 s)
2017-08-10 09:05:14,549 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 11 from persistence list
2017-08-10 09:05:14,549 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326950000 ms.0 from job set of time 1502326950000 ms
2017-08-10 09:05:14,549 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 11
2017-08-10 09:05:14,549 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 10 from persistence list
2017-08-10 09:05:14,550 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 10
2017-08-10 09:05:14,550 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:05:14,550 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326935000 ms
2017-08-10 09:05:14,561 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:05:14,561 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 7 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:05:14,561 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 7 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:05:14,562 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:05:14,562 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:05:14,562 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:05:14,565 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:05:14,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:05:14,575 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_7_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:05:14,575 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:05:14,575 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcess.scala:48)
2017-08-10 09:05:14,576 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 7.0 with 2 tasks
2017-08-10 09:05:14,577 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 7.0 (TID 14, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:05:14,578 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 7.0 (TID 15, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:05:14,579 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 7.0 (TID 14)
2017-08-10 09:05:14,579 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 7.0 (TID 15)
2017-08-10 09:05:14,585 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 175 -> 200
2017-08-10 09:05:14,585 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 175 -> 200
2017-08-10 09:05:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327115000 ms
2017-08-10 09:05:17,032 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_6_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:05:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327120000 ms
2017-08-10 09:05:25,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327125000 ms
2017-08-10 09:05:26,578 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 7.0 (TID 14). 787 bytes result sent to driver
2017-08-10 09:05:26,580 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 7.0 (TID 14) in 12004 ms on localhost (1/2)
2017-08-10 09:05:30,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327130000 ms
2017-08-10 09:05:32,102 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 7.0 (TID 15). 787 bytes result sent to driver
2017-08-10 09:05:32,104 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 7.0 (TID 15) in 17526 ms on localhost (2/2)
2017-08-10 09:05:32,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 7 (foreachPartition at streamingProcess.scala:50) finished in 17.529 s
2017-08-10 09:05:32,105 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2017-08-10 09:05:32,105 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 7 finished: foreachPartition at streamingProcess.scala:50, took 17.543210 s
2017-08-10 09:05:32,105 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326950000 ms.0 from job set of time 1502326950000 ms
2017-08-10 09:05:32,106 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 182.105 s for time 1502326950000 ms (execution: 17.556 s)
2017-08-10 09:05:32,106 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 13 from persistence list
2017-08-10 09:05:32,106 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326955000 ms.0 from job set of time 1502326955000 ms
2017-08-10 09:05:32,106 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 13
2017-08-10 09:05:32,106 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 12 from persistence list
2017-08-10 09:05:32,107 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 12
2017-08-10 09:05:32,107 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:05:32,107 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326940000 ms
2017-08-10 09:05:32,117 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:05:32,117 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 8 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:05:32,117 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 8 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:05:32,117 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:05:32,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:05:32,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:05:32,121 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:05:32,125 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:05:32,126 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_8_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:05:32,126 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:05:32,127 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcess.scala:48)
2017-08-10 09:05:32,127 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 8.0 with 2 tasks
2017-08-10 09:05:32,128 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 8.0 (TID 16, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:05:32,129 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 8.0 (TID 17, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:05:32,129 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 8.0 (TID 16)
2017-08-10 09:05:32,129 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 8.0 (TID 17)
2017-08-10 09:05:32,132 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 200 -> 225
2017-08-10 09:05:32,132 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 200 -> 225
2017-08-10 09:05:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327135000 ms
2017-08-10 09:05:40,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327140000 ms
2017-08-10 09:05:40,910 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_7_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:05:45,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327145000 ms
2017-08-10 09:05:49,875 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 8.0 (TID 16). 787 bytes result sent to driver
2017-08-10 09:05:49,877 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 8.0 (TID 16) in 17750 ms on localhost (1/2)
2017-08-10 09:05:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327150000 ms
2017-08-10 09:05:54,453 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 8.0 (TID 17). 787 bytes result sent to driver
2017-08-10 09:05:54,460 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 8.0 (TID 17) in 22331 ms on localhost (2/2)
2017-08-10 09:05:54,460 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 8 (foreachPartition at streamingProcess.scala:50) finished in 22.333 s
2017-08-10 09:05:54,460 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2017-08-10 09:05:54,461 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 8 finished: foreachPartition at streamingProcess.scala:50, took 22.343216 s
2017-08-10 09:05:54,463 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326955000 ms.0 from job set of time 1502326955000 ms
2017-08-10 09:05:54,463 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 199.463 s for time 1502326955000 ms (execution: 22.357 s)
2017-08-10 09:05:54,464 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 15 from persistence list
2017-08-10 09:05:54,464 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326960000 ms.0 from job set of time 1502326960000 ms
2017-08-10 09:05:54,465 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 15
2017-08-10 09:05:54,465 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 14 from persistence list
2017-08-10 09:05:54,466 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 14
2017-08-10 09:05:54,467 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:05:54,467 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326945000 ms
2017-08-10 09:05:54,505 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:05:54,507 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 9 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:05:54,508 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 9 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:05:54,508 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:05:54,508 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:05:54,509 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:05:54,519 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:05:54,531 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:05:54,533 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_9_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:05:54,534 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:05:54,535 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcess.scala:48)
2017-08-10 09:05:54,535 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 9.0 with 2 tasks
2017-08-10 09:05:54,538 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 9.0 (TID 18, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:05:54,539 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 9.0 (TID 19, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:05:54,540 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 9.0 (TID 18)
2017-08-10 09:05:54,540 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 9.0 (TID 19)
2017-08-10 09:05:54,545 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 225 -> 250
2017-08-10 09:05:54,546 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 225 -> 250
2017-08-10 09:05:55,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327155000 ms
2017-08-10 09:05:57,908 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_8_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:00,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327160000 ms
2017-08-10 09:06:05,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327165000 ms
2017-08-10 09:06:06,389 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 9.0 (TID 18). 787 bytes result sent to driver
2017-08-10 09:06:06,391 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 9.0 (TID 18) in 11855 ms on localhost (1/2)
2017-08-10 09:06:09,996 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 9.0 (TID 19). 787 bytes result sent to driver
2017-08-10 09:06:09,998 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 9.0 (TID 19) in 15460 ms on localhost (2/2)
2017-08-10 09:06:09,998 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2017-08-10 09:06:09,999 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 9 (foreachPartition at streamingProcess.scala:50) finished in 15.462 s
2017-08-10 09:06:09,999 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 9 finished: foreachPartition at streamingProcess.scala:50, took 15.492178 s
2017-08-10 09:06:09,999 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326960000 ms.0 from job set of time 1502326960000 ms
2017-08-10 09:06:09,999 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 209.999 s for time 1502326960000 ms (execution: 15.535 s)
2017-08-10 09:06:09,999 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 17 from persistence list
2017-08-10 09:06:09,999 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326965000 ms.0 from job set of time 1502326965000 ms
2017-08-10 09:06:10,000 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 17
2017-08-10 09:06:10,000 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 16 from persistence list
2017-08-10 09:06:10,000 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 16
2017-08-10 09:06:10,000 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:06:10,000 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326950000 ms
2017-08-10 09:06:10,011 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:06:10,012 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 10 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:06:10,012 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 10 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:06:10,012 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:06:10,012 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:06:10,012 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:06:10,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327170000 ms
2017-08-10 09:06:10,015 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:06:10,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:06:10,021 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_10_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:10,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:06:10,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcess.scala:48)
2017-08-10 09:06:10,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 10.0 with 2 tasks
2017-08-10 09:06:10,023 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 10.0 (TID 20, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:06:10,023 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 10.0 (TID 21, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:06:10,024 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 10.0 (TID 20)
2017-08-10 09:06:10,024 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 10.0 (TID 21)
2017-08-10 09:06:10,026 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 250 -> 275
2017-08-10 09:06:10,026 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 250 -> 275
2017-08-10 09:06:13,880 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_9_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327175000 ms
2017-08-10 09:06:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327180000 ms
2017-08-10 09:06:22,558 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 10.0 (TID 20). 787 bytes result sent to driver
2017-08-10 09:06:22,560 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 10.0 (TID 20) in 12538 ms on localhost (1/2)
2017-08-10 09:06:24,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 10.0 (TID 21). 787 bytes result sent to driver
2017-08-10 09:06:24,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 10.0 (TID 21) in 14043 ms on localhost (2/2)
2017-08-10 09:06:24,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2017-08-10 09:06:24,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 10 (foreachPartition at streamingProcess.scala:50) finished in 14.044 s
2017-08-10 09:06:24,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 10 finished: foreachPartition at streamingProcess.scala:50, took 14.054894 s
2017-08-10 09:06:24,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326965000 ms.0 from job set of time 1502326965000 ms
2017-08-10 09:06:24,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 219.068 s for time 1502326965000 ms (execution: 14.069 s)
2017-08-10 09:06:24,068 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 19 from persistence list
2017-08-10 09:06:24,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326970000 ms.0 from job set of time 1502326970000 ms
2017-08-10 09:06:24,068 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 19
2017-08-10 09:06:24,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 18 from persistence list
2017-08-10 09:06:24,069 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 18
2017-08-10 09:06:24,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:06:24,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326955000 ms
2017-08-10 09:06:24,080 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:06:24,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 11 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:06:24,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 11 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:06:24,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:06:24,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:06:24,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:06:24,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:06:24,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:06:24,089 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_11_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:24,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 11 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:06:24,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcess.scala:48)
2017-08-10 09:06:24,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 11.0 with 2 tasks
2017-08-10 09:06:24,091 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 11.0 (TID 22, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:06:24,091 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 11.0 (TID 23, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:06:24,091 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 11.0 (TID 22)
2017-08-10 09:06:24,091 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 11.0 (TID 23)
2017-08-10 09:06:24,094 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 275 -> 300
2017-08-10 09:06:24,094 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 275 -> 300
2017-08-10 09:06:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327185000 ms
2017-08-10 09:06:26,429 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_10_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:30,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327190000 ms
2017-08-10 09:06:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327195000 ms
2017-08-10 09:06:37,242 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 11.0 (TID 23). 787 bytes result sent to driver
2017-08-10 09:06:37,245 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 11.0 (TID 23) in 13154 ms on localhost (1/2)
2017-08-10 09:06:40,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327200000 ms
2017-08-10 09:06:44,550 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 11.0 (TID 22). 787 bytes result sent to driver
2017-08-10 09:06:44,553 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 11.0 (TID 22) in 20463 ms on localhost (2/2)
2017-08-10 09:06:44,553 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2017-08-10 09:06:44,553 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 11 (foreachPartition at streamingProcess.scala:50) finished in 20.463 s
2017-08-10 09:06:44,553 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 11 finished: foreachPartition at streamingProcess.scala:50, took 20.472312 s
2017-08-10 09:06:44,554 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326970000 ms.0 from job set of time 1502326970000 ms
2017-08-10 09:06:44,554 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 234.554 s for time 1502326970000 ms (execution: 20.486 s)
2017-08-10 09:06:44,554 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326975000 ms.0 from job set of time 1502326975000 ms
2017-08-10 09:06:44,554 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 21 from persistence list
2017-08-10 09:06:44,555 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 21
2017-08-10 09:06:44,555 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 20 from persistence list
2017-08-10 09:06:44,555 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 20
2017-08-10 09:06:44,555 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:06:44,555 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326960000 ms
2017-08-10 09:06:44,566 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:06:44,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 12 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:06:44,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 12 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:06:44,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:06:44,567 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:06:44,567 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:06:44,570 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:06:44,575 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:06:44,576 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_12_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:44,576 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:06:44,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcess.scala:48)
2017-08-10 09:06:44,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 12.0 with 2 tasks
2017-08-10 09:06:44,578 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 12.0 (TID 24, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:06:44,579 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 12.0 (TID 25, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:06:44,579 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 12.0 (TID 24)
2017-08-10 09:06:44,579 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 12.0 (TID 25)
2017-08-10 09:06:44,582 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 300 -> 325
2017-08-10 09:06:44,582 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 300 -> 325
2017-08-10 09:06:45,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327205000 ms
2017-08-10 09:06:45,333 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_11_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327210000 ms
2017-08-10 09:06:55,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327215000 ms
2017-08-10 09:06:56,920 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 12.0 (TID 24). 787 bytes result sent to driver
2017-08-10 09:06:56,923 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 12.0 (TID 24) in 12346 ms on localhost (1/2)
2017-08-10 09:07:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327220000 ms
2017-08-10 09:07:02,568 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 12.0 (TID 25). 787 bytes result sent to driver
2017-08-10 09:07:02,571 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 12.0 (TID 25) in 17992 ms on localhost (2/2)
2017-08-10 09:07:02,571 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2017-08-10 09:07:02,571 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 12 (foreachPartition at streamingProcess.scala:50) finished in 17.994 s
2017-08-10 09:07:02,571 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 12 finished: foreachPartition at streamingProcess.scala:50, took 18.004436 s
2017-08-10 09:07:02,572 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326975000 ms.0 from job set of time 1502326975000 ms
2017-08-10 09:07:02,572 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 247.572 s for time 1502326975000 ms (execution: 18.018 s)
2017-08-10 09:07:02,572 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326980000 ms.0 from job set of time 1502326980000 ms
2017-08-10 09:07:02,572 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 23 from persistence list
2017-08-10 09:07:02,572 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 23
2017-08-10 09:07:02,572 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 22 from persistence list
2017-08-10 09:07:02,573 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 22
2017-08-10 09:07:02,573 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:07:02,573 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326965000 ms
2017-08-10 09:07:02,583 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:07:02,584 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 13 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:07:02,584 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 13 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:07:02,585 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:07:02,585 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:07:02,585 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:07:02,588 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:07:02,592 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:07:02,593 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_13_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:07:02,594 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 13 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:07:02,594 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcess.scala:48)
2017-08-10 09:07:02,594 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 13.0 with 2 tasks
2017-08-10 09:07:02,595 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 13.0 (TID 26, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:07:02,596 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 13.0 (TID 27, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:07:02,596 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 13.0 (TID 27)
2017-08-10 09:07:02,596 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 13.0 (TID 26)
2017-08-10 09:07:02,599 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 325 -> 350
2017-08-10 09:07:02,599 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 325 -> 350
2017-08-10 09:07:02,843 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_12_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:07:05,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327225000 ms
2017-08-10 09:07:10,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327230000 ms
2017-08-10 09:07:11,628 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 13.0 (TID 26). 787 bytes result sent to driver
2017-08-10 09:07:11,631 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 13.0 (TID 26) in 9036 ms on localhost (1/2)
2017-08-10 09:07:15,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327235000 ms
2017-08-10 09:07:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327240000 ms
2017-08-10 09:07:22,444 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 13.0 (TID 27). 787 bytes result sent to driver
2017-08-10 09:07:22,453 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 13.0 (TID 27) in 19856 ms on localhost (2/2)
2017-08-10 09:07:22,453 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2017-08-10 09:07:22,454 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 13 (foreachPartition at streamingProcess.scala:50) finished in 19.859 s
2017-08-10 09:07:22,455 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 13 finished: foreachPartition at streamingProcess.scala:50, took 19.869890 s
2017-08-10 09:07:22,456 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326980000 ms.0 from job set of time 1502326980000 ms
2017-08-10 09:07:22,457 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 262.456 s for time 1502326980000 ms (execution: 19.884 s)
2017-08-10 09:07:22,457 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 25 from persistence list
2017-08-10 09:07:22,457 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326985000 ms.0 from job set of time 1502326985000 ms
2017-08-10 09:07:22,458 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 25
2017-08-10 09:07:22,458 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 24 from persistence list
2017-08-10 09:07:22,459 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 24
2017-08-10 09:07:22,459 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:07:22,460 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326970000 ms
2017-08-10 09:07:22,483 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:07:22,484 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 14 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:07:22,484 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 14 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:07:22,484 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:07:22,485 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:07:22,485 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:07:22,491 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:07:22,495 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:07:22,496 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_14_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:07:22,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:07:22,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcess.scala:48)
2017-08-10 09:07:22,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 14.0 with 2 tasks
2017-08-10 09:07:22,498 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 14.0 (TID 28, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:07:22,499 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 14.0 (TID 29, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:07:22,499 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 14.0 (TID 29)
2017-08-10 09:07:22,499 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 14.0 (TID 28)
2017-08-10 09:07:22,502 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 350 -> 375
2017-08-10 09:07:22,502 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 350 -> 375
2017-08-10 09:07:24,574 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_13_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:07:25,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327245000 ms
2017-08-10 09:07:30,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327250000 ms
2017-08-10 09:07:35,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327255000 ms
2017-08-10 09:07:39,113 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 14.0 (TID 29). 787 bytes result sent to driver
2017-08-10 09:07:39,116 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 14.0 (TID 29) in 16618 ms on localhost (1/2)
2017-08-10 09:07:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327260000 ms
2017-08-10 09:07:45,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327265000 ms
2017-08-10 09:07:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327270000 ms
2017-08-10 09:07:55,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327275000 ms
2017-08-10 09:08:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327280000 ms
2017-08-10 09:08:05,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327285000 ms
2017-08-10 09:08:10,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327290000 ms
2017-08-10 09:08:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327295000 ms
2017-08-10 09:08:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327300000 ms
2017-08-10 09:08:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327305000 ms
2017-08-10 09:08:30,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327310000 ms
2017-08-10 09:08:35,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327315000 ms
2017-08-10 09:08:40,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327320000 ms
2017-08-10 09:08:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327325000 ms
2017-08-10 09:08:47,798 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 14.0 (TID 28). 787 bytes result sent to driver
2017-08-10 09:08:47,801 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 14.0 (TID 28) in 85304 ms on localhost (2/2)
2017-08-10 09:08:47,801 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 14 (foreachPartition at streamingProcess.scala:50) finished in 85.304 s
2017-08-10 09:08:47,801 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2017-08-10 09:08:47,802 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 14 finished: foreachPartition at streamingProcess.scala:50, took 85.314085 s
2017-08-10 09:08:47,802 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326985000 ms.0 from job set of time 1502326985000 ms
2017-08-10 09:08:47,802 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 342.802 s for time 1502326985000 ms (execution: 85.345 s)
2017-08-10 09:08:47,802 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 27 from persistence list
2017-08-10 09:08:47,802 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326990000 ms.0 from job set of time 1502326990000 ms
2017-08-10 09:08:47,803 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 26 from persistence list
2017-08-10 09:08:47,803 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 27
2017-08-10 09:08:47,804 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:08:47,804 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326975000 ms
2017-08-10 09:08:47,804 [block-manager-slave-async-thread-pool-8] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 26
2017-08-10 09:08:47,818 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:08:47,818 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 15 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:08:47,819 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 15 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:08:47,819 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:08:47,819 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:08:47,819 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:08:47,822 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:08:47,830 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:08:47,830 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_15_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:08:47,831 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 15 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:08:47,831 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcess.scala:48)
2017-08-10 09:08:47,831 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 15.0 with 2 tasks
2017-08-10 09:08:47,833 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 15.0 (TID 30, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:08:47,833 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 15.0 (TID 31, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:08:47,833 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 15.0 (TID 30)
2017-08-10 09:08:47,834 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 15.0 (TID 31)
2017-08-10 09:08:47,840 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 375 -> 400
2017-08-10 09:08:47,840 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 375 -> 400
2017-08-10 09:08:48,907 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_14_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:08:50,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327330000 ms
2017-08-10 09:08:55,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327335000 ms
2017-08-10 09:08:58,202 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 15.0 (TID 30). 874 bytes result sent to driver
2017-08-10 09:08:58,206 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 15.0 (TID 30) in 10374 ms on localhost (1/2)
2017-08-10 09:09:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327340000 ms
2017-08-10 09:09:05,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327345000 ms
2017-08-10 09:09:10,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327350000 ms
2017-08-10 09:09:15,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327355000 ms
2017-08-10 09:09:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327360000 ms
2017-08-10 09:09:25,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327365000 ms
2017-08-10 09:09:30,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327370000 ms
2017-08-10 09:09:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327375000 ms
2017-08-10 09:09:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327380000 ms
2017-08-10 09:09:45,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327385000 ms
2017-08-10 09:09:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327390000 ms
2017-08-10 09:09:55,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327395000 ms
2017-08-10 09:10:00,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327400000 ms
2017-08-10 09:10:05,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327405000 ms
2017-08-10 09:10:10,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327410000 ms
2017-08-10 09:10:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327415000 ms
2017-08-10 09:10:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327420000 ms
2017-08-10 09:10:25,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327425000 ms
2017-08-10 09:10:30,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327430000 ms
2017-08-10 09:10:35,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327435000 ms
2017-08-10 09:10:40,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327440000 ms
2017-08-10 09:10:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327445000 ms
2017-08-10 09:10:50,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327450000 ms
2017-08-10 09:10:51,942 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 15.0 (TID 31). 787 bytes result sent to driver
2017-08-10 09:10:51,945 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 15.0 (TID 31) in 124112 ms on localhost (2/2)
2017-08-10 09:10:51,946 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 15 (foreachPartition at streamingProcess.scala:50) finished in 124.114 s
2017-08-10 09:10:51,946 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2017-08-10 09:10:51,946 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 15 finished: foreachPartition at streamingProcess.scala:50, took 124.121489 s
2017-08-10 09:10:51,946 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326990000 ms.0 from job set of time 1502326990000 ms
2017-08-10 09:10:51,947 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 461.946 s for time 1502326990000 ms (execution: 124.144 s)
2017-08-10 09:10:51,947 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 29 from persistence list
2017-08-10 09:10:51,947 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326995000 ms.0 from job set of time 1502326995000 ms
2017-08-10 09:10:51,947 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 29
2017-08-10 09:10:51,948 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 28 from persistence list
2017-08-10 09:10:51,948 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:10:51,948 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326980000 ms
2017-08-10 09:10:51,949 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 28
2017-08-10 09:10:51,959 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:10:51,960 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 16 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:10:51,960 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 16 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:10:51,960 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:10:51,960 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:10:51,961 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:10:51,964 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:10:51,968 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:10:51,969 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_16_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:10:51,969 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 16 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:10:51,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcess.scala:48)
2017-08-10 09:10:51,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 16.0 with 2 tasks
2017-08-10 09:10:51,971 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 16.0 (TID 32, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:10:51,972 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 16.0 (TID 33, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:10:51,972 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 16.0 (TID 32)
2017-08-10 09:10:51,972 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 16.0 (TID 33)
2017-08-10 09:10:51,975 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 400 -> 425
2017-08-10 09:10:51,975 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 400 -> 425
2017-08-10 09:10:54,937 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_15_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:10:55,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327455000 ms
2017-08-10 09:11:00,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327460000 ms
2017-08-10 09:11:03,531 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 16.0 (TID 32). 874 bytes result sent to driver
2017-08-10 09:11:03,534 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 16.0 (TID 32) in 11564 ms on localhost (1/2)
2017-08-10 09:11:05,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327465000 ms
2017-08-10 09:11:10,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327470000 ms
2017-08-10 09:11:15,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327475000 ms
2017-08-10 09:11:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327480000 ms
2017-08-10 09:11:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327485000 ms
2017-08-10 09:11:30,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327490000 ms
2017-08-10 09:11:35,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327495000 ms
2017-08-10 09:11:40,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327500000 ms
2017-08-10 09:11:41,620 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 16.0 (TID 33). 787 bytes result sent to driver
2017-08-10 09:11:41,629 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 16.0 (TID 33) in 49657 ms on localhost (2/2)
2017-08-10 09:11:41,629 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2017-08-10 09:11:41,630 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 16 (foreachPartition at streamingProcess.scala:50) finished in 49.659 s
2017-08-10 09:11:41,631 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 16 finished: foreachPartition at streamingProcess.scala:50, took 49.668722 s
2017-08-10 09:11:41,632 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326995000 ms.0 from job set of time 1502326995000 ms
2017-08-10 09:11:41,633 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 31 from persistence list
2017-08-10 09:11:41,633 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 506.632 s for time 1502326995000 ms (execution: 49.685 s)
2017-08-10 09:11:41,634 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327000000 ms.0 from job set of time 1502327000000 ms
2017-08-10 09:11:41,636 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 31
2017-08-10 09:11:41,636 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 30 from persistence list
2017-08-10 09:11:41,637 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 30
2017-08-10 09:11:41,637 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:11:41,638 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326985000 ms
2017-08-10 09:11:41,663 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_16_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:11:41,668 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:11:41,669 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 17 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:11:41,669 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 17 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:11:41,669 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:11:41,669 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:11:41,670 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:11:41,673 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:11:41,677 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:11:41,678 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_17_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:11:41,678 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 17 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:11:41,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcess.scala:48)
2017-08-10 09:11:41,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 17.0 with 2 tasks
2017-08-10 09:11:41,680 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 17.0 (TID 34, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:11:41,681 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 17.0 (TID 35, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:11:41,681 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 17.0 (TID 34)
2017-08-10 09:11:41,681 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 17.0 (TID 35)
2017-08-10 09:11:41,684 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 425 -> 450
2017-08-10 09:11:41,685 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 425 -> 450
2017-08-10 09:11:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327505000 ms
2017-08-10 09:11:45,866 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 17.0 (TID 35). 787 bytes result sent to driver
2017-08-10 09:11:45,869 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 17.0 (TID 35) in 4189 ms on localhost (1/2)
2017-08-10 09:11:47,554 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 17.0 (TID 34). 787 bytes result sent to driver
2017-08-10 09:11:47,556 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 17.0 (TID 34) in 5877 ms on localhost (2/2)
2017-08-10 09:11:47,556 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2017-08-10 09:11:47,556 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 17 (foreachPartition at streamingProcess.scala:50) finished in 5.877 s
2017-08-10 09:11:47,556 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 17 finished: foreachPartition at streamingProcess.scala:50, took 5.887586 s
2017-08-10 09:11:47,557 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327000000 ms.0 from job set of time 1502327000000 ms
2017-08-10 09:11:47,557 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 507.557 s for time 1502327000000 ms (execution: 5.923 s)
2017-08-10 09:11:47,557 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 33 from persistence list
2017-08-10 09:11:47,557 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327005000 ms.0 from job set of time 1502327005000 ms
2017-08-10 09:11:47,558 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 33
2017-08-10 09:11:47,558 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 32 from persistence list
2017-08-10 09:11:47,558 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 32
2017-08-10 09:11:47,558 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:11:47,558 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326990000 ms
2017-08-10 09:11:47,568 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:11:47,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 18 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:11:47,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 18 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:11:47,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:11:47,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:11:47,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:11:47,572 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:11:47,576 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:11:47,576 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_18_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:11:47,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 18 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:11:47,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcess.scala:48)
2017-08-10 09:11:47,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 18.0 with 2 tasks
2017-08-10 09:11:47,579 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 18.0 (TID 36, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:11:47,580 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 18.0 (TID 37, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:11:47,580 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 18.0 (TID 37)
2017-08-10 09:11:47,580 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 18.0 (TID 36)
2017-08-10 09:11:47,583 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 450 -> 475
2017-08-10 09:11:47,583 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 450 -> 475
2017-08-10 09:11:48,677 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_17_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:11:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327510000 ms
2017-08-10 09:11:54,124 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 18.0 (TID 37). 787 bytes result sent to driver
2017-08-10 09:11:54,127 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 18.0 (TID 37) in 6548 ms on localhost (1/2)
2017-08-10 09:11:55,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327515000 ms
2017-08-10 09:12:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327520000 ms
2017-08-10 09:12:05,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327525000 ms
2017-08-10 09:12:05,099 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 18.0 (TID 36). 787 bytes result sent to driver
2017-08-10 09:12:05,102 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 18.0 (TID 36) in 17524 ms on localhost (2/2)
2017-08-10 09:12:05,102 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 18 (foreachPartition at streamingProcess.scala:50) finished in 17.524 s
2017-08-10 09:12:05,102 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2017-08-10 09:12:05,102 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 18 finished: foreachPartition at streamingProcess.scala:50, took 17.532954 s
2017-08-10 09:12:05,103 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327005000 ms.0 from job set of time 1502327005000 ms
2017-08-10 09:12:05,103 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 520.103 s for time 1502327005000 ms (execution: 17.546 s)
2017-08-10 09:12:05,103 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 35 from persistence list
2017-08-10 09:12:05,103 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327010000 ms.0 from job set of time 1502327010000 ms
2017-08-10 09:12:05,104 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 35
2017-08-10 09:12:05,104 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 34 from persistence list
2017-08-10 09:12:05,104 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 34
2017-08-10 09:12:05,104 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:12:05,104 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326995000 ms
2017-08-10 09:12:05,115 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:12:05,116 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 19 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:12:05,116 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 19 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:12:05,116 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:12:05,116 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:12:05,116 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 19 (MapPartitionsRDD[39] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:12:05,119 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:12:05,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:12:05,124 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_19_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:05,124 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 19 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:12:05,125 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at map at streamingProcess.scala:48)
2017-08-10 09:12:05,125 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 19.0 with 2 tasks
2017-08-10 09:12:05,126 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 19.0 (TID 38, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:12:05,127 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 19.0 (TID 39, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:12:05,127 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 19.0 (TID 39)
2017-08-10 09:12:05,127 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 19.0 (TID 38)
2017-08-10 09:12:05,130 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 475 -> 500
2017-08-10 09:12:05,131 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 475 -> 500
2017-08-10 09:12:06,111 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_18_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:09,358 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 19.0 (TID 38). 787 bytes result sent to driver
2017-08-10 09:12:09,361 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 19.0 (TID 38) in 4235 ms on localhost (1/2)
2017-08-10 09:12:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327530000 ms
2017-08-10 09:12:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327535000 ms
2017-08-10 09:12:16,029 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 19.0 (TID 39). 787 bytes result sent to driver
2017-08-10 09:12:16,032 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 19.0 (TID 39) in 10905 ms on localhost (2/2)
2017-08-10 09:12:16,032 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 19.0, whose tasks have all completed, from pool 
2017-08-10 09:12:16,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 19 (foreachPartition at streamingProcess.scala:50) finished in 10.907 s
2017-08-10 09:12:16,032 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 19 finished: foreachPartition at streamingProcess.scala:50, took 10.916616 s
2017-08-10 09:12:16,033 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327010000 ms.0 from job set of time 1502327010000 ms
2017-08-10 09:12:16,033 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 526.033 s for time 1502327010000 ms (execution: 10.930 s)
2017-08-10 09:12:16,033 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 37 from persistence list
2017-08-10 09:12:16,033 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327015000 ms.0 from job set of time 1502327015000 ms
2017-08-10 09:12:16,033 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 37
2017-08-10 09:12:16,034 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 36 from persistence list
2017-08-10 09:12:16,034 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 36
2017-08-10 09:12:16,034 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:12:16,034 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327000000 ms
2017-08-10 09:12:16,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:12:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 20 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:12:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 20 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:12:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:12:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:12:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 20 (MapPartitionsRDD[41] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:12:16,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:12:16,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:12:16,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_20_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 20 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:12:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at map at streamingProcess.scala:48)
2017-08-10 09:12:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 20.0 with 2 tasks
2017-08-10 09:12:16,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 20.0 (TID 40, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:12:16,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 20.0 (TID 41, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:12:16,060 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 20.0 (TID 40)
2017-08-10 09:12:16,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 20.0 (TID 41)
2017-08-10 09:12:16,063 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 500 -> 525
2017-08-10 09:12:16,063 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 500 -> 525
2017-08-10 09:12:16,684 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_19_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:17,364 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 20.0 (TID 40). 787 bytes result sent to driver
2017-08-10 09:12:17,366 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 20.0 (TID 40) in 1307 ms on localhost (1/2)
2017-08-10 09:12:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327540000 ms
2017-08-10 09:12:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327545000 ms
2017-08-10 09:12:30,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327550000 ms
2017-08-10 09:12:34,159 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 20.0 (TID 41). 787 bytes result sent to driver
2017-08-10 09:12:34,162 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 20.0 (TID 41) in 18103 ms on localhost (2/2)
2017-08-10 09:12:34,162 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 20.0, whose tasks have all completed, from pool 
2017-08-10 09:12:34,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 20 (foreachPartition at streamingProcess.scala:50) finished in 18.104 s
2017-08-10 09:12:34,162 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 20 finished: foreachPartition at streamingProcess.scala:50, took 18.114135 s
2017-08-10 09:12:34,163 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327015000 ms.0 from job set of time 1502327015000 ms
2017-08-10 09:12:34,163 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 539.163 s for time 1502327015000 ms (execution: 18.130 s)
2017-08-10 09:12:34,163 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 39 from persistence list
2017-08-10 09:12:34,163 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327020000 ms.0 from job set of time 1502327020000 ms
2017-08-10 09:12:34,163 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 39
2017-08-10 09:12:34,163 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 38 from persistence list
2017-08-10 09:12:34,164 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 38
2017-08-10 09:12:34,164 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:12:34,164 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327005000 ms
2017-08-10 09:12:34,176 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:12:34,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 21 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:12:34,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 21 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:12:34,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:12:34,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:12:34,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 21 (MapPartitionsRDD[43] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:12:34,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:12:34,185 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:12:34,188 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_21_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:34,189 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 21 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:12:34,190 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at map at streamingProcess.scala:48)
2017-08-10 09:12:34,190 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 21.0 with 2 tasks
2017-08-10 09:12:34,192 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 21.0 (TID 42, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:12:34,192 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 21.0 (TID 43, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:12:34,193 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 21.0 (TID 42)
2017-08-10 09:12:34,193 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 21.0 (TID 43)
2017-08-10 09:12:34,195 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 525 -> 550
2017-08-10 09:12:34,195 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 525 -> 550
2017-08-10 09:12:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327555000 ms
2017-08-10 09:12:35,435 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_20_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:35,948 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 21.0 (TID 43). 787 bytes result sent to driver
2017-08-10 09:12:35,950 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 21.0 (TID 43) in 1758 ms on localhost (1/2)
2017-08-10 09:12:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327560000 ms
2017-08-10 09:12:40,817 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 21.0 (TID 42). 787 bytes result sent to driver
2017-08-10 09:12:40,821 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 21.0 (TID 42) in 6630 ms on localhost (2/2)
2017-08-10 09:12:40,821 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 21 (foreachPartition at streamingProcess.scala:50) finished in 6.631 s
2017-08-10 09:12:40,821 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 21.0, whose tasks have all completed, from pool 
2017-08-10 09:12:40,822 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 21 finished: foreachPartition at streamingProcess.scala:50, took 6.644346 s
2017-08-10 09:12:40,822 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327020000 ms.0 from job set of time 1502327020000 ms
2017-08-10 09:12:40,822 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 540.822 s for time 1502327020000 ms (execution: 6.659 s)
2017-08-10 09:12:40,822 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 41 from persistence list
2017-08-10 09:12:40,823 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327025000 ms.0 from job set of time 1502327025000 ms
2017-08-10 09:12:40,823 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 41
2017-08-10 09:12:40,823 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 40 from persistence list
2017-08-10 09:12:40,823 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 40
2017-08-10 09:12:40,823 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:12:40,824 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327010000 ms
2017-08-10 09:12:40,837 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:12:40,838 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 22 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:12:40,838 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 22 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:12:40,838 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:12:40,839 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:12:40,839 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 22 (MapPartitionsRDD[45] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:12:40,843 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:12:40,848 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:12:40,848 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_22_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:40,849 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 22 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:12:40,849 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at map at streamingProcess.scala:48)
2017-08-10 09:12:40,849 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 22.0 with 2 tasks
2017-08-10 09:12:40,851 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 22.0 (TID 44, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:12:40,851 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 22.0 (TID 45, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:12:40,851 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 22.0 (TID 44)
2017-08-10 09:12:40,851 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 22.0 (TID 45)
2017-08-10 09:12:40,855 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 550 -> 575
2017-08-10 09:12:40,855 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 550 -> 575
2017-08-10 09:12:41,216 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_21_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:45,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327565000 ms
2017-08-10 09:12:46,527 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 22.0 (TID 45). 787 bytes result sent to driver
2017-08-10 09:12:46,530 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 22.0 (TID 45) in 5679 ms on localhost (1/2)
2017-08-10 09:12:50,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327570000 ms
2017-08-10 09:12:50,700 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 22.0 (TID 44). 787 bytes result sent to driver
2017-08-10 09:12:50,702 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 22.0 (TID 44) in 9852 ms on localhost (2/2)
2017-08-10 09:12:50,703 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 22.0, whose tasks have all completed, from pool 
2017-08-10 09:12:50,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 22 (foreachPartition at streamingProcess.scala:50) finished in 9.853 s
2017-08-10 09:12:50,703 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 22 finished: foreachPartition at streamingProcess.scala:50, took 9.864846 s
2017-08-10 09:12:50,704 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327025000 ms.0 from job set of time 1502327025000 ms
2017-08-10 09:12:50,704 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 545.704 s for time 1502327025000 ms (execution: 9.882 s)
2017-08-10 09:12:50,704 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 43 from persistence list
2017-08-10 09:12:50,704 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327030000 ms.0 from job set of time 1502327030000 ms
2017-08-10 09:12:50,704 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 43
2017-08-10 09:12:50,704 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 42 from persistence list
2017-08-10 09:12:50,705 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 42
2017-08-10 09:12:50,705 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:12:50,705 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327015000 ms
2017-08-10 09:12:50,716 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:12:50,717 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 23 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:12:50,717 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 23 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:12:50,717 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:12:50,717 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:12:50,718 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 23 (MapPartitionsRDD[47] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:12:50,720 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:12:50,724 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:12:50,725 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_23_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:50,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 23 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:12:50,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 23 (MapPartitionsRDD[47] at map at streamingProcess.scala:48)
2017-08-10 09:12:50,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 23.0 with 2 tasks
2017-08-10 09:12:50,727 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 23.0 (TID 46, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:12:50,728 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 23.0 (TID 47, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:12:50,728 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 23.0 (TID 47)
2017-08-10 09:12:50,728 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 23.0 (TID 46)
2017-08-10 09:12:50,730 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 575 -> 600
2017-08-10 09:12:50,730 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 575 -> 600
2017-08-10 09:12:55,033 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327575000 ms
2017-08-10 09:12:56,331 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_22_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:13:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327580000 ms
2017-08-10 09:13:04,907 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 23.0 (TID 47). 787 bytes result sent to driver
2017-08-10 09:13:04,911 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 23.0 (TID 47) in 14184 ms on localhost (1/2)
2017-08-10 09:13:05,265 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327585000 ms
2017-08-10 09:13:08,443 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 23.0 (TID 46). 787 bytes result sent to driver
2017-08-10 09:13:08,445 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 23.0 (TID 46) in 17719 ms on localhost (2/2)
2017-08-10 09:13:08,445 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 23.0, whose tasks have all completed, from pool 
2017-08-10 09:13:08,445 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 23 (foreachPartition at streamingProcess.scala:50) finished in 17.719 s
2017-08-10 09:13:08,446 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 23 finished: foreachPartition at streamingProcess.scala:50, took 17.727886 s
2017-08-10 09:13:08,446 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327030000 ms.0 from job set of time 1502327030000 ms
2017-08-10 09:13:08,446 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 558.446 s for time 1502327030000 ms (execution: 17.742 s)
2017-08-10 09:13:08,446 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327035000 ms.0 from job set of time 1502327035000 ms
2017-08-10 09:13:08,448 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 45 from persistence list
2017-08-10 09:13:08,449 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 44 from persistence list
2017-08-10 09:13:08,449 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 45
2017-08-10 09:13:08,450 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 44
2017-08-10 09:13:08,450 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:13:08,450 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327020000 ms
2017-08-10 09:13:08,460 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:13:08,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 24 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:13:08,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 24 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:13:08,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:13:08,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:13:08,462 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 24 (MapPartitionsRDD[49] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:13:08,465 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:13:08,474 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:13:08,475 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_24_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:13:08,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 24 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:13:08,476 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at map at streamingProcess.scala:48)
2017-08-10 09:13:08,476 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 24.0 with 2 tasks
2017-08-10 09:13:08,477 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 24.0 (TID 48, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:13:08,478 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 24.0 (TID 49, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:13:08,478 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 24.0 (TID 48)
2017-08-10 09:13:08,478 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 24.0 (TID 49)
2017-08-10 09:13:08,484 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 600 -> 625
2017-08-10 09:13:08,484 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 600 -> 625
2017-08-10 09:13:10,177 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327590000 ms
2017-08-10 09:13:15,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327595000 ms
2017-08-10 09:13:15,926 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 24.0 (TID 49). 714 bytes result sent to driver
2017-08-10 09:13:15,929 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 24.0 (TID 49) in 7451 ms on localhost (1/2)
2017-08-10 09:13:20,141 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327600000 ms
2017-08-10 09:13:22,269 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_23_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:13:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327605000 ms
2017-08-10 09:13:30,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327610000 ms
2017-08-10 09:13:32,143 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 24.0 (TID 48). 787 bytes result sent to driver
2017-08-10 09:13:32,145 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 24.0 (TID 48) in 23668 ms on localhost (2/2)
2017-08-10 09:13:32,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 24 (foreachPartition at streamingProcess.scala:50) finished in 23.669 s
2017-08-10 09:13:32,145 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 24.0, whose tasks have all completed, from pool 
2017-08-10 09:13:32,145 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 24 finished: foreachPartition at streamingProcess.scala:50, took 23.683740 s
2017-08-10 09:13:32,146 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327035000 ms.0 from job set of time 1502327035000 ms
2017-08-10 09:13:32,146 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 577.146 s for time 1502327035000 ms (execution: 23.700 s)
2017-08-10 09:13:32,146 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 47 from persistence list
2017-08-10 09:13:32,147 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327040000 ms.0 from job set of time 1502327040000 ms
2017-08-10 09:13:32,147 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 47
2017-08-10 09:13:32,147 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 46 from persistence list
2017-08-10 09:13:32,148 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 46
2017-08-10 09:13:32,148 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:13:32,148 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327025000 ms
2017-08-10 09:13:32,158 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:13:32,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 25 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:13:32,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 25 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:13:32,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:13:32,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:13:32,160 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 25 (MapPartitionsRDD[51] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:13:32,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:13:32,166 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:13:32,167 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_25_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:13:32,167 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 25 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:13:32,167 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at map at streamingProcess.scala:48)
2017-08-10 09:13:32,167 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 25.0 with 2 tasks
2017-08-10 09:13:32,169 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 25.0 (TID 50, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:13:32,169 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 25.0 (TID 51, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:13:32,170 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 25.0 (TID 51)
2017-08-10 09:13:32,170 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 25.0 (TID 50)
2017-08-10 09:13:32,173 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 625 -> 650
2017-08-10 09:13:32,173 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 625 -> 650
2017-08-10 09:13:32,316 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_24_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:13:35,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327615000 ms
2017-08-10 09:13:40,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327620000 ms
2017-08-10 09:13:42,986 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 25.0 (TID 51). 787 bytes result sent to driver
2017-08-10 09:13:42,988 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 25.0 (TID 51) in 10819 ms on localhost (1/2)
2017-08-10 09:13:45,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327625000 ms
2017-08-10 09:13:46,205 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 25.0 (TID 50). 787 bytes result sent to driver
2017-08-10 09:13:46,207 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 25.0 (TID 50) in 14038 ms on localhost (2/2)
2017-08-10 09:13:46,207 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 25.0, whose tasks have all completed, from pool 
2017-08-10 09:13:46,207 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 25 (foreachPartition at streamingProcess.scala:50) finished in 14.039 s
2017-08-10 09:13:46,207 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 25 finished: foreachPartition at streamingProcess.scala:50, took 14.047745 s
2017-08-10 09:13:46,208 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327040000 ms.0 from job set of time 1502327040000 ms
2017-08-10 09:13:46,208 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 586.208 s for time 1502327040000 ms (execution: 14.061 s)
2017-08-10 09:13:46,208 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 49 from persistence list
2017-08-10 09:13:46,208 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327045000 ms.0 from job set of time 1502327045000 ms
2017-08-10 09:13:46,208 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 49
2017-08-10 09:13:46,208 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 48 from persistence list
2017-08-10 09:13:46,209 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 48
2017-08-10 09:13:46,209 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:13:46,209 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327030000 ms
2017-08-10 09:13:46,219 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:13:46,219 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 26 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:13:46,219 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 26 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:13:46,220 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:13:46,220 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:13:46,220 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 26 (MapPartitionsRDD[53] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:13:46,223 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:13:46,226 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:13:46,226 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_26_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:13:46,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 26 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:13:46,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 26 (MapPartitionsRDD[53] at map at streamingProcess.scala:48)
2017-08-10 09:13:46,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 26.0 with 2 tasks
2017-08-10 09:13:46,229 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 26.0 (TID 52, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:13:46,230 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 26.0 (TID 53, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:13:46,230 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 26.0 (TID 52)
2017-08-10 09:13:46,230 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 26.0 (TID 53)
2017-08-10 09:13:46,232 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 650 -> 653
2017-08-10 09:13:46,232 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 650 -> 675
2017-08-10 09:13:46,995 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 26.0 (TID 52). 714 bytes result sent to driver
2017-08-10 09:13:46,997 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 26.0 (TID 52) in 769 ms on localhost (1/2)
2017-08-10 09:13:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327630000 ms
2017-08-10 09:13:55,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327635000 ms
2017-08-10 09:13:58,292 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_25_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327640000 ms
2017-08-10 09:14:05,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327645000 ms
2017-08-10 09:14:07,133 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 26.0 (TID 53). 787 bytes result sent to driver
2017-08-10 09:14:07,135 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 26.0 (TID 53) in 20906 ms on localhost (2/2)
2017-08-10 09:14:07,136 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 26.0, whose tasks have all completed, from pool 
2017-08-10 09:14:07,136 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 26 (foreachPartition at streamingProcess.scala:50) finished in 20.909 s
2017-08-10 09:14:07,136 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 26 finished: foreachPartition at streamingProcess.scala:50, took 20.916254 s
2017-08-10 09:14:07,137 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327045000 ms.0 from job set of time 1502327045000 ms
2017-08-10 09:14:07,137 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 602.137 s for time 1502327045000 ms (execution: 20.929 s)
2017-08-10 09:14:07,137 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 51 from persistence list
2017-08-10 09:14:07,137 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327050000 ms.0 from job set of time 1502327050000 ms
2017-08-10 09:14:07,137 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 51
2017-08-10 09:14:07,137 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 50 from persistence list
2017-08-10 09:14:07,138 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 50
2017-08-10 09:14:07,138 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:07,138 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327035000 ms
2017-08-10 09:14:07,152 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:07,152 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 27 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:07,153 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 27 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:07,153 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:07,153 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:07,153 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 27 (MapPartitionsRDD[55] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:07,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:07,161 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:07,162 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_27_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:07,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 27 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:07,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at map at streamingProcess.scala:48)
2017-08-10 09:14:07,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 27.0 with 2 tasks
2017-08-10 09:14:07,164 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 27.0 (TID 54, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:14:07,165 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 27.0 (TID 55, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:14:07,165 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 27.0 (TID 54)
2017-08-10 09:14:07,165 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 27.0 (TID 55)
2017-08-10 09:14:07,167 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:07,167 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 675 -> 698
2017-08-10 09:14:07,170 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 27.0 (TID 54). 714 bytes result sent to driver
2017-08-10 09:14:07,173 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 27.0 (TID 54) in 9 ms on localhost (1/2)
2017-08-10 09:14:10,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327650000 ms
2017-08-10 09:14:10,694 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_26_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:15,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327655000 ms
2017-08-10 09:14:16,196 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 27.0 (TID 55). 787 bytes result sent to driver
2017-08-10 09:14:16,198 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 27.0 (TID 55) in 9034 ms on localhost (2/2)
2017-08-10 09:14:16,198 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 27.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 27 (foreachPartition at streamingProcess.scala:50) finished in 9.035 s
2017-08-10 09:14:16,199 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 27 finished: foreachPartition at streamingProcess.scala:50, took 9.046204 s
2017-08-10 09:14:16,199 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327050000 ms.0 from job set of time 1502327050000 ms
2017-08-10 09:14:16,199 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 606.199 s for time 1502327050000 ms (execution: 9.062 s)
2017-08-10 09:14:16,199 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 53 from persistence list
2017-08-10 09:14:16,199 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327055000 ms.0 from job set of time 1502327055000 ms
2017-08-10 09:14:16,200 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 53
2017-08-10 09:14:16,200 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 52 from persistence list
2017-08-10 09:14:16,200 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 52
2017-08-10 09:14:16,200 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,200 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327040000 ms
2017-08-10 09:14:16,210 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,211 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 28 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,211 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 28 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,211 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,211 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,211 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 28 (MapPartitionsRDD[57] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,214 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:16,218 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:16,219 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_28_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,219 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 28 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,219 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,219 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 28.0 with 2 tasks
2017-08-10 09:14:16,221 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 28.0 (TID 56, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:14:16,221 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 28.0 (TID 57, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:14:16,221 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 28.0 (TID 57)
2017-08-10 09:14:16,221 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 28.0 (TID 56)
2017-08-10 09:14:16,224 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,224 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,227 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 28.0 (TID 56). 714 bytes result sent to driver
2017-08-10 09:14:16,227 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 28.0 (TID 57). 714 bytes result sent to driver
2017-08-10 09:14:16,229 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 28.0 (TID 56) in 9 ms on localhost (1/2)
2017-08-10 09:14:16,229 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 28.0 (TID 57) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,229 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 28.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,229 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 28 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:14:16,230 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 28 finished: foreachPartition at streamingProcess.scala:50, took 0.019560 s
2017-08-10 09:14:16,230 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327055000 ms.0 from job set of time 1502327055000 ms
2017-08-10 09:14:16,230 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 601.230 s for time 1502327055000 ms (execution: 0.031 s)
2017-08-10 09:14:16,230 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 55 from persistence list
2017-08-10 09:14:16,230 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327060000 ms.0 from job set of time 1502327060000 ms
2017-08-10 09:14:16,231 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 55
2017-08-10 09:14:16,231 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 54 from persistence list
2017-08-10 09:14:16,231 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 54
2017-08-10 09:14:16,231 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,231 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327045000 ms
2017-08-10 09:14:16,241 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 29 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 29 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 29 (MapPartitionsRDD[59] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,245 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:16,249 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:16,249 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_29_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,250 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 29 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,250 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 29 (MapPartitionsRDD[59] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,250 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 29.0 with 2 tasks
2017-08-10 09:14:16,251 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 29.0 (TID 58, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:14:16,252 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 29.0 (TID 59, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:14:16,252 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 29.0 (TID 59)
2017-08-10 09:14:16,252 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 29.0 (TID 58)
2017-08-10 09:14:16,255 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,255 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,258 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 29.0 (TID 59). 714 bytes result sent to driver
2017-08-10 09:14:16,258 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 29.0 (TID 58). 714 bytes result sent to driver
2017-08-10 09:14:16,260 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 29.0 (TID 59) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,260 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 29.0 (TID 58) in 10 ms on localhost (2/2)
2017-08-10 09:14:16,260 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 29.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 29 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:14:16,260 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 29 finished: foreachPartition at streamingProcess.scala:50, took 0.019168 s
2017-08-10 09:14:16,261 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327060000 ms.0 from job set of time 1502327060000 ms
2017-08-10 09:14:16,261 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 596.261 s for time 1502327060000 ms (execution: 0.031 s)
2017-08-10 09:14:16,261 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 57 from persistence list
2017-08-10 09:14:16,261 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327065000 ms.0 from job set of time 1502327065000 ms
2017-08-10 09:14:16,261 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 57
2017-08-10 09:14:16,262 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 56 from persistence list
2017-08-10 09:14:16,262 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 56
2017-08-10 09:14:16,262 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,262 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327050000 ms
2017-08-10 09:14:16,273 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,273 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 30 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,274 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 30 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,274 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,274 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,274 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 30 (MapPartitionsRDD[61] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:16,280 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:16,282 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_30_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,282 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 30 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 30.0 with 2 tasks
2017-08-10 09:14:16,284 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 30.0 (TID 60, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:14:16,285 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 30.0 (TID 61, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:14:16,285 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 30.0 (TID 60)
2017-08-10 09:14:16,285 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 30.0 (TID 61)
2017-08-10 09:14:16,287 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,287 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,290 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 30.0 (TID 60). 714 bytes result sent to driver
2017-08-10 09:14:16,290 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 30.0 (TID 61). 714 bytes result sent to driver
2017-08-10 09:14:16,292 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 30.0 (TID 61) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,292 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 30.0 (TID 60) in 9 ms on localhost (2/2)
2017-08-10 09:14:16,292 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 30.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,292 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 30 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,293 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 30 finished: foreachPartition at streamingProcess.scala:50, took 0.019895 s
2017-08-10 09:14:16,293 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327065000 ms.0 from job set of time 1502327065000 ms
2017-08-10 09:14:16,293 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 591.293 s for time 1502327065000 ms (execution: 0.032 s)
2017-08-10 09:14:16,293 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 59 from persistence list
2017-08-10 09:14:16,293 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327070000 ms.0 from job set of time 1502327070000 ms
2017-08-10 09:14:16,294 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 59
2017-08-10 09:14:16,294 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 58 from persistence list
2017-08-10 09:14:16,294 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 58
2017-08-10 09:14:16,294 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,294 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327055000 ms
2017-08-10 09:14:16,304 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,305 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 31 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,305 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 31 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,305 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,305 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,305 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 31 (MapPartitionsRDD[63] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,308 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:16,312 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:16,312 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_31_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 31 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 31.0 with 2 tasks
2017-08-10 09:14:16,314 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 31.0 (TID 62, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:14:16,315 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 31.0 (TID 63, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:14:16,315 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 31.0 (TID 62)
2017-08-10 09:14:16,315 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 31.0 (TID 63)
2017-08-10 09:14:16,317 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,317 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,320 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 31.0 (TID 62). 714 bytes result sent to driver
2017-08-10 09:14:16,320 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 31.0 (TID 63). 714 bytes result sent to driver
2017-08-10 09:14:16,322 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 31.0 (TID 62) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,322 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 31.0 (TID 63) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,322 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 31.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,322 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 31 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,322 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 31 finished: foreachPartition at streamingProcess.scala:50, took 0.017955 s
2017-08-10 09:14:16,323 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327070000 ms.0 from job set of time 1502327070000 ms
2017-08-10 09:14:16,323 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 586.323 s for time 1502327070000 ms (execution: 0.030 s)
2017-08-10 09:14:16,323 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327075000 ms.0 from job set of time 1502327075000 ms
2017-08-10 09:14:16,323 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 61 from persistence list
2017-08-10 09:14:16,324 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 60 from persistence list
2017-08-10 09:14:16,324 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 61
2017-08-10 09:14:16,324 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,324 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327060000 ms
2017-08-10 09:14:16,324 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 60
2017-08-10 09:14:16,334 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 32 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 32 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 32 (MapPartitionsRDD[65] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,338 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:16,341 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:16,342 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_32_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 32 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,343 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 32.0 with 2 tasks
2017-08-10 09:14:16,344 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 32.0 (TID 64, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:14:16,345 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 32.0 (TID 65, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:14:16,345 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 32.0 (TID 64)
2017-08-10 09:14:16,345 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 32.0 (TID 65)
2017-08-10 09:14:16,347 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,347 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,350 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 32.0 (TID 65). 714 bytes result sent to driver
2017-08-10 09:14:16,350 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 32.0 (TID 64). 714 bytes result sent to driver
2017-08-10 09:14:16,352 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 32.0 (TID 65) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,352 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 32.0 (TID 64) in 9 ms on localhost (2/2)
2017-08-10 09:14:16,352 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 32.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,352 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 32 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,353 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 32 finished: foreachPartition at streamingProcess.scala:50, took 0.018232 s
2017-08-10 09:14:16,353 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327075000 ms.0 from job set of time 1502327075000 ms
2017-08-10 09:14:16,353 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 581.353 s for time 1502327075000 ms (execution: 0.030 s)
2017-08-10 09:14:16,353 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 63 from persistence list
2017-08-10 09:14:16,353 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327080000 ms.0 from job set of time 1502327080000 ms
2017-08-10 09:14:16,354 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 62 from persistence list
2017-08-10 09:14:16,354 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 63
2017-08-10 09:14:16,354 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,354 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 62
2017-08-10 09:14:16,354 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327065000 ms
2017-08-10 09:14:16,364 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 33 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 33 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 33 (MapPartitionsRDD[67] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,368 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:16,371 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:16,372 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_33_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,373 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 33 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,373 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 33 (MapPartitionsRDD[67] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,373 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 33.0 with 2 tasks
2017-08-10 09:14:16,374 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 33.0 (TID 66, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,374 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 33.0 (TID 67, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,375 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 33.0 (TID 66)
2017-08-10 09:14:16,375 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 33.0 (TID 67)
2017-08-10 09:14:16,377 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,377 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,380 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 33.0 (TID 66). 714 bytes result sent to driver
2017-08-10 09:14:16,380 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 33.0 (TID 67). 714 bytes result sent to driver
2017-08-10 09:14:16,381 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 33.0 (TID 67) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,382 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 33.0 (TID 66) in 9 ms on localhost (2/2)
2017-08-10 09:14:16,382 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 33.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,382 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 33 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,382 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 33 finished: foreachPartition at streamingProcess.scala:50, took 0.018034 s
2017-08-10 09:14:16,382 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327080000 ms.0 from job set of time 1502327080000 ms
2017-08-10 09:14:16,383 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 65 from persistence list
2017-08-10 09:14:16,383 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 576.382 s for time 1502327080000 ms (execution: 0.029 s)
2017-08-10 09:14:16,383 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327085000 ms.0 from job set of time 1502327085000 ms
2017-08-10 09:14:16,383 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 65
2017-08-10 09:14:16,383 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 64 from persistence list
2017-08-10 09:14:16,383 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 64
2017-08-10 09:14:16,384 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,384 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327070000 ms
2017-08-10 09:14:16,394 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 34 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 34 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,395 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,395 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,395 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 34 (MapPartitionsRDD[69] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,398 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_34 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:16,400 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_34_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:16,401 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_34_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,402 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 34 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,402 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 34 (MapPartitionsRDD[69] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,402 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 34.0 with 2 tasks
2017-08-10 09:14:16,403 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 34.0 (TID 68, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,404 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 34.0 (TID 69, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,404 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 34.0 (TID 68)
2017-08-10 09:14:16,404 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 34.0 (TID 69)
2017-08-10 09:14:16,406 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,406 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,409 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 34.0 (TID 68). 714 bytes result sent to driver
2017-08-10 09:14:16,409 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 34.0 (TID 69). 714 bytes result sent to driver
2017-08-10 09:14:16,411 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 34.0 (TID 69) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,411 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 34.0 (TID 68) in 9 ms on localhost (2/2)
2017-08-10 09:14:16,411 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 34.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,411 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 34 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,412 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 34 finished: foreachPartition at streamingProcess.scala:50, took 0.017645 s
2017-08-10 09:14:16,412 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327085000 ms.0 from job set of time 1502327085000 ms
2017-08-10 09:14:16,412 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 571.412 s for time 1502327085000 ms (execution: 0.029 s)
2017-08-10 09:14:16,412 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 67 from persistence list
2017-08-10 09:14:16,412 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327090000 ms.0 from job set of time 1502327090000 ms
2017-08-10 09:14:16,412 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 67
2017-08-10 09:14:16,413 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 66 from persistence list
2017-08-10 09:14:16,413 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 66
2017-08-10 09:14:16,413 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,413 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327075000 ms
2017-08-10 09:14:16,423 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,424 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 35 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,424 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 35 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,424 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,424 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,424 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 35 (MapPartitionsRDD[71] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,427 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_35 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:16,430 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_35_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:16,430 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_35_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,430 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 35 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,430 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 35 (MapPartitionsRDD[71] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,431 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 35.0 with 2 tasks
2017-08-10 09:14:16,432 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 35.0 (TID 70, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,432 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 35.0 (TID 71, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,432 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 35.0 (TID 70)
2017-08-10 09:14:16,432 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 35.0 (TID 71)
2017-08-10 09:14:16,434 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,434 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,437 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 35.0 (TID 71). 714 bytes result sent to driver
2017-08-10 09:14:16,437 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 35.0 (TID 70). 714 bytes result sent to driver
2017-08-10 09:14:16,439 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 35.0 (TID 70) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,439 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 35.0 (TID 71) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,439 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 35.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,439 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 35 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,440 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 35 finished: foreachPartition at streamingProcess.scala:50, took 0.016866 s
2017-08-10 09:14:16,440 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327090000 ms.0 from job set of time 1502327090000 ms
2017-08-10 09:14:16,440 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 566.440 s for time 1502327090000 ms (execution: 0.028 s)
2017-08-10 09:14:16,440 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 69 from persistence list
2017-08-10 09:14:16,440 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327095000 ms.0 from job set of time 1502327095000 ms
2017-08-10 09:14:16,441 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 69
2017-08-10 09:14:16,441 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 68 from persistence list
2017-08-10 09:14:16,441 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,441 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327080000 ms
2017-08-10 09:14:16,442 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 68
2017-08-10 09:14:16,452 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,452 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 36 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,452 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 36 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,452 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,452 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,453 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 36 (MapPartitionsRDD[73] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_36 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:16,463 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_36_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:16,464 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_36_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,464 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_27_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,465 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 36 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,465 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 36 (MapPartitionsRDD[73] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,465 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 36.0 with 2 tasks
2017-08-10 09:14:16,466 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_28_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,466 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 36.0 (TID 72, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,467 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 36.0 (TID 73, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,468 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 36.0 (TID 72)
2017-08-10 09:14:16,468 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 36.0 (TID 73)
2017-08-10 09:14:16,469 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_29_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,470 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_30_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,471 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,471 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,472 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_31_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,474 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_32_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,474 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 36.0 (TID 73). 714 bytes result sent to driver
2017-08-10 09:14:16,474 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 36.0 (TID 72). 714 bytes result sent to driver
2017-08-10 09:14:16,475 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_33_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,476 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 36.0 (TID 73) in 9 ms on localhost (1/2)
2017-08-10 09:14:16,476 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 36.0 (TID 72) in 11 ms on localhost (2/2)
2017-08-10 09:14:16,476 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 36.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,477 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_34_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,477 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 36 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:14:16,477 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 36 finished: foreachPartition at streamingProcess.scala:50, took 0.025238 s
2017-08-10 09:14:16,478 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327095000 ms.0 from job set of time 1502327095000 ms
2017-08-10 09:14:16,478 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 561.478 s for time 1502327095000 ms (execution: 0.038 s)
2017-08-10 09:14:16,478 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327100000 ms.0 from job set of time 1502327100000 ms
2017-08-10 09:14:16,478 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_35_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,478 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 71 from persistence list
2017-08-10 09:14:16,479 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 71
2017-08-10 09:14:16,479 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 70 from persistence list
2017-08-10 09:14:16,479 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 70
2017-08-10 09:14:16,479 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,479 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327085000 ms
2017-08-10 09:14:16,490 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,490 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 37 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,490 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 37 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,491 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,491 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,491 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 37 (MapPartitionsRDD[75] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,494 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_37 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:16,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_37_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:16,497 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_37_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 37 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 37 (MapPartitionsRDD[75] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 37.0 with 2 tasks
2017-08-10 09:14:16,498 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 37.0 (TID 74, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,499 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 37.0 (TID 75, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,499 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 37.0 (TID 75)
2017-08-10 09:14:16,499 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 37.0 (TID 74)
2017-08-10 09:14:16,501 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,501 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,504 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 37.0 (TID 74). 714 bytes result sent to driver
2017-08-10 09:14:16,504 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 37.0 (TID 75). 714 bytes result sent to driver
2017-08-10 09:14:16,505 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 37.0 (TID 74) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,506 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 37.0 (TID 75) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,506 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 37.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,506 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 37 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,506 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 37 finished: foreachPartition at streamingProcess.scala:50, took 0.016092 s
2017-08-10 09:14:16,506 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327100000 ms.0 from job set of time 1502327100000 ms
2017-08-10 09:14:16,507 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 556.506 s for time 1502327100000 ms (execution: 0.028 s)
2017-08-10 09:14:16,507 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 73 from persistence list
2017-08-10 09:14:16,507 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327105000 ms.0 from job set of time 1502327105000 ms
2017-08-10 09:14:16,507 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 73
2017-08-10 09:14:16,507 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 72 from persistence list
2017-08-10 09:14:16,507 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 72
2017-08-10 09:14:16,507 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,507 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327090000 ms
2017-08-10 09:14:16,517 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 38 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 38 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 38 (MapPartitionsRDD[77] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,520 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_38 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:16,523 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_38_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:16,523 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_38_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,524 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 38 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,524 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 38 (MapPartitionsRDD[77] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,524 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 38.0 with 2 tasks
2017-08-10 09:14:16,525 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 38.0 (TID 76, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,525 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 38.0 (TID 77, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,525 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 38.0 (TID 76)
2017-08-10 09:14:16,525 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 38.0 (TID 77)
2017-08-10 09:14:16,527 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,527 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,530 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 38.0 (TID 76). 714 bytes result sent to driver
2017-08-10 09:14:16,530 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 38.0 (TID 77). 714 bytes result sent to driver
2017-08-10 09:14:16,532 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 38.0 (TID 77) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,532 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 38.0 (TID 76) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,532 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 38.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,532 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 38 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,532 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 38 finished: foreachPartition at streamingProcess.scala:50, took 0.015284 s
2017-08-10 09:14:16,533 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327105000 ms.0 from job set of time 1502327105000 ms
2017-08-10 09:14:16,533 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 551.533 s for time 1502327105000 ms (execution: 0.026 s)
2017-08-10 09:14:16,533 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 75 from persistence list
2017-08-10 09:14:16,533 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327110000 ms.0 from job set of time 1502327110000 ms
2017-08-10 09:14:16,533 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 75
2017-08-10 09:14:16,533 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 74 from persistence list
2017-08-10 09:14:16,534 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 74
2017-08-10 09:14:16,534 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,534 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327095000 ms
2017-08-10 09:14:16,544 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,544 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 39 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,544 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 39 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,544 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,545 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,545 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 39 (MapPartitionsRDD[79] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,548 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_39 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:16,550 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_39_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:16,551 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_39_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,551 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 39 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,552 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 39 (MapPartitionsRDD[79] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,552 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 39.0 with 2 tasks
2017-08-10 09:14:16,553 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 39.0 (TID 78, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,554 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 39.0 (TID 79, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,554 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 39.0 (TID 78)
2017-08-10 09:14:16,554 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 39.0 (TID 79)
2017-08-10 09:14:16,556 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,556 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,559 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 39.0 (TID 78). 714 bytes result sent to driver
2017-08-10 09:14:16,559 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 39.0 (TID 79). 714 bytes result sent to driver
2017-08-10 09:14:16,560 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 39.0 (TID 78) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,560 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 39.0 (TID 79) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,560 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 39.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,560 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 39 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,561 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 39 finished: foreachPartition at streamingProcess.scala:50, took 0.017084 s
2017-08-10 09:14:16,562 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327110000 ms.0 from job set of time 1502327110000 ms
2017-08-10 09:14:16,562 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 77 from persistence list
2017-08-10 09:14:16,562 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 546.562 s for time 1502327110000 ms (execution: 0.029 s)
2017-08-10 09:14:16,562 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327115000 ms.0 from job set of time 1502327115000 ms
2017-08-10 09:14:16,562 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 77
2017-08-10 09:14:16,562 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 76 from persistence list
2017-08-10 09:14:16,563 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 76
2017-08-10 09:14:16,563 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,563 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327100000 ms
2017-08-10 09:14:16,572 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 40 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 40 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 40 (MapPartitionsRDD[81] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,576 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_40 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:16,580 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_40_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:16,581 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_40_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,583 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 40 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,583 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 40 (MapPartitionsRDD[81] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,583 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 40.0 with 2 tasks
2017-08-10 09:14:16,588 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 40.0 (TID 80, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,589 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 40.0 (TID 81, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,590 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 40.0 (TID 80)
2017-08-10 09:14:16,591 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 40.0 (TID 81)
2017-08-10 09:14:16,592 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,593 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,596 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 40.0 (TID 81). 714 bytes result sent to driver
2017-08-10 09:14:16,597 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 40.0 (TID 80). 714 bytes result sent to driver
2017-08-10 09:14:16,599 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 40.0 (TID 81) in 10 ms on localhost (1/2)
2017-08-10 09:14:16,600 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 40.0 (TID 80) in 15 ms on localhost (2/2)
2017-08-10 09:14:16,601 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 40.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,601 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 40 (foreachPartition at streamingProcess.scala:50) finished in 0.017 s
2017-08-10 09:14:16,602 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 40 finished: foreachPartition at streamingProcess.scala:50, took 0.028899 s
2017-08-10 09:14:16,602 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327115000 ms.0 from job set of time 1502327115000 ms
2017-08-10 09:14:16,602 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 541.602 s for time 1502327115000 ms (execution: 0.040 s)
2017-08-10 09:14:16,602 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327120000 ms.0 from job set of time 1502327120000 ms
2017-08-10 09:14:16,603 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 79 from persistence list
2017-08-10 09:14:16,603 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 79
2017-08-10 09:14:16,603 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 78 from persistence list
2017-08-10 09:14:16,604 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 78
2017-08-10 09:14:16,604 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,604 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327105000 ms
2017-08-10 09:14:16,612 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,613 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 41 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,613 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 41 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,613 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,613 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,613 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 41 (MapPartitionsRDD[83] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,616 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_41 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:16,619 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_41_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:16,619 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_41_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,620 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 41 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,620 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 41 (MapPartitionsRDD[83] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,620 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 41.0 with 2 tasks
2017-08-10 09:14:16,621 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 41.0 (TID 82, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,622 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 41.0 (TID 83, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,622 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 41.0 (TID 82)
2017-08-10 09:14:16,622 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 41.0 (TID 83)
2017-08-10 09:14:16,623 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,623 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,626 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 41.0 (TID 82). 714 bytes result sent to driver
2017-08-10 09:14:16,626 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 41.0 (TID 83). 714 bytes result sent to driver
2017-08-10 09:14:16,628 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 41.0 (TID 83) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,628 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 41.0 (TID 82) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,628 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 41.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,628 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 41 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,629 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 41 finished: foreachPartition at streamingProcess.scala:50, took 0.015756 s
2017-08-10 09:14:16,629 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327120000 ms.0 from job set of time 1502327120000 ms
2017-08-10 09:14:16,629 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 81 from persistence list
2017-08-10 09:14:16,629 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 536.629 s for time 1502327120000 ms (execution: 0.027 s)
2017-08-10 09:14:16,629 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327125000 ms.0 from job set of time 1502327125000 ms
2017-08-10 09:14:16,629 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 81
2017-08-10 09:14:16,629 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 80 from persistence list
2017-08-10 09:14:16,630 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 80
2017-08-10 09:14:16,630 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,630 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327110000 ms
2017-08-10 09:14:16,640 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,640 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 42 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,640 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 42 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,640 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,640 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,641 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 42 (MapPartitionsRDD[85] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,644 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_42 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:16,647 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_42_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:16,647 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_42_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,647 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 42 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,647 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 42 (MapPartitionsRDD[85] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,648 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 42.0 with 2 tasks
2017-08-10 09:14:16,648 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 42.0 (TID 84, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,649 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 42.0 (TID 85, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,649 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 42.0 (TID 85)
2017-08-10 09:14:16,649 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 42.0 (TID 84)
2017-08-10 09:14:16,651 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,651 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,654 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 42.0 (TID 85). 714 bytes result sent to driver
2017-08-10 09:14:16,654 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 42.0 (TID 84). 714 bytes result sent to driver
2017-08-10 09:14:16,655 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 42.0 (TID 85) in 6 ms on localhost (1/2)
2017-08-10 09:14:16,656 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 42.0 (TID 84) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,656 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 42.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,656 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 42 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,656 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 42 finished: foreachPartition at streamingProcess.scala:50, took 0.016277 s
2017-08-10 09:14:16,656 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327125000 ms.0 from job set of time 1502327125000 ms
2017-08-10 09:14:16,657 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 531.656 s for time 1502327125000 ms (execution: 0.027 s)
2017-08-10 09:14:16,657 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 83 from persistence list
2017-08-10 09:14:16,657 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327130000 ms.0 from job set of time 1502327130000 ms
2017-08-10 09:14:16,657 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 83
2017-08-10 09:14:16,657 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 82 from persistence list
2017-08-10 09:14:16,658 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 82
2017-08-10 09:14:16,658 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,658 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327115000 ms
2017-08-10 09:14:16,667 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 43 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 43 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 43 (MapPartitionsRDD[87] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,671 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_43 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:16,673 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_43_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:16,674 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_43_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,674 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 43 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,674 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 43 (MapPartitionsRDD[87] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,674 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 43.0 with 2 tasks
2017-08-10 09:14:16,675 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 43.0 (TID 86, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,675 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 43.0 (TID 87, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,676 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 43.0 (TID 86)
2017-08-10 09:14:16,676 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 43.0 (TID 87)
2017-08-10 09:14:16,677 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,677 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,680 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 43.0 (TID 86). 714 bytes result sent to driver
2017-08-10 09:14:16,680 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 43.0 (TID 87). 714 bytes result sent to driver
2017-08-10 09:14:16,682 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 43.0 (TID 86) in 6 ms on localhost (1/2)
2017-08-10 09:14:16,682 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 43.0 (TID 87) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,682 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 43.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,682 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 43 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,682 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 43 finished: foreachPartition at streamingProcess.scala:50, took 0.015164 s
2017-08-10 09:14:16,683 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327130000 ms.0 from job set of time 1502327130000 ms
2017-08-10 09:14:16,683 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 85 from persistence list
2017-08-10 09:14:16,683 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 526.683 s for time 1502327130000 ms (execution: 0.026 s)
2017-08-10 09:14:16,683 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327135000 ms.0 from job set of time 1502327135000 ms
2017-08-10 09:14:16,683 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 85
2017-08-10 09:14:16,683 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 84 from persistence list
2017-08-10 09:14:16,684 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 84
2017-08-10 09:14:16,684 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,684 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327120000 ms
2017-08-10 09:14:16,693 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,693 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 44 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,694 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 44 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,694 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,694 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,694 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 44 (MapPartitionsRDD[89] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,696 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_44 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:16,699 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_44_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:16,700 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_44_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 44 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 44 (MapPartitionsRDD[89] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 44.0 with 2 tasks
2017-08-10 09:14:16,701 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 44.0 (TID 88, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,702 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 44.0 (TID 89, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,702 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 44.0 (TID 89)
2017-08-10 09:14:16,702 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 44.0 (TID 88)
2017-08-10 09:14:16,704 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,704 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,706 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 44.0 (TID 88). 714 bytes result sent to driver
2017-08-10 09:14:16,706 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 44.0 (TID 89). 714 bytes result sent to driver
2017-08-10 09:14:16,708 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 44.0 (TID 88) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,708 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 44.0 (TID 89) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,708 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 44.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,708 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 44 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,708 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 44 finished: foreachPartition at streamingProcess.scala:50, took 0.015280 s
2017-08-10 09:14:16,709 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327135000 ms.0 from job set of time 1502327135000 ms
2017-08-10 09:14:16,709 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 521.709 s for time 1502327135000 ms (execution: 0.026 s)
2017-08-10 09:14:16,709 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 87 from persistence list
2017-08-10 09:14:16,709 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327140000 ms.0 from job set of time 1502327140000 ms
2017-08-10 09:14:16,709 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 87
2017-08-10 09:14:16,709 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 86 from persistence list
2017-08-10 09:14:16,710 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 86
2017-08-10 09:14:16,710 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,710 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327125000 ms
2017-08-10 09:14:16,719 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,720 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 45 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,720 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 45 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,720 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,720 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,720 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 45 (MapPartitionsRDD[91] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,723 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_45 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:16,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_45_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:16,726 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_45_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 45 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 45 (MapPartitionsRDD[91] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 45.0 with 2 tasks
2017-08-10 09:14:16,727 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 45.0 (TID 90, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,728 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 45.0 (TID 91, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,728 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 45.0 (TID 90)
2017-08-10 09:14:16,728 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 45.0 (TID 91)
2017-08-10 09:14:16,730 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,730 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,733 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 45.0 (TID 91). 714 bytes result sent to driver
2017-08-10 09:14:16,733 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 45.0 (TID 90). 714 bytes result sent to driver
2017-08-10 09:14:16,734 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 45.0 (TID 91) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,734 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 45.0 (TID 90) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,734 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 45.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,734 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 45 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:16,735 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 45 finished: foreachPartition at streamingProcess.scala:50, took 0.015176 s
2017-08-10 09:14:16,735 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327140000 ms.0 from job set of time 1502327140000 ms
2017-08-10 09:14:16,735 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 516.735 s for time 1502327140000 ms (execution: 0.026 s)
2017-08-10 09:14:16,735 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 89 from persistence list
2017-08-10 09:14:16,735 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327145000 ms.0 from job set of time 1502327145000 ms
2017-08-10 09:14:16,736 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 89
2017-08-10 09:14:16,736 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 88 from persistence list
2017-08-10 09:14:16,736 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 88
2017-08-10 09:14:16,736 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,736 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327130000 ms
2017-08-10 09:14:16,746 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,747 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 46 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,747 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 46 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,747 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,747 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,747 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 46 (MapPartitionsRDD[93] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,750 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_46 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:16,752 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_46_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:16,753 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_46_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,754 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 46 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,754 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 46 (MapPartitionsRDD[93] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,754 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 46.0 with 2 tasks
2017-08-10 09:14:16,755 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 46.0 (TID 92, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,755 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 46.0 (TID 93, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,755 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 46.0 (TID 92)
2017-08-10 09:14:16,755 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 46.0 (TID 93)
2017-08-10 09:14:16,757 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,757 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,759 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 46.0 (TID 93). 714 bytes result sent to driver
2017-08-10 09:14:16,759 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 46.0 (TID 92). 714 bytes result sent to driver
2017-08-10 09:14:16,761 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 46.0 (TID 92) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,763 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 46.0 (TID 93) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,763 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 46.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,763 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 46 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,763 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 46 finished: foreachPartition at streamingProcess.scala:50, took 0.016675 s
2017-08-10 09:14:16,763 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327145000 ms.0 from job set of time 1502327145000 ms
2017-08-10 09:14:16,763 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 511.763 s for time 1502327145000 ms (execution: 0.028 s)
2017-08-10 09:14:16,763 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 91 from persistence list
2017-08-10 09:14:16,764 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327150000 ms.0 from job set of time 1502327150000 ms
2017-08-10 09:14:16,764 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 91
2017-08-10 09:14:16,764 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 90 from persistence list
2017-08-10 09:14:16,765 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 90
2017-08-10 09:14:16,765 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,765 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327135000 ms
2017-08-10 09:14:16,775 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,775 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 47 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,775 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 47 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,775 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,776 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,776 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 47 (MapPartitionsRDD[95] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_47 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:16,782 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_47_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:16,782 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_47_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,783 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 47 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,783 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 47 (MapPartitionsRDD[95] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,783 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 47.0 with 2 tasks
2017-08-10 09:14:16,784 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 47.0 (TID 94, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,784 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 47.0 (TID 95, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,784 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 47.0 (TID 94)
2017-08-10 09:14:16,784 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 47.0 (TID 95)
2017-08-10 09:14:16,786 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,786 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,789 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 47.0 (TID 95). 714 bytes result sent to driver
2017-08-10 09:14:16,789 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 47.0 (TID 94). 714 bytes result sent to driver
2017-08-10 09:14:16,790 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 47.0 (TID 95) in 6 ms on localhost (1/2)
2017-08-10 09:14:16,790 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 47.0 (TID 94) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,791 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 47.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,791 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 47 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,791 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 47 finished: foreachPartition at streamingProcess.scala:50, took 0.015932 s
2017-08-10 09:14:16,791 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327150000 ms.0 from job set of time 1502327150000 ms
2017-08-10 09:14:16,792 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 506.791 s for time 1502327150000 ms (execution: 0.027 s)
2017-08-10 09:14:16,792 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 93 from persistence list
2017-08-10 09:14:16,792 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327155000 ms.0 from job set of time 1502327155000 ms
2017-08-10 09:14:16,792 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 93
2017-08-10 09:14:16,792 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 92 from persistence list
2017-08-10 09:14:16,792 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 92
2017-08-10 09:14:16,792 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,793 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327140000 ms
2017-08-10 09:14:16,803 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 48 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 48 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,804 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,804 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 48 (MapPartitionsRDD[97] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,806 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_48 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:16,809 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_48_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:16,809 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_48_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,809 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 48 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 48 (MapPartitionsRDD[97] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 48.0 with 2 tasks
2017-08-10 09:14:16,811 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 48.0 (TID 96, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,811 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 48.0 (TID 97, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,811 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 48.0 (TID 96)
2017-08-10 09:14:16,811 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 48.0 (TID 97)
2017-08-10 09:14:16,813 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,813 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,816 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 48.0 (TID 97). 714 bytes result sent to driver
2017-08-10 09:14:16,816 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 48.0 (TID 96). 714 bytes result sent to driver
2017-08-10 09:14:16,817 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 48.0 (TID 97) in 6 ms on localhost (1/2)
2017-08-10 09:14:16,817 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 48.0 (TID 96) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,818 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 48.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,818 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 48 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,818 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 48 finished: foreachPartition at streamingProcess.scala:50, took 0.014986 s
2017-08-10 09:14:16,818 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327155000 ms.0 from job set of time 1502327155000 ms
2017-08-10 09:14:16,818 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 501.818 s for time 1502327155000 ms (execution: 0.026 s)
2017-08-10 09:14:16,818 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 95 from persistence list
2017-08-10 09:14:16,819 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327160000 ms.0 from job set of time 1502327160000 ms
2017-08-10 09:14:16,819 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 95
2017-08-10 09:14:16,819 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 94 from persistence list
2017-08-10 09:14:16,819 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 94
2017-08-10 09:14:16,819 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,819 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327145000 ms
2017-08-10 09:14:16,830 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,830 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 49 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,830 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 49 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,830 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,830 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,830 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 49 (MapPartitionsRDD[99] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,833 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_49 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:16,836 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_49_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:16,836 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_49_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,836 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 49 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,836 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 49 (MapPartitionsRDD[99] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,836 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 49.0 with 2 tasks
2017-08-10 09:14:16,837 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 49.0 (TID 98, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,838 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 49.0 (TID 99, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,838 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 49.0 (TID 98)
2017-08-10 09:14:16,838 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 49.0 (TID 99)
2017-08-10 09:14:16,840 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,840 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,842 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 49.0 (TID 99). 714 bytes result sent to driver
2017-08-10 09:14:16,842 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 49.0 (TID 98). 714 bytes result sent to driver
2017-08-10 09:14:16,844 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 49.0 (TID 99) in 6 ms on localhost (1/2)
2017-08-10 09:14:16,844 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 49.0 (TID 98) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,844 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 49.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,844 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 49 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:16,844 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 49 finished: foreachPartition at streamingProcess.scala:50, took 0.014362 s
2017-08-10 09:14:16,844 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327160000 ms.0 from job set of time 1502327160000 ms
2017-08-10 09:14:16,845 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 496.844 s for time 1502327160000 ms (execution: 0.025 s)
2017-08-10 09:14:16,845 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 97 from persistence list
2017-08-10 09:14:16,845 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327165000 ms.0 from job set of time 1502327165000 ms
2017-08-10 09:14:16,845 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 97
2017-08-10 09:14:16,845 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 96 from persistence list
2017-08-10 09:14:16,845 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 96
2017-08-10 09:14:16,845 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,845 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327150000 ms
2017-08-10 09:14:16,856 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,856 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 50 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,856 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 50 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,856 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,857 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,857 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 50 (MapPartitionsRDD[101] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,860 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_50 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:16,863 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_50_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:16,864 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_50_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:16,864 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 50 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,864 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 50 (MapPartitionsRDD[101] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,864 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 50.0 with 2 tasks
2017-08-10 09:14:16,865 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 50.0 (TID 100, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,866 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 50.0 (TID 101, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,866 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 50.0 (TID 100)
2017-08-10 09:14:16,866 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 50.0 (TID 101)
2017-08-10 09:14:16,868 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,868 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,870 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 50.0 (TID 100). 714 bytes result sent to driver
2017-08-10 09:14:16,870 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 50.0 (TID 101). 714 bytes result sent to driver
2017-08-10 09:14:16,872 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 50.0 (TID 101) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,872 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 50.0 (TID 100) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,872 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 50.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,872 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 50 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,872 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 50 finished: foreachPartition at streamingProcess.scala:50, took 0.016212 s
2017-08-10 09:14:16,873 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327165000 ms.0 from job set of time 1502327165000 ms
2017-08-10 09:14:16,873 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 491.873 s for time 1502327165000 ms (execution: 0.028 s)
2017-08-10 09:14:16,873 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 99 from persistence list
2017-08-10 09:14:16,873 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327170000 ms.0 from job set of time 1502327170000 ms
2017-08-10 09:14:16,874 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 99
2017-08-10 09:14:16,874 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 98 from persistence list
2017-08-10 09:14:16,874 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 98
2017-08-10 09:14:16,874 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,874 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327155000 ms
2017-08-10 09:14:16,884 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,884 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 51 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,884 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 51 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,884 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,885 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,885 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 51 (MapPartitionsRDD[103] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_51 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:16,890 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_51_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:16,891 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_51_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:16,891 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 51 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,891 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 51 (MapPartitionsRDD[103] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,891 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 51.0 with 2 tasks
2017-08-10 09:14:16,892 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 51.0 (TID 102, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,892 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 51.0 (TID 103, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,893 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 51.0 (TID 102)
2017-08-10 09:14:16,893 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 51.0 (TID 103)
2017-08-10 09:14:16,894 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,894 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,897 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 51.0 (TID 102). 714 bytes result sent to driver
2017-08-10 09:14:16,897 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 51.0 (TID 103). 714 bytes result sent to driver
2017-08-10 09:14:16,899 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 51.0 (TID 103) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,899 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 51.0 (TID 102) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,899 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 51.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,899 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 51 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,900 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 51 finished: foreachPartition at streamingProcess.scala:50, took 0.015441 s
2017-08-10 09:14:16,900 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327170000 ms.0 from job set of time 1502327170000 ms
2017-08-10 09:14:16,900 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 486.900 s for time 1502327170000 ms (execution: 0.027 s)
2017-08-10 09:14:16,900 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 101 from persistence list
2017-08-10 09:14:16,900 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327175000 ms.0 from job set of time 1502327175000 ms
2017-08-10 09:14:16,900 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 101
2017-08-10 09:14:16,900 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 100 from persistence list
2017-08-10 09:14:16,901 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 100
2017-08-10 09:14:16,901 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,901 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327160000 ms
2017-08-10 09:14:16,912 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_37_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:16,914 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_51_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,915 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_36_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,916 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_38_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,918 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_39_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,919 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_40_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,921 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_41_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,923 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,923 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_42_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,923 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 52 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,923 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 52 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,923 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,923 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,924 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_43_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,924 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 52 (MapPartitionsRDD[105] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,925 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_44_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,927 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_45_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,928 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_46_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,928 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_47_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,929 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_48_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,929 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_52 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:16,930 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_49_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,931 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_50_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,933 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_52_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:16,933 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_52_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,933 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 52 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,933 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 52 (MapPartitionsRDD[105] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,933 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 52.0 with 2 tasks
2017-08-10 09:14:16,934 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 52.0 (TID 104, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,935 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 52.0 (TID 105, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,935 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 52.0 (TID 104)
2017-08-10 09:14:16,935 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 52.0 (TID 105)
2017-08-10 09:14:16,937 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,937 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,941 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 52.0 (TID 104). 714 bytes result sent to driver
2017-08-10 09:14:16,941 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 52.0 (TID 105). 714 bytes result sent to driver
2017-08-10 09:14:16,942 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 52.0 (TID 104) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,942 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 52.0 (TID 105) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,942 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 52.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,943 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 52 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,943 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 52 finished: foreachPartition at streamingProcess.scala:50, took 0.020058 s
2017-08-10 09:14:16,943 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327175000 ms.0 from job set of time 1502327175000 ms
2017-08-10 09:14:16,943 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 481.943 s for time 1502327175000 ms (execution: 0.043 s)
2017-08-10 09:14:16,943 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 103 from persistence list
2017-08-10 09:14:16,943 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327180000 ms.0 from job set of time 1502327180000 ms
2017-08-10 09:14:16,944 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 102 from persistence list
2017-08-10 09:14:16,944 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 103
2017-08-10 09:14:16,944 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,944 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 102
2017-08-10 09:14:16,944 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327165000 ms
2017-08-10 09:14:16,955 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,955 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 53 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,955 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 53 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,955 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,955 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,956 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 53 (MapPartitionsRDD[107] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,958 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_53 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:16,962 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_53_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:16,962 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_53_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,962 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 53 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 53 (MapPartitionsRDD[107] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 53.0 with 2 tasks
2017-08-10 09:14:16,964 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 53.0 (TID 106, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,964 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 53.0 (TID 107, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,964 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 53.0 (TID 106)
2017-08-10 09:14:16,964 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 53.0 (TID 107)
2017-08-10 09:14:16,966 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,966 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,969 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 53.0 (TID 107). 714 bytes result sent to driver
2017-08-10 09:14:16,969 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 53.0 (TID 106). 714 bytes result sent to driver
2017-08-10 09:14:16,971 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 53.0 (TID 107) in 6 ms on localhost (1/2)
2017-08-10 09:14:16,971 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 53.0 (TID 106) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,971 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 53.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,971 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 53 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,971 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 53 finished: foreachPartition at streamingProcess.scala:50, took 0.016374 s
2017-08-10 09:14:16,972 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327180000 ms.0 from job set of time 1502327180000 ms
2017-08-10 09:14:16,972 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 476.972 s for time 1502327180000 ms (execution: 0.029 s)
2017-08-10 09:14:16,972 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 105 from persistence list
2017-08-10 09:14:16,972 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327185000 ms.0 from job set of time 1502327185000 ms
2017-08-10 09:14:16,972 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 105
2017-08-10 09:14:16,972 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 104 from persistence list
2017-08-10 09:14:16,972 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 104
2017-08-10 09:14:16,972 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,972 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327170000 ms
2017-08-10 09:14:16,983 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,983 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 54 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,983 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 54 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,983 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,983 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,983 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 54 (MapPartitionsRDD[109] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,986 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_54 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:16,989 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_54_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:16,990 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_54_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,990 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 54 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,990 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 54 (MapPartitionsRDD[109] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,990 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 54.0 with 2 tasks
2017-08-10 09:14:16,991 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 54.0 (TID 108, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,992 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 54.0 (TID 109, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,992 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 54.0 (TID 108)
2017-08-10 09:14:16,992 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 54.0 (TID 109)
2017-08-10 09:14:16,994 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,994 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,996 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 54.0 (TID 109). 714 bytes result sent to driver
2017-08-10 09:14:16,996 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 54.0 (TID 108). 714 bytes result sent to driver
2017-08-10 09:14:16,998 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 54.0 (TID 108) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,998 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 54.0 (TID 109) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,998 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 54.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,998 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 54 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,999 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 54 finished: foreachPartition at streamingProcess.scala:50, took 0.015782 s
2017-08-10 09:14:16,999 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327185000 ms.0 from job set of time 1502327185000 ms
2017-08-10 09:14:16,999 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 471.999 s for time 1502327185000 ms (execution: 0.027 s)
2017-08-10 09:14:16,999 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 107 from persistence list
2017-08-10 09:14:16,999 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327190000 ms.0 from job set of time 1502327190000 ms
2017-08-10 09:14:16,999 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 107
2017-08-10 09:14:16,999 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 106 from persistence list
2017-08-10 09:14:17,000 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 106
2017-08-10 09:14:17,000 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,000 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327175000 ms
2017-08-10 09:14:17,010 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,011 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 55 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,011 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 55 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,011 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,011 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,011 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 55 (MapPartitionsRDD[111] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,014 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_55 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:17,016 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_55_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:17,017 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_55_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 55 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 55 (MapPartitionsRDD[111] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 55.0 with 2 tasks
2017-08-10 09:14:17,019 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 55.0 (TID 110, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,019 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 55.0 (TID 111, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,019 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 55.0 (TID 110)
2017-08-10 09:14:17,019 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 55.0 (TID 111)
2017-08-10 09:14:17,021 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,021 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,024 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 55.0 (TID 111). 714 bytes result sent to driver
2017-08-10 09:14:17,024 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 55.0 (TID 110). 714 bytes result sent to driver
2017-08-10 09:14:17,025 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 55.0 (TID 111) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,026 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 55.0 (TID 110) in 8 ms on localhost (2/2)
2017-08-10 09:14:17,026 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 55.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 55 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:17,026 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 55 finished: foreachPartition at streamingProcess.scala:50, took 0.015448 s
2017-08-10 09:14:17,027 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327190000 ms.0 from job set of time 1502327190000 ms
2017-08-10 09:14:17,027 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 467.026 s for time 1502327190000 ms (execution: 0.027 s)
2017-08-10 09:14:17,027 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 109 from persistence list
2017-08-10 09:14:17,027 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327195000 ms.0 from job set of time 1502327195000 ms
2017-08-10 09:14:17,027 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 109
2017-08-10 09:14:17,027 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 108 from persistence list
2017-08-10 09:14:17,027 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 108
2017-08-10 09:14:17,027 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,027 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327180000 ms
2017-08-10 09:14:17,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 56 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 56 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 56 (MapPartitionsRDD[113] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_56 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:17,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_56_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:17,044 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_56_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 56 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 56 (MapPartitionsRDD[113] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 56.0 with 2 tasks
2017-08-10 09:14:17,046 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 56.0 (TID 112, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,046 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 56.0 (TID 113, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,046 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 56.0 (TID 112)
2017-08-10 09:14:17,046 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 56.0 (TID 113)
2017-08-10 09:14:17,048 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,048 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,051 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 56.0 (TID 112). 714 bytes result sent to driver
2017-08-10 09:14:17,051 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 56.0 (TID 113). 714 bytes result sent to driver
2017-08-10 09:14:17,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 56.0 (TID 112) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,052 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 56.0 (TID 113) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,053 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 56.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 56 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:17,053 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 56 finished: foreachPartition at streamingProcess.scala:50, took 0.015215 s
2017-08-10 09:14:17,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327195000 ms.0 from job set of time 1502327195000 ms
2017-08-10 09:14:17,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 462.053 s for time 1502327195000 ms (execution: 0.026 s)
2017-08-10 09:14:17,053 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 111 from persistence list
2017-08-10 09:14:17,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327200000 ms.0 from job set of time 1502327200000 ms
2017-08-10 09:14:17,054 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 111
2017-08-10 09:14:17,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 110 from persistence list
2017-08-10 09:14:17,054 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 110
2017-08-10 09:14:17,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327185000 ms
2017-08-10 09:14:17,064 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 57 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 57 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 57 (MapPartitionsRDD[115] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_57 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:17,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_57_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:17,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_57_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 57 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 57 (MapPartitionsRDD[115] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 57.0 with 2 tasks
2017-08-10 09:14:17,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 57.0 (TID 114, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 57.0 (TID 115, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,072 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 57.0 (TID 114)
2017-08-10 09:14:17,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 57.0 (TID 115)
2017-08-10 09:14:17,073 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,074 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,076 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 57.0 (TID 115). 714 bytes result sent to driver
2017-08-10 09:14:17,076 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 57.0 (TID 114). 714 bytes result sent to driver
2017-08-10 09:14:17,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 57.0 (TID 115) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 57.0 (TID 114) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 57.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 57 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 57 finished: foreachPartition at streamingProcess.scala:50, took 0.014384 s
2017-08-10 09:14:17,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327200000 ms.0 from job set of time 1502327200000 ms
2017-08-10 09:14:17,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 457.079 s for time 1502327200000 ms (execution: 0.026 s)
2017-08-10 09:14:17,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 113 from persistence list
2017-08-10 09:14:17,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327205000 ms.0 from job set of time 1502327205000 ms
2017-08-10 09:14:17,079 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 113
2017-08-10 09:14:17,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 112 from persistence list
2017-08-10 09:14:17,079 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 112
2017-08-10 09:14:17,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327190000 ms
2017-08-10 09:14:17,089 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 58 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 58 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 58 (MapPartitionsRDD[117] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_58 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:17,095 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_58_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:17,096 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_58_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 58 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 58 (MapPartitionsRDD[117] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 58.0 with 2 tasks
2017-08-10 09:14:17,097 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 58.0 (TID 116, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,097 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 58.0 (TID 117, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,097 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 58.0 (TID 116)
2017-08-10 09:14:17,097 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 58.0 (TID 117)
2017-08-10 09:14:17,099 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,099 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,102 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 58.0 (TID 117). 714 bytes result sent to driver
2017-08-10 09:14:17,102 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 58.0 (TID 116). 714 bytes result sent to driver
2017-08-10 09:14:17,103 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 58.0 (TID 117) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,103 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 58.0 (TID 116) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,103 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 58.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,104 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 58 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,104 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 58 finished: foreachPartition at streamingProcess.scala:50, took 0.014781 s
2017-08-10 09:14:17,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327205000 ms.0 from job set of time 1502327205000 ms
2017-08-10 09:14:17,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 452.104 s for time 1502327205000 ms (execution: 0.025 s)
2017-08-10 09:14:17,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327210000 ms.0 from job set of time 1502327210000 ms
2017-08-10 09:14:17,105 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 115 from persistence list
2017-08-10 09:14:17,105 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 115
2017-08-10 09:14:17,106 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 114 from persistence list
2017-08-10 09:14:17,106 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 114
2017-08-10 09:14:17,106 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,106 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327195000 ms
2017-08-10 09:14:17,114 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 59 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 59 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 59 (MapPartitionsRDD[119] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_59 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:17,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_59_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:17,120 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_59_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,121 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 59 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,121 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 59 (MapPartitionsRDD[119] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,121 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 59.0 with 2 tasks
2017-08-10 09:14:17,122 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 59.0 (TID 118, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,123 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 59.0 (TID 119, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,123 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 59.0 (TID 118)
2017-08-10 09:14:17,123 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 59.0 (TID 119)
2017-08-10 09:14:17,125 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,125 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,127 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 59.0 (TID 119). 714 bytes result sent to driver
2017-08-10 09:14:17,127 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 59.0 (TID 118). 714 bytes result sent to driver
2017-08-10 09:14:17,129 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 59.0 (TID 119) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,129 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 59.0 (TID 118) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,129 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 59.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,130 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 59 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,130 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 59 finished: foreachPartition at streamingProcess.scala:50, took 0.015477 s
2017-08-10 09:14:17,130 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327210000 ms.0 from job set of time 1502327210000 ms
2017-08-10 09:14:17,131 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 117 from persistence list
2017-08-10 09:14:17,131 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 447.130 s for time 1502327210000 ms (execution: 0.026 s)
2017-08-10 09:14:17,131 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327215000 ms.0 from job set of time 1502327215000 ms
2017-08-10 09:14:17,131 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 117
2017-08-10 09:14:17,131 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 116 from persistence list
2017-08-10 09:14:17,131 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 116
2017-08-10 09:14:17,131 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,131 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327200000 ms
2017-08-10 09:14:17,141 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,141 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 60 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,141 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 60 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,142 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,142 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,142 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 60 (MapPartitionsRDD[121] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,144 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_60 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:17,147 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_60_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:17,148 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_60_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,148 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 60 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,148 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 60 (MapPartitionsRDD[121] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,148 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 60.0 with 2 tasks
2017-08-10 09:14:17,149 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 60.0 (TID 120, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,150 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 60.0 (TID 121, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,150 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 60.0 (TID 121)
2017-08-10 09:14:17,150 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 60.0 (TID 120)
2017-08-10 09:14:17,152 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,152 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,155 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 60.0 (TID 120). 714 bytes result sent to driver
2017-08-10 09:14:17,155 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 60.0 (TID 121). 714 bytes result sent to driver
2017-08-10 09:14:17,157 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 60.0 (TID 120) in 9 ms on localhost (1/2)
2017-08-10 09:14:17,157 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 60.0 (TID 121) in 8 ms on localhost (2/2)
2017-08-10 09:14:17,157 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 60.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 60 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:17,158 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 60 finished: foreachPartition at streamingProcess.scala:50, took 0.016413 s
2017-08-10 09:14:17,158 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327215000 ms.0 from job set of time 1502327215000 ms
2017-08-10 09:14:17,158 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 442.158 s for time 1502327215000 ms (execution: 0.027 s)
2017-08-10 09:14:17,158 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327220000 ms.0 from job set of time 1502327220000 ms
2017-08-10 09:14:17,158 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 119 from persistence list
2017-08-10 09:14:17,158 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 119
2017-08-10 09:14:17,158 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 118 from persistence list
2017-08-10 09:14:17,159 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 118
2017-08-10 09:14:17,159 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,159 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327205000 ms
2017-08-10 09:14:17,169 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 61 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 61 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 61 (MapPartitionsRDD[123] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,173 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_61 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:17,176 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_61_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:17,177 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_61_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,177 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 61 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,177 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 61 (MapPartitionsRDD[123] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,177 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 61.0 with 2 tasks
2017-08-10 09:14:17,178 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 61.0 (TID 122, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,178 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 61.0 (TID 123, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,179 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 61.0 (TID 122)
2017-08-10 09:14:17,179 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 61.0 (TID 123)
2017-08-10 09:14:17,180 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,180 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,183 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 61.0 (TID 123). 714 bytes result sent to driver
2017-08-10 09:14:17,183 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 61.0 (TID 122). 714 bytes result sent to driver
2017-08-10 09:14:17,185 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 61.0 (TID 123) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,185 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 61.0 (TID 122) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,185 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 61.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,186 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 61 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,186 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 61 finished: foreachPartition at streamingProcess.scala:50, took 0.016108 s
2017-08-10 09:14:17,186 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327220000 ms.0 from job set of time 1502327220000 ms
2017-08-10 09:14:17,186 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 437.186 s for time 1502327220000 ms (execution: 0.028 s)
2017-08-10 09:14:17,186 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 121 from persistence list
2017-08-10 09:14:17,186 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327225000 ms.0 from job set of time 1502327225000 ms
2017-08-10 09:14:17,187 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 120 from persistence list
2017-08-10 09:14:17,187 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,187 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 121
2017-08-10 09:14:17,187 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 120
2017-08-10 09:14:17,187 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327210000 ms
2017-08-10 09:14:17,197 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 62 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 62 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 62 (MapPartitionsRDD[125] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,201 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_62 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:17,204 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_62_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:17,206 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_62_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,206 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 62 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,206 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 62 (MapPartitionsRDD[125] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,206 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 62.0 with 2 tasks
2017-08-10 09:14:17,207 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 62.0 (TID 124, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,208 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 62.0 (TID 125, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,208 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 62.0 (TID 125)
2017-08-10 09:14:17,208 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 62.0 (TID 124)
2017-08-10 09:14:17,210 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,210 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,213 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 62.0 (TID 124). 714 bytes result sent to driver
2017-08-10 09:14:17,213 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 62.0 (TID 125). 714 bytes result sent to driver
2017-08-10 09:14:17,215 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 62.0 (TID 124) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,215 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 62.0 (TID 125) in 8 ms on localhost (2/2)
2017-08-10 09:14:17,215 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 62.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,215 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 62 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:17,215 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 62 finished: foreachPartition at streamingProcess.scala:50, took 0.017714 s
2017-08-10 09:14:17,215 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327225000 ms.0 from job set of time 1502327225000 ms
2017-08-10 09:14:17,216 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 432.215 s for time 1502327225000 ms (execution: 0.029 s)
2017-08-10 09:14:17,216 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327230000 ms.0 from job set of time 1502327230000 ms
2017-08-10 09:14:17,216 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 123 from persistence list
2017-08-10 09:14:17,216 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 123
2017-08-10 09:14:17,216 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 122 from persistence list
2017-08-10 09:14:17,216 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 122
2017-08-10 09:14:17,216 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,216 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327215000 ms
2017-08-10 09:14:17,226 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 63 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 63 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 63 (MapPartitionsRDD[127] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,230 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_63 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:17,232 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_63_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:17,232 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_63_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,233 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 63 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,233 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 63 (MapPartitionsRDD[127] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,233 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 63.0 with 2 tasks
2017-08-10 09:14:17,233 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 63.0 (TID 126, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,234 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 63.0 (TID 127, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,234 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 63.0 (TID 126)
2017-08-10 09:14:17,234 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 63.0 (TID 127)
2017-08-10 09:14:17,235 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,235 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,238 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 63.0 (TID 126). 714 bytes result sent to driver
2017-08-10 09:14:17,238 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 63.0 (TID 127). 714 bytes result sent to driver
2017-08-10 09:14:17,239 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 63.0 (TID 127) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,239 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 63.0 (TID 126) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,240 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 63.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,240 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 63 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,240 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 63 finished: foreachPartition at streamingProcess.scala:50, took 0.013917 s
2017-08-10 09:14:17,240 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327230000 ms.0 from job set of time 1502327230000 ms
2017-08-10 09:14:17,240 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 427.240 s for time 1502327230000 ms (execution: 0.024 s)
2017-08-10 09:14:17,240 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 125 from persistence list
2017-08-10 09:14:17,240 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327235000 ms.0 from job set of time 1502327235000 ms
2017-08-10 09:14:17,240 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 125
2017-08-10 09:14:17,241 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 124 from persistence list
2017-08-10 09:14:17,241 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,241 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327220000 ms
2017-08-10 09:14:17,241 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 124
2017-08-10 09:14:17,250 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,251 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 64 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,251 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 64 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,251 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,251 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,251 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 64 (MapPartitionsRDD[129] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,254 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_64 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:17,257 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_64_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:17,257 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_64_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,258 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 64 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,258 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 64 (MapPartitionsRDD[129] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,258 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 64.0 with 2 tasks
2017-08-10 09:14:17,258 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 64.0 (TID 128, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,259 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 64.0 (TID 129, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,259 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 64.0 (TID 128)
2017-08-10 09:14:17,259 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 64.0 (TID 129)
2017-08-10 09:14:17,261 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,261 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,263 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 64.0 (TID 129). 714 bytes result sent to driver
2017-08-10 09:14:17,263 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 64.0 (TID 128). 714 bytes result sent to driver
2017-08-10 09:14:17,264 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 64.0 (TID 129) in 5 ms on localhost (1/2)
2017-08-10 09:14:17,264 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 64.0 (TID 128) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,264 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 64.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 64 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,265 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 64 finished: foreachPartition at streamingProcess.scala:50, took 0.014364 s
2017-08-10 09:14:17,265 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327235000 ms.0 from job set of time 1502327235000 ms
2017-08-10 09:14:17,265 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 422.265 s for time 1502327235000 ms (execution: 0.025 s)
2017-08-10 09:14:17,265 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327240000 ms.0 from job set of time 1502327240000 ms
2017-08-10 09:14:17,266 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 127 from persistence list
2017-08-10 09:14:17,266 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 127
2017-08-10 09:14:17,266 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 126 from persistence list
2017-08-10 09:14:17,266 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 126
2017-08-10 09:14:17,266 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,266 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327225000 ms
2017-08-10 09:14:17,275 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 65 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 65 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 65 (MapPartitionsRDD[131] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,279 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_65 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:17,281 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_65_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:17,281 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_65_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,282 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 65 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,282 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 65 (MapPartitionsRDD[131] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,282 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 65.0 with 2 tasks
2017-08-10 09:14:17,282 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 65.0 (TID 130, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,283 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 65.0 (TID 131, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,283 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 65.0 (TID 130)
2017-08-10 09:14:17,283 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 65.0 (TID 131)
2017-08-10 09:14:17,284 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,284 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,287 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 65.0 (TID 130). 714 bytes result sent to driver
2017-08-10 09:14:17,288 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 65.0 (TID 131). 714 bytes result sent to driver
2017-08-10 09:14:17,289 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 65.0 (TID 131) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,289 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 65.0 (TID 130) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,289 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 65.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,289 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 65 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,290 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 65 finished: foreachPartition at streamingProcess.scala:50, took 0.014262 s
2017-08-10 09:14:17,290 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327240000 ms.0 from job set of time 1502327240000 ms
2017-08-10 09:14:17,290 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 417.290 s for time 1502327240000 ms (execution: 0.025 s)
2017-08-10 09:14:17,290 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327245000 ms.0 from job set of time 1502327245000 ms
2017-08-10 09:14:17,290 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 129 from persistence list
2017-08-10 09:14:17,290 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 129
2017-08-10 09:14:17,290 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 128 from persistence list
2017-08-10 09:14:17,291 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 128
2017-08-10 09:14:17,291 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,291 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327230000 ms
2017-08-10 09:14:17,300 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 66 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 66 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 66 (MapPartitionsRDD[133] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,303 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_66 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:17,305 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_66_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:17,305 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_66_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 66 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 66 (MapPartitionsRDD[133] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 66.0 with 2 tasks
2017-08-10 09:14:17,306 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 66.0 (TID 132, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,307 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 66.0 (TID 133, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,307 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 66.0 (TID 133)
2017-08-10 09:14:17,307 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 66.0 (TID 132)
2017-08-10 09:14:17,309 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,309 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,311 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 66.0 (TID 133). 714 bytes result sent to driver
2017-08-10 09:14:17,311 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 66.0 (TID 132). 714 bytes result sent to driver
2017-08-10 09:14:17,313 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 66.0 (TID 133) in 5 ms on localhost (1/2)
2017-08-10 09:14:17,313 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 66.0 (TID 132) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,313 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 66.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 66 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,313 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 66 finished: foreachPartition at streamingProcess.scala:50, took 0.013281 s
2017-08-10 09:14:17,313 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327245000 ms.0 from job set of time 1502327245000 ms
2017-08-10 09:14:17,314 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 412.313 s for time 1502327245000 ms (execution: 0.023 s)
2017-08-10 09:14:17,314 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 131 from persistence list
2017-08-10 09:14:17,314 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327250000 ms.0 from job set of time 1502327250000 ms
2017-08-10 09:14:17,314 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 131
2017-08-10 09:14:17,314 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 130 from persistence list
2017-08-10 09:14:17,314 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 130
2017-08-10 09:14:17,314 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,314 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327235000 ms
2017-08-10 09:14:17,323 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,324 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 67 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,324 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 67 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,324 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,324 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,324 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 67 (MapPartitionsRDD[135] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,327 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_67 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:17,329 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_67_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:17,330 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_67_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,330 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 67 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,330 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 67 (MapPartitionsRDD[135] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,331 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 67.0 with 2 tasks
2017-08-10 09:14:17,331 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 67.0 (TID 134, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,331 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 67.0 (TID 135, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,332 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 67.0 (TID 134)
2017-08-10 09:14:17,332 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 67.0 (TID 135)
2017-08-10 09:14:17,333 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,333 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,336 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 67.0 (TID 134). 714 bytes result sent to driver
2017-08-10 09:14:17,336 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 67.0 (TID 135). 714 bytes result sent to driver
2017-08-10 09:14:17,344 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_52_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,344 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 67.0 (TID 134) in 13 ms on localhost (1/2)
2017-08-10 09:14:17,344 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 67.0 (TID 135) in 13 ms on localhost (2/2)
2017-08-10 09:14:17,344 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 67.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,345 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 67 (foreachPartition at streamingProcess.scala:50) finished in 0.014 s
2017-08-10 09:14:17,345 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 67 finished: foreachPartition at streamingProcess.scala:50, took 0.021404 s
2017-08-10 09:14:17,345 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_53_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,345 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327250000 ms.0 from job set of time 1502327250000 ms
2017-08-10 09:14:17,345 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 407.345 s for time 1502327250000 ms (execution: 0.031 s)
2017-08-10 09:14:17,345 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327255000 ms.0 from job set of time 1502327255000 ms
2017-08-10 09:14:17,345 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 133 from persistence list
2017-08-10 09:14:17,346 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 133
2017-08-10 09:14:17,346 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 132 from persistence list
2017-08-10 09:14:17,346 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 132
2017-08-10 09:14:17,346 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_54_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,346 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,346 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327240000 ms
2017-08-10 09:14:17,347 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_55_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,348 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_56_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,349 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_57_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,350 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_58_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,351 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_59_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,352 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_60_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,353 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_61_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,354 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_62_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,355 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_63_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,356 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_64_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,357 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_65_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:17,357 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_66_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:17,358 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,358 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 68 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,358 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 68 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,358 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,358 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,358 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 68 (MapPartitionsRDD[137] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,361 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_68 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:17,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_68_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:17,364 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_68_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:17,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 68 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 68 (MapPartitionsRDD[137] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 68.0 with 2 tasks
2017-08-10 09:14:17,365 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 68.0 (TID 136, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,365 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 68.0 (TID 137, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,365 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 68.0 (TID 137)
2017-08-10 09:14:17,365 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 68.0 (TID 136)
2017-08-10 09:14:17,367 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,367 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,370 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 68.0 (TID 136). 714 bytes result sent to driver
2017-08-10 09:14:17,370 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 68.0 (TID 137). 714 bytes result sent to driver
2017-08-10 09:14:17,371 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 68.0 (TID 137) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,372 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 68.0 (TID 136) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,372 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 68 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:17,372 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 68.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,372 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 68 finished: foreachPartition at streamingProcess.scala:50, took 0.013912 s
2017-08-10 09:14:17,372 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327255000 ms.0 from job set of time 1502327255000 ms
2017-08-10 09:14:17,372 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 402.372 s for time 1502327255000 ms (execution: 0.027 s)
2017-08-10 09:14:17,372 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 135 from persistence list
2017-08-10 09:14:17,372 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327260000 ms.0 from job set of time 1502327260000 ms
2017-08-10 09:14:17,373 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 135
2017-08-10 09:14:17,373 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 134 from persistence list
2017-08-10 09:14:17,373 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 134
2017-08-10 09:14:17,373 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,373 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327245000 ms
2017-08-10 09:14:17,382 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,383 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 69 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,383 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 69 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,383 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,383 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,383 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 69 (MapPartitionsRDD[139] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_69 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:17,388 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_69_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:17,389 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_69_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,389 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 69 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,389 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 69 (MapPartitionsRDD[139] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,389 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 69.0 with 2 tasks
2017-08-10 09:14:17,390 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 69.0 (TID 138, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,390 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 69.0 (TID 139, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,390 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 69.0 (TID 138)
2017-08-10 09:14:17,390 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 69.0 (TID 139)
2017-08-10 09:14:17,392 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,392 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,394 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 69.0 (TID 139). 714 bytes result sent to driver
2017-08-10 09:14:17,394 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 69.0 (TID 138). 714 bytes result sent to driver
2017-08-10 09:14:17,396 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 69.0 (TID 139) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,396 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 69.0 (TID 138) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,396 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 69.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,396 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 69 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,396 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 69 finished: foreachPartition at streamingProcess.scala:50, took 0.013804 s
2017-08-10 09:14:17,397 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327260000 ms.0 from job set of time 1502327260000 ms
2017-08-10 09:14:17,397 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 397.397 s for time 1502327260000 ms (execution: 0.025 s)
2017-08-10 09:14:17,397 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 137 from persistence list
2017-08-10 09:14:17,397 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327265000 ms.0 from job set of time 1502327265000 ms
2017-08-10 09:14:17,397 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 137
2017-08-10 09:14:17,397 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 136 from persistence list
2017-08-10 09:14:17,397 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 136
2017-08-10 09:14:17,397 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,397 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327250000 ms
2017-08-10 09:14:17,407 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 70 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 70 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 70 (MapPartitionsRDD[141] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,410 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_70 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:17,413 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_70_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:17,415 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_70_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 70 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 70 (MapPartitionsRDD[141] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 70.0 with 2 tasks
2017-08-10 09:14:17,416 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 70.0 (TID 140, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,416 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 70.0 (TID 141, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,416 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 70.0 (TID 140)
2017-08-10 09:14:17,416 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 70.0 (TID 141)
2017-08-10 09:14:17,418 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,418 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,420 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 70.0 (TID 140). 714 bytes result sent to driver
2017-08-10 09:14:17,420 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 70.0 (TID 141). 714 bytes result sent to driver
2017-08-10 09:14:17,421 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 70.0 (TID 140) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,422 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 70.0 (TID 141) in 5 ms on localhost (2/2)
2017-08-10 09:14:17,422 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 70.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,422 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 70 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,422 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 70 finished: foreachPartition at streamingProcess.scala:50, took 0.015144 s
2017-08-10 09:14:17,422 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327265000 ms.0 from job set of time 1502327265000 ms
2017-08-10 09:14:17,422 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 392.422 s for time 1502327265000 ms (execution: 0.025 s)
2017-08-10 09:14:17,422 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 139 from persistence list
2017-08-10 09:14:17,422 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327270000 ms.0 from job set of time 1502327270000 ms
2017-08-10 09:14:17,423 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 139
2017-08-10 09:14:17,423 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 138 from persistence list
2017-08-10 09:14:17,423 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 138
2017-08-10 09:14:17,423 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,423 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327255000 ms
2017-08-10 09:14:17,432 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,432 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 71 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,432 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 71 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,432 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,432 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,432 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 71 (MapPartitionsRDD[143] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,435 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_71 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:17,437 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_71_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:17,438 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_71_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,438 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 71 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,438 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 71 (MapPartitionsRDD[143] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,438 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 71.0 with 2 tasks
2017-08-10 09:14:17,439 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 71.0 (TID 142, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,439 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 71.0 (TID 143, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,439 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 71.0 (TID 142)
2017-08-10 09:14:17,439 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 71.0 (TID 143)
2017-08-10 09:14:17,441 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,441 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,443 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 71.0 (TID 143). 714 bytes result sent to driver
2017-08-10 09:14:17,443 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 71.0 (TID 142). 714 bytes result sent to driver
2017-08-10 09:14:17,445 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 71.0 (TID 143) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,445 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 71.0 (TID 142) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,445 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 71.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,445 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 71 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,445 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 71 finished: foreachPartition at streamingProcess.scala:50, took 0.013298 s
2017-08-10 09:14:17,445 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327270000 ms.0 from job set of time 1502327270000 ms
2017-08-10 09:14:17,446 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 387.445 s for time 1502327270000 ms (execution: 0.023 s)
2017-08-10 09:14:17,446 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 141 from persistence list
2017-08-10 09:14:17,446 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327275000 ms.0 from job set of time 1502327275000 ms
2017-08-10 09:14:17,446 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 141
2017-08-10 09:14:17,446 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 140 from persistence list
2017-08-10 09:14:17,446 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 140
2017-08-10 09:14:17,446 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,446 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327260000 ms
2017-08-10 09:14:17,455 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 72 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 72 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 72 (MapPartitionsRDD[145] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,458 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_72 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:17,460 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_72_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:17,461 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_72_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 72 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 72 (MapPartitionsRDD[145] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 72.0 with 2 tasks
2017-08-10 09:14:17,462 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 72.0 (TID 144, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,462 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 72.0 (TID 145, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,462 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 72.0 (TID 144)
2017-08-10 09:14:17,462 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 72.0 (TID 145)
2017-08-10 09:14:17,464 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,464 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,467 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 72.0 (TID 144). 714 bytes result sent to driver
2017-08-10 09:14:17,467 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 72.0 (TID 145). 714 bytes result sent to driver
2017-08-10 09:14:17,468 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 72.0 (TID 144) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,468 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 72.0 (TID 145) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,468 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 72.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 72 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,468 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 72 finished: foreachPartition at streamingProcess.scala:50, took 0.012928 s
2017-08-10 09:14:17,469 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327275000 ms.0 from job set of time 1502327275000 ms
2017-08-10 09:14:17,469 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 382.469 s for time 1502327275000 ms (execution: 0.023 s)
2017-08-10 09:14:17,469 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 143 from persistence list
2017-08-10 09:14:17,469 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327280000 ms.0 from job set of time 1502327280000 ms
2017-08-10 09:14:17,469 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 143
2017-08-10 09:14:17,469 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 142 from persistence list
2017-08-10 09:14:17,469 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 142
2017-08-10 09:14:17,469 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,470 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327265000 ms
2017-08-10 09:14:17,479 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,480 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 73 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,480 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 73 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,480 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,480 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,480 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 73 (MapPartitionsRDD[147] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,482 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_73 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:17,485 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_73_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:17,485 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_73_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,485 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 73 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,485 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 73 (MapPartitionsRDD[147] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,485 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 73.0 with 2 tasks
2017-08-10 09:14:17,486 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 73.0 (TID 146, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,486 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 73.0 (TID 147, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,486 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 73.0 (TID 146)
2017-08-10 09:14:17,486 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 73.0 (TID 147)
2017-08-10 09:14:17,488 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,488 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,490 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 73.0 (TID 146). 714 bytes result sent to driver
2017-08-10 09:14:17,491 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 73.0 (TID 147). 714 bytes result sent to driver
2017-08-10 09:14:17,491 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 73.0 (TID 146) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,492 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 73.0 (TID 147) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,492 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 73.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,492 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 73 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,492 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 73 finished: foreachPartition at streamingProcess.scala:50, took 0.012635 s
2017-08-10 09:14:17,492 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327280000 ms.0 from job set of time 1502327280000 ms
2017-08-10 09:14:17,492 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 377.492 s for time 1502327280000 ms (execution: 0.023 s)
2017-08-10 09:14:17,493 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327285000 ms.0 from job set of time 1502327285000 ms
2017-08-10 09:14:17,493 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 145 from persistence list
2017-08-10 09:14:17,493 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 145
2017-08-10 09:14:17,493 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 144 from persistence list
2017-08-10 09:14:17,493 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 144
2017-08-10 09:14:17,493 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,493 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327270000 ms
2017-08-10 09:14:17,504 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,504 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 74 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,505 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 74 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,505 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,505 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,505 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 74 (MapPartitionsRDD[149] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,507 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_74 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:17,511 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_74_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:17,511 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_74_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,511 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 74 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,511 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 74 (MapPartitionsRDD[149] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,511 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 74.0 with 2 tasks
2017-08-10 09:14:17,512 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 74.0 (TID 148, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,512 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 74.0 (TID 149, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,512 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 74.0 (TID 148)
2017-08-10 09:14:17,512 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 74.0 (TID 149)
2017-08-10 09:14:17,514 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,514 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,516 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 74.0 (TID 148). 714 bytes result sent to driver
2017-08-10 09:14:17,516 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 74.0 (TID 149). 714 bytes result sent to driver
2017-08-10 09:14:17,518 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 74.0 (TID 149) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,518 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 74.0 (TID 148) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,518 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 74.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 74 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:17,518 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 74 finished: foreachPartition at streamingProcess.scala:50, took 0.014125 s
2017-08-10 09:14:17,519 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327285000 ms.0 from job set of time 1502327285000 ms
2017-08-10 09:14:17,519 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 372.519 s for time 1502327285000 ms (execution: 0.026 s)
2017-08-10 09:14:17,519 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327290000 ms.0 from job set of time 1502327290000 ms
2017-08-10 09:14:17,519 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 147 from persistence list
2017-08-10 09:14:17,519 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 147
2017-08-10 09:14:17,519 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 146 from persistence list
2017-08-10 09:14:17,519 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 146
2017-08-10 09:14:17,519 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,520 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327275000 ms
2017-08-10 09:14:17,531 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,532 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 75 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,532 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 75 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,532 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,532 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,532 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 75 (MapPartitionsRDD[151] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,535 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_75 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:17,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_75_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:17,538 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_75_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,538 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 75 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,538 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 75 (MapPartitionsRDD[151] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,538 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 75.0 with 2 tasks
2017-08-10 09:14:17,539 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 75.0 (TID 150, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,539 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 75.0 (TID 151, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,539 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 75.0 (TID 151)
2017-08-10 09:14:17,539 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 75.0 (TID 150)
2017-08-10 09:14:17,541 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,541 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,543 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 75.0 (TID 151). 714 bytes result sent to driver
2017-08-10 09:14:17,543 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 75.0 (TID 150). 714 bytes result sent to driver
2017-08-10 09:14:17,545 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 75.0 (TID 150) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,545 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 75.0 (TID 151) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,545 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 75.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,545 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 75 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,545 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 75 finished: foreachPartition at streamingProcess.scala:50, took 0.013676 s
2017-08-10 09:14:17,545 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327290000 ms.0 from job set of time 1502327290000 ms
2017-08-10 09:14:17,545 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 367.545 s for time 1502327290000 ms (execution: 0.026 s)
2017-08-10 09:14:17,545 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 149 from persistence list
2017-08-10 09:14:17,546 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327295000 ms.0 from job set of time 1502327295000 ms
2017-08-10 09:14:17,546 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 149
2017-08-10 09:14:17,546 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 148 from persistence list
2017-08-10 09:14:17,546 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 148
2017-08-10 09:14:17,546 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,546 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327280000 ms
2017-08-10 09:14:17,557 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,558 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 76 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,558 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 76 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,558 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,558 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,558 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 76 (MapPartitionsRDD[153] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,561 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_76 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:17,565 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_76_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:17,565 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_76_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,565 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 76 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 76 (MapPartitionsRDD[153] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 76.0 with 2 tasks
2017-08-10 09:14:17,566 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 76.0 (TID 152, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,567 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 76.0 (TID 153, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,567 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 76.0 (TID 152)
2017-08-10 09:14:17,567 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 76.0 (TID 153)
2017-08-10 09:14:17,569 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,569 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,572 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 76.0 (TID 153). 714 bytes result sent to driver
2017-08-10 09:14:17,572 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 76.0 (TID 152). 714 bytes result sent to driver
2017-08-10 09:14:17,573 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 76.0 (TID 153) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,574 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 76.0 (TID 152) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,574 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 76.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,574 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 76 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:17,574 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 76 finished: foreachPartition at streamingProcess.scala:50, took 0.016331 s
2017-08-10 09:14:17,574 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327295000 ms.0 from job set of time 1502327295000 ms
2017-08-10 09:14:17,574 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 362.574 s for time 1502327295000 ms (execution: 0.028 s)
2017-08-10 09:14:17,574 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 151 from persistence list
2017-08-10 09:14:17,574 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327300000 ms.0 from job set of time 1502327300000 ms
2017-08-10 09:14:17,575 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 151
2017-08-10 09:14:17,575 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 150 from persistence list
2017-08-10 09:14:17,575 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 150
2017-08-10 09:14:17,575 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,575 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327285000 ms
2017-08-10 09:14:17,585 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,586 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 77 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,586 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 77 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,586 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,586 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,586 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 77 (MapPartitionsRDD[155] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,588 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_77 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:17,591 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_77_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:17,591 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_77_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,591 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 77 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,591 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 77 (MapPartitionsRDD[155] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,591 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 77.0 with 2 tasks
2017-08-10 09:14:17,592 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 77.0 (TID 154, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,592 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 77.0 (TID 155, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,592 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 77.0 (TID 154)
2017-08-10 09:14:17,592 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 77.0 (TID 155)
2017-08-10 09:14:17,594 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,594 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,597 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 77.0 (TID 155). 714 bytes result sent to driver
2017-08-10 09:14:17,597 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 77.0 (TID 154). 714 bytes result sent to driver
2017-08-10 09:14:17,598 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 77.0 (TID 155) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,598 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 77.0 (TID 154) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,598 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 77.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,598 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 77 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:17,598 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 77 finished: foreachPartition at streamingProcess.scala:50, took 0.013027 s
2017-08-10 09:14:17,599 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327300000 ms.0 from job set of time 1502327300000 ms
2017-08-10 09:14:17,599 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 357.599 s for time 1502327300000 ms (execution: 0.025 s)
2017-08-10 09:14:17,599 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 153 from persistence list
2017-08-10 09:14:17,599 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327305000 ms.0 from job set of time 1502327305000 ms
2017-08-10 09:14:17,599 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 153
2017-08-10 09:14:17,599 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 152 from persistence list
2017-08-10 09:14:17,599 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 152
2017-08-10 09:14:17,599 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,599 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327290000 ms
2017-08-10 09:14:17,609 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,609 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 78 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,609 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 78 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,609 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,610 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,610 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 78 (MapPartitionsRDD[157] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,612 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_78 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:17,614 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_78_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:17,614 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_78_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,615 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 78 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,615 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 78 (MapPartitionsRDD[157] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,615 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 78.0 with 2 tasks
2017-08-10 09:14:17,616 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 78.0 (TID 156, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,616 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 78.0 (TID 157, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,616 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 78.0 (TID 156)
2017-08-10 09:14:17,616 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 78.0 (TID 157)
2017-08-10 09:14:17,618 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,618 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,620 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 78.0 (TID 156). 714 bytes result sent to driver
2017-08-10 09:14:17,620 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 78.0 (TID 157). 714 bytes result sent to driver
2017-08-10 09:14:17,621 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 78.0 (TID 156) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,622 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 78.0 (TID 157) in 5 ms on localhost (2/2)
2017-08-10 09:14:17,622 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 78.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,622 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 78 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,622 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 78 finished: foreachPartition at streamingProcess.scala:50, took 0.012678 s
2017-08-10 09:14:17,622 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327305000 ms.0 from job set of time 1502327305000 ms
2017-08-10 09:14:17,622 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 352.622 s for time 1502327305000 ms (execution: 0.023 s)
2017-08-10 09:14:17,622 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 155 from persistence list
2017-08-10 09:14:17,622 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327310000 ms.0 from job set of time 1502327310000 ms
2017-08-10 09:14:17,623 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 154 from persistence list
2017-08-10 09:14:17,623 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 155
2017-08-10 09:14:17,623 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 154
2017-08-10 09:14:17,623 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,623 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327295000 ms
2017-08-10 09:14:17,632 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,632 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 79 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,632 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 79 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,632 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,632 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,632 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 79 (MapPartitionsRDD[159] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,635 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_79 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:17,637 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_79_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:17,638 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_79_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,638 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 79 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,638 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 79 (MapPartitionsRDD[159] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,638 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 79.0 with 2 tasks
2017-08-10 09:14:17,639 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 79.0 (TID 158, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,639 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 79.0 (TID 159, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,639 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 79.0 (TID 158)
2017-08-10 09:14:17,639 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 79.0 (TID 159)
2017-08-10 09:14:17,640 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,640 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,643 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 79.0 (TID 158). 714 bytes result sent to driver
2017-08-10 09:14:17,643 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 79.0 (TID 159). 714 bytes result sent to driver
2017-08-10 09:14:17,644 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 79.0 (TID 158) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,644 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 79.0 (TID 159) in 5 ms on localhost (2/2)
2017-08-10 09:14:17,644 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 79.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,645 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 79 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:17,645 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 79 finished: foreachPartition at streamingProcess.scala:50, took 0.012947 s
2017-08-10 09:14:17,645 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327310000 ms.0 from job set of time 1502327310000 ms
2017-08-10 09:14:17,645 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 347.645 s for time 1502327310000 ms (execution: 0.023 s)
2017-08-10 09:14:17,645 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 157 from persistence list
2017-08-10 09:14:17,645 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327315000 ms.0 from job set of time 1502327315000 ms
2017-08-10 09:14:17,645 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 157
2017-08-10 09:14:17,645 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 156 from persistence list
2017-08-10 09:14:17,646 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 156
2017-08-10 09:14:17,646 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,646 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327300000 ms
2017-08-10 09:14:17,655 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 80 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 80 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 80 (MapPartitionsRDD[161] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,658 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_80 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:17,660 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_80_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:17,661 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_80_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 80 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 80 (MapPartitionsRDD[161] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 80.0 with 2 tasks
2017-08-10 09:14:17,662 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 80.0 (TID 160, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,662 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 80.0 (TID 161, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,662 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 80.0 (TID 160)
2017-08-10 09:14:17,662 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 80.0 (TID 161)
2017-08-10 09:14:17,664 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,664 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,667 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 80.0 (TID 160). 714 bytes result sent to driver
2017-08-10 09:14:17,667 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 80.0 (TID 161). 714 bytes result sent to driver
2017-08-10 09:14:17,668 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 80.0 (TID 160) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,668 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 80.0 (TID 161) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,668 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 80.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 80 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,668 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 80 finished: foreachPartition at streamingProcess.scala:50, took 0.013292 s
2017-08-10 09:14:17,669 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327315000 ms.0 from job set of time 1502327315000 ms
2017-08-10 09:14:17,669 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 342.669 s for time 1502327315000 ms (execution: 0.024 s)
2017-08-10 09:14:17,669 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 159 from persistence list
2017-08-10 09:14:17,669 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327320000 ms.0 from job set of time 1502327320000 ms
2017-08-10 09:14:17,669 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 159
2017-08-10 09:14:17,669 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 158 from persistence list
2017-08-10 09:14:17,669 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 158
2017-08-10 09:14:17,669 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,669 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327305000 ms
2017-08-10 09:14:17,679 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 81 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 81 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,680 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 81 (MapPartitionsRDD[163] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,682 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_81 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:17,685 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_81_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:17,685 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_81_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 81 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 81 (MapPartitionsRDD[163] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 81.0 with 2 tasks
2017-08-10 09:14:17,686 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 81.0 (TID 162, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,687 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 81.0 (TID 163, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,687 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 81.0 (TID 162)
2017-08-10 09:14:17,687 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 81.0 (TID 163)
2017-08-10 09:14:17,688 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,688 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,691 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 81.0 (TID 162). 714 bytes result sent to driver
2017-08-10 09:14:17,691 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 81.0 (TID 163). 714 bytes result sent to driver
2017-08-10 09:14:17,692 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 81.0 (TID 162) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,692 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 81.0 (TID 163) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,692 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 81.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,692 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 81 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:17,692 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 81 finished: foreachPartition at streamingProcess.scala:50, took 0.013431 s
2017-08-10 09:14:17,693 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327320000 ms.0 from job set of time 1502327320000 ms
2017-08-10 09:14:17,693 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 337.693 s for time 1502327320000 ms (execution: 0.024 s)
2017-08-10 09:14:17,693 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327325000 ms.0 from job set of time 1502327325000 ms
2017-08-10 09:14:17,693 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 161 from persistence list
2017-08-10 09:14:17,693 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 161
2017-08-10 09:14:17,693 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 160 from persistence list
2017-08-10 09:14:17,694 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 160
2017-08-10 09:14:17,694 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,694 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327310000 ms
2017-08-10 09:14:17,703 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 82 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 82 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 82 (MapPartitionsRDD[165] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,706 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_82 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:17,708 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_82_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:17,708 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_82_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,708 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 82 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,708 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 82 (MapPartitionsRDD[165] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,709 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 82.0 with 2 tasks
2017-08-10 09:14:17,709 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 82.0 (TID 164, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,709 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 82.0 (TID 165, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,709 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 82.0 (TID 164)
2017-08-10 09:14:17,709 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 82.0 (TID 165)
2017-08-10 09:14:17,711 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,711 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,714 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 82.0 (TID 164). 714 bytes result sent to driver
2017-08-10 09:14:17,714 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 82.0 (TID 165). 714 bytes result sent to driver
2017-08-10 09:14:17,715 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 82.0 (TID 164) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,715 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 82.0 (TID 165) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,715 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 82.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,715 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 82 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:17,715 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 82 finished: foreachPartition at streamingProcess.scala:50, took 0.012116 s
2017-08-10 09:14:17,715 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327325000 ms.0 from job set of time 1502327325000 ms
2017-08-10 09:14:17,716 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 332.715 s for time 1502327325000 ms (execution: 0.022 s)
2017-08-10 09:14:17,716 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 163 from persistence list
2017-08-10 09:14:17,716 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327330000 ms.0 from job set of time 1502327330000 ms
2017-08-10 09:14:17,716 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 163
2017-08-10 09:14:17,716 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 162 from persistence list
2017-08-10 09:14:17,716 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 162
2017-08-10 09:14:17,716 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,716 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327315000 ms
2017-08-10 09:14:17,725 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 83 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 83 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 83 (MapPartitionsRDD[167] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,728 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_83 stored as values in memory (estimated size 34.5 KB, free 412.9 MB)
2017-08-10 09:14:17,740 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_83_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:17,741 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_83_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,741 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 83 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,741 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_67_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,742 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 83 (MapPartitionsRDD[167] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,742 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 83.0 with 2 tasks
2017-08-10 09:14:17,742 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_68_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,742 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 83.0 (TID 166, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,743 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 83.0 (TID 167, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,743 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 83.0 (TID 166)
2017-08-10 09:14:17,743 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 83.0 (TID 167)
2017-08-10 09:14:17,743 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_69_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,744 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_70_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,744 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_71_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,745 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,745 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,745 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_72_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,746 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_73_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,747 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_74_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,747 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 83.0 (TID 166). 714 bytes result sent to driver
2017-08-10 09:14:17,747 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 83.0 (TID 167). 714 bytes result sent to driver
2017-08-10 09:14:17,747 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_75_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,748 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_76_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,749 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 83.0 (TID 166) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,749 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 83.0 (TID 167) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,749 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 83.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,749 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 83 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,749 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_77_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,749 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 83 finished: foreachPartition at streamingProcess.scala:50, took 0.023934 s
2017-08-10 09:14:17,750 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327330000 ms.0 from job set of time 1502327330000 ms
2017-08-10 09:14:17,750 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 327.750 s for time 1502327330000 ms (execution: 0.034 s)
2017-08-10 09:14:17,750 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327335000 ms.0 from job set of time 1502327335000 ms
2017-08-10 09:14:17,750 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 165 from persistence list
2017-08-10 09:14:17,750 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 164 from persistence list
2017-08-10 09:14:17,750 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_78_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,750 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,750 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327320000 ms
2017-08-10 09:14:17,750 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 165
2017-08-10 09:14:17,750 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 164
2017-08-10 09:14:17,751 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_79_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,752 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_80_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,753 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_81_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:17,753 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_82_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:17,760 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,760 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 84 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,760 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 84 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,760 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,760 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,760 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 84 (MapPartitionsRDD[169] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,763 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_84 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:17,765 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_84_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:17,766 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_84_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:17,766 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 84 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,766 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 84 (MapPartitionsRDD[169] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,766 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 84.0 with 2 tasks
2017-08-10 09:14:17,767 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 84.0 (TID 168, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,767 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 84.0 (TID 169, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,767 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 84.0 (TID 168)
2017-08-10 09:14:17,768 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 84.0 (TID 169)
2017-08-10 09:14:17,769 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,769 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,771 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 84.0 (TID 169). 714 bytes result sent to driver
2017-08-10 09:14:17,771 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 84.0 (TID 168). 714 bytes result sent to driver
2017-08-10 09:14:17,773 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 84.0 (TID 169) in 5 ms on localhost (1/2)
2017-08-10 09:14:17,773 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 84.0 (TID 168) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,773 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 84.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,773 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 84 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,773 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 84 finished: foreachPartition at streamingProcess.scala:50, took 0.012974 s
2017-08-10 09:14:17,773 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327335000 ms.0 from job set of time 1502327335000 ms
2017-08-10 09:14:17,773 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 322.773 s for time 1502327335000 ms (execution: 0.023 s)
2017-08-10 09:14:17,773 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327340000 ms.0 from job set of time 1502327340000 ms
2017-08-10 09:14:17,774 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 167 from persistence list
2017-08-10 09:14:17,774 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 166 from persistence list
2017-08-10 09:14:17,774 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 166
2017-08-10 09:14:17,774 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 167
2017-08-10 09:14:17,774 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,774 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327325000 ms
2017-08-10 09:14:17,784 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,784 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 85 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,784 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 85 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,784 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,784 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,785 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 85 (MapPartitionsRDD[171] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,787 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_85 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:17,790 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_85_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:17,791 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_85_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,791 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 85 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,791 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 85 (MapPartitionsRDD[171] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,791 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 85.0 with 2 tasks
2017-08-10 09:14:17,792 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 85.0 (TID 170, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,792 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 85.0 (TID 171, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,792 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 85.0 (TID 171)
2017-08-10 09:14:17,792 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 85.0 (TID 170)
2017-08-10 09:14:17,794 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,794 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,796 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 85.0 (TID 170). 714 bytes result sent to driver
2017-08-10 09:14:17,796 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 85.0 (TID 171). 714 bytes result sent to driver
2017-08-10 09:14:17,798 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 85.0 (TID 170) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,799 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 85.0 (TID 171) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,799 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 85.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,799 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 85 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:17,799 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 85 finished: foreachPartition at streamingProcess.scala:50, took 0.015520 s
2017-08-10 09:14:17,800 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327340000 ms.0 from job set of time 1502327340000 ms
2017-08-10 09:14:17,800 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 169 from persistence list
2017-08-10 09:14:17,800 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 317.800 s for time 1502327340000 ms (execution: 0.027 s)
2017-08-10 09:14:17,800 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327345000 ms.0 from job set of time 1502327345000 ms
2017-08-10 09:14:17,800 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 169
2017-08-10 09:14:17,801 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 168 from persistence list
2017-08-10 09:14:17,801 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 168
2017-08-10 09:14:17,801 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,801 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327330000 ms
2017-08-10 09:14:17,810 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 86 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 86 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 86 (MapPartitionsRDD[173] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,813 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_86 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:17,815 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_86_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:17,815 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_86_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,816 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 86 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,816 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 86 (MapPartitionsRDD[173] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,816 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 86.0 with 2 tasks
2017-08-10 09:14:17,816 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 86.0 (TID 172, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,817 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 86.0 (TID 173, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,817 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 86.0 (TID 172)
2017-08-10 09:14:17,817 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 86.0 (TID 173)
2017-08-10 09:14:17,818 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,819 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,822 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 86.0 (TID 172). 714 bytes result sent to driver
2017-08-10 09:14:17,822 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 86.0 (TID 173). 714 bytes result sent to driver
2017-08-10 09:14:17,823 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 86.0 (TID 172) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,823 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 86.0 (TID 173) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,823 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 86.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,823 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 86 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,823 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 86 finished: foreachPartition at streamingProcess.scala:50, took 0.013426 s
2017-08-10 09:14:17,824 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327345000 ms.0 from job set of time 1502327345000 ms
2017-08-10 09:14:17,824 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 312.824 s for time 1502327345000 ms (execution: 0.024 s)
2017-08-10 09:14:17,824 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 171 from persistence list
2017-08-10 09:14:17,824 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327350000 ms.0 from job set of time 1502327350000 ms
2017-08-10 09:14:17,824 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 171
2017-08-10 09:14:17,824 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 170 from persistence list
2017-08-10 09:14:17,824 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 170
2017-08-10 09:14:17,824 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,825 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327335000 ms
2017-08-10 09:14:17,834 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,834 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 87 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,834 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 87 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,834 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,834 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,834 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 87 (MapPartitionsRDD[175] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,837 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_87 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:17,839 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_87_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:17,840 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_87_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,840 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 87 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,840 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 87 (MapPartitionsRDD[175] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,840 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 87.0 with 2 tasks
2017-08-10 09:14:17,841 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 87.0 (TID 174, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,841 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 87.0 (TID 175, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,841 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 87.0 (TID 174)
2017-08-10 09:14:17,841 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 87.0 (TID 175)
2017-08-10 09:14:17,843 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,843 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,846 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 87.0 (TID 175). 714 bytes result sent to driver
2017-08-10 09:14:17,846 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 87.0 (TID 174). 714 bytes result sent to driver
2017-08-10 09:14:17,847 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 87.0 (TID 175) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,847 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 87.0 (TID 174) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,847 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 87.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,847 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 87 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:17,848 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 87 finished: foreachPartition at streamingProcess.scala:50, took 0.013608 s
2017-08-10 09:14:17,848 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327350000 ms.0 from job set of time 1502327350000 ms
2017-08-10 09:14:17,848 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 307.848 s for time 1502327350000 ms (execution: 0.024 s)
2017-08-10 09:14:17,848 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 173 from persistence list
2017-08-10 09:14:17,848 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327355000 ms.0 from job set of time 1502327355000 ms
2017-08-10 09:14:17,848 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 173
2017-08-10 09:14:17,848 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 172 from persistence list
2017-08-10 09:14:17,849 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,849 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 172
2017-08-10 09:14:17,849 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327340000 ms
2017-08-10 09:14:17,858 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 88 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 88 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 88 (MapPartitionsRDD[177] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,861 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_88 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:17,863 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_88_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:17,863 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_88_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,864 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 88 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,864 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 88 (MapPartitionsRDD[177] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,864 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 88.0 with 2 tasks
2017-08-10 09:14:17,864 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 88.0 (TID 176, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,865 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 88.0 (TID 177, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,865 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 88.0 (TID 177)
2017-08-10 09:14:17,865 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 88.0 (TID 176)
2017-08-10 09:14:17,867 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,867 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,869 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 88.0 (TID 177). 714 bytes result sent to driver
2017-08-10 09:14:17,869 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 88.0 (TID 176). 714 bytes result sent to driver
2017-08-10 09:14:17,870 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 88.0 (TID 176) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,870 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 88.0 (TID 177) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,871 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 88.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,871 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 88 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,871 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 88 finished: foreachPartition at streamingProcess.scala:50, took 0.012867 s
2017-08-10 09:14:17,871 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327355000 ms.0 from job set of time 1502327355000 ms
2017-08-10 09:14:17,871 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 302.871 s for time 1502327355000 ms (execution: 0.023 s)
2017-08-10 09:14:17,871 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 175 from persistence list
2017-08-10 09:14:17,871 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327360000 ms.0 from job set of time 1502327360000 ms
2017-08-10 09:14:17,872 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 175
2017-08-10 09:14:17,872 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 174 from persistence list
2017-08-10 09:14:17,872 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 174
2017-08-10 09:14:17,872 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,872 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327345000 ms
2017-08-10 09:14:17,881 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 89 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 89 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,882 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 89 (MapPartitionsRDD[179] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,885 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_89 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:17,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_89_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:17,888 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_89_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,888 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 89 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,888 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 89 (MapPartitionsRDD[179] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,888 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 89.0 with 2 tasks
2017-08-10 09:14:17,889 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 89.0 (TID 178, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,890 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 89.0 (TID 179, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,890 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 89.0 (TID 178)
2017-08-10 09:14:17,890 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 89.0 (TID 179)
2017-08-10 09:14:17,892 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,892 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,895 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 89.0 (TID 179). 714 bytes result sent to driver
2017-08-10 09:14:17,895 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 89.0 (TID 178). 714 bytes result sent to driver
2017-08-10 09:14:17,896 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 89.0 (TID 179) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,896 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 89.0 (TID 178) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,896 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 89.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,896 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 89 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,896 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 89 finished: foreachPartition at streamingProcess.scala:50, took 0.015437 s
2017-08-10 09:14:17,897 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327360000 ms.0 from job set of time 1502327360000 ms
2017-08-10 09:14:17,897 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 297.897 s for time 1502327360000 ms (execution: 0.026 s)
2017-08-10 09:14:17,897 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327365000 ms.0 from job set of time 1502327365000 ms
2017-08-10 09:14:17,897 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 177 from persistence list
2017-08-10 09:14:17,898 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 177
2017-08-10 09:14:17,898 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 176 from persistence list
2017-08-10 09:14:17,898 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 176
2017-08-10 09:14:17,898 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,898 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327350000 ms
2017-08-10 09:14:17,908 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,908 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 90 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,908 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 90 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,908 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,908 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,908 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 90 (MapPartitionsRDD[181] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,911 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_90 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:17,914 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_90_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:17,915 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_90_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,915 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 90 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,915 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 90 (MapPartitionsRDD[181] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,915 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 90.0 with 2 tasks
2017-08-10 09:14:17,916 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 90.0 (TID 180, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,916 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 90.0 (TID 181, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,917 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 90.0 (TID 180)
2017-08-10 09:14:17,917 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 90.0 (TID 181)
2017-08-10 09:14:17,918 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,918 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,921 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 90.0 (TID 181). 714 bytes result sent to driver
2017-08-10 09:14:17,921 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 90.0 (TID 180). 714 bytes result sent to driver
2017-08-10 09:14:17,923 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 90.0 (TID 180) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,923 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 90.0 (TID 181) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,923 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 90.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,923 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 90 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,923 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 90 finished: foreachPartition at streamingProcess.scala:50, took 0.015514 s
2017-08-10 09:14:17,924 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327365000 ms.0 from job set of time 1502327365000 ms
2017-08-10 09:14:17,924 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 292.924 s for time 1502327365000 ms (execution: 0.027 s)
2017-08-10 09:14:17,924 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327370000 ms.0 from job set of time 1502327370000 ms
2017-08-10 09:14:17,924 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 179 from persistence list
2017-08-10 09:14:17,925 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 178 from persistence list
2017-08-10 09:14:17,925 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 178
2017-08-10 09:14:17,925 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 179
2017-08-10 09:14:17,925 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,925 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327355000 ms
2017-08-10 09:14:17,934 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,935 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 91 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,935 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 91 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,935 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,935 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,935 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 91 (MapPartitionsRDD[183] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,938 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_91 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:17,941 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_91_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:17,942 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_91_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,942 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 91 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,942 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 91 (MapPartitionsRDD[183] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,942 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 91.0 with 2 tasks
2017-08-10 09:14:17,943 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 91.0 (TID 182, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,943 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 91.0 (TID 183, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,943 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 91.0 (TID 182)
2017-08-10 09:14:17,945 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,945 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 91.0 (TID 183)
2017-08-10 09:14:17,947 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,949 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 91.0 (TID 183). 714 bytes result sent to driver
2017-08-10 09:14:17,949 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 91.0 (TID 182). 714 bytes result sent to driver
2017-08-10 09:14:17,951 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 91.0 (TID 183) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,951 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 91.0 (TID 182) in 9 ms on localhost (2/2)
2017-08-10 09:14:17,951 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 91.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,951 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 91 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:17,951 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 91 finished: foreachPartition at streamingProcess.scala:50, took 0.016643 s
2017-08-10 09:14:17,951 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327370000 ms.0 from job set of time 1502327370000 ms
2017-08-10 09:14:17,951 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 287.951 s for time 1502327370000 ms (execution: 0.027 s)
2017-08-10 09:14:17,952 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327375000 ms.0 from job set of time 1502327375000 ms
2017-08-10 09:14:17,952 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 181 from persistence list
2017-08-10 09:14:17,952 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 181
2017-08-10 09:14:17,952 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 180 from persistence list
2017-08-10 09:14:17,952 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 180
2017-08-10 09:14:17,952 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,953 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327360000 ms
2017-08-10 09:14:17,963 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 92 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 92 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 92 (MapPartitionsRDD[185] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,967 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_92 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:17,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_92_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:17,971 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_92_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,971 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 92 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,971 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 92 (MapPartitionsRDD[185] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,971 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 92.0 with 2 tasks
2017-08-10 09:14:17,972 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 92.0 (TID 184, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,972 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 92.0 (TID 185, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,972 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 92.0 (TID 184)
2017-08-10 09:14:17,974 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,974 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 92.0 (TID 185)
2017-08-10 09:14:17,976 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,978 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 92.0 (TID 185). 714 bytes result sent to driver
2017-08-10 09:14:17,979 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 92.0 (TID 184). 714 bytes result sent to driver
2017-08-10 09:14:17,981 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 92.0 (TID 185) in 9 ms on localhost (1/2)
2017-08-10 09:14:17,982 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 92.0 (TID 184) in 10 ms on localhost (2/2)
2017-08-10 09:14:17,982 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 92.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,982 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 92 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:14:17,983 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 92 finished: foreachPartition at streamingProcess.scala:50, took 0.019677 s
2017-08-10 09:14:17,983 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327375000 ms.0 from job set of time 1502327375000 ms
2017-08-10 09:14:17,983 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 282.983 s for time 1502327375000 ms (execution: 0.031 s)
2017-08-10 09:14:17,983 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327380000 ms.0 from job set of time 1502327380000 ms
2017-08-10 09:14:17,983 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 183 from persistence list
2017-08-10 09:14:17,983 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 182 from persistence list
2017-08-10 09:14:17,984 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 182
2017-08-10 09:14:17,984 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 183
2017-08-10 09:14:17,984 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,984 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327365000 ms
2017-08-10 09:14:17,993 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,994 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 93 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,994 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 93 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,994 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,994 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,994 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 93 (MapPartitionsRDD[187] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,996 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_93 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:17,999 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_93_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:17,999 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_93_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,999 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 93 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,999 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 93 (MapPartitionsRDD[187] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,000 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 93.0 with 2 tasks
2017-08-10 09:14:18,000 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 93.0 (TID 186, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,000 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 93.0 (TID 187, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,001 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 93.0 (TID 186)
2017-08-10 09:14:18,001 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 93.0 (TID 187)
2017-08-10 09:14:18,002 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,002 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,005 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 93.0 (TID 187). 714 bytes result sent to driver
2017-08-10 09:14:18,005 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 93.0 (TID 186). 714 bytes result sent to driver
2017-08-10 09:14:18,007 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 93.0 (TID 187) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,007 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 93.0 (TID 186) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,007 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 93.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,008 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 93 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:18,008 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 93 finished: foreachPartition at streamingProcess.scala:50, took 0.014688 s
2017-08-10 09:14:18,009 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327380000 ms.0 from job set of time 1502327380000 ms
2017-08-10 09:14:18,009 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 278.009 s for time 1502327380000 ms (execution: 0.026 s)
2017-08-10 09:14:18,009 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 185 from persistence list
2017-08-10 09:14:18,009 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327385000 ms.0 from job set of time 1502327385000 ms
2017-08-10 09:14:18,010 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 185
2017-08-10 09:14:18,010 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 184 from persistence list
2017-08-10 09:14:18,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327370000 ms
2017-08-10 09:14:18,011 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 184
2017-08-10 09:14:18,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 94 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 94 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 94 (MapPartitionsRDD[189] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_94 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:18,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_94_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:18,029 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_94_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 94 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 94 (MapPartitionsRDD[189] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 94.0 with 2 tasks
2017-08-10 09:14:18,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 94.0 (TID 188, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 94.0 (TID 189, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,031 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 94.0 (TID 188)
2017-08-10 09:14:18,031 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 94.0 (TID 189)
2017-08-10 09:14:18,032 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,032 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,035 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 94.0 (TID 189). 801 bytes result sent to driver
2017-08-10 09:14:18,035 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 94.0 (TID 188). 714 bytes result sent to driver
2017-08-10 09:14:18,037 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 94.0 (TID 188) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,037 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 94.0 (TID 189) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,037 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 94.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 94 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,038 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 94 finished: foreachPartition at streamingProcess.scala:50, took 0.013896 s
2017-08-10 09:14:18,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327385000 ms.0 from job set of time 1502327385000 ms
2017-08-10 09:14:18,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 273.040 s for time 1502327385000 ms (execution: 0.031 s)
2017-08-10 09:14:18,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327390000 ms.0 from job set of time 1502327390000 ms
2017-08-10 09:14:18,043 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 187 from persistence list
2017-08-10 09:14:18,048 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 186 from persistence list
2017-08-10 09:14:18,048 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 187
2017-08-10 09:14:18,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327375000 ms
2017-08-10 09:14:18,048 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 186
2017-08-10 09:14:18,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 95 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 95 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 95 (MapPartitionsRDD[191] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_95 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:18,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_95_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:18,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_95_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 95 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 95 (MapPartitionsRDD[191] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 95.0 with 2 tasks
2017-08-10 09:14:18,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 95.0 (TID 190, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 95.0 (TID 191, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,057 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 95.0 (TID 190)
2017-08-10 09:14:18,057 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 95.0 (TID 191)
2017-08-10 09:14:18,059 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,059 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,062 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 95.0 (TID 191). 801 bytes result sent to driver
2017-08-10 09:14:18,062 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 95.0 (TID 190). 714 bytes result sent to driver
2017-08-10 09:14:18,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 95.0 (TID 191) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 95.0 (TID 190) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 95.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 95 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:18,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 95 finished: foreachPartition at streamingProcess.scala:50, took 0.014580 s
2017-08-10 09:14:18,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327390000 ms.0 from job set of time 1502327390000 ms
2017-08-10 09:14:18,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 268.064 s for time 1502327390000 ms (execution: 0.024 s)
2017-08-10 09:14:18,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327395000 ms.0 from job set of time 1502327395000 ms
2017-08-10 09:14:18,064 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 189 from persistence list
2017-08-10 09:14:18,065 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 189
2017-08-10 09:14:18,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 188 from persistence list
2017-08-10 09:14:18,065 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 188
2017-08-10 09:14:18,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327380000 ms
2017-08-10 09:14:18,075 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 96 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 96 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 96 (MapPartitionsRDD[193] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_96 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:18,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_96_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:18,081 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_96_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 96 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 96 (MapPartitionsRDD[193] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 96.0 with 2 tasks
2017-08-10 09:14:18,082 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 96.0 (TID 192, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,082 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 96.0 (TID 193, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,082 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 96.0 (TID 192)
2017-08-10 09:14:18,083 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 96.0 (TID 193)
2017-08-10 09:14:18,084 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,084 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,086 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 96.0 (TID 193). 714 bytes result sent to driver
2017-08-10 09:14:18,086 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 96.0 (TID 192). 714 bytes result sent to driver
2017-08-10 09:14:18,088 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 96.0 (TID 193) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 96.0 (TID 192) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 96.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 96 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 96 finished: foreachPartition at streamingProcess.scala:50, took 0.013247 s
2017-08-10 09:14:18,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327395000 ms.0 from job set of time 1502327395000 ms
2017-08-10 09:14:18,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 263.089 s for time 1502327395000 ms (execution: 0.025 s)
2017-08-10 09:14:18,089 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 191 from persistence list
2017-08-10 09:14:18,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327400000 ms.0 from job set of time 1502327400000 ms
2017-08-10 09:14:18,089 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 191
2017-08-10 09:14:18,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 190 from persistence list
2017-08-10 09:14:18,089 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 190
2017-08-10 09:14:18,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327385000 ms
2017-08-10 09:14:18,098 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,099 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 97 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,099 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 97 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,099 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,099 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,099 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 97 (MapPartitionsRDD[195] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,102 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_97 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:18,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_97_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:18,105 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_97_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 97 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 97 (MapPartitionsRDD[195] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 97.0 with 2 tasks
2017-08-10 09:14:18,106 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 97.0 (TID 194, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,106 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 97.0 (TID 195, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,106 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 97.0 (TID 194)
2017-08-10 09:14:18,106 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 97.0 (TID 195)
2017-08-10 09:14:18,108 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,108 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,110 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 97.0 (TID 195). 714 bytes result sent to driver
2017-08-10 09:14:18,110 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 97.0 (TID 194). 714 bytes result sent to driver
2017-08-10 09:14:18,111 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 97.0 (TID 195) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,112 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 97.0 (TID 194) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,112 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 97 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,112 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 97.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,112 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 97 finished: foreachPartition at streamingProcess.scala:50, took 0.013295 s
2017-08-10 09:14:18,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327400000 ms.0 from job set of time 1502327400000 ms
2017-08-10 09:14:18,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 258.112 s for time 1502327400000 ms (execution: 0.023 s)
2017-08-10 09:14:18,112 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 193 from persistence list
2017-08-10 09:14:18,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327405000 ms.0 from job set of time 1502327405000 ms
2017-08-10 09:14:18,112 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 193
2017-08-10 09:14:18,113 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 192 from persistence list
2017-08-10 09:14:18,113 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 192
2017-08-10 09:14:18,113 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,113 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327390000 ms
2017-08-10 09:14:18,122 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 98 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 98 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 98 (MapPartitionsRDD[197] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,125 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_98 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:18,128 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_98_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:18,128 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_98_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,128 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 98 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,128 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 98 (MapPartitionsRDD[197] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,128 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 98.0 with 2 tasks
2017-08-10 09:14:18,129 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 98.0 (TID 196, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,129 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 98.0 (TID 197, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,129 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 98.0 (TID 197)
2017-08-10 09:14:18,129 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 98.0 (TID 196)
2017-08-10 09:14:18,131 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,131 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,133 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 98.0 (TID 197). 714 bytes result sent to driver
2017-08-10 09:14:18,133 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 98.0 (TID 196). 714 bytes result sent to driver
2017-08-10 09:14:18,134 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 98.0 (TID 197) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,134 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 98.0 (TID 196) in 5 ms on localhost (2/2)
2017-08-10 09:14:18,135 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 98.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,135 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 98 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,135 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 98 finished: foreachPartition at streamingProcess.scala:50, took 0.012459 s
2017-08-10 09:14:18,135 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327405000 ms.0 from job set of time 1502327405000 ms
2017-08-10 09:14:18,135 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 253.135 s for time 1502327405000 ms (execution: 0.023 s)
2017-08-10 09:14:18,135 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 195 from persistence list
2017-08-10 09:14:18,135 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327410000 ms.0 from job set of time 1502327410000 ms
2017-08-10 09:14:18,135 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 195
2017-08-10 09:14:18,136 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 194 from persistence list
2017-08-10 09:14:18,136 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 194
2017-08-10 09:14:18,136 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,136 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327395000 ms
2017-08-10 09:14:18,145 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 99 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 99 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 99 (MapPartitionsRDD[199] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,148 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_99 stored as values in memory (estimated size 34.5 KB, free 412.9 MB)
2017-08-10 09:14:18,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_99_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:18,157 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_99_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,157 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_83_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 99 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 99 (MapPartitionsRDD[199] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 99.0 with 2 tasks
2017-08-10 09:14:18,158 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_84_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,158 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 99.0 (TID 198, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,158 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 99.0 (TID 199, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,158 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 99.0 (TID 198)
2017-08-10 09:14:18,158 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 99.0 (TID 199)
2017-08-10 09:14:18,158 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_85_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,159 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_86_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,160 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,160 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,160 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_87_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,161 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_88_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,162 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 99.0 (TID 199). 714 bytes result sent to driver
2017-08-10 09:14:18,162 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 99.0 (TID 198). 714 bytes result sent to driver
2017-08-10 09:14:18,162 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_89_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,163 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 99.0 (TID 199) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,163 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 99.0 (TID 198) in 5 ms on localhost (2/2)
2017-08-10 09:14:18,164 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 99.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,164 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 99 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,164 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 99 finished: foreachPartition at streamingProcess.scala:50, took 0.018845 s
2017-08-10 09:14:18,164 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327410000 ms.0 from job set of time 1502327410000 ms
2017-08-10 09:14:18,164 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_90_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,164 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 248.164 s for time 1502327410000 ms (execution: 0.029 s)
2017-08-10 09:14:18,164 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327415000 ms.0 from job set of time 1502327415000 ms
2017-08-10 09:14:18,164 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 197 from persistence list
2017-08-10 09:14:18,165 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 197
2017-08-10 09:14:18,165 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 196 from persistence list
2017-08-10 09:14:18,165 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_91_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,165 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 196
2017-08-10 09:14:18,165 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,165 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327400000 ms
2017-08-10 09:14:18,166 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_92_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,166 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_93_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,167 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_94_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,167 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_95_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,168 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_96_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,169 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_97_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,169 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_98_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,175 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,175 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 100 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,175 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 100 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,175 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,175 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,175 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 100 (MapPartitionsRDD[201] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_100 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:18,180 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_100_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:18,181 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_100_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 100 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 100 (MapPartitionsRDD[201] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 100.0 with 2 tasks
2017-08-10 09:14:18,182 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 100.0 (TID 200, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,182 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 100.0 (TID 201, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,182 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 100.0 (TID 200)
2017-08-10 09:14:18,182 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 100.0 (TID 201)
2017-08-10 09:14:18,184 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,184 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,186 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 100.0 (TID 200). 714 bytes result sent to driver
2017-08-10 09:14:18,186 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 100.0 (TID 201). 714 bytes result sent to driver
2017-08-10 09:14:18,188 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 100.0 (TID 200) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,188 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 100.0 (TID 201) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,188 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 100.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,188 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 100 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,188 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 100 finished: foreachPartition at streamingProcess.scala:50, took 0.013301 s
2017-08-10 09:14:18,188 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327415000 ms.0 from job set of time 1502327415000 ms
2017-08-10 09:14:18,188 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 243.188 s for time 1502327415000 ms (execution: 0.024 s)
2017-08-10 09:14:18,188 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 199 from persistence list
2017-08-10 09:14:18,188 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327420000 ms.0 from job set of time 1502327420000 ms
2017-08-10 09:14:18,189 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 198 from persistence list
2017-08-10 09:14:18,189 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 199
2017-08-10 09:14:18,189 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 198
2017-08-10 09:14:18,189 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,189 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327405000 ms
2017-08-10 09:14:18,198 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,199 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 101 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,199 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 101 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,199 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,199 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,199 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 101 (MapPartitionsRDD[203] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,201 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_101 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:18,204 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_101_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:18,205 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_101_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 101 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 101 (MapPartitionsRDD[203] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 101.0 with 2 tasks
2017-08-10 09:14:18,205 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 101.0 (TID 202, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,206 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 101.0 (TID 203, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,206 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 101.0 (TID 202)
2017-08-10 09:14:18,206 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 101.0 (TID 203)
2017-08-10 09:14:18,207 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,207 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,210 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 101.0 (TID 203). 714 bytes result sent to driver
2017-08-10 09:14:18,210 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 101.0 (TID 202). 714 bytes result sent to driver
2017-08-10 09:14:18,212 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 101.0 (TID 202) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,212 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 101.0 (TID 203) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,212 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 101.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,212 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 101 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,212 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 101 finished: foreachPartition at streamingProcess.scala:50, took 0.013843 s
2017-08-10 09:14:18,212 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327420000 ms.0 from job set of time 1502327420000 ms
2017-08-10 09:14:18,212 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 238.212 s for time 1502327420000 ms (execution: 0.024 s)
2017-08-10 09:14:18,212 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 201 from persistence list
2017-08-10 09:14:18,213 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327425000 ms.0 from job set of time 1502327425000 ms
2017-08-10 09:14:18,213 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 201
2017-08-10 09:14:18,213 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 200 from persistence list
2017-08-10 09:14:18,213 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 200
2017-08-10 09:14:18,213 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,213 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327410000 ms
2017-08-10 09:14:18,222 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,222 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 102 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,222 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 102 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,222 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,222 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,223 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 102 (MapPartitionsRDD[205] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,225 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_102 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:18,228 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_102_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:18,228 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_102_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,229 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 102 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,229 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 102 (MapPartitionsRDD[205] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,229 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 102.0 with 2 tasks
2017-08-10 09:14:18,229 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 102.0 (TID 204, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,230 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 102.0 (TID 205, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,230 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 102.0 (TID 204)
2017-08-10 09:14:18,230 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 102.0 (TID 205)
2017-08-10 09:14:18,231 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,231 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,234 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 102.0 (TID 204). 714 bytes result sent to driver
2017-08-10 09:14:18,234 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 102.0 (TID 205). 714 bytes result sent to driver
2017-08-10 09:14:18,235 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 102.0 (TID 205) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,236 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 102.0 (TID 204) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,236 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 102.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,236 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 102 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,236 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 102 finished: foreachPartition at streamingProcess.scala:50, took 0.013728 s
2017-08-10 09:14:18,236 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327425000 ms.0 from job set of time 1502327425000 ms
2017-08-10 09:14:18,236 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 233.236 s for time 1502327425000 ms (execution: 0.023 s)
2017-08-10 09:14:18,236 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 203 from persistence list
2017-08-10 09:14:18,236 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327430000 ms.0 from job set of time 1502327430000 ms
2017-08-10 09:14:18,237 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 202 from persistence list
2017-08-10 09:14:18,237 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 203
2017-08-10 09:14:18,237 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 202
2017-08-10 09:14:18,237 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,237 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327415000 ms
2017-08-10 09:14:18,246 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,246 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 103 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,246 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 103 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,246 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,246 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,246 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 103 (MapPartitionsRDD[207] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,249 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_103 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:18,251 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_103_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:18,252 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_103_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,252 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 103 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,252 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 103 (MapPartitionsRDD[207] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,252 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 103.0 with 2 tasks
2017-08-10 09:14:18,253 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 103.0 (TID 206, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,253 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 103.0 (TID 207, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,253 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 103.0 (TID 206)
2017-08-10 09:14:18,253 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 103.0 (TID 207)
2017-08-10 09:14:18,255 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,255 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,257 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 103.0 (TID 206). 714 bytes result sent to driver
2017-08-10 09:14:18,257 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 103.0 (TID 207). 714 bytes result sent to driver
2017-08-10 09:14:18,258 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 103.0 (TID 207) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,258 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 103.0 (TID 206) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,259 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 103.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,259 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 103 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,259 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 103 finished: foreachPartition at streamingProcess.scala:50, took 0.012942 s
2017-08-10 09:14:18,259 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327430000 ms.0 from job set of time 1502327430000 ms
2017-08-10 09:14:18,259 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 228.259 s for time 1502327430000 ms (execution: 0.023 s)
2017-08-10 09:14:18,259 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 205 from persistence list
2017-08-10 09:14:18,259 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327435000 ms.0 from job set of time 1502327435000 ms
2017-08-10 09:14:18,259 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 205
2017-08-10 09:14:18,260 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 204 from persistence list
2017-08-10 09:14:18,260 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 204
2017-08-10 09:14:18,260 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,260 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327420000 ms
2017-08-10 09:14:18,271 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 104 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 104 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 104 (MapPartitionsRDD[209] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,274 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_104 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:18,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_104_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:18,276 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_104_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 104 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 104 (MapPartitionsRDD[209] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 104.0 with 2 tasks
2017-08-10 09:14:18,277 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 104.0 (TID 208, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,278 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 104.0 (TID 209, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,278 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 104.0 (TID 209)
2017-08-10 09:14:18,278 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 104.0 (TID 208)
2017-08-10 09:14:18,279 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,279 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,282 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 104.0 (TID 208). 714 bytes result sent to driver
2017-08-10 09:14:18,282 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 104.0 (TID 209). 714 bytes result sent to driver
2017-08-10 09:14:18,283 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 104.0 (TID 209) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,283 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 104.0 (TID 208) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,283 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 104.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 104 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,283 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 104 finished: foreachPartition at streamingProcess.scala:50, took 0.012557 s
2017-08-10 09:14:18,283 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327435000 ms.0 from job set of time 1502327435000 ms
2017-08-10 09:14:18,284 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 223.283 s for time 1502327435000 ms (execution: 0.024 s)
2017-08-10 09:14:18,284 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327440000 ms.0 from job set of time 1502327440000 ms
2017-08-10 09:14:18,284 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 207 from persistence list
2017-08-10 09:14:18,284 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 206 from persistence list
2017-08-10 09:14:18,284 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 207
2017-08-10 09:14:18,285 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,285 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327425000 ms
2017-08-10 09:14:18,285 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 206
2017-08-10 09:14:18,295 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,295 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 105 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,295 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 105 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,295 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,295 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,295 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 105 (MapPartitionsRDD[211] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,298 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_105 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:18,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_105_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:18,301 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_105_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 105 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 105 (MapPartitionsRDD[211] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 105.0 with 2 tasks
2017-08-10 09:14:18,302 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 105.0 (TID 210, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,302 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 105.0 (TID 211, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,303 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 105.0 (TID 210)
2017-08-10 09:14:18,303 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 105.0 (TID 211)
2017-08-10 09:14:18,304 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,304 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,307 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 105.0 (TID 211). 714 bytes result sent to driver
2017-08-10 09:14:18,307 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 105.0 (TID 210). 714 bytes result sent to driver
2017-08-10 09:14:18,308 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 105.0 (TID 210) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,308 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 105.0 (TID 211) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,308 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 105.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,308 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 105 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,309 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 105 finished: foreachPartition at streamingProcess.scala:50, took 0.013841 s
2017-08-10 09:14:18,309 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327440000 ms.0 from job set of time 1502327440000 ms
2017-08-10 09:14:18,309 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 218.309 s for time 1502327440000 ms (execution: 0.025 s)
2017-08-10 09:14:18,309 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 209 from persistence list
2017-08-10 09:14:18,309 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327445000 ms.0 from job set of time 1502327445000 ms
2017-08-10 09:14:18,309 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 209
2017-08-10 09:14:18,309 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 208 from persistence list
2017-08-10 09:14:18,310 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,310 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 208
2017-08-10 09:14:18,310 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327430000 ms
2017-08-10 09:14:18,319 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 106 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 106 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,320 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 106 (MapPartitionsRDD[213] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,322 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_106 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:18,325 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_106_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:18,325 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_106_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,325 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 106 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,325 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 106 (MapPartitionsRDD[213] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,325 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 106.0 with 2 tasks
2017-08-10 09:14:18,326 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 106.0 (TID 212, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,326 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 106.0 (TID 213, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,326 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 106.0 (TID 213)
2017-08-10 09:14:18,326 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 106.0 (TID 212)
2017-08-10 09:14:18,328 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,328 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,330 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 106.0 (TID 212). 714 bytes result sent to driver
2017-08-10 09:14:18,330 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 106.0 (TID 213). 714 bytes result sent to driver
2017-08-10 09:14:18,331 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 106.0 (TID 212) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,331 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 106.0 (TID 213) in 5 ms on localhost (2/2)
2017-08-10 09:14:18,331 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 106.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,332 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 106 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,332 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 106 finished: foreachPartition at streamingProcess.scala:50, took 0.012630 s
2017-08-10 09:14:18,332 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327445000 ms.0 from job set of time 1502327445000 ms
2017-08-10 09:14:18,332 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 213.332 s for time 1502327445000 ms (execution: 0.023 s)
2017-08-10 09:14:18,332 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 211 from persistence list
2017-08-10 09:14:18,332 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327450000 ms.0 from job set of time 1502327450000 ms
2017-08-10 09:14:18,332 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 211
2017-08-10 09:14:18,332 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 210 from persistence list
2017-08-10 09:14:18,333 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 210
2017-08-10 09:14:18,333 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,333 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327435000 ms
2017-08-10 09:14:18,342 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,343 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 107 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,343 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 107 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,343 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,343 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,343 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 107 (MapPartitionsRDD[215] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,345 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_107 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:18,349 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_107_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:18,349 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_107_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,349 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 107 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,349 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 107 (MapPartitionsRDD[215] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,349 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 107.0 with 2 tasks
2017-08-10 09:14:18,350 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 107.0 (TID 214, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,350 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 107.0 (TID 215, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,350 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 107.0 (TID 215)
2017-08-10 09:14:18,350 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 107.0 (TID 214)
2017-08-10 09:14:18,352 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,352 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,355 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 107.0 (TID 215). 714 bytes result sent to driver
2017-08-10 09:14:18,355 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 107.0 (TID 214). 714 bytes result sent to driver
2017-08-10 09:14:18,356 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 107.0 (TID 214) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,356 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 107.0 (TID 215) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,356 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 107.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,356 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 107 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,357 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 107 finished: foreachPartition at streamingProcess.scala:50, took 0.014193 s
2017-08-10 09:14:18,357 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327450000 ms.0 from job set of time 1502327450000 ms
2017-08-10 09:14:18,357 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 208.357 s for time 1502327450000 ms (execution: 0.025 s)
2017-08-10 09:14:18,357 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 213 from persistence list
2017-08-10 09:14:18,357 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327455000 ms.0 from job set of time 1502327455000 ms
2017-08-10 09:14:18,357 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 212 from persistence list
2017-08-10 09:14:18,357 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 213
2017-08-10 09:14:18,357 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,358 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327440000 ms
2017-08-10 09:14:18,358 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 212
2017-08-10 09:14:18,367 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,367 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 108 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,367 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 108 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,367 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,367 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,367 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 108 (MapPartitionsRDD[217] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,370 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_108 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:18,373 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_108_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:18,373 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_108_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,374 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 108 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,374 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 108 (MapPartitionsRDD[217] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,374 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 108.0 with 2 tasks
2017-08-10 09:14:18,374 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 108.0 (TID 216, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,375 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 108.0 (TID 217, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,375 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 108.0 (TID 217)
2017-08-10 09:14:18,375 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 108.0 (TID 216)
2017-08-10 09:14:18,377 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,377 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,379 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 108.0 (TID 217). 714 bytes result sent to driver
2017-08-10 09:14:18,379 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 108.0 (TID 216). 714 bytes result sent to driver
2017-08-10 09:14:18,380 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 108.0 (TID 217) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,381 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 108.0 (TID 216) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,381 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 108.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,381 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 108 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,381 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 108 finished: foreachPartition at streamingProcess.scala:50, took 0.014034 s
2017-08-10 09:14:18,381 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327455000 ms.0 from job set of time 1502327455000 ms
2017-08-10 09:14:18,381 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 203.381 s for time 1502327455000 ms (execution: 0.024 s)
2017-08-10 09:14:18,381 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 215 from persistence list
2017-08-10 09:14:18,381 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327460000 ms.0 from job set of time 1502327460000 ms
2017-08-10 09:14:18,382 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 215
2017-08-10 09:14:18,382 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 214 from persistence list
2017-08-10 09:14:18,382 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 214
2017-08-10 09:14:18,382 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,382 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327445000 ms
2017-08-10 09:14:18,392 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,393 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 109 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,393 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 109 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,393 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,393 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,393 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 109 (MapPartitionsRDD[219] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,396 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_109 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:18,398 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_109_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:18,399 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_109_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 109 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 109 (MapPartitionsRDD[219] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 109.0 with 2 tasks
2017-08-10 09:14:18,400 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 109.0 (TID 218, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,400 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 109.0 (TID 219, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,400 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 109.0 (TID 219)
2017-08-10 09:14:18,400 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 109.0 (TID 218)
2017-08-10 09:14:18,402 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,402 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,405 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 109.0 (TID 218). 714 bytes result sent to driver
2017-08-10 09:14:18,405 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 109.0 (TID 219). 714 bytes result sent to driver
2017-08-10 09:14:18,406 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 109.0 (TID 219) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,407 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 109.0 (TID 218) in 8 ms on localhost (2/2)
2017-08-10 09:14:18,407 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 109.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 109 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:18,407 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 109 finished: foreachPartition at streamingProcess.scala:50, took 0.014495 s
2017-08-10 09:14:18,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327460000 ms.0 from job set of time 1502327460000 ms
2017-08-10 09:14:18,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 198.407 s for time 1502327460000 ms (execution: 0.026 s)
2017-08-10 09:14:18,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327465000 ms.0 from job set of time 1502327465000 ms
2017-08-10 09:14:18,407 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 217 from persistence list
2017-08-10 09:14:18,409 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 217
2017-08-10 09:14:18,409 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 216 from persistence list
2017-08-10 09:14:18,410 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 216
2017-08-10 09:14:18,410 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,410 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327450000 ms
2017-08-10 09:14:18,420 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 110 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 110 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 110 (MapPartitionsRDD[221] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,423 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_110 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:18,426 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_110_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:18,427 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_110_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,427 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 110 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,427 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 110 (MapPartitionsRDD[221] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,427 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 110.0 with 2 tasks
2017-08-10 09:14:18,428 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 110.0 (TID 220, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,429 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 110.0 (TID 221, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,429 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 110.0 (TID 220)
2017-08-10 09:14:18,429 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 110.0 (TID 221)
2017-08-10 09:14:18,431 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,431 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,433 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 110.0 (TID 221). 714 bytes result sent to driver
2017-08-10 09:14:18,433 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 110.0 (TID 220). 714 bytes result sent to driver
2017-08-10 09:14:18,435 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 110.0 (TID 221) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,435 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 110.0 (TID 220) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,435 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 110.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,436 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 110 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:18,436 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 110 finished: foreachPartition at streamingProcess.scala:50, took 0.016447 s
2017-08-10 09:14:18,437 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327465000 ms.0 from job set of time 1502327465000 ms
2017-08-10 09:14:18,437 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 193.437 s for time 1502327465000 ms (execution: 0.030 s)
2017-08-10 09:14:18,437 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327470000 ms.0 from job set of time 1502327470000 ms
2017-08-10 09:14:18,438 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 219 from persistence list
2017-08-10 09:14:18,438 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 218 from persistence list
2017-08-10 09:14:18,438 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 218
2017-08-10 09:14:18,438 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 219
2017-08-10 09:14:18,438 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,438 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327455000 ms
2017-08-10 09:14:18,447 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 111 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 111 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 111 (MapPartitionsRDD[223] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,451 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_111 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:18,454 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_111_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:18,455 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_111_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,455 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 111 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 111 (MapPartitionsRDD[223] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 111.0 with 2 tasks
2017-08-10 09:14:18,456 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 111.0 (TID 222, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,456 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 111.0 (TID 223, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,457 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 111.0 (TID 222)
2017-08-10 09:14:18,457 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 111.0 (TID 223)
2017-08-10 09:14:18,458 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,458 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,461 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 111.0 (TID 222). 714 bytes result sent to driver
2017-08-10 09:14:18,461 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 111.0 (TID 223). 714 bytes result sent to driver
2017-08-10 09:14:18,462 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 111.0 (TID 223) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,463 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 111.0 (TID 222) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,463 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 111.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,463 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 111 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,463 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 111 finished: foreachPartition at streamingProcess.scala:50, took 0.016160 s
2017-08-10 09:14:18,464 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327470000 ms.0 from job set of time 1502327470000 ms
2017-08-10 09:14:18,464 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 188.464 s for time 1502327470000 ms (execution: 0.027 s)
2017-08-10 09:14:18,464 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327475000 ms.0 from job set of time 1502327475000 ms
2017-08-10 09:14:18,464 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 221 from persistence list
2017-08-10 09:14:18,464 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 221
2017-08-10 09:14:18,464 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 220 from persistence list
2017-08-10 09:14:18,464 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 220
2017-08-10 09:14:18,465 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,465 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327460000 ms
2017-08-10 09:14:18,475 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 112 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 112 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 112 (MapPartitionsRDD[225] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,478 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_112 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:18,481 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_112_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:18,481 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_112_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,482 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 112 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,482 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 112 (MapPartitionsRDD[225] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,482 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 112.0 with 2 tasks
2017-08-10 09:14:18,482 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 112.0 (TID 224, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,483 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 112.0 (TID 225, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,483 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 112.0 (TID 224)
2017-08-10 09:14:18,483 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 112.0 (TID 225)
2017-08-10 09:14:18,485 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,485 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,487 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 112.0 (TID 225). 714 bytes result sent to driver
2017-08-10 09:14:18,487 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 112.0 (TID 224). 714 bytes result sent to driver
2017-08-10 09:14:18,488 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 112.0 (TID 224) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,489 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 112.0 (TID 225) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,489 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 112.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,489 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 112 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,489 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 112 finished: foreachPartition at streamingProcess.scala:50, took 0.014449 s
2017-08-10 09:14:18,489 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327475000 ms.0 from job set of time 1502327475000 ms
2017-08-10 09:14:18,490 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 183.489 s for time 1502327475000 ms (execution: 0.025 s)
2017-08-10 09:14:18,490 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327480000 ms.0 from job set of time 1502327480000 ms
2017-08-10 09:14:18,490 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 223 from persistence list
2017-08-10 09:14:18,490 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 222 from persistence list
2017-08-10 09:14:18,490 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 223
2017-08-10 09:14:18,490 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 222
2017-08-10 09:14:18,491 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,491 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327465000 ms
2017-08-10 09:14:18,499 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,500 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 113 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,500 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 113 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,500 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,500 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,500 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 113 (MapPartitionsRDD[227] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,503 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_113 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:18,506 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_113_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:18,506 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_113_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,507 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 113 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,507 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 113 (MapPartitionsRDD[227] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,507 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 113.0 with 2 tasks
2017-08-10 09:14:18,507 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 113.0 (TID 226, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,508 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 113.0 (TID 227, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,508 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 113.0 (TID 226)
2017-08-10 09:14:18,508 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 113.0 (TID 227)
2017-08-10 09:14:18,509 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,509 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,513 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 113.0 (TID 227). 714 bytes result sent to driver
2017-08-10 09:14:18,514 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 113.0 (TID 226). 714 bytes result sent to driver
2017-08-10 09:14:18,515 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 113.0 (TID 227) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,515 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 113.0 (TID 226) in 8 ms on localhost (2/2)
2017-08-10 09:14:18,516 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 113.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,516 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 113 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:18,516 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 113 finished: foreachPartition at streamingProcess.scala:50, took 0.016475 s
2017-08-10 09:14:18,516 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327480000 ms.0 from job set of time 1502327480000 ms
2017-08-10 09:14:18,516 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 178.516 s for time 1502327480000 ms (execution: 0.026 s)
2017-08-10 09:14:18,516 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327485000 ms.0 from job set of time 1502327485000 ms
2017-08-10 09:14:18,516 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 225 from persistence list
2017-08-10 09:14:18,517 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 224 from persistence list
2017-08-10 09:14:18,517 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 224
2017-08-10 09:14:18,517 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 225
2017-08-10 09:14:18,517 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,517 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327470000 ms
2017-08-10 09:14:18,527 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,527 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 114 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,527 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 114 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,527 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,527 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,527 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 114 (MapPartitionsRDD[229] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,530 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_114 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:18,533 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_114_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:18,533 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_114_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,533 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 114 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,533 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 114 (MapPartitionsRDD[229] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,533 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 114.0 with 2 tasks
2017-08-10 09:14:18,534 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 114.0 (TID 228, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,534 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 114.0 (TID 229, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,534 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 114.0 (TID 229)
2017-08-10 09:14:18,534 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 114.0 (TID 228)
2017-08-10 09:14:18,536 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,536 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,538 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 114.0 (TID 228). 714 bytes result sent to driver
2017-08-10 09:14:18,538 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 114.0 (TID 229). 714 bytes result sent to driver
2017-08-10 09:14:18,539 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 114.0 (TID 228) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,540 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 114.0 (TID 229) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,540 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 114.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,540 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 114 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,540 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 114 finished: foreachPartition at streamingProcess.scala:50, took 0.013302 s
2017-08-10 09:14:18,540 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327485000 ms.0 from job set of time 1502327485000 ms
2017-08-10 09:14:18,540 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 173.540 s for time 1502327485000 ms (execution: 0.024 s)
2017-08-10 09:14:18,540 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 227 from persistence list
2017-08-10 09:14:18,540 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327490000 ms.0 from job set of time 1502327490000 ms
2017-08-10 09:14:18,541 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 227
2017-08-10 09:14:18,541 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 226 from persistence list
2017-08-10 09:14:18,541 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,542 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327475000 ms
2017-08-10 09:14:18,542 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 226
2017-08-10 09:14:18,557 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_99_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,558 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_114_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,558 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_100_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,559 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_101_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,559 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,559 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 115 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,559 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 115 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,559 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,559 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,559 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 115 (MapPartitionsRDD[231] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,560 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_102_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,560 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_103_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,561 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_104_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,562 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_105_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,562 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_106_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,562 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_115 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:18,562 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_107_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,563 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_108_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,564 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_109_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,564 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_110_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,565 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_111_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,565 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_115_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:18,565 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_112_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,565 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_115_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 115 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 115 (MapPartitionsRDD[231] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 115.0 with 2 tasks
2017-08-10 09:14:18,566 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_113_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,566 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 115.0 (TID 230, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,567 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 115.0 (TID 231, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,567 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 115.0 (TID 231)
2017-08-10 09:14:18,567 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 115.0 (TID 230)
2017-08-10 09:14:18,568 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,568 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,571 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 115.0 (TID 230). 714 bytes result sent to driver
2017-08-10 09:14:18,571 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 115.0 (TID 231). 714 bytes result sent to driver
2017-08-10 09:14:18,572 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 115.0 (TID 230) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,572 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 115.0 (TID 231) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,572 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 115.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,572 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 115 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,572 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 115 finished: foreachPartition at streamingProcess.scala:50, took 0.013420 s
2017-08-10 09:14:18,573 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327490000 ms.0 from job set of time 1502327490000 ms
2017-08-10 09:14:18,573 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 168.573 s for time 1502327490000 ms (execution: 0.033 s)
2017-08-10 09:14:18,573 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327495000 ms.0 from job set of time 1502327495000 ms
2017-08-10 09:14:18,573 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 229 from persistence list
2017-08-10 09:14:18,573 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 229
2017-08-10 09:14:18,573 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 228 from persistence list
2017-08-10 09:14:18,573 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 228
2017-08-10 09:14:18,573 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,573 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327480000 ms
2017-08-10 09:14:18,582 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,582 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 116 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,582 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 116 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,583 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,583 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,583 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 116 (MapPartitionsRDD[233] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,586 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_116 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:18,589 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_116_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:18,589 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_116_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,590 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 116 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,590 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 116 (MapPartitionsRDD[233] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,590 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 116.0 with 2 tasks
2017-08-10 09:14:18,590 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 116.0 (TID 232, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,591 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 116.0 (TID 233, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,591 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 116.0 (TID 232)
2017-08-10 09:14:18,591 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 116.0 (TID 233)
2017-08-10 09:14:18,592 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,592 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,595 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 116.0 (TID 233). 714 bytes result sent to driver
2017-08-10 09:14:18,595 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 116.0 (TID 232). 714 bytes result sent to driver
2017-08-10 09:14:18,596 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 116.0 (TID 233) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,597 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 116.0 (TID 232) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,597 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 116.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,597 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 116 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,597 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 116 finished: foreachPartition at streamingProcess.scala:50, took 0.014811 s
2017-08-10 09:14:18,597 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327495000 ms.0 from job set of time 1502327495000 ms
2017-08-10 09:14:18,597 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 163.597 s for time 1502327495000 ms (execution: 0.024 s)
2017-08-10 09:14:18,597 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327500000 ms.0 from job set of time 1502327500000 ms
2017-08-10 09:14:18,598 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 231 from persistence list
2017-08-10 09:14:18,598 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 231
2017-08-10 09:14:18,598 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 230 from persistence list
2017-08-10 09:14:18,598 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 230
2017-08-10 09:14:18,598 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,598 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327485000 ms
2017-08-10 09:14:18,608 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,608 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 117 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,608 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 117 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,608 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,608 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,608 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 117 (MapPartitionsRDD[235] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,611 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_117 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:18,614 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_117_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:18,614 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_117_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,614 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 117 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,614 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 117 (MapPartitionsRDD[235] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,614 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 117.0 with 2 tasks
2017-08-10 09:14:18,615 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 117.0 (TID 234, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,615 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 117.0 (TID 235, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,615 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 117.0 (TID 234)
2017-08-10 09:14:18,615 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 117.0 (TID 235)
2017-08-10 09:14:18,617 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,617 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,620 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 117.0 (TID 235). 714 bytes result sent to driver
2017-08-10 09:14:18,620 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 117.0 (TID 234). 714 bytes result sent to driver
2017-08-10 09:14:18,622 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 117.0 (TID 235) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,622 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 117.0 (TID 234) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,622 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 117.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,622 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 117 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:18,622 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 117 finished: foreachPartition at streamingProcess.scala:50, took 0.014409 s
2017-08-10 09:14:18,623 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327500000 ms.0 from job set of time 1502327500000 ms
2017-08-10 09:14:18,623 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 158.622 s for time 1502327500000 ms (execution: 0.025 s)
2017-08-10 09:14:18,623 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 233 from persistence list
2017-08-10 09:14:18,623 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327505000 ms.0 from job set of time 1502327505000 ms
2017-08-10 09:14:18,623 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 233
2017-08-10 09:14:18,623 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 232 from persistence list
2017-08-10 09:14:18,623 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,623 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327490000 ms
2017-08-10 09:14:18,623 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 232
2017-08-10 09:14:18,635 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,636 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 118 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,636 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 118 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,636 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,636 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,636 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 118 (MapPartitionsRDD[237] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,639 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_118 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:18,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_118_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:18,642 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_118_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 118 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,643 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 118 (MapPartitionsRDD[237] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,643 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 118.0 with 2 tasks
2017-08-10 09:14:18,643 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 118.0 (TID 236, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,644 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 118.0 (TID 237, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,644 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 118.0 (TID 236)
2017-08-10 09:14:18,644 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 118.0 (TID 237)
2017-08-10 09:14:18,645 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,645 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,648 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 118.0 (TID 236). 714 bytes result sent to driver
2017-08-10 09:14:18,648 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 118.0 (TID 237). 714 bytes result sent to driver
2017-08-10 09:14:18,649 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 118.0 (TID 236) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,650 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 118.0 (TID 237) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,650 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 118.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,650 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 118 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,650 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 118 finished: foreachPartition at streamingProcess.scala:50, took 0.014763 s
2017-08-10 09:14:18,650 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327505000 ms.0 from job set of time 1502327505000 ms
2017-08-10 09:14:18,650 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 153.650 s for time 1502327505000 ms (execution: 0.027 s)
2017-08-10 09:14:18,650 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327510000 ms.0 from job set of time 1502327510000 ms
2017-08-10 09:14:18,650 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 235 from persistence list
2017-08-10 09:14:18,651 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 234 from persistence list
2017-08-10 09:14:18,651 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,651 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327495000 ms
2017-08-10 09:14:18,651 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 234
2017-08-10 09:14:18,652 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 235
2017-08-10 09:14:18,661 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,662 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 119 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,662 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 119 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,662 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,662 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,662 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 119 (MapPartitionsRDD[239] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,664 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_119 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:18,667 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_119_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:18,667 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_119_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,667 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 119 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 119 (MapPartitionsRDD[239] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 119.0 with 2 tasks
2017-08-10 09:14:18,668 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 119.0 (TID 238, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,669 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 119.0 (TID 239, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,669 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 119.0 (TID 239)
2017-08-10 09:14:18,669 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 119.0 (TID 238)
2017-08-10 09:14:18,670 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,670 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,673 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 119.0 (TID 238). 714 bytes result sent to driver
2017-08-10 09:14:18,673 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 119.0 (TID 239). 714 bytes result sent to driver
2017-08-10 09:14:18,675 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 119.0 (TID 239) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,675 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 119.0 (TID 238) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,675 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 119.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,675 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 119 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,675 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 119 finished: foreachPartition at streamingProcess.scala:50, took 0.014284 s
2017-08-10 09:14:18,676 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327510000 ms.0 from job set of time 1502327510000 ms
2017-08-10 09:14:18,676 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 148.676 s for time 1502327510000 ms (execution: 0.026 s)
2017-08-10 09:14:18,676 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 237 from persistence list
2017-08-10 09:14:18,676 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327515000 ms.0 from job set of time 1502327515000 ms
2017-08-10 09:14:18,676 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 237
2017-08-10 09:14:18,676 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 236 from persistence list
2017-08-10 09:14:18,676 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,676 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327500000 ms
2017-08-10 09:14:18,677 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 236
2017-08-10 09:14:18,686 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 120 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 120 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 120 (MapPartitionsRDD[241] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,689 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_120 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:18,692 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_120_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:18,693 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_120_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,693 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 120 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,693 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 120 (MapPartitionsRDD[241] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,693 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 120.0 with 2 tasks
2017-08-10 09:14:18,694 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 120.0 (TID 240, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,694 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 120.0 (TID 241, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,694 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 120.0 (TID 240)
2017-08-10 09:14:18,694 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 120.0 (TID 241)
2017-08-10 09:14:18,696 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,696 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,698 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 120.0 (TID 240). 714 bytes result sent to driver
2017-08-10 09:14:18,698 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 120.0 (TID 241). 714 bytes result sent to driver
2017-08-10 09:14:18,699 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 120.0 (TID 240) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,699 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 120.0 (TID 241) in 5 ms on localhost (2/2)
2017-08-10 09:14:18,699 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 120.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 120 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,700 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 120 finished: foreachPartition at streamingProcess.scala:50, took 0.013941 s
2017-08-10 09:14:18,700 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327515000 ms.0 from job set of time 1502327515000 ms
2017-08-10 09:14:18,700 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 143.700 s for time 1502327515000 ms (execution: 0.024 s)
2017-08-10 09:14:18,700 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 239 from persistence list
2017-08-10 09:14:18,700 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327520000 ms.0 from job set of time 1502327520000 ms
2017-08-10 09:14:18,700 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 239
2017-08-10 09:14:18,700 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 238 from persistence list
2017-08-10 09:14:18,700 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 238
2017-08-10 09:14:18,701 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,701 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327505000 ms
2017-08-10 09:14:18,709 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 121 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 121 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 121 (MapPartitionsRDD[243] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,712 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_121 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:18,715 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_121_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:18,716 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_121_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,716 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 121 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,716 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 121 (MapPartitionsRDD[243] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,716 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 121.0 with 2 tasks
2017-08-10 09:14:18,717 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 121.0 (TID 242, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,717 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 121.0 (TID 243, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,717 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 121.0 (TID 242)
2017-08-10 09:14:18,717 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 121.0 (TID 243)
2017-08-10 09:14:18,719 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,719 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,722 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 121.0 (TID 243). 714 bytes result sent to driver
2017-08-10 09:14:18,722 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 121.0 (TID 242). 714 bytes result sent to driver
2017-08-10 09:14:18,723 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 121.0 (TID 243) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,723 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 121.0 (TID 242) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,723 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 121.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,723 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 121 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,723 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 121 finished: foreachPartition at streamingProcess.scala:50, took 0.013915 s
2017-08-10 09:14:18,724 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327520000 ms.0 from job set of time 1502327520000 ms
2017-08-10 09:14:18,724 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 138.724 s for time 1502327520000 ms (execution: 0.024 s)
2017-08-10 09:14:18,724 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 241 from persistence list
2017-08-10 09:14:18,724 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327525000 ms.0 from job set of time 1502327525000 ms
2017-08-10 09:14:18,724 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 241
2017-08-10 09:14:18,724 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 240 from persistence list
2017-08-10 09:14:18,724 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 240
2017-08-10 09:14:18,724 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,724 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327510000 ms
2017-08-10 09:14:18,733 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,733 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 122 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,734 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 122 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,734 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,734 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,734 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 122 (MapPartitionsRDD[245] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,736 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_122 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:18,738 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_122_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:18,738 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_122_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,739 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 122 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,739 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 122 (MapPartitionsRDD[245] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,739 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 122.0 with 2 tasks
2017-08-10 09:14:18,739 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 122.0 (TID 244, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,740 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 122.0 (TID 245, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,740 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 122.0 (TID 245)
2017-08-10 09:14:18,740 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 122.0 (TID 244)
2017-08-10 09:14:18,741 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,741 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,744 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 122.0 (TID 245). 714 bytes result sent to driver
2017-08-10 09:14:18,744 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 122.0 (TID 244). 714 bytes result sent to driver
2017-08-10 09:14:18,745 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 122.0 (TID 245) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,745 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 122.0 (TID 244) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,745 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 122.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,745 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 122 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,745 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 122 finished: foreachPartition at streamingProcess.scala:50, took 0.012006 s
2017-08-10 09:14:18,746 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327525000 ms.0 from job set of time 1502327525000 ms
2017-08-10 09:14:18,746 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 133.745 s for time 1502327525000 ms (execution: 0.021 s)
2017-08-10 09:14:18,746 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 243 from persistence list
2017-08-10 09:14:18,746 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327530000 ms.0 from job set of time 1502327530000 ms
2017-08-10 09:14:18,746 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 243
2017-08-10 09:14:18,746 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 242 from persistence list
2017-08-10 09:14:18,746 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 242
2017-08-10 09:14:18,746 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,746 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327515000 ms
2017-08-10 09:14:18,755 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,755 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 123 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,755 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 123 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,755 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,755 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,755 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 123 (MapPartitionsRDD[247] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,758 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_123 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:18,761 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_123_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:18,761 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_123_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,761 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 123 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,761 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 123 (MapPartitionsRDD[247] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,762 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 123.0 with 2 tasks
2017-08-10 09:14:18,762 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 123.0 (TID 246, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,762 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 123.0 (TID 247, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,762 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 123.0 (TID 246)
2017-08-10 09:14:18,762 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 123.0 (TID 247)
2017-08-10 09:14:18,764 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,764 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,766 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 123.0 (TID 246). 714 bytes result sent to driver
2017-08-10 09:14:18,766 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 123.0 (TID 247). 714 bytes result sent to driver
2017-08-10 09:14:18,768 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 123.0 (TID 246) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,768 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 123.0 (TID 247) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,768 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 123.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,768 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 123 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,768 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 123 finished: foreachPartition at streamingProcess.scala:50, took 0.013228 s
2017-08-10 09:14:18,768 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327530000 ms.0 from job set of time 1502327530000 ms
2017-08-10 09:14:18,768 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 128.768 s for time 1502327530000 ms (execution: 0.022 s)
2017-08-10 09:14:18,768 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 245 from persistence list
2017-08-10 09:14:18,768 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327535000 ms.0 from job set of time 1502327535000 ms
2017-08-10 09:14:18,769 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 245
2017-08-10 09:14:18,769 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 244 from persistence list
2017-08-10 09:14:18,769 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 244
2017-08-10 09:14:18,769 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,769 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327520000 ms
2017-08-10 09:14:18,778 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 124 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 124 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 124 (MapPartitionsRDD[249] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,781 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_124 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:18,784 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_124_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:18,784 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_124_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,784 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 124 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,785 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 124 (MapPartitionsRDD[249] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,785 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 124.0 with 2 tasks
2017-08-10 09:14:18,786 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 124.0 (TID 248, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,786 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 124.0 (TID 249, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,786 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 124.0 (TID 249)
2017-08-10 09:14:18,786 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 124.0 (TID 248)
2017-08-10 09:14:18,788 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,788 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,791 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 124.0 (TID 248). 714 bytes result sent to driver
2017-08-10 09:14:18,791 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 124.0 (TID 249). 714 bytes result sent to driver
2017-08-10 09:14:18,792 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 124.0 (TID 249) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,792 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 124.0 (TID 248) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,792 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 124.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,792 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 124 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,792 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 124 finished: foreachPartition at streamingProcess.scala:50, took 0.013748 s
2017-08-10 09:14:18,792 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327535000 ms.0 from job set of time 1502327535000 ms
2017-08-10 09:14:18,793 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 123.792 s for time 1502327535000 ms (execution: 0.024 s)
2017-08-10 09:14:18,793 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 247 from persistence list
2017-08-10 09:14:18,793 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327540000 ms.0 from job set of time 1502327540000 ms
2017-08-10 09:14:18,793 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 247
2017-08-10 09:14:18,793 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 246 from persistence list
2017-08-10 09:14:18,793 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 246
2017-08-10 09:14:18,793 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,793 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327525000 ms
2017-08-10 09:14:18,803 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 125 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 125 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,804 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 125 (MapPartitionsRDD[251] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,806 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_125 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:18,808 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_125_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:18,809 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_125_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,809 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 125 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,809 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 125 (MapPartitionsRDD[251] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,809 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 125.0 with 2 tasks
2017-08-10 09:14:18,810 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 125.0 (TID 250, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,810 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 125.0 (TID 251, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,810 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 125.0 (TID 250)
2017-08-10 09:14:18,810 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 125.0 (TID 251)
2017-08-10 09:14:18,812 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,812 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,814 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 125.0 (TID 251). 714 bytes result sent to driver
2017-08-10 09:14:18,814 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 125.0 (TID 250). 714 bytes result sent to driver
2017-08-10 09:14:18,815 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 125.0 (TID 251) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,816 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 125.0 (TID 250) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,816 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 125.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,816 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 125 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,816 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 125 finished: foreachPartition at streamingProcess.scala:50, took 0.012914 s
2017-08-10 09:14:18,816 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327540000 ms.0 from job set of time 1502327540000 ms
2017-08-10 09:14:18,816 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 118.816 s for time 1502327540000 ms (execution: 0.023 s)
2017-08-10 09:14:18,816 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 249 from persistence list
2017-08-10 09:14:18,816 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327545000 ms.0 from job set of time 1502327545000 ms
2017-08-10 09:14:18,817 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 249
2017-08-10 09:14:18,817 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 248 from persistence list
2017-08-10 09:14:18,817 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,817 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 248
2017-08-10 09:14:18,817 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327530000 ms
2017-08-10 09:14:18,826 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,826 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 126 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,826 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 126 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,826 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,826 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,827 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 126 (MapPartitionsRDD[253] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,829 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_126 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:18,831 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_126_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:18,831 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_126_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,832 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 126 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,832 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 126 (MapPartitionsRDD[253] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,832 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 126.0 with 2 tasks
2017-08-10 09:14:18,832 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 126.0 (TID 252, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,833 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 126.0 (TID 253, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,833 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 126.0 (TID 252)
2017-08-10 09:14:18,833 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 126.0 (TID 253)
2017-08-10 09:14:18,835 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,835 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,838 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 126.0 (TID 253). 714 bytes result sent to driver
2017-08-10 09:14:18,838 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 126.0 (TID 252). 714 bytes result sent to driver
2017-08-10 09:14:18,839 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 126.0 (TID 253) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,839 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 126.0 (TID 252) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,839 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 126.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,839 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 126 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,840 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 126 finished: foreachPartition at streamingProcess.scala:50, took 0.014066 s
2017-08-10 09:14:18,841 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327545000 ms.0 from job set of time 1502327545000 ms
2017-08-10 09:14:18,841 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 113.841 s for time 1502327545000 ms (execution: 0.025 s)
2017-08-10 09:14:18,841 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327550000 ms.0 from job set of time 1502327550000 ms
2017-08-10 09:14:18,842 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 251 from persistence list
2017-08-10 09:14:18,842 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 251
2017-08-10 09:14:18,842 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 250 from persistence list
2017-08-10 09:14:18,843 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 250
2017-08-10 09:14:18,843 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,843 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327535000 ms
2017-08-10 09:14:18,852 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,853 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 127 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,853 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 127 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,853 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,853 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,853 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 127 (MapPartitionsRDD[255] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,855 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_127 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:18,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_127_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:18,858 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_127_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 127 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 127 (MapPartitionsRDD[255] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 127.0 with 2 tasks
2017-08-10 09:14:18,859 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 127.0 (TID 254, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,859 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 127.0 (TID 255, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,859 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 127.0 (TID 254)
2017-08-10 09:14:18,859 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 127.0 (TID 255)
2017-08-10 09:14:18,861 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,861 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,863 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 127.0 (TID 254). 714 bytes result sent to driver
2017-08-10 09:14:18,863 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 127.0 (TID 255). 714 bytes result sent to driver
2017-08-10 09:14:18,864 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 127.0 (TID 254) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,865 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 127.0 (TID 255) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,865 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 127.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,865 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 127 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,865 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 127 finished: foreachPartition at streamingProcess.scala:50, took 0.012651 s
2017-08-10 09:14:18,865 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327550000 ms.0 from job set of time 1502327550000 ms
2017-08-10 09:14:18,865 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 108.865 s for time 1502327550000 ms (execution: 0.024 s)
2017-08-10 09:14:18,865 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 253 from persistence list
2017-08-10 09:14:18,865 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327555000 ms.0 from job set of time 1502327555000 ms
2017-08-10 09:14:18,866 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 253
2017-08-10 09:14:18,866 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 252 from persistence list
2017-08-10 09:14:18,866 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 252
2017-08-10 09:14:18,866 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,866 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327540000 ms
2017-08-10 09:14:18,875 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 128 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 128 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 128 (MapPartitionsRDD[257] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,878 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_128 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:18,880 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_128_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:18,881 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_128_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 128 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 128 (MapPartitionsRDD[257] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 128.0 with 2 tasks
2017-08-10 09:14:18,882 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 128.0 (TID 256, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,882 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 128.0 (TID 257, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,882 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 128.0 (TID 256)
2017-08-10 09:14:18,882 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 128.0 (TID 257)
2017-08-10 09:14:18,884 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,884 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,887 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 128.0 (TID 256). 714 bytes result sent to driver
2017-08-10 09:14:18,887 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 128.0 (TID 257). 714 bytes result sent to driver
2017-08-10 09:14:18,888 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 128.0 (TID 256) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,888 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 128.0 (TID 257) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,888 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 128.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,888 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 128 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,889 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 128 finished: foreachPartition at streamingProcess.scala:50, took 0.013632 s
2017-08-10 09:14:18,889 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327555000 ms.0 from job set of time 1502327555000 ms
2017-08-10 09:14:18,889 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 103.889 s for time 1502327555000 ms (execution: 0.024 s)
2017-08-10 09:14:18,889 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 255 from persistence list
2017-08-10 09:14:18,889 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327560000 ms.0 from job set of time 1502327560000 ms
2017-08-10 09:14:18,889 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 255
2017-08-10 09:14:18,889 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 254 from persistence list
2017-08-10 09:14:18,889 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 254
2017-08-10 09:14:18,889 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,890 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327545000 ms
2017-08-10 09:14:18,898 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,898 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 129 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,898 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 129 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,898 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,898 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,898 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 129 (MapPartitionsRDD[259] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,901 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_129 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:18,905 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_129_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:18,905 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_129_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,906 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 129 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,906 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 129 (MapPartitionsRDD[259] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,906 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 129.0 with 2 tasks
2017-08-10 09:14:18,907 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 129.0 (TID 258, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,908 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 129.0 (TID 259, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,908 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 129.0 (TID 258)
2017-08-10 09:14:18,908 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 129.0 (TID 259)
2017-08-10 09:14:18,910 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,910 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,912 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 129.0 (TID 259). 714 bytes result sent to driver
2017-08-10 09:14:18,912 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 129.0 (TID 258). 714 bytes result sent to driver
2017-08-10 09:14:18,919 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 129.0 (TID 259) in 11 ms on localhost (1/2)
2017-08-10 09:14:18,919 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 129.0 (TID 258) in 12 ms on localhost (2/2)
2017-08-10 09:14:18,920 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 129.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,920 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 129 (foreachPartition at streamingProcess.scala:50) finished in 0.014 s
2017-08-10 09:14:18,920 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 129 finished: foreachPartition at streamingProcess.scala:50, took 0.022277 s
2017-08-10 09:14:18,921 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327560000 ms.0 from job set of time 1502327560000 ms
2017-08-10 09:14:18,921 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 98.921 s for time 1502327560000 ms (execution: 0.032 s)
2017-08-10 09:14:18,921 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327565000 ms.0 from job set of time 1502327565000 ms
2017-08-10 09:14:18,921 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 257 from persistence list
2017-08-10 09:14:18,922 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 257
2017-08-10 09:14:18,922 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 256 from persistence list
2017-08-10 09:14:18,922 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 256
2017-08-10 09:14:18,922 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,922 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327550000 ms
2017-08-10 09:14:18,933 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,933 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 130 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,934 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 130 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,934 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,934 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,934 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 130 (MapPartitionsRDD[261] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,937 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_130 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:18,940 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_130_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:18,940 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_130_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,941 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 130 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,941 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 130 (MapPartitionsRDD[261] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,941 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 130.0 with 2 tasks
2017-08-10 09:14:18,942 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 130.0 (TID 260, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,942 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 130.0 (TID 261, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,942 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 130.0 (TID 260)
2017-08-10 09:14:18,942 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 130.0 (TID 261)
2017-08-10 09:14:18,944 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,944 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,947 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 130.0 (TID 261). 714 bytes result sent to driver
2017-08-10 09:14:18,947 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 130.0 (TID 260). 714 bytes result sent to driver
2017-08-10 09:14:18,948 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 130.0 (TID 261) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,948 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 130.0 (TID 260) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,948 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 130.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,948 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 130 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,949 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 130 finished: foreachPartition at streamingProcess.scala:50, took 0.015467 s
2017-08-10 09:14:18,949 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327565000 ms.0 from job set of time 1502327565000 ms
2017-08-10 09:14:18,949 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 93.949 s for time 1502327565000 ms (execution: 0.028 s)
2017-08-10 09:14:18,958 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 259 from persistence list
2017-08-10 09:14:18,958 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327570000 ms.0 from job set of time 1502327570000 ms
2017-08-10 09:14:18,958 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 259
2017-08-10 09:14:18,958 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 258 from persistence list
2017-08-10 09:14:18,959 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 258
2017-08-10 09:14:18,959 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,959 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327555000 ms
2017-08-10 09:14:18,959 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_130_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,959 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_115_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,962 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_116_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,964 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_117_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,965 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_118_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,966 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_119_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,967 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_120_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,968 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_121_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,969 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_122_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,970 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,970 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_123_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 131 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 131 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 131 (MapPartitionsRDD[263] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,970 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_124_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,971 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_125_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,972 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_126_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,972 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_127_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,972 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_131 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:18,975 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_128_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,976 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_131_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:18,976 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_129_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,976 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_131_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,977 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 131 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,977 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 131 (MapPartitionsRDD[263] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,977 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 131.0 with 2 tasks
2017-08-10 09:14:18,978 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 131.0 (TID 262, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,978 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 131.0 (TID 263, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,978 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 131.0 (TID 262)
2017-08-10 09:14:18,978 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 131.0 (TID 263)
2017-08-10 09:14:18,980 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,980 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,983 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 131.0 (TID 262). 714 bytes result sent to driver
2017-08-10 09:14:18,983 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 131.0 (TID 263). 714 bytes result sent to driver
2017-08-10 09:14:18,984 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 131.0 (TID 263) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,984 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 131.0 (TID 262) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,984 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 131.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,984 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 131 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,985 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 131 finished: foreachPartition at streamingProcess.scala:50, took 0.014939 s
2017-08-10 09:14:18,985 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327570000 ms.0 from job set of time 1502327570000 ms
2017-08-10 09:14:18,985 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 88.985 s for time 1502327570000 ms (execution: 0.027 s)
2017-08-10 09:14:18,985 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 261 from persistence list
2017-08-10 09:14:18,985 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327575000 ms.0 from job set of time 1502327575000 ms
2017-08-10 09:14:18,986 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 261
2017-08-10 09:14:18,986 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 260 from persistence list
2017-08-10 09:14:18,986 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,986 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327560000 ms
2017-08-10 09:14:18,986 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 260
2017-08-10 09:14:18,995 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,995 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 132 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,995 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 132 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,995 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,995 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,996 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 132 (MapPartitionsRDD[265] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,998 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_132 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:19,000 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_132_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:19,000 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_132_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:19,000 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 132 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,000 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 132 (MapPartitionsRDD[265] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,000 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 132.0 with 2 tasks
2017-08-10 09:14:19,001 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 132.0 (TID 264, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,001 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 132.0 (TID 265, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,002 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 132.0 (TID 264)
2017-08-10 09:14:19,002 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 132.0 (TID 265)
2017-08-10 09:14:19,003 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,003 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,006 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 132.0 (TID 265). 714 bytes result sent to driver
2017-08-10 09:14:19,006 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 132.0 (TID 264). 714 bytes result sent to driver
2017-08-10 09:14:19,007 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 132.0 (TID 265) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,007 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 132.0 (TID 264) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,007 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 132.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,007 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 132 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:19,007 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 132 finished: foreachPartition at streamingProcess.scala:50, took 0.012014 s
2017-08-10 09:14:19,007 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327575000 ms.0 from job set of time 1502327575000 ms
2017-08-10 09:14:19,007 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 84.007 s for time 1502327575000 ms (execution: 0.022 s)
2017-08-10 09:14:19,007 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 263 from persistence list
2017-08-10 09:14:19,008 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327580000 ms.0 from job set of time 1502327580000 ms
2017-08-10 09:14:19,008 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 263
2017-08-10 09:14:19,008 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 262 from persistence list
2017-08-10 09:14:19,008 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 262
2017-08-10 09:14:19,008 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,008 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327565000 ms
2017-08-10 09:14:19,017 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 133 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 133 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 133 (MapPartitionsRDD[267] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_133 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:19,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_133_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:19,022 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_133_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 133 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 133 (MapPartitionsRDD[267] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 133.0 with 2 tasks
2017-08-10 09:14:19,023 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 133.0 (TID 266, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,024 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 133.0 (TID 267, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,024 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 133.0 (TID 267)
2017-08-10 09:14:19,024 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 133.0 (TID 266)
2017-08-10 09:14:19,025 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,025 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,028 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 133.0 (TID 266). 714 bytes result sent to driver
2017-08-10 09:14:19,028 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 133.0 (TID 267). 714 bytes result sent to driver
2017-08-10 09:14:19,029 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 133.0 (TID 266) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,029 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 133.0 (TID 267) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,029 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 133.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 133 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:19,029 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 133 finished: foreachPartition at streamingProcess.scala:50, took 0.012429 s
2017-08-10 09:14:19,029 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327580000 ms.0 from job set of time 1502327580000 ms
2017-08-10 09:14:19,030 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 79.029 s for time 1502327580000 ms (execution: 0.021 s)
2017-08-10 09:14:19,030 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 265 from persistence list
2017-08-10 09:14:19,030 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327585000 ms.0 from job set of time 1502327585000 ms
2017-08-10 09:14:19,030 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 265
2017-08-10 09:14:19,030 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 264 from persistence list
2017-08-10 09:14:19,030 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 264
2017-08-10 09:14:19,030 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,030 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327570000 ms
2017-08-10 09:14:19,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 134 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 134 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 134 (MapPartitionsRDD[269] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_134 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:19,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_134_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:19,045 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_134_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 134 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 134 (MapPartitionsRDD[269] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 134.0 with 2 tasks
2017-08-10 09:14:19,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 134.0 (TID 268, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 134.0 (TID 269, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,046 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 134.0 (TID 268)
2017-08-10 09:14:19,046 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 134.0 (TID 269)
2017-08-10 09:14:19,047 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,047 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,050 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 134.0 (TID 269). 714 bytes result sent to driver
2017-08-10 09:14:19,050 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 134.0 (TID 268). 714 bytes result sent to driver
2017-08-10 09:14:19,051 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 134.0 (TID 268) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 134.0 (TID 269) in 5 ms on localhost (2/2)
2017-08-10 09:14:19,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 134.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 134 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 134 finished: foreachPartition at streamingProcess.scala:50, took 0.012598 s
2017-08-10 09:14:19,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327585000 ms.0 from job set of time 1502327585000 ms
2017-08-10 09:14:19,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 74.052 s for time 1502327585000 ms (execution: 0.022 s)
2017-08-10 09:14:19,052 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 267 from persistence list
2017-08-10 09:14:19,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327590000 ms.0 from job set of time 1502327590000 ms
2017-08-10 09:14:19,052 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 267
2017-08-10 09:14:19,053 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 266 from persistence list
2017-08-10 09:14:19,053 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 266
2017-08-10 09:14:19,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327575000 ms
2017-08-10 09:14:19,062 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 135 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 135 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 135 (MapPartitionsRDD[271] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_135 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:19,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_135_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:19,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_135_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 135 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 135 (MapPartitionsRDD[271] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 135.0 with 2 tasks
2017-08-10 09:14:19,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 135.0 (TID 270, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 135.0 (TID 271, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,068 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 135.0 (TID 270)
2017-08-10 09:14:19,068 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 135.0 (TID 271)
2017-08-10 09:14:19,070 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,070 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 135.0 (TID 270). 714 bytes result sent to driver
2017-08-10 09:14:19,072 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 135.0 (TID 271). 714 bytes result sent to driver
2017-08-10 09:14:19,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 135.0 (TID 271) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 135.0 (TID 270) in 7 ms on localhost (2/2)
2017-08-10 09:14:19,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 135.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 135 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 135 finished: foreachPartition at streamingProcess.scala:50, took 0.012383 s
2017-08-10 09:14:19,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327590000 ms.0 from job set of time 1502327590000 ms
2017-08-10 09:14:19,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 69.074 s for time 1502327590000 ms (execution: 0.022 s)
2017-08-10 09:14:19,074 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 269 from persistence list
2017-08-10 09:14:19,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327595000 ms.0 from job set of time 1502327595000 ms
2017-08-10 09:14:19,075 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 269
2017-08-10 09:14:19,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 268 from persistence list
2017-08-10 09:14:19,075 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 268
2017-08-10 09:14:19,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327580000 ms
2017-08-10 09:14:19,085 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 136 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 136 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 136 (MapPartitionsRDD[273] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_136 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:19,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_136_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:19,091 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_136_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 136 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 136 (MapPartitionsRDD[273] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 136.0 with 2 tasks
2017-08-10 09:14:19,092 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 136.0 (TID 272, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,093 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 136.0 (TID 273, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,093 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 136.0 (TID 272)
2017-08-10 09:14:19,093 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 136.0 (TID 273)
2017-08-10 09:14:19,094 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,094 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,097 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 136.0 (TID 272). 714 bytes result sent to driver
2017-08-10 09:14:19,097 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 136.0 (TID 273). 714 bytes result sent to driver
2017-08-10 09:14:19,099 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 136.0 (TID 272) in 7 ms on localhost (1/2)
2017-08-10 09:14:19,099 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 136.0 (TID 273) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,099 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 136.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,099 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 136 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,099 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 136 finished: foreachPartition at streamingProcess.scala:50, took 0.013744 s
2017-08-10 09:14:19,099 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327595000 ms.0 from job set of time 1502327595000 ms
2017-08-10 09:14:19,100 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 64.099 s for time 1502327595000 ms (execution: 0.025 s)
2017-08-10 09:14:19,100 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 271 from persistence list
2017-08-10 09:14:19,100 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327600000 ms.0 from job set of time 1502327600000 ms
2017-08-10 09:14:19,100 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 271
2017-08-10 09:14:19,100 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 270 from persistence list
2017-08-10 09:14:19,100 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 270
2017-08-10 09:14:19,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327585000 ms
2017-08-10 09:14:19,109 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 137 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 137 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 137 (MapPartitionsRDD[275] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,112 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_137 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:19,114 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_137_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:19,115 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_137_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 137 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 137 (MapPartitionsRDD[275] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 137.0 with 2 tasks
2017-08-10 09:14:19,115 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 137.0 (TID 274, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,116 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 137.0 (TID 275, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,116 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 137.0 (TID 275)
2017-08-10 09:14:19,116 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 137.0 (TID 274)
2017-08-10 09:14:19,117 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,117 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,120 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 137.0 (TID 274). 714 bytes result sent to driver
2017-08-10 09:14:19,120 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 137.0 (TID 275). 714 bytes result sent to driver
2017-08-10 09:14:19,122 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 137.0 (TID 274) in 7 ms on localhost (1/2)
2017-08-10 09:14:19,122 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 137.0 (TID 275) in 7 ms on localhost (2/2)
2017-08-10 09:14:19,122 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 137.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,122 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 137 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,122 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 137 finished: foreachPartition at streamingProcess.scala:50, took 0.013005 s
2017-08-10 09:14:19,123 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327600000 ms.0 from job set of time 1502327600000 ms
2017-08-10 09:14:19,123 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 59.123 s for time 1502327600000 ms (execution: 0.023 s)
2017-08-10 09:14:19,123 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 273 from persistence list
2017-08-10 09:14:19,123 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327605000 ms.0 from job set of time 1502327605000 ms
2017-08-10 09:14:19,123 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 273
2017-08-10 09:14:19,123 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 272 from persistence list
2017-08-10 09:14:19,124 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,124 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327590000 ms
2017-08-10 09:14:19,124 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 272
2017-08-10 09:14:19,132 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,133 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 138 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,133 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 138 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,133 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,133 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,133 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 138 (MapPartitionsRDD[277] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,135 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_138 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:19,138 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_138_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:19,139 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_138_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,139 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 138 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,139 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 138 (MapPartitionsRDD[277] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,139 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 138.0 with 2 tasks
2017-08-10 09:14:19,139 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 138.0 (TID 276, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,140 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 138.0 (TID 277, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,140 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 138.0 (TID 276)
2017-08-10 09:14:19,140 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 138.0 (TID 277)
2017-08-10 09:14:19,141 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,141 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,144 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 138.0 (TID 276). 714 bytes result sent to driver
2017-08-10 09:14:19,144 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 138.0 (TID 277). 714 bytes result sent to driver
2017-08-10 09:14:19,145 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 138.0 (TID 277) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,146 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 138.0 (TID 276) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,146 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 138.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,146 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 138 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,146 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 138 finished: foreachPartition at streamingProcess.scala:50, took 0.013479 s
2017-08-10 09:14:19,146 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327605000 ms.0 from job set of time 1502327605000 ms
2017-08-10 09:14:19,146 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 54.146 s for time 1502327605000 ms (execution: 0.023 s)
2017-08-10 09:14:19,146 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 275 from persistence list
2017-08-10 09:14:19,146 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327610000 ms.0 from job set of time 1502327610000 ms
2017-08-10 09:14:19,147 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 275
2017-08-10 09:14:19,147 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 274 from persistence list
2017-08-10 09:14:19,147 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 274
2017-08-10 09:14:19,147 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,147 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327595000 ms
2017-08-10 09:14:19,156 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,156 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 139 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,156 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 139 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,156 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,156 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,156 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 139 (MapPartitionsRDD[279] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_139 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:19,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_139_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:19,162 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_139_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 139 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 139 (MapPartitionsRDD[279] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 139.0 with 2 tasks
2017-08-10 09:14:19,163 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 139.0 (TID 278, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,163 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 139.0 (TID 279, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,164 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 139.0 (TID 278)
2017-08-10 09:14:19,164 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 139.0 (TID 279)
2017-08-10 09:14:19,165 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,165 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,169 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 139.0 (TID 279). 714 bytes result sent to driver
2017-08-10 09:14:19,169 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 139.0 (TID 278). 714 bytes result sent to driver
2017-08-10 09:14:19,170 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 139.0 (TID 279) in 7 ms on localhost (1/2)
2017-08-10 09:14:19,170 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 139.0 (TID 278) in 7 ms on localhost (2/2)
2017-08-10 09:14:19,170 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 139.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 139 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,170 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 139 finished: foreachPartition at streamingProcess.scala:50, took 0.014583 s
2017-08-10 09:14:19,170 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327610000 ms.0 from job set of time 1502327610000 ms
2017-08-10 09:14:19,171 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 49.170 s for time 1502327610000 ms (execution: 0.024 s)
2017-08-10 09:14:19,171 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327615000 ms.0 from job set of time 1502327615000 ms
2017-08-10 09:14:19,171 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 277 from persistence list
2017-08-10 09:14:19,171 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 277
2017-08-10 09:14:19,171 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 276 from persistence list
2017-08-10 09:14:19,171 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 276
2017-08-10 09:14:19,171 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,171 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327600000 ms
2017-08-10 09:14:19,180 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 140 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 140 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 140 (MapPartitionsRDD[281] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,183 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_140 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:19,186 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_140_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:19,187 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_140_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 140 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 140 (MapPartitionsRDD[281] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 140.0 with 2 tasks
2017-08-10 09:14:19,187 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 140.0 (TID 280, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,188 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 140.0 (TID 281, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,188 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 140.0 (TID 281)
2017-08-10 09:14:19,188 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 140.0 (TID 280)
2017-08-10 09:14:19,189 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,189 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,192 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 140.0 (TID 281). 714 bytes result sent to driver
2017-08-10 09:14:19,192 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 140.0 (TID 280). 714 bytes result sent to driver
2017-08-10 09:14:19,193 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 140.0 (TID 281) in 5 ms on localhost (1/2)
2017-08-10 09:14:19,193 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 140.0 (TID 280) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,194 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 140.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,194 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 140 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,194 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 140 finished: foreachPartition at streamingProcess.scala:50, took 0.013599 s
2017-08-10 09:14:19,194 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327615000 ms.0 from job set of time 1502327615000 ms
2017-08-10 09:14:19,194 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 44.194 s for time 1502327615000 ms (execution: 0.023 s)
2017-08-10 09:14:19,194 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 279 from persistence list
2017-08-10 09:14:19,195 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327620000 ms.0 from job set of time 1502327620000 ms
2017-08-10 09:14:19,195 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 278 from persistence list
2017-08-10 09:14:19,195 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 279
2017-08-10 09:14:19,195 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,195 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 278
2017-08-10 09:14:19,195 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327605000 ms
2017-08-10 09:14:19,207 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,208 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 141 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,208 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 141 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,208 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,208 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,208 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 141 (MapPartitionsRDD[283] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,211 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_141 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:19,214 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_141_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:19,215 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_141_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,215 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 141 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,215 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 141 (MapPartitionsRDD[283] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,215 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 141.0 with 2 tasks
2017-08-10 09:14:19,216 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 141.0 (TID 282, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,216 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 141.0 (TID 283, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,216 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 141.0 (TID 282)
2017-08-10 09:14:19,217 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 141.0 (TID 283)
2017-08-10 09:14:19,218 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,219 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,222 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 141.0 (TID 282). 714 bytes result sent to driver
2017-08-10 09:14:19,222 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 141.0 (TID 283). 714 bytes result sent to driver
2017-08-10 09:14:19,223 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 141.0 (TID 283) in 7 ms on localhost (1/2)
2017-08-10 09:14:19,223 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 141.0 (TID 282) in 7 ms on localhost (2/2)
2017-08-10 09:14:19,223 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 141.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,223 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 141 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:19,223 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 141 finished: foreachPartition at streamingProcess.scala:50, took 0.016274 s
2017-08-10 09:14:19,224 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327620000 ms.0 from job set of time 1502327620000 ms
2017-08-10 09:14:19,224 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 39.224 s for time 1502327620000 ms (execution: 0.029 s)
2017-08-10 09:14:19,224 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327625000 ms.0 from job set of time 1502327625000 ms
2017-08-10 09:14:19,224 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 281 from persistence list
2017-08-10 09:14:19,225 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 281
2017-08-10 09:14:19,225 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 280 from persistence list
2017-08-10 09:14:19,225 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,225 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327610000 ms
2017-08-10 09:14:19,225 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 280
2017-08-10 09:14:19,235 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,235 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 142 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,236 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 142 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,236 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,236 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,236 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 142 (MapPartitionsRDD[285] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,239 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_142 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:19,241 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_142_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:19,241 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_142_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 142 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 142 (MapPartitionsRDD[285] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 142.0 with 2 tasks
2017-08-10 09:14:19,242 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 142.0 (TID 284, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,243 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 142.0 (TID 285, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,243 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 142.0 (TID 284)
2017-08-10 09:14:19,243 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 142.0 (TID 285)
2017-08-10 09:14:19,244 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,244 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,247 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 142.0 (TID 285). 714 bytes result sent to driver
2017-08-10 09:14:19,247 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 142.0 (TID 284). 714 bytes result sent to driver
2017-08-10 09:14:19,249 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 142.0 (TID 285) in 7 ms on localhost (1/2)
2017-08-10 09:14:19,249 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 142.0 (TID 284) in 7 ms on localhost (2/2)
2017-08-10 09:14:19,249 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 142.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,249 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 142 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,249 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 142 finished: foreachPartition at streamingProcess.scala:50, took 0.014097 s
2017-08-10 09:14:19,249 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327625000 ms.0 from job set of time 1502327625000 ms
2017-08-10 09:14:19,250 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 34.249 s for time 1502327625000 ms (execution: 0.025 s)
2017-08-10 09:14:19,250 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 283 from persistence list
2017-08-10 09:14:19,250 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327630000 ms.0 from job set of time 1502327630000 ms
2017-08-10 09:14:19,250 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 283
2017-08-10 09:14:19,250 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 282 from persistence list
2017-08-10 09:14:19,250 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 282
2017-08-10 09:14:19,250 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,250 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327615000 ms
2017-08-10 09:14:19,260 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 143 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 143 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 143 (MapPartitionsRDD[287] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,262 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_143 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:19,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_143_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:19,265 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_143_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 143 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 143 (MapPartitionsRDD[287] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 143.0 with 2 tasks
2017-08-10 09:14:19,266 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 143.0 (TID 286, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,266 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 143.0 (TID 287, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,266 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 143.0 (TID 286)
2017-08-10 09:14:19,266 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 143.0 (TID 287)
2017-08-10 09:14:19,268 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,268 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,271 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 143.0 (TID 286). 714 bytes result sent to driver
2017-08-10 09:14:19,271 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 143.0 (TID 287). 714 bytes result sent to driver
2017-08-10 09:14:19,272 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 143.0 (TID 286) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,272 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 143.0 (TID 287) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,272 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 143.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,272 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 143 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:19,272 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 143 finished: foreachPartition at streamingProcess.scala:50, took 0.012471 s
2017-08-10 09:14:19,273 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327630000 ms.0 from job set of time 1502327630000 ms
2017-08-10 09:14:19,273 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 29.273 s for time 1502327630000 ms (execution: 0.023 s)
2017-08-10 09:14:19,273 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327635000 ms.0 from job set of time 1502327635000 ms
2017-08-10 09:14:19,273 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 285 from persistence list
2017-08-10 09:14:19,273 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 285
2017-08-10 09:14:19,273 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 284 from persistence list
2017-08-10 09:14:19,273 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 284
2017-08-10 09:14:19,273 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,273 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327620000 ms
2017-08-10 09:14:19,282 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 144 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 144 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 144 (MapPartitionsRDD[289] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_144 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:19,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_144_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:19,289 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_144_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,289 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 144 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,289 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 144 (MapPartitionsRDD[289] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,289 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 144.0 with 2 tasks
2017-08-10 09:14:19,290 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 144.0 (TID 288, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,290 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 144.0 (TID 289, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,290 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 144.0 (TID 289)
2017-08-10 09:14:19,290 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 144.0 (TID 288)
2017-08-10 09:14:19,292 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,292 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,294 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 144.0 (TID 289). 714 bytes result sent to driver
2017-08-10 09:14:19,294 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 144.0 (TID 288). 714 bytes result sent to driver
2017-08-10 09:14:19,295 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 144.0 (TID 288) in 5 ms on localhost (1/2)
2017-08-10 09:14:19,295 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 144.0 (TID 289) in 5 ms on localhost (2/2)
2017-08-10 09:14:19,295 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 144.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,295 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 144 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:19,296 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 144 finished: foreachPartition at streamingProcess.scala:50, took 0.013192 s
2017-08-10 09:14:19,296 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327635000 ms.0 from job set of time 1502327635000 ms
2017-08-10 09:14:19,296 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 24.296 s for time 1502327635000 ms (execution: 0.023 s)
2017-08-10 09:14:19,296 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 287 from persistence list
2017-08-10 09:14:19,296 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327640000 ms.0 from job set of time 1502327640000 ms
2017-08-10 09:14:19,296 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 287
2017-08-10 09:14:19,296 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 286 from persistence list
2017-08-10 09:14:19,296 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 286
2017-08-10 09:14:19,297 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,297 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327625000 ms
2017-08-10 09:14:19,306 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 145 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 145 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,307 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,307 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 145 (MapPartitionsRDD[291] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,309 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_145 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:19,311 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_145_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:19,311 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_145_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:19,311 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 145 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,312 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 145 (MapPartitionsRDD[291] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,312 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 145.0 with 2 tasks
2017-08-10 09:14:19,312 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 145.0 (TID 290, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,312 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 145.0 (TID 291, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,312 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 145.0 (TID 290)
2017-08-10 09:14:19,312 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 145.0 (TID 291)
2017-08-10 09:14:19,314 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,314 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,316 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 145.0 (TID 290). 714 bytes result sent to driver
2017-08-10 09:14:19,316 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 145.0 (TID 291). 714 bytes result sent to driver
2017-08-10 09:14:19,318 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 145.0 (TID 290) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,318 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 145.0 (TID 291) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,318 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 145.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,318 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 145 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:19,318 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 145 finished: foreachPartition at streamingProcess.scala:50, took 0.011996 s
2017-08-10 09:14:19,318 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327640000 ms.0 from job set of time 1502327640000 ms
2017-08-10 09:14:19,319 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 19.318 s for time 1502327640000 ms (execution: 0.022 s)
2017-08-10 09:14:19,319 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 289 from persistence list
2017-08-10 09:14:19,319 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327645000 ms.0 from job set of time 1502327645000 ms
2017-08-10 09:14:19,319 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 288 from persistence list
2017-08-10 09:14:19,319 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,319 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327630000 ms
2017-08-10 09:14:19,319 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 289
2017-08-10 09:14:19,319 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 288
2017-08-10 09:14:19,328 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,329 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 146 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,329 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 146 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,329 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,329 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,329 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 146 (MapPartitionsRDD[293] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,332 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_146 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:19,334 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_146_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:19,334 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_146_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:19,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 146 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 146 (MapPartitionsRDD[293] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 146.0 with 2 tasks
2017-08-10 09:14:19,335 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 146.0 (TID 292, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,336 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 146.0 (TID 293, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,336 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 146.0 (TID 292)
2017-08-10 09:14:19,336 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 146.0 (TID 293)
2017-08-10 09:14:19,346 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,346 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,346 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_131_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:19,347 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_132_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,348 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_133_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,349 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_134_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,349 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 146.0 (TID 292). 787 bytes result sent to driver
2017-08-10 09:14:19,349 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 146.0 (TID 293). 787 bytes result sent to driver
2017-08-10 09:14:19,349 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_135_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,350 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_136_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,350 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 146.0 (TID 292) in 15 ms on localhost (1/2)
2017-08-10 09:14:19,350 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 146.0 (TID 293) in 15 ms on localhost (2/2)
2017-08-10 09:14:19,350 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 146.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,350 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 146 (foreachPartition at streamingProcess.scala:50) finished in 0.015 s
2017-08-10 09:14:19,351 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 146 finished: foreachPartition at streamingProcess.scala:50, took 0.022032 s
2017-08-10 09:14:19,351 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_137_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,351 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327645000 ms.0 from job set of time 1502327645000 ms
2017-08-10 09:14:19,351 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 14.351 s for time 1502327645000 ms (execution: 0.032 s)
2017-08-10 09:14:19,351 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327650000 ms.0 from job set of time 1502327650000 ms
2017-08-10 09:14:19,352 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 291 from persistence list
2017-08-10 09:14:19,352 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_138_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,352 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 290 from persistence list
2017-08-10 09:14:19,352 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 290
2017-08-10 09:14:19,352 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 291
2017-08-10 09:14:19,352 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_139_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,353 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,353 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327635000 ms
2017-08-10 09:14:19,353 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_140_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,354 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_141_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,355 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_142_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,355 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_143_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,356 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_144_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:19,356 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_145_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:19,362 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 147 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 147 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 147 (MapPartitionsRDD[295] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_147 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:19,368 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_147_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:19,368 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_147_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:19,368 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 147 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,368 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 147 (MapPartitionsRDD[295] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,368 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 147.0 with 2 tasks
2017-08-10 09:14:19,369 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 147.0 (TID 294, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,370 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 147.0 (TID 295, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,370 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 147.0 (TID 294)
2017-08-10 09:14:19,370 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 147.0 (TID 295)
2017-08-10 09:14:19,371 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,371 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,374 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 147.0 (TID 295). 714 bytes result sent to driver
2017-08-10 09:14:19,374 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 147.0 (TID 294). 714 bytes result sent to driver
2017-08-10 09:14:19,375 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 147.0 (TID 295) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,375 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 147.0 (TID 294) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,375 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 147.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,375 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 147 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:19,376 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 147 finished: foreachPartition at streamingProcess.scala:50, took 0.013337 s
2017-08-10 09:14:19,376 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327650000 ms.0 from job set of time 1502327650000 ms
2017-08-10 09:14:19,376 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 9.376 s for time 1502327650000 ms (execution: 0.025 s)
2017-08-10 09:14:19,376 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 293 from persistence list
2017-08-10 09:14:19,376 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327655000 ms.0 from job set of time 1502327655000 ms
2017-08-10 09:14:19,376 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 293
2017-08-10 09:14:19,376 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 292 from persistence list
2017-08-10 09:14:19,376 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 292
2017-08-10 09:14:19,376 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,376 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327640000 ms
2017-08-10 09:14:19,385 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 148 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 148 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 148 (MapPartitionsRDD[297] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,388 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_148 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:19,390 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_148_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:19,391 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_148_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,391 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 148 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,391 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 148 (MapPartitionsRDD[297] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,391 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 148.0 with 2 tasks
2017-08-10 09:14:19,392 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 148.0 (TID 296, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,392 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 148.0 (TID 297, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,392 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 148.0 (TID 297)
2017-08-10 09:14:19,392 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 148.0 (TID 296)
2017-08-10 09:14:19,394 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,394 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,397 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 148.0 (TID 296). 714 bytes result sent to driver
2017-08-10 09:14:19,397 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 148.0 (TID 297). 714 bytes result sent to driver
2017-08-10 09:14:19,398 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 148.0 (TID 297) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,398 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 148.0 (TID 296) in 7 ms on localhost (2/2)
2017-08-10 09:14:19,398 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 148.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,398 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 148 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,398 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 148 finished: foreachPartition at streamingProcess.scala:50, took 0.012714 s
2017-08-10 09:14:19,398 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327655000 ms.0 from job set of time 1502327655000 ms
2017-08-10 09:14:19,399 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 4.398 s for time 1502327655000 ms (execution: 0.022 s)
2017-08-10 09:14:19,399 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 295 from persistence list
2017-08-10 09:14:19,399 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 295
2017-08-10 09:14:19,399 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 294 from persistence list
2017-08-10 09:14:19,399 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 294
2017-08-10 09:14:19,399 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,399 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327645000 ms
2017-08-10 09:14:20,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327660000 ms
2017-08-10 09:14:20,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327660000 ms.0 from job set of time 1502327660000 ms
2017-08-10 09:14:20,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 149 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 149 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:20,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 149 (MapPartitionsRDD[299] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:20,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_149 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:20,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_149_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:20,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_149_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:20,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 149 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:20,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 149 (MapPartitionsRDD[299] at map at streamingProcess.scala:48)
2017-08-10 09:14:20,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 149.0 with 2 tasks
2017-08-10 09:14:20,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 149.0 (TID 298, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:20,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 149.0 (TID 299, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:20,069 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 149.0 (TID 299)
2017-08-10 09:14:20,069 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 149.0 (TID 298)
2017-08-10 09:14:20,072 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:20,072 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:20,075 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 149.0 (TID 298). 714 bytes result sent to driver
2017-08-10 09:14:20,075 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 149.0 (TID 299). 714 bytes result sent to driver
2017-08-10 09:14:20,077 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 149.0 (TID 298) in 9 ms on localhost (1/2)
2017-08-10 09:14:20,077 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 149.0 (TID 299) in 8 ms on localhost (2/2)
2017-08-10 09:14:20,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 149.0, whose tasks have all completed, from pool 
2017-08-10 09:14:20,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 149 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:14:20,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 149 finished: foreachPartition at streamingProcess.scala:50, took 0.024603 s
2017-08-10 09:14:20,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327660000 ms.0 from job set of time 1502327660000 ms
2017-08-10 09:14:20,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1502327660000 ms (execution: 0.057 s)
2017-08-10 09:14:20,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 297 from persistence list
2017-08-10 09:14:20,079 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 297
2017-08-10 09:14:20,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 296 from persistence list
2017-08-10 09:14:20,079 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 296
2017-08-10 09:14:20,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:20,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327650000 ms
2017-08-10 09:14:25,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327665000 ms
2017-08-10 09:14:25,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327665000 ms.0 from job set of time 1502327665000 ms
2017-08-10 09:14:25,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 150 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 150 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 150 (MapPartitionsRDD[301] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:25,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_150 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:25,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_150_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:25,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_150_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:25,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 150 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:25,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 150 (MapPartitionsRDD[301] at map at streamingProcess.scala:48)
2017-08-10 09:14:25,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 150.0 with 2 tasks
2017-08-10 09:14:25,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 150.0 (TID 300, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:25,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 150.0 (TID 301, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:25,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 150.0 (TID 300)
2017-08-10 09:14:25,075 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 150.0 (TID 301)
2017-08-10 09:14:25,077 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:25,078 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:25,082 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 150.0 (TID 300). 714 bytes result sent to driver
2017-08-10 09:14:25,082 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 150.0 (TID 301). 714 bytes result sent to driver
2017-08-10 09:14:25,084 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 150.0 (TID 300) in 11 ms on localhost (1/2)
2017-08-10 09:14:25,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 150.0 (TID 301) in 10 ms on localhost (2/2)
2017-08-10 09:14:25,085 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 150.0, whose tasks have all completed, from pool 
2017-08-10 09:14:25,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 150 (foreachPartition at streamingProcess.scala:50) finished in 0.012 s
2017-08-10 09:14:25,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 150 finished: foreachPartition at streamingProcess.scala:50, took 0.030985 s
2017-08-10 09:14:25,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327665000 ms.0 from job set of time 1502327665000 ms
2017-08-10 09:14:25,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1502327665000 ms (execution: 0.064 s)
2017-08-10 09:14:25,085 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 299 from persistence list
2017-08-10 09:14:25,086 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 299
2017-08-10 09:14:25,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 298 from persistence list
2017-08-10 09:14:25,086 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 298
2017-08-10 09:14:25,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:25,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327655000 ms
2017-08-10 09:14:30,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327670000 ms
2017-08-10 09:14:30,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327670000 ms.0 from job set of time 1502327670000 ms
2017-08-10 09:14:30,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 151 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 151 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:30,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 151 (MapPartitionsRDD[303] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_151 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_151_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:30,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_151_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 151 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 151 (MapPartitionsRDD[303] at map at streamingProcess.scala:48)
2017-08-10 09:14:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 151.0 with 2 tasks
2017-08-10 09:14:30,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 151.0 (TID 302, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:30,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 151.0 (TID 303, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:30,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 151.0 (TID 303)
2017-08-10 09:14:30,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 151.0 (TID 302)
2017-08-10 09:14:30,068 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:30,068 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:30,073 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 151.0 (TID 303). 714 bytes result sent to driver
2017-08-10 09:14:30,073 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 151.0 (TID 302). 714 bytes result sent to driver
2017-08-10 09:14:30,076 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 151.0 (TID 302) in 15 ms on localhost (1/2)
2017-08-10 09:14:30,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 151.0 (TID 303) in 14 ms on localhost (2/2)
2017-08-10 09:14:30,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 151.0, whose tasks have all completed, from pool 
2017-08-10 09:14:30,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 151 (foreachPartition at streamingProcess.scala:50) finished in 0.015 s
2017-08-10 09:14:30,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 151 finished: foreachPartition at streamingProcess.scala:50, took 0.031242 s
2017-08-10 09:14:30,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327670000 ms.0 from job set of time 1502327670000 ms
2017-08-10 09:14:30,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1502327670000 ms (execution: 0.058 s)
2017-08-10 09:14:30,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 301 from persistence list
2017-08-10 09:14:30,078 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 301
2017-08-10 09:14:30,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 300 from persistence list
2017-08-10 09:14:30,079 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 300
2017-08-10 09:14:30,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:30,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327660000 ms
2017-08-10 09:14:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327675000 ms
2017-08-10 09:14:35,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327675000 ms.0 from job set of time 1502327675000 ms
2017-08-10 09:14:35,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:35,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 152 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:35,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 152 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:35,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:35,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:35,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 152 (MapPartitionsRDD[305] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:35,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_152 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_152_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:35,029 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_152_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:35,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 152 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:35,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 152 (MapPartitionsRDD[305] at map at streamingProcess.scala:48)
2017-08-10 09:14:35,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 152.0 with 2 tasks
2017-08-10 09:14:35,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 152.0 (TID 304, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:35,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 152.0 (TID 305, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:35,030 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 152.0 (TID 305)
2017-08-10 09:14:35,030 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 152.0 (TID 304)
2017-08-10 09:14:35,031 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:35,032 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:35,035 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 152.0 (TID 305). 714 bytes result sent to driver
2017-08-10 09:14:35,035 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 152.0 (TID 304). 714 bytes result sent to driver
2017-08-10 09:14:35,036 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 152.0 (TID 304) in 7 ms on localhost (1/2)
2017-08-10 09:14:35,036 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 152.0 (TID 305) in 6 ms on localhost (2/2)
2017-08-10 09:14:35,036 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 152.0, whose tasks have all completed, from pool 
2017-08-10 09:14:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 152 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:35,036 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 152 finished: foreachPartition at streamingProcess.scala:50, took 0.013454 s
2017-08-10 09:14:35,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327675000 ms.0 from job set of time 1502327675000 ms
2017-08-10 09:14:35,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.037 s for time 1502327675000 ms (execution: 0.024 s)
2017-08-10 09:14:35,037 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 303 from persistence list
2017-08-10 09:14:35,037 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 303
2017-08-10 09:14:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 302 from persistence list
2017-08-10 09:14:35,037 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 302
2017-08-10 09:14:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327665000 ms
2017-08-10 09:14:40,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327680000 ms
2017-08-10 09:14:40,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327680000 ms.0 from job set of time 1502327680000 ms
2017-08-10 09:14:40,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 153 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 153 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 153 (MapPartitionsRDD[307] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:40,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_153 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_153_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:40,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_153_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 153 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 153 (MapPartitionsRDD[307] at map at streamingProcess.scala:48)
2017-08-10 09:14:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 153.0 with 2 tasks
2017-08-10 09:14:40,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 153.0 (TID 306, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:40,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 153.0 (TID 307, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:40,054 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 153.0 (TID 306)
2017-08-10 09:14:40,054 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 153.0 (TID 307)
2017-08-10 09:14:40,056 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:40,056 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:40,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 153.0 (TID 307). 714 bytes result sent to driver
2017-08-10 09:14:40,060 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 153.0 (TID 306). 714 bytes result sent to driver
2017-08-10 09:14:40,061 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 153.0 (TID 307) in 7 ms on localhost (1/2)
2017-08-10 09:14:40,061 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 153.0 (TID 306) in 8 ms on localhost (2/2)
2017-08-10 09:14:40,061 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 153.0, whose tasks have all completed, from pool 
2017-08-10 09:14:40,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 153 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:40,062 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 153 finished: foreachPartition at streamingProcess.scala:50, took 0.015669 s
2017-08-10 09:14:40,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327680000 ms.0 from job set of time 1502327680000 ms
2017-08-10 09:14:40,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.062 s for time 1502327680000 ms (execution: 0.043 s)
2017-08-10 09:14:40,062 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 305 from persistence list
2017-08-10 09:14:40,062 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 305
2017-08-10 09:14:40,062 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 304 from persistence list
2017-08-10 09:14:40,062 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 304
2017-08-10 09:14:40,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:40,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327670000 ms
2017-08-10 09:14:45,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327685000 ms
2017-08-10 09:14:45,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327685000 ms.0 from job set of time 1502327685000 ms
2017-08-10 09:14:45,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 154 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 154 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:45,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 154 (MapPartitionsRDD[309] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:45,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_154 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_154_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:45,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_154_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:45,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 154 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 154 (MapPartitionsRDD[309] at map at streamingProcess.scala:48)
2017-08-10 09:14:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 154.0 with 2 tasks
2017-08-10 09:14:45,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 154.0 (TID 308, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:45,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 154.0 (TID 309, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:45,062 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 154.0 (TID 309)
2017-08-10 09:14:45,062 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 154.0 (TID 308)
2017-08-10 09:14:45,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:45,066 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:45,070 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 154.0 (TID 308). 714 bytes result sent to driver
2017-08-10 09:14:45,070 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 154.0 (TID 309). 714 bytes result sent to driver
2017-08-10 09:14:45,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 154.0 (TID 309) in 12 ms on localhost (1/2)
2017-08-10 09:14:45,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 154.0 (TID 308) in 13 ms on localhost (2/2)
2017-08-10 09:14:45,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 154.0, whose tasks have all completed, from pool 
2017-08-10 09:14:45,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 154 (foreachPartition at streamingProcess.scala:50) finished in 0.014 s
2017-08-10 09:14:45,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 154 finished: foreachPartition at streamingProcess.scala:50, took 0.028631 s
2017-08-10 09:14:45,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327685000 ms.0 from job set of time 1502327685000 ms
2017-08-10 09:14:45,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1502327685000 ms (execution: 0.054 s)
2017-08-10 09:14:45,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 307 from persistence list
2017-08-10 09:14:45,076 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 307
2017-08-10 09:14:45,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 306 from persistence list
2017-08-10 09:14:45,076 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 306
2017-08-10 09:14:45,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:45,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327675000 ms
2017-08-10 09:14:50,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327690000 ms
2017-08-10 09:14:50,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327690000 ms.0 from job set of time 1502327690000 ms
2017-08-10 09:14:50,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 155 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:50,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 155 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:50,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:50,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:50,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 155 (MapPartitionsRDD[311] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_155 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_155_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:50,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_155_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 155 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 155 (MapPartitionsRDD[311] at map at streamingProcess.scala:48)
2017-08-10 09:14:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 155.0 with 2 tasks
2017-08-10 09:14:50,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 155.0 (TID 310, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:50,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 155.0 (TID 311, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:50,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 155.0 (TID 310)
2017-08-10 09:14:50,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 155.0 (TID 311)
2017-08-10 09:14:50,065 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:50,065 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:50,069 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 155.0 (TID 311). 714 bytes result sent to driver
2017-08-10 09:14:50,069 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 155.0 (TID 310). 714 bytes result sent to driver
2017-08-10 09:14:50,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 155.0 (TID 310) in 8 ms on localhost (1/2)
2017-08-10 09:14:50,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 155.0 (TID 311) in 9 ms on localhost (2/2)
2017-08-10 09:14:50,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 155.0, whose tasks have all completed, from pool 
2017-08-10 09:14:50,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 155 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:14:50,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 155 finished: foreachPartition at streamingProcess.scala:50, took 0.023260 s
2017-08-10 09:14:50,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327690000 ms.0 from job set of time 1502327690000 ms
2017-08-10 09:14:50,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1502327690000 ms (execution: 0.052 s)
2017-08-10 09:14:50,073 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 309 from persistence list
2017-08-10 09:14:50,073 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 309
2017-08-10 09:14:50,073 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 308 from persistence list
2017-08-10 09:14:50,073 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 308
2017-08-10 09:14:50,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:50,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327680000 ms
2017-08-10 09:14:55,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327695000 ms
2017-08-10 09:14:55,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327695000 ms.0 from job set of time 1502327695000 ms
2017-08-10 09:14:55,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 156 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 156 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 156 (MapPartitionsRDD[313] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_156 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:55,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_156_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:55,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_156_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 156 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 156 (MapPartitionsRDD[313] at map at streamingProcess.scala:48)
2017-08-10 09:14:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 156.0 with 2 tasks
2017-08-10 09:14:55,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 156.0 (TID 312, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:55,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 156.0 (TID 313, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:55,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 156.0 (TID 313)
2017-08-10 09:14:55,077 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 156.0 (TID 312)
2017-08-10 09:14:55,083 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:55,083 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:55,090 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 156.0 (TID 313). 714 bytes result sent to driver
2017-08-10 09:14:55,090 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 156.0 (TID 312). 714 bytes result sent to driver
2017-08-10 09:14:55,094 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 156.0 (TID 312) in 18 ms on localhost (1/2)
2017-08-10 09:14:55,094 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 156.0 (TID 313) in 18 ms on localhost (2/2)
2017-08-10 09:14:55,094 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 156.0, whose tasks have all completed, from pool 
2017-08-10 09:14:55,094 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 156 (foreachPartition at streamingProcess.scala:50) finished in 0.019 s
2017-08-10 09:14:55,095 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 156 finished: foreachPartition at streamingProcess.scala:50, took 0.040884 s
2017-08-10 09:14:55,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327695000 ms.0 from job set of time 1502327695000 ms
2017-08-10 09:14:55,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.096 s for time 1502327695000 ms (execution: 0.077 s)
2017-08-10 09:14:55,096 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 311 from persistence list
2017-08-10 09:14:55,097 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 311
2017-08-10 09:14:55,097 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 310 from persistence list
2017-08-10 09:14:55,097 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 310
2017-08-10 09:14:55,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:55,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327685000 ms
2017-08-10 09:15:00,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327700000 ms
2017-08-10 09:15:00,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327700000 ms.0 from job set of time 1502327700000 ms
2017-08-10 09:15:00,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 157 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 157 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 157 (MapPartitionsRDD[315] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_157 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:15:00,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_157_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:15:00,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_157_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:00,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 157 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:00,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 157 (MapPartitionsRDD[315] at map at streamingProcess.scala:48)
2017-08-10 09:15:00,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 157.0 with 2 tasks
2017-08-10 09:15:00,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 157.0 (TID 314, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:00,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 157.0 (TID 315, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:00,061 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 157.0 (TID 315)
2017-08-10 09:15:00,061 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 157.0 (TID 314)
2017-08-10 09:15:00,064 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:00,064 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:00,067 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 157.0 (TID 315). 714 bytes result sent to driver
2017-08-10 09:15:00,067 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 157.0 (TID 314). 714 bytes result sent to driver
2017-08-10 09:15:00,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 157.0 (TID 314) in 9 ms on localhost (1/2)
2017-08-10 09:15:00,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 157.0 (TID 315) in 9 ms on localhost (2/2)
2017-08-10 09:15:00,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 157.0, whose tasks have all completed, from pool 
2017-08-10 09:15:00,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 157 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:15:00,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 157 finished: foreachPartition at streamingProcess.scala:50, took 0.018521 s
2017-08-10 09:15:00,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327700000 ms.0 from job set of time 1502327700000 ms
2017-08-10 09:15:00,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1502327700000 ms (execution: 0.051 s)
2017-08-10 09:15:00,071 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 313 from persistence list
2017-08-10 09:15:00,071 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 313
2017-08-10 09:15:00,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 312 from persistence list
2017-08-10 09:15:00,071 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 312
2017-08-10 09:15:00,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:00,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327690000 ms
2017-08-10 09:15:05,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327705000 ms
2017-08-10 09:15:05,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327705000 ms.0 from job set of time 1502327705000 ms
2017-08-10 09:15:05,022 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:05,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 158 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:05,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 158 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:05,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:05,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:05,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 158 (MapPartitionsRDD[317] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_158 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:15:05,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_158_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:15:05,029 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_158_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:05,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 158 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:05,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 158 (MapPartitionsRDD[317] at map at streamingProcess.scala:48)
2017-08-10 09:15:05,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 158.0 with 2 tasks
2017-08-10 09:15:05,030 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 158.0 (TID 316, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:05,030 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 158.0 (TID 317, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:05,030 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 158.0 (TID 317)
2017-08-10 09:15:05,030 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 158.0 (TID 316)
2017-08-10 09:15:05,032 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:05,032 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:05,035 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 158.0 (TID 317). 714 bytes result sent to driver
2017-08-10 09:15:05,035 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 158.0 (TID 316). 714 bytes result sent to driver
2017-08-10 09:15:05,036 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 158.0 (TID 316) in 6 ms on localhost (1/2)
2017-08-10 09:15:05,036 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 158.0 (TID 317) in 6 ms on localhost (2/2)
2017-08-10 09:15:05,036 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 158.0, whose tasks have all completed, from pool 
2017-08-10 09:15:05,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 158 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:15:05,036 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 158 finished: foreachPartition at streamingProcess.scala:50, took 0.014063 s
2017-08-10 09:15:05,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327705000 ms.0 from job set of time 1502327705000 ms
2017-08-10 09:15:05,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.036 s for time 1502327705000 ms (execution: 0.024 s)
2017-08-10 09:15:05,037 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 315 from persistence list
2017-08-10 09:15:05,037 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 315
2017-08-10 09:15:05,037 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 314 from persistence list
2017-08-10 09:15:05,037 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 314
2017-08-10 09:15:05,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:05,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327695000 ms
2017-08-10 09:15:10,032 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327710000 ms
2017-08-10 09:15:10,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327710000 ms.0 from job set of time 1502327710000 ms
2017-08-10 09:15:10,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 159 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 159 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 159 (MapPartitionsRDD[319] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:10,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_159 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:15:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_159_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:15:10,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_159_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 159 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 159 (MapPartitionsRDD[319] at map at streamingProcess.scala:48)
2017-08-10 09:15:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 159.0 with 2 tasks
2017-08-10 09:15:10,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 159.0 (TID 318, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:10,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 159.0 (TID 319, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:10,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 159.0 (TID 318)
2017-08-10 09:15:10,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 159.0 (TID 319)
2017-08-10 09:15:10,067 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:10,067 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:10,071 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 159.0 (TID 319). 714 bytes result sent to driver
2017-08-10 09:15:10,071 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 159.0 (TID 318). 714 bytes result sent to driver
2017-08-10 09:15:10,073 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 159.0 (TID 318) in 9 ms on localhost (1/2)
2017-08-10 09:15:10,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 159.0 (TID 319) in 9 ms on localhost (2/2)
2017-08-10 09:15:10,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 159.0, whose tasks have all completed, from pool 
2017-08-10 09:15:10,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 159 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:15:10,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 159 finished: foreachPartition at streamingProcess.scala:50, took 0.026911 s
2017-08-10 09:15:10,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327710000 ms.0 from job set of time 1502327710000 ms
2017-08-10 09:15:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1502327710000 ms (execution: 0.037 s)
2017-08-10 09:15:10,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 317 from persistence list
2017-08-10 09:15:10,075 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 317
2017-08-10 09:15:10,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 316 from persistence list
2017-08-10 09:15:10,076 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 316
2017-08-10 09:15:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327700000 ms
2017-08-10 09:15:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327715000 ms
2017-08-10 09:15:15,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327715000 ms.0 from job set of time 1502327715000 ms
2017-08-10 09:15:15,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:15,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 160 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:15,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 160 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:15,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:15,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:15,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 160 (MapPartitionsRDD[321] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:15,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_160 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:15:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_160_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:15:15,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_160_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:15:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 160 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 160 (MapPartitionsRDD[321] at map at streamingProcess.scala:48)
2017-08-10 09:15:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 160.0 with 2 tasks
2017-08-10 09:15:15,031 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 160.0 (TID 320, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:15,031 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 160.0 (TID 321, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:15,032 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 160.0 (TID 320)
2017-08-10 09:15:15,032 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 160.0 (TID 321)
2017-08-10 09:15:15,033 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:15,034 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:15,036 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 160.0 (TID 320). 714 bytes result sent to driver
2017-08-10 09:15:15,036 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 160.0 (TID 321). 714 bytes result sent to driver
2017-08-10 09:15:15,038 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 160.0 (TID 320) in 6 ms on localhost (1/2)
2017-08-10 09:15:15,038 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 160.0 (TID 321) in 7 ms on localhost (2/2)
2017-08-10 09:15:15,038 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 160.0, whose tasks have all completed, from pool 
2017-08-10 09:15:15,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 160 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:15:15,038 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 160 finished: foreachPartition at streamingProcess.scala:50, took 0.015105 s
2017-08-10 09:15:15,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327715000 ms.0 from job set of time 1502327715000 ms
2017-08-10 09:15:15,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.038 s for time 1502327715000 ms (execution: 0.025 s)
2017-08-10 09:15:15,038 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 319 from persistence list
2017-08-10 09:15:15,039 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 319
2017-08-10 09:15:15,039 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 318 from persistence list
2017-08-10 09:15:15,039 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 318
2017-08-10 09:15:15,039 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:15,039 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327705000 ms
2017-08-10 09:15:20,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327720000 ms
2017-08-10 09:15:20,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327720000 ms.0 from job set of time 1502327720000 ms
2017-08-10 09:15:20,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 161 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 161 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 161 (MapPartitionsRDD[323] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:20,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_161 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:15:20,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_161_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:15:20,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_161_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:15:20,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_146_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:15:20,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 161 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:20,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 161 (MapPartitionsRDD[323] at map at streamingProcess.scala:48)
2017-08-10 09:15:20,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 161.0 with 2 tasks
2017-08-10 09:15:20,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 161.0 (TID 322, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:20,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_147_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:20,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 161.0 (TID 323, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:20,078 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 161.0 (TID 322)
2017-08-10 09:15:20,078 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 161.0 (TID 323)
2017-08-10 09:15:20,081 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_148_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:20,082 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:20,082 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:20,083 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_149_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:20,084 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_150_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:20,086 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_151_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:15:20,086 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 161.0 (TID 322). 714 bytes result sent to driver
2017-08-10 09:15:20,086 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 161.0 (TID 323). 714 bytes result sent to driver
2017-08-10 09:15:20,087 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_152_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:15:20,088 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_153_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:15:20,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 161.0 (TID 322) in 12 ms on localhost (1/2)
2017-08-10 09:15:20,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 161.0 (TID 323) in 11 ms on localhost (2/2)
2017-08-10 09:15:20,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 161.0, whose tasks have all completed, from pool 
2017-08-10 09:15:20,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 161 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-10 09:15:20,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 161 finished: foreachPartition at streamingProcess.scala:50, took 0.043833 s
2017-08-10 09:15:20,089 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_154_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:15:20,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327720000 ms.0 from job set of time 1502327720000 ms
2017-08-10 09:15:20,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1502327720000 ms (execution: 0.070 s)
2017-08-10 09:15:20,090 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 321 from persistence list
2017-08-10 09:15:20,090 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 321
2017-08-10 09:15:20,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 320 from persistence list
2017-08-10 09:15:20,091 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 320
2017-08-10 09:15:20,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:20,091 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_155_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:20,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327710000 ms
2017-08-10 09:15:20,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_156_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:20,093 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_157_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:20,094 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_158_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:20,095 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_159_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:15:20,096 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_160_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:15:25,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327725000 ms
2017-08-10 09:15:25,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327725000 ms.0 from job set of time 1502327725000 ms
2017-08-10 09:15:25,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 162 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 162 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 162 (MapPartitionsRDD[325] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:25,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_162 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:15:25,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_162_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:15:25,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_162_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:15:25,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 162 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:25,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 162 (MapPartitionsRDD[325] at map at streamingProcess.scala:48)
2017-08-10 09:15:25,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 162.0 with 2 tasks
2017-08-10 09:15:25,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 162.0 (TID 324, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:25,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 162.0 (TID 325, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:25,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 162.0 (TID 325)
2017-08-10 09:15:25,061 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 162.0 (TID 324)
2017-08-10 09:15:25,064 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:25,064 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:25,067 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 162.0 (TID 325). 714 bytes result sent to driver
2017-08-10 09:15:25,067 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 162.0 (TID 324). 714 bytes result sent to driver
2017-08-10 09:15:25,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 162.0 (TID 325) in 9 ms on localhost (1/2)
2017-08-10 09:15:25,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 162.0 (TID 324) in 10 ms on localhost (2/2)
2017-08-10 09:15:25,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 162.0, whose tasks have all completed, from pool 
2017-08-10 09:15:25,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 162 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:15:25,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 162 finished: foreachPartition at streamingProcess.scala:50, took 0.021794 s
2017-08-10 09:15:25,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327725000 ms.0 from job set of time 1502327725000 ms
2017-08-10 09:15:25,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1502327725000 ms (execution: 0.051 s)
2017-08-10 09:15:25,071 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 323 from persistence list
2017-08-10 09:15:25,071 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 323
2017-08-10 09:15:25,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 322 from persistence list
2017-08-10 09:15:25,071 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 322
2017-08-10 09:15:25,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:25,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327715000 ms
2017-08-10 09:15:30,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327730000 ms
2017-08-10 09:15:30,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327730000 ms.0 from job set of time 1502327730000 ms
2017-08-10 09:15:30,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 163 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 163 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 163 (MapPartitionsRDD[327] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_163 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:15:30,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_163_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:15:30,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_163_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:30,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 163 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:30,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 163 (MapPartitionsRDD[327] at map at streamingProcess.scala:48)
2017-08-10 09:15:30,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 163.0 with 2 tasks
2017-08-10 09:15:30,076 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 163.0 (TID 326, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:30,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 163.0 (TID 327, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:30,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 163.0 (TID 327)
2017-08-10 09:15:30,077 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 163.0 (TID 326)
2017-08-10 09:15:30,083 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:30,083 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:30,089 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 163.0 (TID 326). 714 bytes result sent to driver
2017-08-10 09:15:30,089 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 163.0 (TID 327). 714 bytes result sent to driver
2017-08-10 09:15:30,092 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 163.0 (TID 326) in 17 ms on localhost (1/2)
2017-08-10 09:15:30,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 163.0 (TID 327) in 17 ms on localhost (2/2)
2017-08-10 09:15:30,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 163.0, whose tasks have all completed, from pool 
2017-08-10 09:15:30,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 163 (foreachPartition at streamingProcess.scala:50) finished in 0.018 s
2017-08-10 09:15:30,094 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 163 finished: foreachPartition at streamingProcess.scala:50, took 0.039657 s
2017-08-10 09:15:30,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327730000 ms.0 from job set of time 1502327730000 ms
2017-08-10 09:15:30,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.094 s for time 1502327730000 ms (execution: 0.074 s)
2017-08-10 09:15:30,095 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 325 from persistence list
2017-08-10 09:15:30,096 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 324 from persistence list
2017-08-10 09:15:30,096 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 325
2017-08-10 09:15:30,097 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 324
2017-08-10 09:15:30,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:30,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327720000 ms
2017-08-10 09:15:35,027 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327735000 ms
2017-08-10 09:15:35,031 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327735000 ms.0 from job set of time 1502327735000 ms
2017-08-10 09:15:35,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:35,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 164 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:35,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 164 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:35,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:35,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:35,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 164 (MapPartitionsRDD[329] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_164 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:15:35,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_164_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:15:35,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_164_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 164 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 164 (MapPartitionsRDD[329] at map at streamingProcess.scala:48)
2017-08-10 09:15:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 164.0 with 2 tasks
2017-08-10 09:15:35,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 164.0 (TID 328, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:35,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 164.0 (TID 329, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:35,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 164.0 (TID 328)
2017-08-10 09:15:35,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:35,069 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 164.0 (TID 329)
2017-08-10 09:15:35,071 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:35,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 164.0 (TID 328). 714 bytes result sent to driver
2017-08-10 09:15:35,075 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 164.0 (TID 329). 714 bytes result sent to driver
2017-08-10 09:15:35,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 164.0 (TID 329) in 15 ms on localhost (1/2)
2017-08-10 09:15:35,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 164.0 (TID 328) in 15 ms on localhost (2/2)
2017-08-10 09:15:35,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 164.0, whose tasks have all completed, from pool 
2017-08-10 09:15:35,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 164 (foreachPartition at streamingProcess.scala:50) finished in 0.017 s
2017-08-10 09:15:35,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 164 finished: foreachPartition at streamingProcess.scala:50, took 0.040923 s
2017-08-10 09:15:35,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327735000 ms.0 from job set of time 1502327735000 ms
2017-08-10 09:15:35,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1502327735000 ms (execution: 0.049 s)
2017-08-10 09:15:35,080 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 327 from persistence list
2017-08-10 09:15:35,080 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 327
2017-08-10 09:15:35,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 326 from persistence list
2017-08-10 09:15:35,080 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 326
2017-08-10 09:15:35,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:35,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327725000 ms
2017-08-10 09:15:40,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327740000 ms
2017-08-10 09:15:40,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327740000 ms.0 from job set of time 1502327740000 ms
2017-08-10 09:15:40,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 165 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 165 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 165 (MapPartitionsRDD[331] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_165 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:15:40,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_165_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:15:40,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_165_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:40,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 165 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:40,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 165 (MapPartitionsRDD[331] at map at streamingProcess.scala:48)
2017-08-10 09:15:40,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 165.0 with 2 tasks
2017-08-10 09:15:40,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 165.0 (TID 330, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:40,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 165.0 (TID 331, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:40,066 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 165.0 (TID 331)
2017-08-10 09:15:40,066 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 165.0 (TID 330)
2017-08-10 09:15:40,069 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:40,069 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:40,074 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 165.0 (TID 330). 714 bytes result sent to driver
2017-08-10 09:15:40,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 165.0 (TID 331). 714 bytes result sent to driver
2017-08-10 09:15:40,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 165.0 (TID 330) in 12 ms on localhost (1/2)
2017-08-10 09:15:40,076 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 165.0 (TID 331) in 11 ms on localhost (2/2)
2017-08-10 09:15:40,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 165 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-10 09:15:40,077 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 165.0, whose tasks have all completed, from pool 
2017-08-10 09:15:40,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 165 finished: foreachPartition at streamingProcess.scala:50, took 0.026703 s
2017-08-10 09:15:40,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327740000 ms.0 from job set of time 1502327740000 ms
2017-08-10 09:15:40,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1502327740000 ms (execution: 0.058 s)
2017-08-10 09:15:40,078 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 329 from persistence list
2017-08-10 09:15:40,078 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 329
2017-08-10 09:15:40,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 328 from persistence list
2017-08-10 09:15:40,079 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 328
2017-08-10 09:15:40,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:40,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327730000 ms
2017-08-10 09:15:45,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327745000 ms
2017-08-10 09:15:45,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327745000 ms.0 from job set of time 1502327745000 ms
2017-08-10 09:15:45,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:45,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 166 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:45,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 166 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:45,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:45,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:45,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 166 (MapPartitionsRDD[333] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_166 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:15:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_166_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:15:45,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_166_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 166 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 166 (MapPartitionsRDD[333] at map at streamingProcess.scala:48)
2017-08-10 09:15:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 166.0 with 2 tasks
2017-08-10 09:15:45,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 166.0 (TID 332, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:45,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 166.0 (TID 333, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:45,056 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 166.0 (TID 333)
2017-08-10 09:15:45,056 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 166.0 (TID 332)
2017-08-10 09:15:45,058 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:45,058 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:45,061 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 166.0 (TID 332). 714 bytes result sent to driver
2017-08-10 09:15:45,061 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 166.0 (TID 333). 714 bytes result sent to driver
2017-08-10 09:15:45,062 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 166.0 (TID 332) in 6 ms on localhost (1/2)
2017-08-10 09:15:45,062 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 166.0 (TID 333) in 6 ms on localhost (2/2)
2017-08-10 09:15:45,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 166.0, whose tasks have all completed, from pool 
2017-08-10 09:15:45,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 166 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:15:45,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 166 finished: foreachPartition at streamingProcess.scala:50, took 0.015785 s
2017-08-10 09:15:45,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327745000 ms.0 from job set of time 1502327745000 ms
2017-08-10 09:15:45,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1502327745000 ms (execution: 0.043 s)
2017-08-10 09:15:45,063 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 331 from persistence list
2017-08-10 09:15:45,063 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 331
2017-08-10 09:15:45,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 330 from persistence list
2017-08-10 09:15:45,064 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 330
2017-08-10 09:15:45,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:45,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327735000 ms
2017-08-10 09:15:50,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327750000 ms
2017-08-10 09:15:50,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327750000 ms.0 from job set of time 1502327750000 ms
2017-08-10 09:15:50,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 167 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 167 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 167 (MapPartitionsRDD[335] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_167 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:15:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_167_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:15:50,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_167_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:15:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 167 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 167 (MapPartitionsRDD[335] at map at streamingProcess.scala:48)
2017-08-10 09:15:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 167.0 with 2 tasks
2017-08-10 09:15:50,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 167.0 (TID 334, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:50,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 167.0 (TID 335, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:50,056 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 167.0 (TID 334)
2017-08-10 09:15:50,056 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 167.0 (TID 335)
2017-08-10 09:15:50,059 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:50,059 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:50,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 167.0 (TID 335). 714 bytes result sent to driver
2017-08-10 09:15:50,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 167.0 (TID 334). 714 bytes result sent to driver
2017-08-10 09:15:50,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 167.0 (TID 335) in 11 ms on localhost (1/2)
2017-08-10 09:15:50,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 167.0 (TID 334) in 11 ms on localhost (2/2)
2017-08-10 09:15:50,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 167.0, whose tasks have all completed, from pool 
2017-08-10 09:15:50,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 167 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:15:50,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 167 finished: foreachPartition at streamingProcess.scala:50, took 0.023962 s
2017-08-10 09:15:50,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327750000 ms.0 from job set of time 1502327750000 ms
2017-08-10 09:15:50,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1502327750000 ms (execution: 0.048 s)
2017-08-10 09:15:50,067 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 333 from persistence list
2017-08-10 09:15:50,068 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 333
2017-08-10 09:15:50,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 332 from persistence list
2017-08-10 09:15:50,068 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 332
2017-08-10 09:15:50,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:50,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327740000 ms
2017-08-10 09:15:55,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327755000 ms
2017-08-10 09:15:55,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327755000 ms.0 from job set of time 1502327755000 ms
2017-08-10 09:15:55,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:55,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 168 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:55,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 168 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:55,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:55,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:55,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 168 (MapPartitionsRDD[337] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:55,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_168 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:15:55,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_168_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:15:55,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_168_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:15:55,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 168 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:55,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 168 (MapPartitionsRDD[337] at map at streamingProcess.scala:48)
2017-08-10 09:15:55,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 168.0 with 2 tasks
2017-08-10 09:15:55,078 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 168.0 (TID 336, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:55,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 168.0 (TID 337, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:55,079 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 168.0 (TID 336)
2017-08-10 09:15:55,079 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 168.0 (TID 337)
2017-08-10 09:15:55,085 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:55,085 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:55,092 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 168.0 (TID 337). 714 bytes result sent to driver
2017-08-10 09:15:55,092 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 168.0 (TID 336). 714 bytes result sent to driver
2017-08-10 09:15:55,096 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 168.0 (TID 337) in 18 ms on localhost (1/2)
2017-08-10 09:15:55,096 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 168.0 (TID 336) in 19 ms on localhost (2/2)
2017-08-10 09:15:55,096 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 168.0, whose tasks have all completed, from pool 
2017-08-10 09:15:55,097 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 168 (foreachPartition at streamingProcess.scala:50) finished in 0.020 s
2017-08-10 09:15:55,097 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 168 finished: foreachPartition at streamingProcess.scala:50, took 0.041986 s
2017-08-10 09:15:55,098 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327755000 ms.0 from job set of time 1502327755000 ms
2017-08-10 09:15:55,098 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.098 s for time 1502327755000 ms (execution: 0.078 s)
2017-08-10 09:15:55,098 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 335 from persistence list
2017-08-10 09:15:55,099 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 334 from persistence list
2017-08-10 09:15:55,099 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 335
2017-08-10 09:15:55,100 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 334
2017-08-10 09:15:55,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:55,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327745000 ms
2017-08-10 09:16:00,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327760000 ms
2017-08-10 09:16:00,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327760000 ms.0 from job set of time 1502327760000 ms
2017-08-10 09:16:00,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 169 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 169 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 169 (MapPartitionsRDD[339] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_169 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:16:00,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_169_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:16:00,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_169_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:16:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 169 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 169 (MapPartitionsRDD[339] at map at streamingProcess.scala:48)
2017-08-10 09:16:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 169.0 with 2 tasks
2017-08-10 09:16:00,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 169.0 (TID 338, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:00,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 169.0 (TID 339, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:00,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 169.0 (TID 339)
2017-08-10 09:16:00,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 169.0 (TID 338)
2017-08-10 09:16:00,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:00,066 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:00,069 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 169.0 (TID 339). 714 bytes result sent to driver
2017-08-10 09:16:00,069 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 169.0 (TID 338). 714 bytes result sent to driver
2017-08-10 09:16:00,071 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 169.0 (TID 339) in 9 ms on localhost (1/2)
2017-08-10 09:16:00,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 169.0 (TID 338) in 9 ms on localhost (2/2)
2017-08-10 09:16:00,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 169.0, whose tasks have all completed, from pool 
2017-08-10 09:16:00,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 169 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:16:00,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 169 finished: foreachPartition at streamingProcess.scala:50, took 0.019650 s
2017-08-10 09:16:00,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327760000 ms.0 from job set of time 1502327760000 ms
2017-08-10 09:16:00,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1502327760000 ms (execution: 0.053 s)
2017-08-10 09:16:00,072 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 337 from persistence list
2017-08-10 09:16:00,073 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 337
2017-08-10 09:16:00,073 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 336 from persistence list
2017-08-10 09:16:00,073 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 336
2017-08-10 09:16:00,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:00,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327750000 ms
2017-08-10 09:16:05,112 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327765000 ms
2017-08-10 09:16:05,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327765000 ms.0 from job set of time 1502327765000 ms
2017-08-10 09:16:05,124 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:05,124 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 170 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:05,124 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 170 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:05,124 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:05,125 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:05,125 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 170 (MapPartitionsRDD[341] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:05,127 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_170 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:16:05,131 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_170_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:16:05,132 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_170_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:16:05,132 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 170 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:05,132 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 170 (MapPartitionsRDD[341] at map at streamingProcess.scala:48)
2017-08-10 09:16:05,132 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 170.0 with 2 tasks
2017-08-10 09:16:05,133 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 170.0 (TID 340, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:05,133 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 170.0 (TID 341, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:05,134 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 170.0 (TID 340)
2017-08-10 09:16:05,135 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 170.0 (TID 341)
2017-08-10 09:16:05,136 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:05,137 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:05,139 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 170.0 (TID 340). 714 bytes result sent to driver
2017-08-10 09:16:05,139 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 170.0 (TID 341). 714 bytes result sent to driver
2017-08-10 09:16:05,140 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 170.0 (TID 341) in 7 ms on localhost (1/2)
2017-08-10 09:16:05,140 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 170.0 (TID 340) in 8 ms on localhost (2/2)
2017-08-10 09:16:05,141 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 170.0, whose tasks have all completed, from pool 
2017-08-10 09:16:05,141 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 170 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:16:05,141 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 170 finished: foreachPartition at streamingProcess.scala:50, took 0.016921 s
2017-08-10 09:16:05,141 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327765000 ms.0 from job set of time 1502327765000 ms
2017-08-10 09:16:05,141 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.141 s for time 1502327765000 ms (execution: 0.029 s)
2017-08-10 09:16:05,142 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 339 from persistence list
2017-08-10 09:16:05,142 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 338 from persistence list
2017-08-10 09:16:05,142 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 339
2017-08-10 09:16:05,143 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:05,142 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 338
2017-08-10 09:16:05,143 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327755000 ms
2017-08-10 09:16:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327770000 ms
2017-08-10 09:16:10,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327770000 ms.0 from job set of time 1502327770000 ms
2017-08-10 09:16:10,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 171 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 171 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 171 (MapPartitionsRDD[343] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_171 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:16:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_171_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:16:10,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_171_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 171 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 171 (MapPartitionsRDD[343] at map at streamingProcess.scala:48)
2017-08-10 09:16:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 171.0 with 2 tasks
2017-08-10 09:16:10,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 171.0 (TID 342, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:10,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 171.0 (TID 343, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:10,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 171.0 (TID 342)
2017-08-10 09:16:10,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 171.0 (TID 343)
2017-08-10 09:16:10,069 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:10,069 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:10,073 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 171.0 (TID 343). 714 bytes result sent to driver
2017-08-10 09:16:10,073 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 171.0 (TID 342). 714 bytes result sent to driver
2017-08-10 09:16:10,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 171.0 (TID 342) in 10 ms on localhost (1/2)
2017-08-10 09:16:10,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 171.0 (TID 343) in 9 ms on localhost (2/2)
2017-08-10 09:16:10,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 171.0, whose tasks have all completed, from pool 
2017-08-10 09:16:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 171 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:16:10,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 171 finished: foreachPartition at streamingProcess.scala:50, took 0.024406 s
2017-08-10 09:16:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327770000 ms.0 from job set of time 1502327770000 ms
2017-08-10 09:16:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1502327770000 ms (execution: 0.055 s)
2017-08-10 09:16:10,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 341 from persistence list
2017-08-10 09:16:10,075 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 341
2017-08-10 09:16:10,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 340 from persistence list
2017-08-10 09:16:10,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:10,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327760000 ms
2017-08-10 09:16:10,075 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 340
2017-08-10 09:16:15,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327775000 ms
2017-08-10 09:16:15,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327775000 ms.0 from job set of time 1502327775000 ms
2017-08-10 09:16:15,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 172 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 172 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 172 (MapPartitionsRDD[345] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_172 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:16:15,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_172_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:16:15,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_172_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 172 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 172 (MapPartitionsRDD[345] at map at streamingProcess.scala:48)
2017-08-10 09:16:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 172.0 with 2 tasks
2017-08-10 09:16:15,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 172.0 (TID 344, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:15,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 172.0 (TID 345, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:15,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 172.0 (TID 344)
2017-08-10 09:16:15,072 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 172.0 (TID 345)
2017-08-10 09:16:15,076 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:15,076 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:15,082 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 172.0 (TID 344). 714 bytes result sent to driver
2017-08-10 09:16:15,082 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 172.0 (TID 345). 714 bytes result sent to driver
2017-08-10 09:16:15,086 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 172.0 (TID 345) in 15 ms on localhost (1/2)
2017-08-10 09:16:15,086 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 172.0 (TID 344) in 16 ms on localhost (2/2)
2017-08-10 09:16:15,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 172.0, whose tasks have all completed, from pool 
2017-08-10 09:16:15,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 172 (foreachPartition at streamingProcess.scala:50) finished in 0.018 s
2017-08-10 09:16:15,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 172 finished: foreachPartition at streamingProcess.scala:50, took 0.036287 s
2017-08-10 09:16:15,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327775000 ms.0 from job set of time 1502327775000 ms
2017-08-10 09:16:15,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1502327775000 ms (execution: 0.070 s)
2017-08-10 09:16:15,090 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 343 from persistence list
2017-08-10 09:16:15,090 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 343
2017-08-10 09:16:15,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 342 from persistence list
2017-08-10 09:16:15,091 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 342
2017-08-10 09:16:15,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:15,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327765000 ms
2017-08-10 09:16:20,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327780000 ms
2017-08-10 09:16:20,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327780000 ms.0 from job set of time 1502327780000 ms
2017-08-10 09:16:20,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:20,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 173 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:20,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 173 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:20,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:20,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:20,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 173 (MapPartitionsRDD[347] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_173 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:16:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_173_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:16:20,045 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_173_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 173 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 173 (MapPartitionsRDD[347] at map at streamingProcess.scala:48)
2017-08-10 09:16:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 173.0 with 2 tasks
2017-08-10 09:16:20,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 173.0 (TID 346, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:20,047 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 173.0 (TID 347, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:20,047 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 173.0 (TID 346)
2017-08-10 09:16:20,047 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 173.0 (TID 347)
2017-08-10 09:16:20,049 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:20,049 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:20,052 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 173.0 (TID 346). 714 bytes result sent to driver
2017-08-10 09:16:20,052 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 173.0 (TID 347). 714 bytes result sent to driver
2017-08-10 09:16:20,054 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 173.0 (TID 346) in 8 ms on localhost (1/2)
2017-08-10 09:16:20,054 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 173.0 (TID 347) in 8 ms on localhost (2/2)
2017-08-10 09:16:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 173 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:16:20,054 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 173.0, whose tasks have all completed, from pool 
2017-08-10 09:16:20,054 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 173 finished: foreachPartition at streamingProcess.scala:50, took 0.014668 s
2017-08-10 09:16:20,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327780000 ms.0 from job set of time 1502327780000 ms
2017-08-10 09:16:20,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.055 s for time 1502327780000 ms (execution: 0.032 s)
2017-08-10 09:16:20,055 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 345 from persistence list
2017-08-10 09:16:20,055 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 345
2017-08-10 09:16:20,055 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 344 from persistence list
2017-08-10 09:16:20,055 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 344
2017-08-10 09:16:20,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:20,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327770000 ms
2017-08-10 09:16:25,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327785000 ms
2017-08-10 09:16:25,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327785000 ms.0 from job set of time 1502327785000 ms
2017-08-10 09:16:25,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 174 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 174 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 174 (MapPartitionsRDD[349] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:25,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_174 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:16:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_174_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:16:25,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_174_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:25,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 174 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:25,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 174 (MapPartitionsRDD[349] at map at streamingProcess.scala:48)
2017-08-10 09:16:25,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 174.0 with 2 tasks
2017-08-10 09:16:25,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 174.0 (TID 348, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:25,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 174.0 (TID 349, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:25,066 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 174.0 (TID 349)
2017-08-10 09:16:25,066 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 174.0 (TID 348)
2017-08-10 09:16:25,070 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:25,070 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:25,074 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 174.0 (TID 349). 714 bytes result sent to driver
2017-08-10 09:16:25,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 174.0 (TID 348). 714 bytes result sent to driver
2017-08-10 09:16:25,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 174.0 (TID 349) in 10 ms on localhost (1/2)
2017-08-10 09:16:25,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 174.0 (TID 348) in 11 ms on localhost (2/2)
2017-08-10 09:16:25,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 174.0, whose tasks have all completed, from pool 
2017-08-10 09:16:25,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 174 (foreachPartition at streamingProcess.scala:50) finished in 0.012 s
2017-08-10 09:16:25,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 174 finished: foreachPartition at streamingProcess.scala:50, took 0.026199 s
2017-08-10 09:16:25,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327785000 ms.0 from job set of time 1502327785000 ms
2017-08-10 09:16:25,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1502327785000 ms (execution: 0.057 s)
2017-08-10 09:16:25,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 347 from persistence list
2017-08-10 09:16:25,077 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 347
2017-08-10 09:16:25,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 346 from persistence list
2017-08-10 09:16:25,077 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 346
2017-08-10 09:16:25,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:25,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327775000 ms
2017-08-10 09:16:30,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327790000 ms
2017-08-10 09:16:30,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327790000 ms.0 from job set of time 1502327790000 ms
2017-08-10 09:16:30,034 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:30,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 175 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:30,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 175 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:30,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:30,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:30,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 175 (MapPartitionsRDD[351] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:30,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_175 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:16:30,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_175_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:16:30,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_175_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:16:30,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 175 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:30,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 175 (MapPartitionsRDD[351] at map at streamingProcess.scala:48)
2017-08-10 09:16:30,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 175.0 with 2 tasks
2017-08-10 09:16:30,046 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 175.0 (TID 350, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:30,046 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 175.0 (TID 351, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:30,046 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 175.0 (TID 350)
2017-08-10 09:16:30,046 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 175.0 (TID 351)
2017-08-10 09:16:30,049 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:30,049 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:30,053 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 175.0 (TID 351). 714 bytes result sent to driver
2017-08-10 09:16:30,053 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 175.0 (TID 350). 714 bytes result sent to driver
2017-08-10 09:16:30,055 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 175.0 (TID 350) in 10 ms on localhost (1/2)
2017-08-10 09:16:30,055 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 175.0 (TID 351) in 9 ms on localhost (2/2)
2017-08-10 09:16:30,055 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 175.0, whose tasks have all completed, from pool 
2017-08-10 09:16:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 175 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:16:30,056 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 175 finished: foreachPartition at streamingProcess.scala:50, took 0.021816 s
2017-08-10 09:16:30,056 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327790000 ms.0 from job set of time 1502327790000 ms
2017-08-10 09:16:30,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.056 s for time 1502327790000 ms (execution: 0.040 s)
2017-08-10 09:16:30,057 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 349 from persistence list
2017-08-10 09:16:30,057 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 349
2017-08-10 09:16:30,057 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 348 from persistence list
2017-08-10 09:16:30,057 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 348
2017-08-10 09:16:30,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:30,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327780000 ms
2017-08-10 09:16:35,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327795000 ms
2017-08-10 09:16:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327795000 ms.0 from job set of time 1502327795000 ms
2017-08-10 09:16:35,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 176 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 176 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 176 (MapPartitionsRDD[353] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:35,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_176 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:16:35,116 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_176_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:16:35,116 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_176_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:16:35,117 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 176 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:35,117 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_161_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:16:35,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 176 (MapPartitionsRDD[353] at map at streamingProcess.scala:48)
2017-08-10 09:16:35,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 176.0 with 2 tasks
2017-08-10 09:16:35,120 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_162_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:35,120 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 176.0 (TID 352, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:35,121 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 176.0 (TID 353, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:35,122 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 176.0 (TID 352)
2017-08-10 09:16:35,122 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_163_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:35,122 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 176.0 (TID 353)
2017-08-10 09:16:35,123 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_164_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:35,124 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_165_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:35,126 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_166_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:16:35,127 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_167_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:16:35,128 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_168_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:16:35,129 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:35,129 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:35,130 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_169_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:16:35,131 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_170_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:35,132 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_171_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:35,133 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 176.0 (TID 352). 714 bytes result sent to driver
2017-08-10 09:16:35,133 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 176.0 (TID 353). 714 bytes result sent to driver
2017-08-10 09:16:35,133 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_172_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:35,134 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_173_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:35,135 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 176.0 (TID 352) in 16 ms on localhost (1/2)
2017-08-10 09:16:35,135 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 176.0 (TID 353) in 15 ms on localhost (2/2)
2017-08-10 09:16:35,135 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 176.0, whose tasks have all completed, from pool 
2017-08-10 09:16:35,135 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_174_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:16:35,136 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 176 (foreachPartition at streamingProcess.scala:50) finished in 0.016 s
2017-08-10 09:16:35,136 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 176 finished: foreachPartition at streamingProcess.scala:50, took 0.076833 s
2017-08-10 09:16:35,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327795000 ms.0 from job set of time 1502327795000 ms
2017-08-10 09:16:35,137 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.136 s for time 1502327795000 ms (execution: 0.115 s)
2017-08-10 09:16:35,137 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 351 from persistence list
2017-08-10 09:16:35,137 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_175_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:16:35,137 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 351
2017-08-10 09:16:35,137 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 350 from persistence list
2017-08-10 09:16:35,137 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 350
2017-08-10 09:16:35,138 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:35,138 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327785000 ms
2017-08-10 09:16:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327800000 ms
2017-08-10 09:16:40,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327800000 ms.0 from job set of time 1502327800000 ms
2017-08-10 09:16:40,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 177 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 177 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 177 (MapPartitionsRDD[355] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:40,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_177 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:16:40,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_177_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:16:40,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_177_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:16:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 177 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 177 (MapPartitionsRDD[355] at map at streamingProcess.scala:48)
2017-08-10 09:16:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 177.0 with 2 tasks
2017-08-10 09:16:40,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 177.0 (TID 354, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:40,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 177.0 (TID 355, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:40,031 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 177.0 (TID 354)
2017-08-10 09:16:40,032 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 177.0 (TID 355)
2017-08-10 09:16:40,033 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:40,035 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:40,036 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 177.0 (TID 354). 714 bytes result sent to driver
2017-08-10 09:16:40,037 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 177.0 (TID 355). 714 bytes result sent to driver
2017-08-10 09:16:40,037 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 177.0 (TID 354) in 7 ms on localhost (1/2)
2017-08-10 09:16:40,039 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 177.0 (TID 355) in 7 ms on localhost (2/2)
2017-08-10 09:16:40,039 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 177.0, whose tasks have all completed, from pool 
2017-08-10 09:16:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 177 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:16:40,039 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 177 finished: foreachPartition at streamingProcess.scala:50, took 0.015541 s
2017-08-10 09:16:40,039 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327800000 ms.0 from job set of time 1502327800000 ms
2017-08-10 09:16:40,039 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.039 s for time 1502327800000 ms (execution: 0.026 s)
2017-08-10 09:16:40,039 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 353 from persistence list
2017-08-10 09:16:40,039 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 352 from persistence list
2017-08-10 09:16:40,039 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 353
2017-08-10 09:16:40,040 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 352
2017-08-10 09:16:40,040 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:40,040 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327790000 ms
2017-08-10 09:16:45,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327805000 ms
2017-08-10 09:16:45,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327805000 ms.0 from job set of time 1502327805000 ms
2017-08-10 09:16:45,056 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:45,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 178 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:45,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 178 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 178 (MapPartitionsRDD[357] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:45,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_178 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:16:45,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_178_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:16:45,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_178_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:45,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 178 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:45,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 178 (MapPartitionsRDD[357] at map at streamingProcess.scala:48)
2017-08-10 09:16:45,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 178.0 with 2 tasks
2017-08-10 09:16:45,078 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 178.0 (TID 356, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:45,079 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 178.0 (TID 357, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:45,080 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 178.0 (TID 357)
2017-08-10 09:16:45,080 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 178.0 (TID 356)
2017-08-10 09:16:45,085 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:45,085 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:45,090 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 178.0 (TID 357). 714 bytes result sent to driver
2017-08-10 09:16:45,090 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 178.0 (TID 356). 714 bytes result sent to driver
2017-08-10 09:16:45,094 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 178.0 (TID 356) in 15 ms on localhost (1/2)
2017-08-10 09:16:45,094 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 178.0 (TID 357) in 15 ms on localhost (2/2)
2017-08-10 09:16:45,094 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 178.0, whose tasks have all completed, from pool 
2017-08-10 09:16:45,095 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 178 (foreachPartition at streamingProcess.scala:50) finished in 0.018 s
2017-08-10 09:16:45,095 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 178 finished: foreachPartition at streamingProcess.scala:50, took 0.038475 s
2017-08-10 09:16:45,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327805000 ms.0 from job set of time 1502327805000 ms
2017-08-10 09:16:45,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.096 s for time 1502327805000 ms (execution: 0.076 s)
2017-08-10 09:16:45,096 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 355 from persistence list
2017-08-10 09:16:45,097 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 355
2017-08-10 09:16:45,097 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 354 from persistence list
2017-08-10 09:16:45,097 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 354
2017-08-10 09:16:45,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:45,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327795000 ms
2017-08-10 09:16:50,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327810000 ms
2017-08-10 09:16:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327810000 ms.0 from job set of time 1502327810000 ms
2017-08-10 09:16:50,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 179 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 179 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:50,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 179 (MapPartitionsRDD[359] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_179 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:16:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_179_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:16:50,048 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_179_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 179 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 179 (MapPartitionsRDD[359] at map at streamingProcess.scala:48)
2017-08-10 09:16:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 179.0 with 2 tasks
2017-08-10 09:16:50,049 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 179.0 (TID 358, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:50,049 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 179.0 (TID 359, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:50,050 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 179.0 (TID 358)
2017-08-10 09:16:50,050 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 179.0 (TID 359)
2017-08-10 09:16:50,052 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:50,052 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:50,055 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 179.0 (TID 358). 714 bytes result sent to driver
2017-08-10 09:16:50,055 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 179.0 (TID 359). 714 bytes result sent to driver
2017-08-10 09:16:50,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 179.0 (TID 359) in 8 ms on localhost (1/2)
2017-08-10 09:16:50,057 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 179.0 (TID 358) in 8 ms on localhost (2/2)
2017-08-10 09:16:50,057 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 179.0, whose tasks have all completed, from pool 
2017-08-10 09:16:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 179 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:16:50,058 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 179 finished: foreachPartition at streamingProcess.scala:50, took 0.017592 s
2017-08-10 09:16:50,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327810000 ms.0 from job set of time 1502327810000 ms
2017-08-10 09:16:50,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.058 s for time 1502327810000 ms (execution: 0.037 s)
2017-08-10 09:16:50,058 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 357 from persistence list
2017-08-10 09:16:50,058 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 357
2017-08-10 09:16:50,058 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 356 from persistence list
2017-08-10 09:16:50,059 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 356
2017-08-10 09:16:50,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:50,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327800000 ms
2017-08-10 09:16:55,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327815000 ms
2017-08-10 09:16:55,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327815000 ms.0 from job set of time 1502327815000 ms
2017-08-10 09:16:55,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:55,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 180 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 180 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 180 (MapPartitionsRDD[361] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_180 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:16:55,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_180_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:16:55,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_180_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:55,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 180 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:55,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 180 (MapPartitionsRDD[361] at map at streamingProcess.scala:48)
2017-08-10 09:16:55,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 180.0 with 2 tasks
2017-08-10 09:16:55,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 180.0 (TID 360, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:55,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 180.0 (TID 361, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:55,074 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 180.0 (TID 360)
2017-08-10 09:16:55,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 180.0 (TID 361)
2017-08-10 09:16:55,077 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:55,077 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:55,081 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 180.0 (TID 361). 714 bytes result sent to driver
2017-08-10 09:16:55,081 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 180.0 (TID 360). 714 bytes result sent to driver
2017-08-10 09:16:55,084 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 180.0 (TID 361) in 10 ms on localhost (1/2)
2017-08-10 09:16:55,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 180.0 (TID 360) in 11 ms on localhost (2/2)
2017-08-10 09:16:55,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 180.0, whose tasks have all completed, from pool 
2017-08-10 09:16:55,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 180 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:16:55,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 180 finished: foreachPartition at streamingProcess.scala:50, took 0.031105 s
2017-08-10 09:16:55,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327815000 ms.0 from job set of time 1502327815000 ms
2017-08-10 09:16:55,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1502327815000 ms (execution: 0.065 s)
2017-08-10 09:16:55,086 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 359 from persistence list
2017-08-10 09:16:55,086 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 359
2017-08-10 09:16:55,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 358 from persistence list
2017-08-10 09:16:55,087 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 358
2017-08-10 09:16:55,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:55,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327805000 ms
2017-08-10 09:17:00,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327820000 ms
2017-08-10 09:17:00,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327820000 ms.0 from job set of time 1502327820000 ms
2017-08-10 09:17:00,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 181 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 181 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 181 (MapPartitionsRDD[363] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_181 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:17:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_181_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:17:00,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_181_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:17:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 181 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 181 (MapPartitionsRDD[363] at map at streamingProcess.scala:48)
2017-08-10 09:17:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 181.0 with 2 tasks
2017-08-10 09:17:00,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 181.0 (TID 362, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:00,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 181.0 (TID 363, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:00,064 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 181.0 (TID 363)
2017-08-10 09:17:00,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 181.0 (TID 362)
2017-08-10 09:17:00,068 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:00,068 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:00,071 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 181.0 (TID 363). 714 bytes result sent to driver
2017-08-10 09:17:00,071 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 181.0 (TID 362). 714 bytes result sent to driver
2017-08-10 09:17:00,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 181.0 (TID 363) in 9 ms on localhost (1/2)
2017-08-10 09:17:00,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 181.0 (TID 362) in 11 ms on localhost (2/2)
2017-08-10 09:17:00,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 181.0, whose tasks have all completed, from pool 
2017-08-10 09:17:00,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 181 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:17:00,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 181 finished: foreachPartition at streamingProcess.scala:50, took 0.022377 s
2017-08-10 09:17:00,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327820000 ms.0 from job set of time 1502327820000 ms
2017-08-10 09:17:00,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1502327820000 ms (execution: 0.056 s)
2017-08-10 09:17:00,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 361 from persistence list
2017-08-10 09:17:00,075 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 361
2017-08-10 09:17:00,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 360 from persistence list
2017-08-10 09:17:00,076 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 360
2017-08-10 09:17:00,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:00,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327810000 ms
2017-08-10 09:17:05,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327825000 ms
2017-08-10 09:17:05,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327825000 ms.0 from job set of time 1502327825000 ms
2017-08-10 09:17:05,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 182 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 182 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 182 (MapPartitionsRDD[365] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:05,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_182 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:17:05,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_182_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:17:05,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_182_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:05,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 182 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 182 (MapPartitionsRDD[365] at map at streamingProcess.scala:48)
2017-08-10 09:17:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 182.0 with 2 tasks
2017-08-10 09:17:05,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 182.0 (TID 364, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:05,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 182.0 (TID 365, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:05,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 182.0 (TID 365)
2017-08-10 09:17:05,077 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 182.0 (TID 364)
2017-08-10 09:17:05,082 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:05,082 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:05,088 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 182.0 (TID 365). 714 bytes result sent to driver
2017-08-10 09:17:05,088 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 182.0 (TID 364). 714 bytes result sent to driver
2017-08-10 09:17:05,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 182.0 (TID 365) in 15 ms on localhost (1/2)
2017-08-10 09:17:05,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 182.0 (TID 364) in 17 ms on localhost (2/2)
2017-08-10 09:17:05,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 182.0, whose tasks have all completed, from pool 
2017-08-10 09:17:05,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 182 (foreachPartition at streamingProcess.scala:50) finished in 0.018 s
2017-08-10 09:17:05,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 182 finished: foreachPartition at streamingProcess.scala:50, took 0.038201 s
2017-08-10 09:17:05,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327825000 ms.0 from job set of time 1502327825000 ms
2017-08-10 09:17:05,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.093 s for time 1502327825000 ms (execution: 0.073 s)
2017-08-10 09:17:05,094 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 363 from persistence list
2017-08-10 09:17:05,094 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 363
2017-08-10 09:17:05,094 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 362 from persistence list
2017-08-10 09:17:05,095 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 362
2017-08-10 09:17:05,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:05,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327815000 ms
2017-08-10 09:17:10,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327830000 ms
2017-08-10 09:17:10,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327830000 ms.0 from job set of time 1502327830000 ms
2017-08-10 09:17:10,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 183 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 183 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 183 (MapPartitionsRDD[367] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_183 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:17:10,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_183_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:17:10,053 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_183_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:10,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 183 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 183 (MapPartitionsRDD[367] at map at streamingProcess.scala:48)
2017-08-10 09:17:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 183.0 with 2 tasks
2017-08-10 09:17:10,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 183.0 (TID 366, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:10,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 183.0 (TID 367, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:10,055 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 183.0 (TID 367)
2017-08-10 09:17:10,055 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 183.0 (TID 366)
2017-08-10 09:17:10,057 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:10,057 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:10,060 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 183.0 (TID 366). 714 bytes result sent to driver
2017-08-10 09:17:10,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 183.0 (TID 367). 714 bytes result sent to driver
2017-08-10 09:17:10,062 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 183.0 (TID 366) in 8 ms on localhost (1/2)
2017-08-10 09:17:10,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 183.0 (TID 367) in 8 ms on localhost (2/2)
2017-08-10 09:17:10,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 183.0, whose tasks have all completed, from pool 
2017-08-10 09:17:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 183 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:17:10,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 183 finished: foreachPartition at streamingProcess.scala:50, took 0.017295 s
2017-08-10 09:17:10,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327830000 ms.0 from job set of time 1502327830000 ms
2017-08-10 09:17:10,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1502327830000 ms (execution: 0.045 s)
2017-08-10 09:17:10,063 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 365 from persistence list
2017-08-10 09:17:10,063 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 365
2017-08-10 09:17:10,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 364 from persistence list
2017-08-10 09:17:10,064 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 364
2017-08-10 09:17:10,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:10,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327820000 ms
2017-08-10 09:17:15,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327835000 ms
2017-08-10 09:17:15,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327835000 ms.0 from job set of time 1502327835000 ms
2017-08-10 09:17:15,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 184 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 184 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 184 (MapPartitionsRDD[369] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:15,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_184 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:17:15,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_184_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:17:15,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_184_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:15,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 184 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:15,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 184 (MapPartitionsRDD[369] at map at streamingProcess.scala:48)
2017-08-10 09:17:15,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 184.0 with 2 tasks
2017-08-10 09:17:15,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 184.0 (TID 368, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:15,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 184.0 (TID 369, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:15,067 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 184.0 (TID 368)
2017-08-10 09:17:15,067 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 184.0 (TID 369)
2017-08-10 09:17:15,070 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:15,070 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:15,074 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 184.0 (TID 369). 714 bytes result sent to driver
2017-08-10 09:17:15,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 184.0 (TID 368). 714 bytes result sent to driver
2017-08-10 09:17:15,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 184.0 (TID 368) in 11 ms on localhost (1/2)
2017-08-10 09:17:15,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 184.0 (TID 369) in 10 ms on localhost (2/2)
2017-08-10 09:17:15,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 184.0, whose tasks have all completed, from pool 
2017-08-10 09:17:15,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 184 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:17:15,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 184 finished: foreachPartition at streamingProcess.scala:50, took 0.022662 s
2017-08-10 09:17:15,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327835000 ms.0 from job set of time 1502327835000 ms
2017-08-10 09:17:15,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1502327835000 ms (execution: 0.058 s)
2017-08-10 09:17:15,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 367 from persistence list
2017-08-10 09:17:15,078 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 367
2017-08-10 09:17:15,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 366 from persistence list
2017-08-10 09:17:15,078 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 366
2017-08-10 09:17:15,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:15,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327825000 ms
2017-08-10 09:17:20,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327840000 ms
2017-08-10 09:17:20,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327840000 ms.0 from job set of time 1502327840000 ms
2017-08-10 09:17:20,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 185 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 185 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:20,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:20,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 185 (MapPartitionsRDD[371] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_185 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:17:20,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_185_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:17:20,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_185_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:20,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 185 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:20,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 185 (MapPartitionsRDD[371] at map at streamingProcess.scala:48)
2017-08-10 09:17:20,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 185.0 with 2 tasks
2017-08-10 09:17:20,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 185.0 (TID 370, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:20,076 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 185.0 (TID 371, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:20,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 185.0 (TID 370)
2017-08-10 09:17:20,077 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 185.0 (TID 371)
2017-08-10 09:17:20,083 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:20,083 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:20,089 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 185.0 (TID 371). 714 bytes result sent to driver
2017-08-10 09:17:20,089 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 185.0 (TID 370). 714 bytes result sent to driver
2017-08-10 09:17:20,092 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 185.0 (TID 371) in 16 ms on localhost (1/2)
2017-08-10 09:17:20,093 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 185.0 (TID 370) in 17 ms on localhost (2/2)
2017-08-10 09:17:20,093 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 185.0, whose tasks have all completed, from pool 
2017-08-10 09:17:20,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 185 (foreachPartition at streamingProcess.scala:50) finished in 0.019 s
2017-08-10 09:17:20,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 185 finished: foreachPartition at streamingProcess.scala:50, took 0.039601 s
2017-08-10 09:17:20,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327840000 ms.0 from job set of time 1502327840000 ms
2017-08-10 09:17:20,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.093 s for time 1502327840000 ms (execution: 0.074 s)
2017-08-10 09:17:20,094 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 369 from persistence list
2017-08-10 09:17:20,094 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 369
2017-08-10 09:17:20,094 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 368 from persistence list
2017-08-10 09:17:20,095 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 368
2017-08-10 09:17:20,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:20,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327830000 ms
2017-08-10 09:17:25,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327845000 ms
2017-08-10 09:17:25,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327845000 ms.0 from job set of time 1502327845000 ms
2017-08-10 09:17:25,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 186 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 186 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 186 (MapPartitionsRDD[373] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_186 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:17:25,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_186_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:17:25,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_186_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:25,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 186 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:25,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 186 (MapPartitionsRDD[373] at map at streamingProcess.scala:48)
2017-08-10 09:17:25,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 186.0 with 2 tasks
2017-08-10 09:17:25,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 186.0 (TID 372, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:25,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 186.0 (TID 373, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:25,055 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 186.0 (TID 372)
2017-08-10 09:17:25,055 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 186.0 (TID 373)
2017-08-10 09:17:25,058 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:25,058 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:25,061 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 186.0 (TID 373). 714 bytes result sent to driver
2017-08-10 09:17:25,061 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 186.0 (TID 372). 714 bytes result sent to driver
2017-08-10 09:17:25,063 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 186.0 (TID 373) in 8 ms on localhost (1/2)
2017-08-10 09:17:25,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 186.0 (TID 372) in 9 ms on localhost (2/2)
2017-08-10 09:17:25,064 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 186.0, whose tasks have all completed, from pool 
2017-08-10 09:17:25,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 186 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:17:25,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 186 finished: foreachPartition at streamingProcess.scala:50, took 0.019028 s
2017-08-10 09:17:25,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327845000 ms.0 from job set of time 1502327845000 ms
2017-08-10 09:17:25,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1502327845000 ms (execution: 0.048 s)
2017-08-10 09:17:25,065 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 371 from persistence list
2017-08-10 09:17:25,065 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 371
2017-08-10 09:17:25,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 370 from persistence list
2017-08-10 09:17:25,065 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 370
2017-08-10 09:17:25,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:25,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327835000 ms
2017-08-10 09:17:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327850000 ms
2017-08-10 09:17:30,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327850000 ms.0 from job set of time 1502327850000 ms
2017-08-10 09:17:30,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 187 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 187 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 187 (MapPartitionsRDD[375] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_187 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:17:30,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_187_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:17:30,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_187_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:30,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 187 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:30,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 187 (MapPartitionsRDD[375] at map at streamingProcess.scala:48)
2017-08-10 09:17:30,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 187.0 with 2 tasks
2017-08-10 09:17:30,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 187.0 (TID 374, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:30,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 187.0 (TID 375, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:30,075 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 187.0 (TID 375)
2017-08-10 09:17:30,075 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 187.0 (TID 374)
2017-08-10 09:17:30,080 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:30,080 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:30,084 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 187.0 (TID 375). 714 bytes result sent to driver
2017-08-10 09:17:30,085 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 187.0 (TID 374). 714 bytes result sent to driver
2017-08-10 09:17:30,087 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 187.0 (TID 375) in 13 ms on localhost (1/2)
2017-08-10 09:17:30,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 187.0 (TID 374) in 16 ms on localhost (2/2)
2017-08-10 09:17:30,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 187.0, whose tasks have all completed, from pool 
2017-08-10 09:17:30,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 187 (foreachPartition at streamingProcess.scala:50) finished in 0.016 s
2017-08-10 09:17:30,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 187 finished: foreachPartition at streamingProcess.scala:50, took 0.034897 s
2017-08-10 09:17:30,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327850000 ms.0 from job set of time 1502327850000 ms
2017-08-10 09:17:30,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1502327850000 ms (execution: 0.069 s)
2017-08-10 09:17:30,089 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 373 from persistence list
2017-08-10 09:17:30,090 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 373
2017-08-10 09:17:30,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 372 from persistence list
2017-08-10 09:17:30,090 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 372
2017-08-10 09:17:30,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:30,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327840000 ms
2017-08-10 09:17:35,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327855000 ms
2017-08-10 09:17:35,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327855000 ms.0 from job set of time 1502327855000 ms
2017-08-10 09:17:35,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:35,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 188 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:35,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 188 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:35,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:35,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:35,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 188 (MapPartitionsRDD[377] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_188 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:17:35,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_188_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:17:35,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_188_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:35,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 188 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:35,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 188 (MapPartitionsRDD[377] at map at streamingProcess.scala:48)
2017-08-10 09:17:35,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 188.0 with 2 tasks
2017-08-10 09:17:35,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 188.0 (TID 376, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:35,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 188.0 (TID 377, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:35,055 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 188.0 (TID 376)
2017-08-10 09:17:35,055 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 188.0 (TID 377)
2017-08-10 09:17:35,059 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:35,059 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:35,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 188.0 (TID 377). 714 bytes result sent to driver
2017-08-10 09:17:35,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 188.0 (TID 376). 714 bytes result sent to driver
2017-08-10 09:17:35,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 188.0 (TID 377) in 11 ms on localhost (1/2)
2017-08-10 09:17:35,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 188.0 (TID 376) in 13 ms on localhost (2/2)
2017-08-10 09:17:35,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 188.0, whose tasks have all completed, from pool 
2017-08-10 09:17:35,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 188 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-10 09:17:35,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 188 finished: foreachPartition at streamingProcess.scala:50, took 0.027509 s
2017-08-10 09:17:35,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327855000 ms.0 from job set of time 1502327855000 ms
2017-08-10 09:17:35,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1502327855000 ms (execution: 0.051 s)
2017-08-10 09:17:35,067 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 375 from persistence list
2017-08-10 09:17:35,068 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 375
2017-08-10 09:17:35,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 374 from persistence list
2017-08-10 09:17:35,069 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 374
2017-08-10 09:17:35,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:35,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327845000 ms
2017-08-10 09:17:40,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327860000 ms
2017-08-10 09:17:40,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327860000 ms.0 from job set of time 1502327860000 ms
2017-08-10 09:17:40,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 189 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 189 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 189 (MapPartitionsRDD[379] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:40,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_189 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:17:40,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_189_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:17:40,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_189_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 189 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 189 (MapPartitionsRDD[379] at map at streamingProcess.scala:48)
2017-08-10 09:17:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 189.0 with 2 tasks
2017-08-10 09:17:40,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 189.0 (TID 378, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:40,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 189.0 (TID 379, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:40,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 189.0 (TID 378)
2017-08-10 09:17:40,074 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 189.0 (TID 379)
2017-08-10 09:17:40,077 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:40,077 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:40,080 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 189.0 (TID 378). 714 bytes result sent to driver
2017-08-10 09:17:40,080 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 189.0 (TID 379). 714 bytes result sent to driver
2017-08-10 09:17:40,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 189.0 (TID 379) in 8 ms on localhost (1/2)
2017-08-10 09:17:40,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 189.0 (TID 378) in 10 ms on localhost (2/2)
2017-08-10 09:17:40,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 189.0, whose tasks have all completed, from pool 
2017-08-10 09:17:40,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 189 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:17:40,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 189 finished: foreachPartition at streamingProcess.scala:50, took 0.027758 s
2017-08-10 09:17:40,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327860000 ms.0 from job set of time 1502327860000 ms
2017-08-10 09:17:40,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1502327860000 ms (execution: 0.063 s)
2017-08-10 09:17:40,084 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 377 from persistence list
2017-08-10 09:17:40,084 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 377
2017-08-10 09:17:40,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 376 from persistence list
2017-08-10 09:17:40,084 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 376
2017-08-10 09:17:40,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:40,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327850000 ms
2017-08-10 09:17:45,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327865000 ms
2017-08-10 09:17:45,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327865000 ms.0 from job set of time 1502327865000 ms
2017-08-10 09:17:45,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 190 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 190 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 190 (MapPartitionsRDD[381] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_190 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:17:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_190_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:17:45,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_190_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:17:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 190 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 190 (MapPartitionsRDD[381] at map at streamingProcess.scala:48)
2017-08-10 09:17:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 190.0 with 2 tasks
2017-08-10 09:17:45,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 190.0 (TID 380, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:45,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 190.0 (TID 381, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:45,057 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 190.0 (TID 381)
2017-08-10 09:17:45,057 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 190.0 (TID 380)
2017-08-10 09:17:45,059 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:45,059 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:45,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 190.0 (TID 381). 714 bytes result sent to driver
2017-08-10 09:17:45,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 190.0 (TID 380). 714 bytes result sent to driver
2017-08-10 09:17:45,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 190.0 (TID 381) in 7 ms on localhost (1/2)
2017-08-10 09:17:45,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 190.0 (TID 380) in 9 ms on localhost (2/2)
2017-08-10 09:17:45,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 190.0, whose tasks have all completed, from pool 
2017-08-10 09:17:45,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 190 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:17:45,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 190 finished: foreachPartition at streamingProcess.scala:50, took 0.020680 s
2017-08-10 09:17:45,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327865000 ms.0 from job set of time 1502327865000 ms
2017-08-10 09:17:45,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1502327865000 ms (execution: 0.047 s)
2017-08-10 09:17:45,066 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 379 from persistence list
2017-08-10 09:17:45,066 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 379
2017-08-10 09:17:45,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 378 from persistence list
2017-08-10 09:17:45,066 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 378
2017-08-10 09:17:45,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:45,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327855000 ms
2017-08-10 09:17:50,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327870000 ms
2017-08-10 09:17:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327870000 ms.0 from job set of time 1502327870000 ms
2017-08-10 09:17:50,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 191 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 191 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 191 (MapPartitionsRDD[383] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:50,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_191 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:17:50,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_191_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:17:50,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_176_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:50,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_191_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:17:50,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 191 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:50,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 191 (MapPartitionsRDD[383] at map at streamingProcess.scala:48)
2017-08-10 09:17:50,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 191.0 with 2 tasks
2017-08-10 09:17:50,093 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_177_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:50,093 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 191.0 (TID 382, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:50,093 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 191.0 (TID 383, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:50,093 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_178_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:50,093 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 191.0 (TID 382)
2017-08-10 09:17:50,093 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 191.0 (TID 383)
2017-08-10 09:17:50,094 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_179_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:50,095 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_180_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:50,095 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:50,095 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:50,096 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_181_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:50,097 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_182_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:50,098 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_183_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:50,098 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 191.0 (TID 383). 714 bytes result sent to driver
2017-08-10 09:17:50,098 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 191.0 (TID 382). 714 bytes result sent to driver
2017-08-10 09:17:50,099 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_184_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:50,100 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 191.0 (TID 383) in 7 ms on localhost (1/2)
2017-08-10 09:17:50,100 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_185_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:17:50,100 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 191.0 (TID 382) in 7 ms on localhost (2/2)
2017-08-10 09:17:50,100 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 191.0, whose tasks have all completed, from pool 
2017-08-10 09:17:50,100 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 191 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:17:50,100 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 191 finished: foreachPartition at streamingProcess.scala:50, took 0.045406 s
2017-08-10 09:17:50,100 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_186_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:17:50,100 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327870000 ms.0 from job set of time 1502327870000 ms
2017-08-10 09:17:50,100 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.100 s for time 1502327870000 ms (execution: 0.079 s)
2017-08-10 09:17:50,100 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 381 from persistence list
2017-08-10 09:17:50,101 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 381
2017-08-10 09:17:50,101 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 380 from persistence list
2017-08-10 09:17:50,101 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 380
2017-08-10 09:17:50,101 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:50,101 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_187_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:17:50,101 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327860000 ms
2017-08-10 09:17:50,102 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_188_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:17:50,104 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_189_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:17:50,105 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_190_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:17:55,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327875000 ms
2017-08-10 09:17:55,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327875000 ms.0 from job set of time 1502327875000 ms
2017-08-10 09:17:55,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:55,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 192 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:55,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 192 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:55,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:55,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:55,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 192 (MapPartitionsRDD[385] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:55,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_192 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:17:55,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_192_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:17:55,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_192_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:17:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 192 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 192 (MapPartitionsRDD[385] at map at streamingProcess.scala:48)
2017-08-10 09:17:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 192.0 with 2 tasks
2017-08-10 09:17:55,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 192.0 (TID 384, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:55,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 192.0 (TID 385, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:55,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 192.0 (TID 385)
2017-08-10 09:17:55,060 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 192.0 (TID 384)
2017-08-10 09:17:55,062 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:55,063 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:55,066 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 192.0 (TID 384). 714 bytes result sent to driver
2017-08-10 09:17:55,066 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 192.0 (TID 385). 714 bytes result sent to driver
2017-08-10 09:17:55,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 192.0 (TID 385) in 9 ms on localhost (1/2)
2017-08-10 09:17:55,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 192.0 (TID 384) in 10 ms on localhost (2/2)
2017-08-10 09:17:55,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 192.0, whose tasks have all completed, from pool 
2017-08-10 09:17:55,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 192 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:17:55,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 192 finished: foreachPartition at streamingProcess.scala:50, took 0.021325 s
2017-08-10 09:17:55,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327875000 ms.0 from job set of time 1502327875000 ms
2017-08-10 09:17:55,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1502327875000 ms (execution: 0.050 s)
2017-08-10 09:17:55,070 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 383 from persistence list
2017-08-10 09:17:55,070 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 383
2017-08-10 09:17:55,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 382 from persistence list
2017-08-10 09:17:55,070 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 382
2017-08-10 09:17:55,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:55,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327865000 ms
2017-08-10 09:18:00,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327880000 ms
2017-08-10 09:18:00,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327880000 ms.0 from job set of time 1502327880000 ms
2017-08-10 09:18:00,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 193 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 193 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 193 (MapPartitionsRDD[387] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_193 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:18:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_193_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:18:00,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_193_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:18:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 193 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 193 (MapPartitionsRDD[387] at map at streamingProcess.scala:48)
2017-08-10 09:18:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 193.0 with 2 tasks
2017-08-10 09:18:00,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 193.0 (TID 386, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:00,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 193.0 (TID 387, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:00,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 193.0 (TID 387)
2017-08-10 09:18:00,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 193.0 (TID 386)
2017-08-10 09:18:00,066 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:00,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:00,070 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 193.0 (TID 387). 714 bytes result sent to driver
2017-08-10 09:18:00,070 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 193.0 (TID 386). 714 bytes result sent to driver
2017-08-10 09:18:00,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 193.0 (TID 387) in 9 ms on localhost (1/2)
2017-08-10 09:18:00,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 193.0 (TID 386) in 10 ms on localhost (2/2)
2017-08-10 09:18:00,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 193.0, whose tasks have all completed, from pool 
2017-08-10 09:18:00,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 193 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:18:00,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 193 finished: foreachPartition at streamingProcess.scala:50, took 0.021657 s
2017-08-10 09:18:00,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327880000 ms.0 from job set of time 1502327880000 ms
2017-08-10 09:18:00,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1502327880000 ms (execution: 0.055 s)
2017-08-10 09:18:00,073 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 385 from persistence list
2017-08-10 09:18:00,073 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 385
2017-08-10 09:18:00,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 384 from persistence list
2017-08-10 09:18:00,074 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 384
2017-08-10 09:18:00,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:00,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327870000 ms
2017-08-10 09:18:05,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327885000 ms
2017-08-10 09:18:05,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327885000 ms.0 from job set of time 1502327885000 ms
2017-08-10 09:18:05,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 194 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 194 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 194 (MapPartitionsRDD[389] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:05,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_194 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:18:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_194_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:18:05,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_194_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:18:05,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 194 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:05,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 194 (MapPartitionsRDD[389] at map at streamingProcess.scala:48)
2017-08-10 09:18:05,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 194.0 with 2 tasks
2017-08-10 09:18:05,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 194.0 (TID 388, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:05,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 194.0 (TID 389, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:05,059 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 194.0 (TID 389)
2017-08-10 09:18:05,059 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 194.0 (TID 388)
2017-08-10 09:18:05,063 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:05,063 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:05,067 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 194.0 (TID 388). 714 bytes result sent to driver
2017-08-10 09:18:05,067 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 194.0 (TID 389). 714 bytes result sent to driver
2017-08-10 09:18:05,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 194.0 (TID 388) in 11 ms on localhost (1/2)
2017-08-10 09:18:05,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 194.0 (TID 389) in 10 ms on localhost (2/2)
2017-08-10 09:18:05,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 194.0, whose tasks have all completed, from pool 
2017-08-10 09:18:05,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 194 (foreachPartition at streamingProcess.scala:50) finished in 0.012 s
2017-08-10 09:18:05,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 194 finished: foreachPartition at streamingProcess.scala:50, took 0.026009 s
2017-08-10 09:18:05,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327885000 ms.0 from job set of time 1502327885000 ms
2017-08-10 09:18:05,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1502327885000 ms (execution: 0.053 s)
2017-08-10 09:18:05,071 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 387 from persistence list
2017-08-10 09:18:05,071 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 387
2017-08-10 09:18:05,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 386 from persistence list
2017-08-10 09:18:05,072 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 386
2017-08-10 09:18:05,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:05,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327875000 ms
2017-08-10 09:18:10,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327890000 ms
2017-08-10 09:18:10,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327890000 ms.0 from job set of time 1502327890000 ms
2017-08-10 09:18:10,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 195 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 195 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 195 (MapPartitionsRDD[391] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_195 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:18:10,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_195_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:18:10,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_195_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:18:10,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 195 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:10,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 195 (MapPartitionsRDD[391] at map at streamingProcess.scala:48)
2017-08-10 09:18:10,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 195.0 with 2 tasks
2017-08-10 09:18:10,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 195.0 (TID 390, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:10,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 195.0 (TID 391, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:10,059 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 195.0 (TID 390)
2017-08-10 09:18:10,059 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 195.0 (TID 391)
2017-08-10 09:18:10,062 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:10,062 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:10,066 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 195.0 (TID 391). 714 bytes result sent to driver
2017-08-10 09:18:10,066 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 195.0 (TID 390). 714 bytes result sent to driver
2017-08-10 09:18:10,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 195.0 (TID 391) in 9 ms on localhost (1/2)
2017-08-10 09:18:10,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 195.0 (TID 390) in 10 ms on localhost (2/2)
2017-08-10 09:18:10,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 195 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:18:10,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 195.0, whose tasks have all completed, from pool 
2017-08-10 09:18:10,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 195 finished: foreachPartition at streamingProcess.scala:50, took 0.021892 s
2017-08-10 09:18:10,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327890000 ms.0 from job set of time 1502327890000 ms
2017-08-10 09:18:10,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1502327890000 ms (execution: 0.051 s)
2017-08-10 09:18:10,070 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 389 from persistence list
2017-08-10 09:18:10,070 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 389
2017-08-10 09:18:10,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 388 from persistence list
2017-08-10 09:18:10,071 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 388
2017-08-10 09:18:10,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:10,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327880000 ms
2017-08-10 09:18:15,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327895000 ms
2017-08-10 09:18:15,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327895000 ms.0 from job set of time 1502327895000 ms
2017-08-10 09:18:15,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 196 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 196 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 196 (MapPartitionsRDD[393] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:15,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_196 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:18:15,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_196_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:18:15,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_196_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:18:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 196 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 196 (MapPartitionsRDD[393] at map at streamingProcess.scala:48)
2017-08-10 09:18:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 196.0 with 2 tasks
2017-08-10 09:18:15,064 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 196.0 (TID 392, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:15,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 196.0 (TID 393, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:15,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 196.0 (TID 393)
2017-08-10 09:18:15,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 196.0 (TID 392)
2017-08-10 09:18:15,068 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:15,068 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:15,071 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 196.0 (TID 393). 714 bytes result sent to driver
2017-08-10 09:18:15,071 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 196.0 (TID 392). 714 bytes result sent to driver
2017-08-10 09:18:15,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 196.0 (TID 393) in 9 ms on localhost (1/2)
2017-08-10 09:18:15,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 196.0 (TID 392) in 10 ms on localhost (2/2)
2017-08-10 09:18:15,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 196.0, whose tasks have all completed, from pool 
2017-08-10 09:18:15,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 196 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:18:15,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 196 finished: foreachPartition at streamingProcess.scala:50, took 0.021758 s
2017-08-10 09:18:15,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327895000 ms.0 from job set of time 1502327895000 ms
2017-08-10 09:18:15,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1502327895000 ms (execution: 0.056 s)
2017-08-10 09:18:15,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 391 from persistence list
2017-08-10 09:18:15,075 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 391
2017-08-10 09:18:15,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 390 from persistence list
2017-08-10 09:18:15,076 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 390
2017-08-10 09:18:15,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:15,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327885000 ms
2017-08-10 09:18:20,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327900000 ms
2017-08-10 09:18:20,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327900000 ms.0 from job set of time 1502327900000 ms
2017-08-10 09:18:20,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 197 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 197 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 197 (MapPartitionsRDD[395] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:20,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_197 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:18:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_197_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:18:20,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_197_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:18:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 197 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 197 (MapPartitionsRDD[395] at map at streamingProcess.scala:48)
2017-08-10 09:18:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 197.0 with 2 tasks
2017-08-10 09:18:20,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 197.0 (TID 394, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:20,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 197.0 (TID 395, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:20,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 197.0 (TID 395)
2017-08-10 09:18:20,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 197.0 (TID 394)
2017-08-10 09:18:20,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:20,066 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:20,071 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 197.0 (TID 395). 714 bytes result sent to driver
2017-08-10 09:18:20,071 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 197.0 (TID 394). 714 bytes result sent to driver
2017-08-10 09:18:20,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 197.0 (TID 395) in 12 ms on localhost (1/2)
2017-08-10 09:18:20,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 197.0 (TID 394) in 13 ms on localhost (2/2)
2017-08-10 09:18:20,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 197.0, whose tasks have all completed, from pool 
2017-08-10 09:18:20,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 197 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-10 09:18:20,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 197 finished: foreachPartition at streamingProcess.scala:50, took 0.029346 s
2017-08-10 09:18:20,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327900000 ms.0 from job set of time 1502327900000 ms
2017-08-10 09:18:20,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1502327900000 ms (execution: 0.058 s)
2017-08-10 09:18:20,076 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 393 from persistence list
2017-08-10 09:18:20,076 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 393
2017-08-10 09:18:20,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 392 from persistence list
2017-08-10 09:18:20,077 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 392
2017-08-10 09:18:20,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:20,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327890000 ms
2017-08-10 09:18:25,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327905000 ms
2017-08-10 09:18:25,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327905000 ms.0 from job set of time 1502327905000 ms
2017-08-10 09:18:25,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:25,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 198 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:25,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 198 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:25,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:25,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 198 (MapPartitionsRDD[397] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:25,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_198 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:18:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_198_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:18:25,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_198_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:18:25,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 198 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:25,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 198 (MapPartitionsRDD[397] at map at streamingProcess.scala:48)
2017-08-10 09:18:25,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 198.0 with 2 tasks
2017-08-10 09:18:25,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 198.0 (TID 396, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:25,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 198.0 (TID 397, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:25,059 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 198.0 (TID 396)
2017-08-10 09:18:25,059 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 198.0 (TID 397)
2017-08-10 09:18:25,061 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:25,062 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:25,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 198.0 (TID 397). 714 bytes result sent to driver
2017-08-10 09:18:25,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 198.0 (TID 396). 714 bytes result sent to driver
2017-08-10 09:18:25,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 198.0 (TID 397) in 9 ms on localhost (1/2)
2017-08-10 09:18:25,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 198.0 (TID 396) in 10 ms on localhost (2/2)
2017-08-10 09:18:25,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 198.0, whose tasks have all completed, from pool 
2017-08-10 09:18:25,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 198 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:18:25,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 198 finished: foreachPartition at streamingProcess.scala:50, took 0.021178 s
2017-08-10 09:18:25,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327905000 ms.0 from job set of time 1502327905000 ms
2017-08-10 09:18:25,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1502327905000 ms (execution: 0.050 s)
2017-08-10 09:18:25,069 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 395 from persistence list
2017-08-10 09:18:25,069 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 395
2017-08-10 09:18:25,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 394 from persistence list
2017-08-10 09:18:25,069 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 394
2017-08-10 09:18:25,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:25,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327895000 ms
2017-08-10 09:18:30,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327910000 ms
2017-08-10 09:18:30,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327910000 ms.0 from job set of time 1502327910000 ms
2017-08-10 09:18:30,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 199 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 199 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 199 (MapPartitionsRDD[399] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:30,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_199 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:18:30,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_199_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:18:30,033 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_199_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:18:30,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 199 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 199 (MapPartitionsRDD[399] at map at streamingProcess.scala:48)
2017-08-10 09:18:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 199.0 with 2 tasks
2017-08-10 09:18:30,034 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 199.0 (TID 398, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:30,034 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 199.0 (TID 399, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:30,035 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 199.0 (TID 398)
2017-08-10 09:18:30,035 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 199.0 (TID 399)
2017-08-10 09:18:30,036 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:30,036 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:30,039 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 199.0 (TID 399). 714 bytes result sent to driver
2017-08-10 09:18:30,039 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 199.0 (TID 398). 714 bytes result sent to driver
2017-08-10 09:18:30,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 199.0 (TID 399) in 6 ms on localhost (1/2)
2017-08-10 09:18:30,040 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 199.0 (TID 398) in 6 ms on localhost (2/2)
2017-08-10 09:18:30,041 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 199.0, whose tasks have all completed, from pool 
2017-08-10 09:18:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 199 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:18:30,041 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 199 finished: foreachPartition at streamingProcess.scala:50, took 0.013302 s
2017-08-10 09:18:30,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327910000 ms.0 from job set of time 1502327910000 ms
2017-08-10 09:18:30,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1502327910000 ms (execution: 0.027 s)
2017-08-10 09:18:30,041 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 397 from persistence list
2017-08-10 09:18:30,042 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 397
2017-08-10 09:18:30,042 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 396 from persistence list
2017-08-10 09:18:30,042 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 396
2017-08-10 09:18:30,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:30,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327900000 ms
2017-08-10 09:18:35,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327915000 ms
2017-08-10 09:18:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327915000 ms.0 from job set of time 1502327915000 ms
2017-08-10 09:18:35,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:35,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 200 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:35,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 200 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:35,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:35,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:35,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 200 (MapPartitionsRDD[401] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:35,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_200 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:18:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_200_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:18:35,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_200_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:18:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 200 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 200 (MapPartitionsRDD[401] at map at streamingProcess.scala:48)
2017-08-10 09:18:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 200.0 with 2 tasks
2017-08-10 09:18:35,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 200.0 (TID 400, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:35,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 200.0 (TID 401, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:35,062 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 200.0 (TID 401)
2017-08-10 09:18:35,062 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 200.0 (TID 400)
2017-08-10 09:18:35,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:35,066 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:35,070 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 200.0 (TID 400). 714 bytes result sent to driver
2017-08-10 09:18:35,070 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 200.0 (TID 401). 714 bytes result sent to driver
2017-08-10 09:18:35,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 200.0 (TID 400) in 13 ms on localhost (1/2)
2017-08-10 09:18:35,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 200.0 (TID 401) in 12 ms on localhost (2/2)
2017-08-10 09:18:35,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 200.0, whose tasks have all completed, from pool 
2017-08-10 09:18:35,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 200 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-10 09:18:35,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 200 finished: foreachPartition at streamingProcess.scala:50, took 0.026860 s
2017-08-10 09:18:35,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327915000 ms.0 from job set of time 1502327915000 ms
2017-08-10 09:18:35,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1502327915000 ms (execution: 0.053 s)
2017-08-10 09:18:35,074 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 399 from persistence list
2017-08-10 09:18:35,075 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 399
2017-08-10 09:18:35,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 398 from persistence list
2017-08-10 09:18:35,075 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 398
2017-08-10 09:18:35,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:35,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327905000 ms
2017-08-10 09:18:40,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327920000 ms
2017-08-10 09:18:40,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327920000 ms.0 from job set of time 1502327920000 ms
2017-08-10 09:18:40,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 201 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 201 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 201 (MapPartitionsRDD[403] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_201 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:18:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_201_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:18:40,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_201_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:18:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 201 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 201 (MapPartitionsRDD[403] at map at streamingProcess.scala:48)
2017-08-10 09:18:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 201.0 with 2 tasks
2017-08-10 09:18:40,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 201.0 (TID 402, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:40,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 201.0 (TID 403, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:40,058 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 201.0 (TID 403)
2017-08-10 09:18:40,058 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 201.0 (TID 402)
2017-08-10 09:18:40,061 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:40,061 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:40,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 201.0 (TID 402). 714 bytes result sent to driver
2017-08-10 09:18:40,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 201.0 (TID 403). 714 bytes result sent to driver
2017-08-10 09:18:40,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 201.0 (TID 402) in 10 ms on localhost (1/2)
2017-08-10 09:18:40,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 201.0 (TID 403) in 9 ms on localhost (2/2)
2017-08-10 09:18:40,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 201.0, whose tasks have all completed, from pool 
2017-08-10 09:18:40,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 201 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:18:40,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 201 finished: foreachPartition at streamingProcess.scala:50, took 0.020780 s
2017-08-10 09:18:40,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327920000 ms.0 from job set of time 1502327920000 ms
2017-08-10 09:18:40,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1502327920000 ms (execution: 0.049 s)
2017-08-10 09:18:40,068 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 401 from persistence list
2017-08-10 09:18:40,068 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 401
2017-08-10 09:18:40,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 400 from persistence list
2017-08-10 09:18:40,069 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 400
2017-08-10 09:18:40,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:40,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327910000 ms
2017-08-10 09:18:45,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327925000 ms
2017-08-10 09:18:45,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327925000 ms.0 from job set of time 1502327925000 ms
2017-08-10 09:18:45,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 202 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 202 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 202 (MapPartitionsRDD[405] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:45,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_202 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:18:45,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_202_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:18:45,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_202_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:18:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 202 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 202 (MapPartitionsRDD[405] at map at streamingProcess.scala:48)
2017-08-10 09:18:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 202.0 with 2 tasks
2017-08-10 09:18:45,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 202.0 (TID 404, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:45,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 202.0 (TID 405, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:45,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 202.0 (TID 404)
2017-08-10 09:18:45,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 202.0 (TID 405)
2017-08-10 09:18:45,067 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:45,067 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:45,071 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 202.0 (TID 404). 714 bytes result sent to driver
2017-08-10 09:18:45,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 202.0 (TID 405). 714 bytes result sent to driver
2017-08-10 09:18:45,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 202.0 (TID 405) in 12 ms on localhost (1/2)
2017-08-10 09:18:45,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 202.0 (TID 404) in 14 ms on localhost (2/2)
2017-08-10 09:18:45,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 202.0, whose tasks have all completed, from pool 
2017-08-10 09:18:45,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 202 (foreachPartition at streamingProcess.scala:50) finished in 0.015 s
2017-08-10 09:18:45,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 202 finished: foreachPartition at streamingProcess.scala:50, took 0.031642 s
2017-08-10 09:18:45,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327925000 ms.0 from job set of time 1502327925000 ms
2017-08-10 09:18:45,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1502327925000 ms (execution: 0.058 s)
2017-08-10 09:18:45,078 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 403 from persistence list
2017-08-10 09:18:45,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 402 from persistence list
2017-08-10 09:18:45,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:45,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327915000 ms
2017-08-10 09:18:45,081 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 402
2017-08-10 09:18:45,081 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 403
2017-08-10 09:18:50,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327930000 ms
2017-08-10 09:18:50,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327930000 ms.0 from job set of time 1502327930000 ms
2017-08-10 09:18:50,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 203 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 203 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 203 (MapPartitionsRDD[407] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:50,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_203 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:18:50,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_203_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:18:50,073 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_203_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:18:50,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 203 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:50,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 203 (MapPartitionsRDD[407] at map at streamingProcess.scala:48)
2017-08-10 09:18:50,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 203.0 with 2 tasks
2017-08-10 09:18:50,076 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 203.0 (TID 406, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:50,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 203.0 (TID 407, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:50,078 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 203.0 (TID 407)
2017-08-10 09:18:50,078 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 203.0 (TID 406)
2017-08-10 09:18:50,083 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:50,083 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:50,089 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 203.0 (TID 406). 714 bytes result sent to driver
2017-08-10 09:18:50,089 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 203.0 (TID 407). 714 bytes result sent to driver
2017-08-10 09:18:50,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 203.0 (TID 407) in 17 ms on localhost (1/2)
2017-08-10 09:18:50,093 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 203.0 (TID 406) in 18 ms on localhost (2/2)
2017-08-10 09:18:50,094 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 203 (foreachPartition at streamingProcess.scala:50) finished in 0.019 s
2017-08-10 09:18:50,094 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 203.0, whose tasks have all completed, from pool 
2017-08-10 09:18:50,094 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 203 finished: foreachPartition at streamingProcess.scala:50, took 0.040459 s
2017-08-10 09:18:50,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327930000 ms.0 from job set of time 1502327930000 ms
2017-08-10 09:18:50,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.095 s for time 1502327930000 ms (execution: 0.076 s)
2017-08-10 09:18:50,095 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 405 from persistence list
2017-08-10 09:18:50,095 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 405
2017-08-10 09:18:50,095 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 404 from persistence list
2017-08-10 09:18:50,096 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 404
2017-08-10 09:18:50,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:50,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327920000 ms
2017-08-10 09:18:55,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327935000 ms
2017-08-10 09:18:55,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327935000 ms.0 from job set of time 1502327935000 ms
2017-08-10 09:18:55,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:55,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 204 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:55,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 204 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:55,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:55,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:55,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 204 (MapPartitionsRDD[409] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_204 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:18:55,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_204_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:18:55,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_204_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:18:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 204 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 204 (MapPartitionsRDD[409] at map at streamingProcess.scala:48)
2017-08-10 09:18:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 204.0 with 2 tasks
2017-08-10 09:18:55,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 204.0 (TID 408, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:55,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 204.0 (TID 409, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:55,059 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 204.0 (TID 409)
2017-08-10 09:18:55,060 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 204.0 (TID 408)
2017-08-10 09:18:55,062 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:55,062 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:55,066 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 204.0 (TID 408). 714 bytes result sent to driver
2017-08-10 09:18:55,066 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 204.0 (TID 409). 714 bytes result sent to driver
2017-08-10 09:18:55,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 204.0 (TID 408) in 10 ms on localhost (1/2)
2017-08-10 09:18:55,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 204.0 (TID 409) in 10 ms on localhost (2/2)
2017-08-10 09:18:55,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 204.0, whose tasks have all completed, from pool 
2017-08-10 09:18:55,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 204 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:18:55,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 204 finished: foreachPartition at streamingProcess.scala:50, took 0.022442 s
2017-08-10 09:18:55,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327935000 ms.0 from job set of time 1502327935000 ms
2017-08-10 09:18:55,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1502327935000 ms (execution: 0.051 s)
2017-08-10 09:18:55,070 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 407 from persistence list
2017-08-10 09:18:55,071 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 407
2017-08-10 09:18:55,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 406 from persistence list
2017-08-10 09:18:55,071 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 406
2017-08-10 09:18:55,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:55,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327925000 ms
2017-08-10 09:19:00,109 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327940000 ms
2017-08-10 09:19:00,109 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327940000 ms.0 from job set of time 1502327940000 ms
2017-08-10 09:19:00,120 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:00,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 205 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:00,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 205 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:00,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:00,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:00,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 205 (MapPartitionsRDD[411] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:00,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_205 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:19:00,126 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_205_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:19:00,126 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_205_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:19:00,127 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 205 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:00,127 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 205 (MapPartitionsRDD[411] at map at streamingProcess.scala:48)
2017-08-10 09:19:00,127 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 205.0 with 2 tasks
2017-08-10 09:19:00,128 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 205.0 (TID 410, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:00,128 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 205.0 (TID 411, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:00,128 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 205.0 (TID 410)
2017-08-10 09:19:00,128 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 205.0 (TID 411)
2017-08-10 09:19:00,130 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:00,130 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:00,133 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 205.0 (TID 410). 714 bytes result sent to driver
2017-08-10 09:19:00,133 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 205.0 (TID 411). 714 bytes result sent to driver
2017-08-10 09:19:00,135 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 205.0 (TID 410) in 8 ms on localhost (1/2)
2017-08-10 09:19:00,135 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 205.0 (TID 411) in 7 ms on localhost (2/2)
2017-08-10 09:19:00,135 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 205.0, whose tasks have all completed, from pool 
2017-08-10 09:19:00,135 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 205 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:19:00,135 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 205 finished: foreachPartition at streamingProcess.scala:50, took 0.015729 s
2017-08-10 09:19:00,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327940000 ms.0 from job set of time 1502327940000 ms
2017-08-10 09:19:00,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.136 s for time 1502327940000 ms (execution: 0.027 s)
2017-08-10 09:19:00,136 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 409 from persistence list
2017-08-10 09:19:00,136 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 409
2017-08-10 09:19:00,136 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 408 from persistence list
2017-08-10 09:19:00,136 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 408
2017-08-10 09:19:00,136 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:00,136 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327930000 ms
2017-08-10 09:19:05,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327945000 ms
2017-08-10 09:19:05,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327945000 ms.0 from job set of time 1502327945000 ms
2017-08-10 09:19:05,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:05,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 206 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:05,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 206 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:05,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:05,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:05,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 206 (MapPartitionsRDD[413] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:05,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_206 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:19:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_206_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:19:05,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_206_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:19:05,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_191_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:19:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 206 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 206 (MapPartitionsRDD[413] at map at streamingProcess.scala:48)
2017-08-10 09:19:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 206.0 with 2 tasks
2017-08-10 09:19:05,045 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_192_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:19:05,045 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 206.0 (TID 412, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:05,046 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 206.0 (TID 413, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:05,046 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 206.0 (TID 413)
2017-08-10 09:19:05,046 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 206.0 (TID 412)
2017-08-10 09:19:05,046 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_193_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:19:05,047 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_194_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:19:05,048 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:05,048 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_195_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:19:05,048 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:05,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_196_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:05,051 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 206.0 (TID 412). 714 bytes result sent to driver
2017-08-10 09:19:05,051 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 206.0 (TID 413). 714 bytes result sent to driver
2017-08-10 09:19:05,051 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_197_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:05,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 206.0 (TID 412) in 7 ms on localhost (1/2)
2017-08-10 09:19:05,053 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 206.0 (TID 413) in 7 ms on localhost (2/2)
2017-08-10 09:19:05,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 206 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:19:05,053 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 206.0, whose tasks have all completed, from pool 
2017-08-10 09:19:05,053 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 206 finished: foreachPartition at streamingProcess.scala:50, took 0.026181 s
2017-08-10 09:19:05,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_198_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:05,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327945000 ms.0 from job set of time 1502327945000 ms
2017-08-10 09:19:05,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.053 s for time 1502327945000 ms (execution: 0.037 s)
2017-08-10 09:19:05,053 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 411 from persistence list
2017-08-10 09:19:05,054 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 411
2017-08-10 09:19:05,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 410 from persistence list
2017-08-10 09:19:05,054 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 410
2017-08-10 09:19:05,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:05,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_199_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:05,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327935000 ms
2017-08-10 09:19:05,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_200_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:05,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_201_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:05,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_202_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:05,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_203_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:05,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_204_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:19:05,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_205_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:19:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327950000 ms
2017-08-10 09:19:10,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327950000 ms.0 from job set of time 1502327950000 ms
2017-08-10 09:19:10,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 207 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 207 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 207 (MapPartitionsRDD[415] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_207 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:19:10,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_207_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:19:10,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_207_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:19:10,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 207 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:10,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 207 (MapPartitionsRDD[415] at map at streamingProcess.scala:48)
2017-08-10 09:19:10,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 207.0 with 2 tasks
2017-08-10 09:19:10,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 207.0 (TID 414, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:10,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 207.0 (TID 415, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:10,074 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 207.0 (TID 415)
2017-08-10 09:19:10,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 207.0 (TID 414)
2017-08-10 09:19:10,080 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:10,080 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:10,086 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 207.0 (TID 415). 714 bytes result sent to driver
2017-08-10 09:19:10,086 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 207.0 (TID 414). 714 bytes result sent to driver
2017-08-10 09:19:10,089 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 207.0 (TID 414) in 16 ms on localhost (1/2)
2017-08-10 09:19:10,089 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 207.0 (TID 415) in 15 ms on localhost (2/2)
2017-08-10 09:19:10,089 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 207.0, whose tasks have all completed, from pool 
2017-08-10 09:19:10,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 207 (foreachPartition at streamingProcess.scala:50) finished in 0.017 s
2017-08-10 09:19:10,090 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 207 finished: foreachPartition at streamingProcess.scala:50, took 0.037067 s
2017-08-10 09:19:10,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327950000 ms.0 from job set of time 1502327950000 ms
2017-08-10 09:19:10,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.091 s for time 1502327950000 ms (execution: 0.071 s)
2017-08-10 09:19:10,091 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 413 from persistence list
2017-08-10 09:19:10,092 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 413
2017-08-10 09:19:10,092 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 412 from persistence list
2017-08-10 09:19:10,092 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 412
2017-08-10 09:19:10,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:10,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327940000 ms
2017-08-10 09:19:15,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327955000 ms
2017-08-10 09:19:15,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327955000 ms.0 from job set of time 1502327955000 ms
2017-08-10 09:19:15,021 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:15,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 208 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:15,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 208 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:15,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:15,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:15,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 208 (MapPartitionsRDD[417] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:15,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_208 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:19:15,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_208_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:19:15,027 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_208_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:15,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 208 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:15,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 208 (MapPartitionsRDD[417] at map at streamingProcess.scala:48)
2017-08-10 09:19:15,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 208.0 with 2 tasks
2017-08-10 09:19:15,028 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 208.0 (TID 416, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:15,028 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 208.0 (TID 417, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:15,028 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 208.0 (TID 416)
2017-08-10 09:19:15,028 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 208.0 (TID 417)
2017-08-10 09:19:15,030 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:15,030 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:15,033 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 208.0 (TID 416). 714 bytes result sent to driver
2017-08-10 09:19:15,033 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 208.0 (TID 417). 714 bytes result sent to driver
2017-08-10 09:19:15,034 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 208.0 (TID 416) in 6 ms on localhost (1/2)
2017-08-10 09:19:15,034 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 208.0 (TID 417) in 6 ms on localhost (2/2)
2017-08-10 09:19:15,034 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 208.0, whose tasks have all completed, from pool 
2017-08-10 09:19:15,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 208 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:19:15,034 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 208 finished: foreachPartition at streamingProcess.scala:50, took 0.012840 s
2017-08-10 09:19:15,035 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327955000 ms.0 from job set of time 1502327955000 ms
2017-08-10 09:19:15,035 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.035 s for time 1502327955000 ms (execution: 0.023 s)
2017-08-10 09:19:15,035 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 415 from persistence list
2017-08-10 09:19:15,035 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 415
2017-08-10 09:19:15,035 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 414 from persistence list
2017-08-10 09:19:15,035 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 414
2017-08-10 09:19:15,035 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:15,035 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327945000 ms
2017-08-10 09:19:20,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327960000 ms
2017-08-10 09:19:20,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327960000 ms.0 from job set of time 1502327960000 ms
2017-08-10 09:19:20,021 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:20,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 209 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:20,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 209 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:20,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:20,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:20,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 209 (MapPartitionsRDD[419] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:20,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_209 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:19:20,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_209_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:19:20,026 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_209_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:20,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 209 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:20,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 209 (MapPartitionsRDD[419] at map at streamingProcess.scala:48)
2017-08-10 09:19:20,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 209.0 with 2 tasks
2017-08-10 09:19:20,027 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 209.0 (TID 418, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:20,027 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 209.0 (TID 419, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:20,027 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 209.0 (TID 418)
2017-08-10 09:19:20,027 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 209.0 (TID 419)
2017-08-10 09:19:20,029 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:20,029 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:20,032 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 209.0 (TID 418). 714 bytes result sent to driver
2017-08-10 09:19:20,032 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 209.0 (TID 419). 714 bytes result sent to driver
2017-08-10 09:19:20,033 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 209.0 (TID 418) in 6 ms on localhost (1/2)
2017-08-10 09:19:20,034 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 209.0 (TID 419) in 7 ms on localhost (2/2)
2017-08-10 09:19:20,034 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 209.0, whose tasks have all completed, from pool 
2017-08-10 09:19:20,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 209 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:19:20,034 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 209 finished: foreachPartition at streamingProcess.scala:50, took 0.013297 s
2017-08-10 09:19:20,034 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327960000 ms.0 from job set of time 1502327960000 ms
2017-08-10 09:19:20,034 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.034 s for time 1502327960000 ms (execution: 0.022 s)
2017-08-10 09:19:20,034 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 417 from persistence list
2017-08-10 09:19:20,035 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 417
2017-08-10 09:19:20,035 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 416 from persistence list
2017-08-10 09:19:20,035 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 416
2017-08-10 09:19:20,035 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:20,036 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327950000 ms
2017-08-10 09:19:25,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327965000 ms
2017-08-10 09:19:25,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327965000 ms.0 from job set of time 1502327965000 ms
2017-08-10 09:19:25,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 210 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 210 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 210 (MapPartitionsRDD[421] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:25,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_210 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:19:25,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_210_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:19:25,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_210_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 210 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 210 (MapPartitionsRDD[421] at map at streamingProcess.scala:48)
2017-08-10 09:19:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 210.0 with 2 tasks
2017-08-10 09:19:25,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 210.0 (TID 420, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:25,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 210.0 (TID 421, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:25,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 210.0 (TID 421)
2017-08-10 09:19:25,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 210.0 (TID 420)
2017-08-10 09:19:25,066 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:25,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:25,071 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 210.0 (TID 421). 714 bytes result sent to driver
2017-08-10 09:19:25,071 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 210.0 (TID 420). 714 bytes result sent to driver
2017-08-10 09:19:25,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 210.0 (TID 421) in 12 ms on localhost (1/2)
2017-08-10 09:19:25,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 210.0 (TID 420) in 13 ms on localhost (2/2)
2017-08-10 09:19:25,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 210.0, whose tasks have all completed, from pool 
2017-08-10 09:19:25,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 210 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-10 09:19:25,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 210 finished: foreachPartition at streamingProcess.scala:50, took 0.029252 s
2017-08-10 09:19:25,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327965000 ms.0 from job set of time 1502327965000 ms
2017-08-10 09:19:25,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1502327965000 ms (execution: 0.055 s)
2017-08-10 09:19:25,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 419 from persistence list
2017-08-10 09:19:25,076 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 419
2017-08-10 09:19:25,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 418 from persistence list
2017-08-10 09:19:25,076 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 418
2017-08-10 09:19:25,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:25,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327955000 ms
2017-08-10 09:19:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327970000 ms
2017-08-10 09:19:30,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327970000 ms.0 from job set of time 1502327970000 ms
2017-08-10 09:19:30,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 211 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 211 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 211 (MapPartitionsRDD[423] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_211 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:19:30,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_211_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:19:30,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_211_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:30,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 211 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:30,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 211 (MapPartitionsRDD[423] at map at streamingProcess.scala:48)
2017-08-10 09:19:30,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 211.0 with 2 tasks
2017-08-10 09:19:30,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 211.0 (TID 422, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:30,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 211.0 (TID 423, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:30,076 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 211.0 (TID 422)
2017-08-10 09:19:30,076 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 211.0 (TID 423)
2017-08-10 09:19:30,084 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:30,084 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:30,090 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 211.0 (TID 423). 714 bytes result sent to driver
2017-08-10 09:19:30,090 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 211.0 (TID 422). 714 bytes result sent to driver
2017-08-10 09:19:30,093 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 211.0 (TID 423) in 18 ms on localhost (1/2)
2017-08-10 09:19:30,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 211.0 (TID 422) in 19 ms on localhost (2/2)
2017-08-10 09:19:30,094 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 211.0, whose tasks have all completed, from pool 
2017-08-10 09:19:30,094 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 211 (foreachPartition at streamingProcess.scala:50) finished in 0.020 s
2017-08-10 09:19:30,095 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 211 finished: foreachPartition at streamingProcess.scala:50, took 0.041326 s
2017-08-10 09:19:30,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327970000 ms.0 from job set of time 1502327970000 ms
2017-08-10 09:19:30,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.095 s for time 1502327970000 ms (execution: 0.075 s)
2017-08-10 09:19:30,096 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 421 from persistence list
2017-08-10 09:19:30,096 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 421
2017-08-10 09:19:30,096 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 420 from persistence list
2017-08-10 09:19:30,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:30,097 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 420
2017-08-10 09:19:30,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327960000 ms
2017-08-10 09:19:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327975000 ms
2017-08-10 09:19:35,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327975000 ms.0 from job set of time 1502327975000 ms
2017-08-10 09:19:35,022 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:35,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 212 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:35,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 212 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:35,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:35,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:35,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 212 (MapPartitionsRDD[425] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:35,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_212 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:19:35,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_212_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:19:35,027 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_212_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 212 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 212 (MapPartitionsRDD[425] at map at streamingProcess.scala:48)
2017-08-10 09:19:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 212.0 with 2 tasks
2017-08-10 09:19:35,028 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 212.0 (TID 424, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:35,029 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 212.0 (TID 425, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:35,029 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 212.0 (TID 424)
2017-08-10 09:19:35,029 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 212.0 (TID 425)
2017-08-10 09:19:35,031 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:35,031 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:35,034 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 212.0 (TID 424). 714 bytes result sent to driver
2017-08-10 09:19:35,034 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 212.0 (TID 425). 714 bytes result sent to driver
2017-08-10 09:19:35,035 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 212.0 (TID 425) in 6 ms on localhost (1/2)
2017-08-10 09:19:35,035 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 212.0 (TID 424) in 7 ms on localhost (2/2)
2017-08-10 09:19:35,035 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 212.0, whose tasks have all completed, from pool 
2017-08-10 09:19:35,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 212 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:19:35,035 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 212 finished: foreachPartition at streamingProcess.scala:50, took 0.013180 s
2017-08-10 09:19:35,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327975000 ms.0 from job set of time 1502327975000 ms
2017-08-10 09:19:35,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.036 s for time 1502327975000 ms (execution: 0.024 s)
2017-08-10 09:19:35,036 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 423 from persistence list
2017-08-10 09:19:35,036 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 423
2017-08-10 09:19:35,036 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 422 from persistence list
2017-08-10 09:19:35,037 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 422
2017-08-10 09:19:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327965000 ms
2017-08-10 09:19:40,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327980000 ms
2017-08-10 09:19:40,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327980000 ms.0 from job set of time 1502327980000 ms
2017-08-10 09:19:40,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 213 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 213 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 213 (MapPartitionsRDD[427] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:40,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_213 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:19:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_213_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:19:40,048 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_213_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 213 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 213 (MapPartitionsRDD[427] at map at streamingProcess.scala:48)
2017-08-10 09:19:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 213.0 with 2 tasks
2017-08-10 09:19:40,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 213.0 (TID 426, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:40,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 213.0 (TID 427, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:40,049 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 213.0 (TID 426)
2017-08-10 09:19:40,050 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 213.0 (TID 427)
2017-08-10 09:19:40,051 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:40,051 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:40,054 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 213.0 (TID 427). 714 bytes result sent to driver
2017-08-10 09:19:40,054 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 213.0 (TID 426). 714 bytes result sent to driver
2017-08-10 09:19:40,055 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 213.0 (TID 427) in 6 ms on localhost (1/2)
2017-08-10 09:19:40,056 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 213.0 (TID 426) in 7 ms on localhost (2/2)
2017-08-10 09:19:40,056 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 213.0, whose tasks have all completed, from pool 
2017-08-10 09:19:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 213 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:19:40,056 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 213 finished: foreachPartition at streamingProcess.scala:50, took 0.014322 s
2017-08-10 09:19:40,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327980000 ms.0 from job set of time 1502327980000 ms
2017-08-10 09:19:40,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.056 s for time 1502327980000 ms (execution: 0.036 s)
2017-08-10 09:19:40,057 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 425 from persistence list
2017-08-10 09:19:40,057 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 425
2017-08-10 09:19:40,057 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 424 from persistence list
2017-08-10 09:19:40,057 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 424
2017-08-10 09:19:40,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:40,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327970000 ms
2017-08-10 09:19:45,029 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327985000 ms
2017-08-10 09:19:45,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327985000 ms.0 from job set of time 1502327985000 ms
2017-08-10 09:19:45,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 214 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 214 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 214 (MapPartitionsRDD[429] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_214 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:19:45,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_214_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:19:45,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_214_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 214 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 214 (MapPartitionsRDD[429] at map at streamingProcess.scala:48)
2017-08-10 09:19:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 214.0 with 2 tasks
2017-08-10 09:19:45,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 214.0 (TID 428, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:45,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 214.0 (TID 429, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:45,059 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 214.0 (TID 428)
2017-08-10 09:19:45,061 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:45,061 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 214.0 (TID 429)
2017-08-10 09:19:45,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 214.0 (TID 428). 714 bytes result sent to driver
2017-08-10 09:19:45,067 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:45,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 214.0 (TID 429). 714 bytes result sent to driver
2017-08-10 09:19:45,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 214.0 (TID 429) in 15 ms on localhost (1/2)
2017-08-10 09:19:45,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 214.0 (TID 428) in 19 ms on localhost (2/2)
2017-08-10 09:19:45,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 214.0, whose tasks have all completed, from pool 
2017-08-10 09:19:45,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 214 (foreachPartition at streamingProcess.scala:50) finished in 0.019 s
2017-08-10 09:19:45,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 214 finished: foreachPartition at streamingProcess.scala:50, took 0.028681 s
2017-08-10 09:19:45,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327985000 ms.0 from job set of time 1502327985000 ms
2017-08-10 09:19:45,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1502327985000 ms (execution: 0.036 s)
2017-08-10 09:19:45,074 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 427 from persistence list
2017-08-10 09:19:45,074 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 427
2017-08-10 09:19:45,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 426 from persistence list
2017-08-10 09:19:45,075 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 426
2017-08-10 09:19:45,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:45,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327975000 ms
2017-08-10 09:19:50,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327990000 ms
2017-08-10 09:19:50,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327990000 ms.0 from job set of time 1502327990000 ms
2017-08-10 09:19:50,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:50,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 215 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:50,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 215 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:50,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:50,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 215 (MapPartitionsRDD[431] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_215 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:19:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_215_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:19:50,047 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_215_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 215 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 215 (MapPartitionsRDD[431] at map at streamingProcess.scala:48)
2017-08-10 09:19:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 215.0 with 2 tasks
2017-08-10 09:19:50,049 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 215.0 (TID 430, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:50,049 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 215.0 (TID 431, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:50,049 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 215.0 (TID 430)
2017-08-10 09:19:50,049 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 215.0 (TID 431)
2017-08-10 09:19:50,052 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:50,052 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:50,055 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 215.0 (TID 431). 714 bytes result sent to driver
2017-08-10 09:19:50,055 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 215.0 (TID 430). 714 bytes result sent to driver
2017-08-10 09:19:50,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 215.0 (TID 430) in 9 ms on localhost (1/2)
2017-08-10 09:19:50,058 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 215.0 (TID 431) in 9 ms on localhost (2/2)
2017-08-10 09:19:50,058 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 215.0, whose tasks have all completed, from pool 
2017-08-10 09:19:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 215 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:19:50,058 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 215 finished: foreachPartition at streamingProcess.scala:50, took 0.020586 s
2017-08-10 09:19:50,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327990000 ms.0 from job set of time 1502327990000 ms
2017-08-10 09:19:50,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.058 s for time 1502327990000 ms (execution: 0.042 s)
2017-08-10 09:19:50,059 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 429 from persistence list
2017-08-10 09:19:50,059 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 429
2017-08-10 09:19:50,059 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 428 from persistence list
2017-08-10 09:19:50,059 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 428
2017-08-10 09:19:50,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:50,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327980000 ms
2017-08-10 09:19:55,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327995000 ms
2017-08-10 09:19:55,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327995000 ms.0 from job set of time 1502327995000 ms
2017-08-10 09:19:55,028 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 216 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 216 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 216 (MapPartitionsRDD[433] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:55,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_216 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:19:55,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_216_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:19:55,037 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_216_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:19:55,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 216 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:55,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 216 (MapPartitionsRDD[433] at map at streamingProcess.scala:48)
2017-08-10 09:19:55,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 216.0 with 2 tasks
2017-08-10 09:19:55,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 216.0 (TID 432, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:55,039 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 216.0 (TID 433, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:55,039 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 216.0 (TID 432)
2017-08-10 09:19:55,039 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 216.0 (TID 433)
2017-08-10 09:19:55,041 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:55,041 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:55,044 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 216.0 (TID 433). 714 bytes result sent to driver
2017-08-10 09:19:55,044 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 216.0 (TID 432). 714 bytes result sent to driver
2017-08-10 09:19:55,045 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 216.0 (TID 433) in 7 ms on localhost (1/2)
2017-08-10 09:19:55,045 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 216.0 (TID 432) in 7 ms on localhost (2/2)
2017-08-10 09:19:55,045 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 216.0, whose tasks have all completed, from pool 
2017-08-10 09:19:55,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 216 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:19:55,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 216 finished: foreachPartition at streamingProcess.scala:50, took 0.017026 s
2017-08-10 09:19:55,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327995000 ms.0 from job set of time 1502327995000 ms
2017-08-10 09:19:55,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.046 s for time 1502327995000 ms (execution: 0.032 s)
2017-08-10 09:19:55,046 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 431 from persistence list
2017-08-10 09:19:55,046 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 431
2017-08-10 09:19:55,046 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 430 from persistence list
2017-08-10 09:19:55,046 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 430
2017-08-10 09:19:55,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:55,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327985000 ms
2017-08-10 09:20:00,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502328000000 ms
2017-08-10 09:20:00,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502328000000 ms.0 from job set of time 1502328000000 ms
2017-08-10 09:20:00,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:20:00,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 217 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:20:00,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 217 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:20:00,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:20:00,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:20:00,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 217 (MapPartitionsRDD[435] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:20:00,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_217 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:20:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_217_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:20:00,032 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_217_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:20:00,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 217 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:20:00,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 217 (MapPartitionsRDD[435] at map at streamingProcess.scala:48)
2017-08-10 09:20:00,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 217.0 with 2 tasks
2017-08-10 09:20:00,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 217.0 (TID 434, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:20:00,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 217.0 (TID 435, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:20:00,033 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 217.0 (TID 434)
2017-08-10 09:20:00,033 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 217.0 (TID 435)
2017-08-10 09:20:00,035 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:20:00,035 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:20:00,038 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 217.0 (TID 434). 714 bytes result sent to driver
2017-08-10 09:20:00,038 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 217.0 (TID 435). 714 bytes result sent to driver
2017-08-10 09:20:00,040 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 217.0 (TID 434) in 8 ms on localhost (1/2)
2017-08-10 09:20:00,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 217.0 (TID 435) in 7 ms on localhost (2/2)
2017-08-10 09:20:00,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 217.0, whose tasks have all completed, from pool 
2017-08-10 09:20:00,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 217 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:20:00,040 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 217 finished: foreachPartition at streamingProcess.scala:50, took 0.015414 s
2017-08-10 09:20:00,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502328000000 ms.0 from job set of time 1502328000000 ms
2017-08-10 09:20:00,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1502328000000 ms (execution: 0.029 s)
2017-08-10 09:20:00,041 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 433 from persistence list
2017-08-10 09:20:00,041 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 433
2017-08-10 09:20:00,041 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 432 from persistence list
2017-08-10 09:20:00,041 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 432
2017-08-10 09:20:00,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:20:00,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327990000 ms
2017-08-10 09:20:05,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502328005000 ms
2017-08-10 09:20:05,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502328005000 ms.0 from job set of time 1502328005000 ms
2017-08-10 09:20:05,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:20:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 218 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:20:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 218 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:20:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:20:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:20:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 218 (MapPartitionsRDD[437] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:20:05,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_218 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:20:05,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_218_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:20:05,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_218_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:20:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 218 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:20:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 218 (MapPartitionsRDD[437] at map at streamingProcess.scala:48)
2017-08-10 09:20:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 218.0 with 2 tasks
2017-08-10 09:20:05,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 218.0 (TID 436, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:20:05,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 218.0 (TID 437, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:20:05,057 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 218.0 (TID 436)
2017-08-10 09:20:05,057 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 218.0 (TID 437)
2017-08-10 09:20:05,062 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:20:05,062 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:20:05,067 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 218.0 (TID 437). 714 bytes result sent to driver
2017-08-10 09:20:05,067 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 218.0 (TID 436). 714 bytes result sent to driver
2017-08-10 09:20:05,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 218.0 (TID 437) in 13 ms on localhost (1/2)
2017-08-10 09:20:05,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 218.0 (TID 436) in 13 ms on localhost (2/2)
2017-08-10 09:20:05,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 218.0, whose tasks have all completed, from pool 
2017-08-10 09:20:05,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 218 (foreachPartition at streamingProcess.scala:50) finished in 0.014 s
2017-08-10 09:20:05,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 218 finished: foreachPartition at streamingProcess.scala:50, took 0.026328 s
2017-08-10 09:20:05,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502328005000 ms.0 from job set of time 1502328005000 ms
2017-08-10 09:20:05,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1502328005000 ms (execution: 0.047 s)
2017-08-10 09:20:05,070 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 435 from persistence list
2017-08-10 09:20:05,070 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 435
2017-08-10 09:20:05,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 434 from persistence list
2017-08-10 09:20:05,071 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 434
2017-08-10 09:20:05,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:20:05,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327995000 ms
2017-08-10 09:20:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502328010000 ms
2017-08-10 09:20:10,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502328010000 ms.0 from job set of time 1502328010000 ms
2017-08-10 09:20:10,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:20:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 219 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:20:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 219 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:20:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:20:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:20:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 219 (MapPartitionsRDD[439] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:20:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_219 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:20:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_219_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:20:10,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_219_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:20:10,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 219 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:20:10,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 219 (MapPartitionsRDD[439] at map at streamingProcess.scala:48)
2017-08-10 09:20:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 219.0 with 2 tasks
2017-08-10 09:20:10,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 219.0 (TID 438, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:20:10,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 219.0 (TID 439, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:20:10,077 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 219.0 (TID 438)
2017-08-10 09:20:10,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 219.0 (TID 439)
2017-08-10 09:20:10,080 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:20:10,080 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:20:10,083 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 219.0 (TID 439). 714 bytes result sent to driver
2017-08-10 09:20:10,083 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 219.0 (TID 438). 714 bytes result sent to driver
2017-08-10 09:20:10,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 219.0 (TID 439) in 9 ms on localhost (1/2)
2017-08-10 09:20:10,086 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 219.0 (TID 438) in 10 ms on localhost (2/2)
2017-08-10 09:20:10,086 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 219.0, whose tasks have all completed, from pool 
2017-08-10 09:20:10,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 219 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:20:10,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 219 finished: foreachPartition at streamingProcess.scala:50, took 0.031736 s
2017-08-10 09:20:10,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502328010000 ms.0 from job set of time 1502328010000 ms
2017-08-10 09:20:10,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1502328010000 ms (execution: 0.065 s)
2017-08-10 09:20:10,087 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 437 from persistence list
2017-08-10 09:20:10,087 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 437
2017-08-10 09:20:10,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 436 from persistence list
2017-08-10 09:20:10,088 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 436
2017-08-10 09:20:10,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:20:10,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502328000000 ms
2017-08-10 09:37:13,573 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-10 09:37:13,801 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-10 09:37:13,888 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-10 09:37:13,888 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-10 09:37:13,889 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-10 09:37:13,889 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-10 09:37:13,890 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-10 09:37:14,590 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 51062.
2017-08-10 09:37:14,609 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-10 09:37:14,627 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-10 09:37:14,641 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-57aaa44c-8a1a-49dc-b979-fd7581d11224
2017-08-10 09:37:14,655 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-10 09:37:14,694 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-10 09:37:14,775 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2009ms
2017-08-10 09:37:14,861 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-10 09:37:14,876 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fe7086{/jobs,null,AVAILABLE}
2017-08-10 09:37:14,877 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15d8c96{/jobs/json,null,AVAILABLE}
2017-08-10 09:37:14,877 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/jobs/job,null,AVAILABLE}
2017-08-10 09:37:14,877 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/jobs/job/json,null,AVAILABLE}
2017-08-10 09:37:14,878 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/stages,null,AVAILABLE}
2017-08-10 09:37:14,878 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/stages/json,null,AVAILABLE}
2017-08-10 09:37:14,878 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/stages/stage,null,AVAILABLE}
2017-08-10 09:37:14,878 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/stages/stage/json,null,AVAILABLE}
2017-08-10 09:37:14,879 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/stages/pool,null,AVAILABLE}
2017-08-10 09:37:14,879 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/stages/pool/json,null,AVAILABLE}
2017-08-10 09:37:14,879 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/storage,null,AVAILABLE}
2017-08-10 09:37:14,879 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/storage/json,null,AVAILABLE}
2017-08-10 09:37:14,879 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/storage/rdd,null,AVAILABLE}
2017-08-10 09:37:14,879 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/storage/rdd/json,null,AVAILABLE}
2017-08-10 09:37:14,880 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/environment,null,AVAILABLE}
2017-08-10 09:37:14,880 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/environment/json,null,AVAILABLE}
2017-08-10 09:37:14,880 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/executors,null,AVAILABLE}
2017-08-10 09:37:14,880 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0fba8{/executors/json,null,AVAILABLE}
2017-08-10 09:37:14,880 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5de5a4{/executors/threadDump,null,AVAILABLE}
2017-08-10 09:37:14,881 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1ef6856{/executors/threadDump/json,null,AVAILABLE}
2017-08-10 09:37:14,885 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@b7a938{/static,null,AVAILABLE}
2017-08-10 09:37:14,886 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1af2e7d{/,null,AVAILABLE}
2017-08-10 09:37:14,887 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@648ce9{/api,null,AVAILABLE}
2017-08-10 09:37:14,887 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9bf9eb{/stages/stage/kill,null,AVAILABLE}
2017-08-10 09:37:14,894 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@1465e4b{HTTP/1.1}{0.0.0.0:4040}
2017-08-10 09:37:14,894 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2128ms
2017-08-10 09:37:14,894 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-10 09:37:14,896 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-10 09:37:14,965 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-10 09:37:15,003 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51083.
2017-08-10 09:37:15,003 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:51083
2017-08-10 09:37:15,005 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 51083)
2017-08-10 09:37:15,007 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:51083 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 51083)
2017-08-10 09:37:15,009 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 51083)
2017-08-10 09:37:15,166 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@518890{/metrics/json,null,AVAILABLE}
2017-08-10 09:37:15,827 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-08-10 09:37:15,828 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-10 09:37:15,828 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-10 09:37:15,829 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@1759585
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@8d8cf0
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-08-10 09:37:15,831 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@82aa6a
2017-08-10 09:37:15,880 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1502329040000
2017-08-10 09:37:15,881 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1502329040000 ms
2017-08-10 09:37:15,882 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-10 09:37:15,884 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1740dae{/streaming,null,AVAILABLE}
2017-08-10 09:37:15,884 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cf395{/streaming/json,null,AVAILABLE}
2017-08-10 09:37:15,885 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@82cd4f{/streaming/batch,null,AVAILABLE}
2017-08-10 09:37:15,887 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch/json,null,AVAILABLE}
2017-08-10 09:37:15,890 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cf9028{/static/streaming,null,AVAILABLE}
2017-08-10 09:37:15,890 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-10 09:37:20,121 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329040000 ms
2017-08-10 09:37:20,125 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329040000 ms.0 from job set of time 1502329040000 ms
2017-08-10 09:37:20,153 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:20,167 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:20,168 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:20,168 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:20,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:20,179 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:20,238 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-10 09:37:20,245 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:20,451 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:20,454 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:20,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:20,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:20,462 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-10 09:37:20,506 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-08-10 09:37:20,509 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-08-10 09:37:20,515 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-10 09:37:20,515 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-10 09:37:20,541 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 0 -> 25
2017-08-10 09:37:20,541 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 0 -> 25
2017-08-10 09:37:20,609 [Executor task launch worker-1] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-08-10 09:37:20,849 [Executor task launch worker-1] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-08-10 09:37:21,895 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-08-10 09:37:21,910 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 1425 ms on localhost (1/2)
2017-08-10 09:37:22,214 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-08-10 09:37:22,223 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 1714 ms on localhost (2/2)
2017-08-10 09:37:22,224 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.749 s
2017-08-10 09:37:22,225 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-08-10 09:37:22,231 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcessNoReg.scala:50, took 2.076342 s
2017-08-10 09:37:22,239 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329040000 ms.0 from job set of time 1502329040000 ms
2017-08-10 09:37:22,243 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.235 s for time 1502329040000 ms (execution: 2.112 s)
2017-08-10 09:37:22,252 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:22,259 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-10 09:37:25,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329045000 ms
2017-08-10 09:37:25,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329045000 ms.0 from job set of time 1502329045000 ms
2017-08-10 09:37:25,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:25,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:25,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:25,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:25,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:25,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:25,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-08-10 09:37:25,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-08-10 09:37:25,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-08-10 09:37:25,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-08-10 09:37:25,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-08-10 09:37:25,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 25 -> 50
2017-08-10 09:37:25,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 25 -> 50
2017-08-10 09:37:25,804 [Executor task launch worker-1] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-10 09:37:25,986 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 787 bytes result sent to driver
2017-08-10 09:37:25,995 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 920 ms on localhost (1/2)
2017-08-10 09:37:26,386 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 787 bytes result sent to driver
2017-08-10 09:37:26,390 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 1324 ms on localhost (2/2)
2017-08-10 09:37:26,391 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-08-10 09:37:26,391 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.325 s
2017-08-10 09:37:26,391 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.344613 s
2017-08-10 09:37:26,392 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329045000 ms.0 from job set of time 1502329045000 ms
2017-08-10 09:37:26,392 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.392 s for time 1502329045000 ms (execution: 1.368 s)
2017-08-10 09:37:26,392 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-08-10 09:37:26,400 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-08-10 09:37:26,402 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:26,403 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-08-10 09:37:26,403 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-10 09:37:26,403 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-08-10 09:37:30,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329050000 ms
2017-08-10 09:37:30,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329050000 ms.0 from job set of time 1502329050000 ms
2017-08-10 09:37:30,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:30,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2017-08-10 09:37:30,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5655 bytes)
2017-08-10 09:37:30,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5655 bytes)
2017-08-10 09:37:30,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2017-08-10 09:37:30,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2017-08-10 09:37:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 50 -> 75
2017-08-10 09:37:30,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 50 -> 75
2017-08-10 09:37:30,190 [Executor task launch worker-0] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-10 09:37:30,382 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:30,953 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2.0 (TID 5). 787 bytes result sent to driver
2017-08-10 09:37:30,957 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2.0 (TID 5) in 890 ms on localhost (1/2)
2017-08-10 09:37:30,966 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 787 bytes result sent to driver
2017-08-10 09:37:30,969 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 904 ms on localhost (2/2)
2017-08-10 09:37:30,969 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-08-10 09:37:30,969 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.905 s
2017-08-10 09:37:30,970 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.922008 s
2017-08-10 09:37:30,971 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329050000 ms.0 from job set of time 1502329050000 ms
2017-08-10 09:37:30,971 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.971 s for time 1502329050000 ms (execution: 0.948 s)
2017-08-10 09:37:30,971 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 3 from persistence list
2017-08-10 09:37:30,971 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 3
2017-08-10 09:37:30,972 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2 from persistence list
2017-08-10 09:37:30,972 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2
2017-08-10 09:37:30,972 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:30,973 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329040000 ms
2017-08-10 09:37:35,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329055000 ms
2017-08-10 09:37:35,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329055000 ms.0 from job set of time 1502329055000 ms
2017-08-10 09:37:35,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:35,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:35,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 3 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:35,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:35,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:35,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:35,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:35,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:35,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 3.0 with 2 tasks
2017-08-10 09:37:35,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 3.0 (TID 6, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:37:35,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 3.0 (TID 7, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:37:35,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 3.0 (TID 6)
2017-08-10 09:37:35,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 3.0 (TID 7)
2017-08-10 09:37:35,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 75 -> 100
2017-08-10 09:37:35,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 75 -> 100
2017-08-10 09:37:35,212 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:35,391 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 3.0 (TID 7). 787 bytes result sent to driver
2017-08-10 09:37:35,393 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 3.0 (TID 7) in 320 ms on localhost (1/2)
2017-08-10 09:37:35,456 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 3.0 (TID 6). 787 bytes result sent to driver
2017-08-10 09:37:35,458 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 3.0 (TID 6) in 388 ms on localhost (2/2)
2017-08-10 09:37:35,459 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-08-10 09:37:35,459 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 3 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.388 s
2017-08-10 09:37:35,459 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.411242 s
2017-08-10 09:37:35,459 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329055000 ms.0 from job set of time 1502329055000 ms
2017-08-10 09:37:35,460 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.459 s for time 1502329055000 ms (execution: 0.436 s)
2017-08-10 09:37:35,460 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 5 from persistence list
2017-08-10 09:37:35,460 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 5
2017-08-10 09:37:35,460 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 4 from persistence list
2017-08-10 09:37:35,460 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 4
2017-08-10 09:37:35,461 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:35,461 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329045000 ms
2017-08-10 09:37:40,025 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329060000 ms
2017-08-10 09:37:40,026 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329060000 ms.0 from job set of time 1502329060000 ms
2017-08-10 09:37:40,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 4 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:40,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:40,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:40,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:40,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:40,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 2 tasks
2017-08-10 09:37:40,076 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 8, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:37:40,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 4.0 (TID 9, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:37:40,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 8)
2017-08-10 09:37:40,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 4.0 (TID 9)
2017-08-10 09:37:40,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 100 -> 125
2017-08-10 09:37:40,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 100 -> 125
2017-08-10 09:37:40,217 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_3_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:40,787 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 4.0 (TID 9). 787 bytes result sent to driver
2017-08-10 09:37:40,790 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 4.0 (TID 9) in 712 ms on localhost (1/2)
2017-08-10 09:37:41,116 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 8). 787 bytes result sent to driver
2017-08-10 09:37:41,118 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 8) in 1043 ms on localhost (2/2)
2017-08-10 09:37:41,118 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-08-10 09:37:41,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.044 s
2017-08-10 09:37:41,118 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 4 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.068134 s
2017-08-10 09:37:41,118 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329060000 ms.0 from job set of time 1502329060000 ms
2017-08-10 09:37:41,119 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.118 s for time 1502329060000 ms (execution: 1.092 s)
2017-08-10 09:37:41,119 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 7 from persistence list
2017-08-10 09:37:41,119 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 7
2017-08-10 09:37:41,119 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 6 from persistence list
2017-08-10 09:37:41,120 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 6
2017-08-10 09:37:41,120 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:41,120 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329050000 ms
2017-08-10 09:37:45,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329065000 ms
2017-08-10 09:37:45,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329065000 ms.0 from job set of time 1502329065000 ms
2017-08-10 09:37:45,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:45,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 5 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:45,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 5 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:45,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:45,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:45,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:45,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:45,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:45,042 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:45,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:45,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:45,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 5.0 with 2 tasks
2017-08-10 09:37:45,048 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 5.0 (TID 10, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:37:45,050 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 5.0 (TID 11, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:37:45,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 5.0 (TID 10)
2017-08-10 09:37:45,052 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 5.0 (TID 11)
2017-08-10 09:37:45,054 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 125 -> 150
2017-08-10 09:37:45,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 125 -> 150
2017-08-10 09:37:45,382 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_4_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:45,533 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 5.0 (TID 11). 787 bytes result sent to driver
2017-08-10 09:37:45,534 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 5.0 (TID 11) in 485 ms on localhost (1/2)
2017-08-10 09:37:46,095 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 5.0 (TID 10). 787 bytes result sent to driver
2017-08-10 09:37:46,097 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 5.0 (TID 10) in 1052 ms on localhost (2/2)
2017-08-10 09:37:46,098 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-08-10 09:37:46,098 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 5 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.053 s
2017-08-10 09:37:46,098 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 5 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.068717 s
2017-08-10 09:37:46,098 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329065000 ms.0 from job set of time 1502329065000 ms
2017-08-10 09:37:46,099 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.098 s for time 1502329065000 ms (execution: 1.082 s)
2017-08-10 09:37:46,099 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 9 from persistence list
2017-08-10 09:37:46,099 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 9
2017-08-10 09:37:46,099 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 8 from persistence list
2017-08-10 09:37:46,100 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 8
2017-08-10 09:37:46,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:46,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329055000 ms
2017-08-10 09:37:50,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329070000 ms
2017-08-10 09:37:50,025 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329070000 ms.0 from job set of time 1502329070000 ms
2017-08-10 09:37:50,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:50,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 6 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 6 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:50,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:50,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_6_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:50,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:50,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:50,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 6.0 with 2 tasks
2017-08-10 09:37:50,080 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 6.0 (TID 12, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:37:50,082 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 6.0 (TID 13, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:37:50,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 6.0 (TID 13)
2017-08-10 09:37:50,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 6.0 (TID 12)
2017-08-10 09:37:50,086 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 150 -> 175
2017-08-10 09:37:50,086 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 150 -> 175
2017-08-10 09:37:50,226 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_5_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:50,985 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 6.0 (TID 12). 787 bytes result sent to driver
2017-08-10 09:37:50,987 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 6.0 (TID 12) in 908 ms on localhost (1/2)
2017-08-10 09:37:51,559 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 6.0 (TID 13). 787 bytes result sent to driver
2017-08-10 09:37:51,562 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 6.0 (TID 13) in 1481 ms on localhost (2/2)
2017-08-10 09:37:51,563 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2017-08-10 09:37:51,563 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 6 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.484 s
2017-08-10 09:37:51,563 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 6 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.518105 s
2017-08-10 09:37:51,564 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329070000 ms.0 from job set of time 1502329070000 ms
2017-08-10 09:37:51,564 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 11 from persistence list
2017-08-10 09:37:51,564 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.564 s for time 1502329070000 ms (execution: 1.540 s)
2017-08-10 09:37:51,565 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 11
2017-08-10 09:37:51,565 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 10 from persistence list
2017-08-10 09:37:51,565 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 10
2017-08-10 09:37:51,565 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:51,566 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329060000 ms
2017-08-10 09:37:55,025 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329075000 ms
2017-08-10 09:37:55,026 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329075000 ms.0 from job set of time 1502329075000 ms
2017-08-10 09:37:55,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:55,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 7 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:55,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 7 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:55,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:55,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:55,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:55,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:55,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_7_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 7.0 with 2 tasks
2017-08-10 09:37:55,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 7.0 (TID 14, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:37:55,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 7.0 (TID 15, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:37:55,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 7.0 (TID 15)
2017-08-10 09:37:55,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 7.0 (TID 14)
2017-08-10 09:37:55,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 175 -> 200
2017-08-10 09:37:55,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 175 -> 200
2017-08-10 09:37:55,375 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_6_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:56,392 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 7.0 (TID 15). 787 bytes result sent to driver
2017-08-10 09:37:56,395 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 7.0 (TID 15) in 1335 ms on localhost (1/2)
2017-08-10 09:37:56,665 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 7.0 (TID 14). 787 bytes result sent to driver
2017-08-10 09:37:56,669 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 7.0 (TID 14) in 1610 ms on localhost (2/2)
2017-08-10 09:37:56,670 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 7 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.611 s
2017-08-10 09:37:56,671 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2017-08-10 09:37:56,672 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 7 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.626714 s
2017-08-10 09:37:56,673 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329075000 ms.0 from job set of time 1502329075000 ms
2017-08-10 09:37:56,673 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 13 from persistence list
2017-08-10 09:37:56,673 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.673 s for time 1502329075000 ms (execution: 1.647 s)
2017-08-10 09:37:56,674 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 13
2017-08-10 09:37:56,674 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 12 from persistence list
2017-08-10 09:37:56,675 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 12
2017-08-10 09:37:56,675 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:56,675 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329065000 ms
2017-08-10 09:38:00,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329080000 ms
2017-08-10 09:38:00,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329080000 ms.0 from job set of time 1502329080000 ms
2017-08-10 09:38:00,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 8 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 8 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:00,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:00,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:00,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:00,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_7_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:00,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_8_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 8.0 with 2 tasks
2017-08-10 09:38:00,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 8.0 (TID 16, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:00,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 8.0 (TID 17, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:00,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 8.0 (TID 17)
2017-08-10 09:38:00,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 8.0 (TID 16)
2017-08-10 09:38:00,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 200 -> 225
2017-08-10 09:38:00,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 200 -> 225
2017-08-10 09:38:01,274 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 8.0 (TID 16). 787 bytes result sent to driver
2017-08-10 09:38:01,276 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 8.0 (TID 16) in 1212 ms on localhost (1/2)
2017-08-10 09:38:01,633 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 8.0 (TID 17). 787 bytes result sent to driver
2017-08-10 09:38:01,634 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 8.0 (TID 17) in 1570 ms on localhost (2/2)
2017-08-10 09:38:01,635 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 8 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.573 s
2017-08-10 09:38:01,635 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2017-08-10 09:38:01,635 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 8 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.595577 s
2017-08-10 09:38:01,636 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329080000 ms.0 from job set of time 1502329080000 ms
2017-08-10 09:38:01,636 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.636 s for time 1502329080000 ms (execution: 1.614 s)
2017-08-10 09:38:01,636 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 15 from persistence list
2017-08-10 09:38:01,637 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 15
2017-08-10 09:38:01,637 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 14 from persistence list
2017-08-10 09:38:01,638 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 14
2017-08-10 09:38:01,639 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:01,639 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329070000 ms
2017-08-10 09:38:05,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329085000 ms
2017-08-10 09:38:05,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329085000 ms.0 from job set of time 1502329085000 ms
2017-08-10 09:38:05,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:05,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 9 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:05,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 9 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:05,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:05,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:05,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:05,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:05,033 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_9_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:05,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:05,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:05,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 9.0 with 2 tasks
2017-08-10 09:38:05,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 9.0 (TID 18, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:05,036 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 9.0 (TID 19, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:05,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 9.0 (TID 18)
2017-08-10 09:38:05,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 9.0 (TID 19)
2017-08-10 09:38:05,037 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 225 -> 250
2017-08-10 09:38:05,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 225 -> 250
2017-08-10 09:38:05,123 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_8_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:06,456 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 9.0 (TID 19). 787 bytes result sent to driver
2017-08-10 09:38:06,457 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 9.0 (TID 19) in 1422 ms on localhost (1/2)
2017-08-10 09:38:06,547 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 9.0 (TID 18). 787 bytes result sent to driver
2017-08-10 09:38:06,550 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 9.0 (TID 18) in 1516 ms on localhost (2/2)
2017-08-10 09:38:06,550 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2017-08-10 09:38:06,551 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 9 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.517 s
2017-08-10 09:38:06,551 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 9 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.527295 s
2017-08-10 09:38:06,552 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329085000 ms.0 from job set of time 1502329085000 ms
2017-08-10 09:38:06,552 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 17 from persistence list
2017-08-10 09:38:06,552 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.552 s for time 1502329085000 ms (execution: 1.538 s)
2017-08-10 09:38:06,552 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 17
2017-08-10 09:38:06,552 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 16 from persistence list
2017-08-10 09:38:06,553 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 16
2017-08-10 09:38:06,553 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:06,553 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329075000 ms
2017-08-10 09:38:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329090000 ms
2017-08-10 09:38:10,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329090000 ms.0 from job set of time 1502329090000 ms
2017-08-10 09:38:10,024 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:10,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 10 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:10,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 10 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:10,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:10,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:10,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:10,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:10,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:10,031 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_10_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:10,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:10,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:10,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 10.0 with 2 tasks
2017-08-10 09:38:10,034 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 10.0 (TID 20, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:10,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 10.0 (TID 21, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:10,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 10.0 (TID 20)
2017-08-10 09:38:10,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 10.0 (TID 21)
2017-08-10 09:38:10,036 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 250 -> 275
2017-08-10 09:38:10,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 250 -> 275
2017-08-10 09:38:10,272 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_9_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:10,773 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 10.0 (TID 20). 787 bytes result sent to driver
2017-08-10 09:38:10,774 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 10.0 (TID 20) in 741 ms on localhost (1/2)
2017-08-10 09:38:11,458 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 10.0 (TID 21). 787 bytes result sent to driver
2017-08-10 09:38:11,459 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 10.0 (TID 21) in 1425 ms on localhost (2/2)
2017-08-10 09:38:11,459 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2017-08-10 09:38:11,459 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 10 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.426 s
2017-08-10 09:38:11,460 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 10 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.435389 s
2017-08-10 09:38:11,460 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329090000 ms.0 from job set of time 1502329090000 ms
2017-08-10 09:38:11,460 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.460 s for time 1502329090000 ms (execution: 1.445 s)
2017-08-10 09:38:11,460 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 19 from persistence list
2017-08-10 09:38:11,461 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 19
2017-08-10 09:38:11,461 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 18 from persistence list
2017-08-10 09:38:11,461 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 18
2017-08-10 09:38:11,461 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:11,462 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329080000 ms
2017-08-10 09:38:15,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329095000 ms
2017-08-10 09:38:15,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329095000 ms.0 from job set of time 1502329095000 ms
2017-08-10 09:38:15,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:15,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 11 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:15,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 11 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:15,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:15,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:15,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:15,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:15,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_11_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 11 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 11.0 with 2 tasks
2017-08-10 09:38:15,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 11.0 (TID 22, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:15,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 11.0 (TID 23, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:15,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 11.0 (TID 22)
2017-08-10 09:38:15,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 11.0 (TID 23)
2017-08-10 09:38:15,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 275 -> 300
2017-08-10 09:38:15,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 275 -> 300
2017-08-10 09:38:15,082 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_10_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:15,977 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 11.0 (TID 23). 787 bytes result sent to driver
2017-08-10 09:38:15,979 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 11.0 (TID 23) in 916 ms on localhost (1/2)
2017-08-10 09:38:16,369 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 11.0 (TID 22). 787 bytes result sent to driver
2017-08-10 09:38:16,370 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 11.0 (TID 22) in 1309 ms on localhost (2/2)
2017-08-10 09:38:16,371 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 11 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.309 s
2017-08-10 09:38:16,371 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2017-08-10 09:38:16,371 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 11 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.327583 s
2017-08-10 09:38:16,371 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329095000 ms.0 from job set of time 1502329095000 ms
2017-08-10 09:38:16,372 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.371 s for time 1502329095000 ms (execution: 1.349 s)
2017-08-10 09:38:16,372 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 21 from persistence list
2017-08-10 09:38:16,372 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 21
2017-08-10 09:38:16,372 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 20 from persistence list
2017-08-10 09:38:16,372 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 20
2017-08-10 09:38:16,373 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:16,373 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329085000 ms
2017-08-10 09:38:20,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329100000 ms
2017-08-10 09:38:20,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329100000 ms.0 from job set of time 1502329100000 ms
2017-08-10 09:38:20,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:20,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 12 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 12 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:20,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:20,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_12_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 12.0 with 2 tasks
2017-08-10 09:38:20,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 12.0 (TID 24, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:20,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 12.0 (TID 25, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:20,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 12.0 (TID 25)
2017-08-10 09:38:20,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 12.0 (TID 24)
2017-08-10 09:38:20,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 300 -> 325
2017-08-10 09:38:20,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 300 -> 325
2017-08-10 09:38:20,903 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 12.0 (TID 25). 787 bytes result sent to driver
2017-08-10 09:38:20,907 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 12.0 (TID 25) in 841 ms on localhost (1/2)
2017-08-10 09:38:21,402 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 12.0 (TID 24). 787 bytes result sent to driver
2017-08-10 09:38:21,406 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 12.0 (TID 24) in 1344 ms on localhost (2/2)
2017-08-10 09:38:21,406 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2017-08-10 09:38:21,406 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 12 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.344 s
2017-08-10 09:38:21,406 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 12 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.364911 s
2017-08-10 09:38:21,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329100000 ms.0 from job set of time 1502329100000 ms
2017-08-10 09:38:21,408 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 23 from persistence list
2017-08-10 09:38:21,408 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.407 s for time 1502329100000 ms (execution: 1.385 s)
2017-08-10 09:38:21,408 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 22 from persistence list
2017-08-10 09:38:21,408 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 23
2017-08-10 09:38:21,409 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 22
2017-08-10 09:38:21,409 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:21,409 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329090000 ms
2017-08-10 09:38:25,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329105000 ms
2017-08-10 09:38:25,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329105000 ms.0 from job set of time 1502329105000 ms
2017-08-10 09:38:25,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:25,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 13 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:25,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 13 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:25,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:25,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:25,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:25,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:25,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:25,030 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_13_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:25,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 13 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:25,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:25,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 13.0 with 2 tasks
2017-08-10 09:38:25,032 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 13.0 (TID 26, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:25,033 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 13.0 (TID 27, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:25,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 13.0 (TID 27)
2017-08-10 09:38:25,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 13.0 (TID 26)
2017-08-10 09:38:25,034 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 325 -> 350
2017-08-10 09:38:25,035 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 325 -> 350
2017-08-10 09:38:25,767 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_12_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:26,182 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 13.0 (TID 26). 787 bytes result sent to driver
2017-08-10 09:38:26,184 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 13.0 (TID 26) in 1153 ms on localhost (1/2)
2017-08-10 09:38:26,617 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 13.0 (TID 27). 787 bytes result sent to driver
2017-08-10 09:38:26,618 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 13.0 (TID 27) in 1586 ms on localhost (2/2)
2017-08-10 09:38:26,618 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2017-08-10 09:38:26,619 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 13 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.587 s
2017-08-10 09:38:26,619 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 13 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.595765 s
2017-08-10 09:38:26,619 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329105000 ms.0 from job set of time 1502329105000 ms
2017-08-10 09:38:26,619 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.619 s for time 1502329105000 ms (execution: 1.603 s)
2017-08-10 09:38:26,619 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 25 from persistence list
2017-08-10 09:38:26,620 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 25
2017-08-10 09:38:26,620 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 24 from persistence list
2017-08-10 09:38:26,620 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 24
2017-08-10 09:38:26,620 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:26,620 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329095000 ms
2017-08-10 09:38:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329110000 ms
2017-08-10 09:38:30,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329110000 ms.0 from job set of time 1502329110000 ms
2017-08-10 09:38:30,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 14 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 14 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:30,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:30,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:30,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_14_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 14.0 with 2 tasks
2017-08-10 09:38:30,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 14.0 (TID 28, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:30,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 14.0 (TID 29, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:30,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 14.0 (TID 28)
2017-08-10 09:38:30,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 14.0 (TID 29)
2017-08-10 09:38:30,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 350 -> 375
2017-08-10 09:38:30,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 350 -> 375
2017-08-10 09:38:30,179 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_13_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:31,122 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 14.0 (TID 29). 787 bytes result sent to driver
2017-08-10 09:38:31,123 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 14.0 (TID 29) in 1062 ms on localhost (1/2)
2017-08-10 09:38:31,225 [Executor task launch worker-1] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-10 09:38:31,767 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 14.0 (TID 28). 787 bytes result sent to driver
2017-08-10 09:38:31,769 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 14.0 (TID 28) in 1709 ms on localhost (2/2)
2017-08-10 09:38:31,769 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2017-08-10 09:38:31,769 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 14 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.709 s
2017-08-10 09:38:31,770 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 14 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.729885 s
2017-08-10 09:38:31,770 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329110000 ms.0 from job set of time 1502329110000 ms
2017-08-10 09:38:31,770 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 27 from persistence list
2017-08-10 09:38:31,770 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.770 s for time 1502329110000 ms (execution: 1.750 s)
2017-08-10 09:38:31,770 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 27
2017-08-10 09:38:31,771 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 26 from persistence list
2017-08-10 09:38:31,771 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 26
2017-08-10 09:38:31,771 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:31,771 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329100000 ms
2017-08-10 09:38:35,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329115000 ms
2017-08-10 09:38:35,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329115000 ms.0 from job set of time 1502329115000 ms
2017-08-10 09:38:35,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:35,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 15 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 15 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:35,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:35,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:35,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:35,035 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_15_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 15 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:35,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 15.0 with 2 tasks
2017-08-10 09:38:35,038 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 15.0 (TID 30, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:35,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 15.0 (TID 31, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:35,040 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 15.0 (TID 30)
2017-08-10 09:38:35,040 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 15.0 (TID 31)
2017-08-10 09:38:35,042 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 375 -> 400
2017-08-10 09:38:35,042 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 375 -> 400
2017-08-10 09:38:35,389 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_14_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:35,924 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 15.0 (TID 30). 787 bytes result sent to driver
2017-08-10 09:38:35,925 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 15.0 (TID 30) in 888 ms on localhost (1/2)
2017-08-10 09:38:36,390 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 15.0 (TID 31). 787 bytes result sent to driver
2017-08-10 09:38:36,392 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 15.0 (TID 31) in 1353 ms on localhost (2/2)
2017-08-10 09:38:36,392 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2017-08-10 09:38:36,392 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 15 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.355 s
2017-08-10 09:38:36,393 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 15 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.366142 s
2017-08-10 09:38:36,393 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329115000 ms.0 from job set of time 1502329115000 ms
2017-08-10 09:38:36,394 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.393 s for time 1502329115000 ms (execution: 1.376 s)
2017-08-10 09:38:36,394 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 29 from persistence list
2017-08-10 09:38:36,394 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 29
2017-08-10 09:38:36,394 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 28 from persistence list
2017-08-10 09:38:36,395 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 28
2017-08-10 09:38:36,395 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:36,395 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329105000 ms
2017-08-10 09:38:40,024 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329120000 ms
2017-08-10 09:38:40,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329120000 ms.0 from job set of time 1502329120000 ms
2017-08-10 09:38:40,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 16 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 16 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:40,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:40,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:40,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:40,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_16_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 16 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 16.0 with 2 tasks
2017-08-10 09:38:40,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 16.0 (TID 32, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:40,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 16.0 (TID 33, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:40,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 16.0 (TID 33)
2017-08-10 09:38:40,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 16.0 (TID 32)
2017-08-10 09:38:40,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 400 -> 425
2017-08-10 09:38:40,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 400 -> 425
2017-08-10 09:38:40,341 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_15_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:41,593 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 16.0 (TID 33). 787 bytes result sent to driver
2017-08-10 09:38:41,595 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 16.0 (TID 33) in 1539 ms on localhost (1/2)
2017-08-10 09:38:41,740 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 16.0 (TID 32). 787 bytes result sent to driver
2017-08-10 09:38:41,742 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 16.0 (TID 32) in 1687 ms on localhost (2/2)
2017-08-10 09:38:41,742 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2017-08-10 09:38:41,742 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 16 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.688 s
2017-08-10 09:38:41,742 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 16 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.704181 s
2017-08-10 09:38:41,743 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329120000 ms.0 from job set of time 1502329120000 ms
2017-08-10 09:38:41,743 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.743 s for time 1502329120000 ms (execution: 1.719 s)
2017-08-10 09:38:41,743 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 31 from persistence list
2017-08-10 09:38:41,744 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 31
2017-08-10 09:38:41,744 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 30 from persistence list
2017-08-10 09:38:41,744 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 30
2017-08-10 09:38:41,744 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:41,744 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329110000 ms
2017-08-10 09:38:45,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329125000 ms
2017-08-10 09:38:45,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329125000 ms.0 from job set of time 1502329125000 ms
2017-08-10 09:38:45,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:45,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 17 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:45,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 17 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:45,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:45,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:45,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:45,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:45,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:45,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_17_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 17 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 17.0 with 2 tasks
2017-08-10 09:38:45,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 17.0 (TID 34, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:45,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 17.0 (TID 35, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:45,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 17.0 (TID 35)
2017-08-10 09:38:45,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 17.0 (TID 34)
2017-08-10 09:38:45,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 425 -> 450
2017-08-10 09:38:45,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 425 -> 450
2017-08-10 09:38:45,359 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_16_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:45,854 [Executor task launch worker-0] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-10 09:38:45,964 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 17.0 (TID 34). 787 bytes result sent to driver
2017-08-10 09:38:45,967 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 17.0 (TID 34) in 912 ms on localhost (1/2)
2017-08-10 09:38:46,733 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 17.0 (TID 35). 787 bytes result sent to driver
2017-08-10 09:38:46,735 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 17.0 (TID 35) in 1679 ms on localhost (2/2)
2017-08-10 09:38:46,735 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2017-08-10 09:38:46,735 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 17 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.680 s
2017-08-10 09:38:46,735 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 17 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.695230 s
2017-08-10 09:38:46,736 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329125000 ms.0 from job set of time 1502329125000 ms
2017-08-10 09:38:46,736 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.736 s for time 1502329125000 ms (execution: 1.716 s)
2017-08-10 09:38:46,736 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 33 from persistence list
2017-08-10 09:38:46,736 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 33
2017-08-10 09:38:46,736 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 32 from persistence list
2017-08-10 09:38:46,737 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 32
2017-08-10 09:38:46,737 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:46,737 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329115000 ms
2017-08-10 09:38:50,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329130000 ms
2017-08-10 09:38:50,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329130000 ms.0 from job set of time 1502329130000 ms
2017-08-10 09:38:50,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:50,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 18 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:50,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 18 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:50,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:50,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:50,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:50,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:50,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:50,032 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_18_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:50,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 18 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:50,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:50,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 18.0 with 2 tasks
2017-08-10 09:38:50,034 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 18.0 (TID 36, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:50,034 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 18.0 (TID 37, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:50,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 18.0 (TID 36)
2017-08-10 09:38:50,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 18.0 (TID 37)
2017-08-10 09:38:50,036 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 450 -> 475
2017-08-10 09:38:50,036 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 450 -> 475
2017-08-10 09:38:50,158 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_17_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:50,796 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 18.0 (TID 36). 787 bytes result sent to driver
2017-08-10 09:38:50,797 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 18.0 (TID 36) in 764 ms on localhost (1/2)
2017-08-10 09:38:51,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 18.0 (TID 37). 787 bytes result sent to driver
2017-08-10 09:38:51,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 18.0 (TID 37) in 1042 ms on localhost (2/2)
2017-08-10 09:38:51,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 18 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.044 s
2017-08-10 09:38:51,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2017-08-10 09:38:51,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 18 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.051505 s
2017-08-10 09:38:51,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329130000 ms.0 from job set of time 1502329130000 ms
2017-08-10 09:38:51,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.078 s for time 1502329130000 ms (execution: 1.063 s)
2017-08-10 09:38:51,078 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 35 from persistence list
2017-08-10 09:38:51,078 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 35
2017-08-10 09:38:51,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 34 from persistence list
2017-08-10 09:38:51,079 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 34
2017-08-10 09:38:51,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:51,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329120000 ms
2017-08-10 09:38:55,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329135000 ms
2017-08-10 09:38:55,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329135000 ms.0 from job set of time 1502329135000 ms
2017-08-10 09:38:55,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 19 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 19 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:55,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 19 (MapPartitionsRDD[39] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:55,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:55,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_19_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:55,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 19 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:55,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:55,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 19.0 with 2 tasks
2017-08-10 09:38:55,032 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 19.0 (TID 38, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:55,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 19.0 (TID 39, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:55,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 19.0 (TID 38)
2017-08-10 09:38:55,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 19.0 (TID 39)
2017-08-10 09:38:55,034 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 475 -> 500
2017-08-10 09:38:55,034 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 475 -> 500
2017-08-10 09:38:55,400 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_18_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:55,623 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 19.0 (TID 38). 787 bytes result sent to driver
2017-08-10 09:38:55,624 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 19.0 (TID 38) in 593 ms on localhost (1/2)
2017-08-10 09:38:56,133 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 19.0 (TID 39). 787 bytes result sent to driver
2017-08-10 09:38:56,135 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 19.0 (TID 39) in 1102 ms on localhost (2/2)
2017-08-10 09:38:56,135 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 19 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.104 s
2017-08-10 09:38:56,135 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 19.0, whose tasks have all completed, from pool 
2017-08-10 09:38:56,135 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 19 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.111852 s
2017-08-10 09:38:56,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329135000 ms.0 from job set of time 1502329135000 ms
2017-08-10 09:38:56,137 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 37 from persistence list
2017-08-10 09:38:56,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.136 s for time 1502329135000 ms (execution: 1.121 s)
2017-08-10 09:38:56,139 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 37
2017-08-10 09:38:56,139 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 36 from persistence list
2017-08-10 09:38:56,139 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 36
2017-08-10 09:38:56,139 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:56,140 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329125000 ms
2017-08-10 09:39:00,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329140000 ms
2017-08-10 09:39:00,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329140000 ms.0 from job set of time 1502329140000 ms
2017-08-10 09:39:00,036 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:00,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 20 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:00,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 20 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:00,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:00,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:00,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 20 (MapPartitionsRDD[41] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:00,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:00,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:00,047 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_20_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 20 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 20.0 with 2 tasks
2017-08-10 09:39:00,050 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 20.0 (TID 40, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:00,051 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 20.0 (TID 41, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:00,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 20.0 (TID 40)
2017-08-10 09:39:00,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 20.0 (TID 41)
2017-08-10 09:39:00,053 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 500 -> 525
2017-08-10 09:39:00,053 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 500 -> 525
2017-08-10 09:39:00,507 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_19_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:00,680 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 20.0 (TID 40). 787 bytes result sent to driver
2017-08-10 09:39:00,682 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 20.0 (TID 40) in 633 ms on localhost (1/2)
2017-08-10 09:39:01,009 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 20.0 (TID 41). 787 bytes result sent to driver
2017-08-10 09:39:01,010 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 20.0 (TID 41) in 960 ms on localhost (2/2)
2017-08-10 09:39:01,010 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 20.0, whose tasks have all completed, from pool 
2017-08-10 09:39:01,010 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 20 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.961 s
2017-08-10 09:39:01,010 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 20 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.973905 s
2017-08-10 09:39:01,011 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329140000 ms.0 from job set of time 1502329140000 ms
2017-08-10 09:39:01,011 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.011 s for time 1502329140000 ms (execution: 0.987 s)
2017-08-10 09:39:01,011 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 39 from persistence list
2017-08-10 09:39:01,011 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 39
2017-08-10 09:39:01,011 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 38 from persistence list
2017-08-10 09:39:01,012 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 38
2017-08-10 09:39:01,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:01,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329130000 ms
2017-08-10 09:39:05,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329145000 ms
2017-08-10 09:39:05,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329145000 ms.0 from job set of time 1502329145000 ms
2017-08-10 09:39:05,019 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:05,019 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 21 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:05,019 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 21 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:05,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:05,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:05,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 21 (MapPartitionsRDD[43] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:05,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:05,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:05,025 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_21_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 21 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 21.0 with 2 tasks
2017-08-10 09:39:05,027 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 21.0 (TID 42, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:05,028 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 21.0 (TID 43, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:05,028 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 21.0 (TID 43)
2017-08-10 09:39:05,028 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 21.0 (TID 42)
2017-08-10 09:39:05,029 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 525 -> 550
2017-08-10 09:39:05,029 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 525 -> 550
2017-08-10 09:39:05,406 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_20_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:05,460 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 21.0 (TID 43). 787 bytes result sent to driver
2017-08-10 09:39:05,462 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 21.0 (TID 43) in 434 ms on localhost (1/2)
2017-08-10 09:39:05,754 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 21.0 (TID 42). 787 bytes result sent to driver
2017-08-10 09:39:05,755 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 21.0 (TID 42) in 728 ms on localhost (2/2)
2017-08-10 09:39:05,756 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 21.0, whose tasks have all completed, from pool 
2017-08-10 09:39:05,756 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 21 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.730 s
2017-08-10 09:39:05,756 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 21 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.736904 s
2017-08-10 09:39:05,756 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329145000 ms.0 from job set of time 1502329145000 ms
2017-08-10 09:39:05,756 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.756 s for time 1502329145000 ms (execution: 0.743 s)
2017-08-10 09:39:05,757 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 41 from persistence list
2017-08-10 09:39:05,757 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 41
2017-08-10 09:39:05,757 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 40 from persistence list
2017-08-10 09:39:05,757 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 40
2017-08-10 09:39:05,757 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:05,758 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329135000 ms
2017-08-10 09:39:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329150000 ms
2017-08-10 09:39:10,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329150000 ms.0 from job set of time 1502329150000 ms
2017-08-10 09:39:10,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:10,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 22 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:10,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 22 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:10,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:10,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:10,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 22 (MapPartitionsRDD[45] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:10,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:10,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_22_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 22 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:10,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:10,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 22.0 with 2 tasks
2017-08-10 09:39:10,060 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 22.0 (TID 44, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:10,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 22.0 (TID 45, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:10,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 22.0 (TID 45)
2017-08-10 09:39:10,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 22.0 (TID 44)
2017-08-10 09:39:10,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 550 -> 575
2017-08-10 09:39:10,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 550 -> 575
2017-08-10 09:39:10,279 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_21_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:10,709 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 22.0 (TID 45). 787 bytes result sent to driver
2017-08-10 09:39:10,711 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 22.0 (TID 45) in 650 ms on localhost (1/2)
2017-08-10 09:39:11,268 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 22.0 (TID 44). 787 bytes result sent to driver
2017-08-10 09:39:11,270 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 22.0 (TID 44) in 1210 ms on localhost (2/2)
2017-08-10 09:39:11,270 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 22.0, whose tasks have all completed, from pool 
2017-08-10 09:39:11,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 22 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.213 s
2017-08-10 09:39:11,272 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 22 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.231221 s
2017-08-10 09:39:11,274 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329150000 ms.0 from job set of time 1502329150000 ms
2017-08-10 09:39:11,274 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 43 from persistence list
2017-08-10 09:39:11,274 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.273 s for time 1502329150000 ms (execution: 1.251 s)
2017-08-10 09:39:11,274 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 42 from persistence list
2017-08-10 09:39:11,275 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:11,275 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329140000 ms
2017-08-10 09:39:11,275 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 42
2017-08-10 09:39:11,275 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 43
2017-08-10 09:39:15,024 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329155000 ms
2017-08-10 09:39:15,027 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329155000 ms.0 from job set of time 1502329155000 ms
2017-08-10 09:39:15,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:15,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 23 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:15,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 23 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:15,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 23 (MapPartitionsRDD[47] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:15,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:15,060 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_23_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 23 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 23 (MapPartitionsRDD[47] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 23.0 with 2 tasks
2017-08-10 09:39:15,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 23.0 (TID 46, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:15,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 23.0 (TID 47, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:15,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 23.0 (TID 47)
2017-08-10 09:39:15,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 23.0 (TID 46)
2017-08-10 09:39:15,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 575 -> 600
2017-08-10 09:39:15,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 575 -> 600
2017-08-10 09:39:15,267 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_22_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:16,291 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 23.0 (TID 46). 787 bytes result sent to driver
2017-08-10 09:39:16,293 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 23.0 (TID 46) in 1232 ms on localhost (1/2)
2017-08-10 09:39:16,617 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 23.0 (TID 47). 787 bytes result sent to driver
2017-08-10 09:39:16,618 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 23.0 (TID 47) in 1555 ms on localhost (2/2)
2017-08-10 09:39:16,619 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 23 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.558 s
2017-08-10 09:39:16,619 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 23.0, whose tasks have all completed, from pool 
2017-08-10 09:39:16,619 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 23 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.567878 s
2017-08-10 09:39:16,620 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329155000 ms.0 from job set of time 1502329155000 ms
2017-08-10 09:39:16,620 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.620 s for time 1502329155000 ms (execution: 1.594 s)
2017-08-10 09:39:16,620 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 45 from persistence list
2017-08-10 09:39:16,620 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 45
2017-08-10 09:39:16,621 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 44 from persistence list
2017-08-10 09:39:16,621 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 44
2017-08-10 09:39:16,621 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:16,621 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329145000 ms
2017-08-10 09:39:20,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329160000 ms
2017-08-10 09:39:20,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329160000 ms.0 from job set of time 1502329160000 ms
2017-08-10 09:39:20,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 24 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 24 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 24 (MapPartitionsRDD[49] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:20,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:20,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_24_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 24 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 24.0 with 2 tasks
2017-08-10 09:39:20,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 24.0 (TID 48, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:20,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 24.0 (TID 49, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:20,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 24.0 (TID 49)
2017-08-10 09:39:20,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 24.0 (TID 48)
2017-08-10 09:39:20,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 600 -> 625
2017-08-10 09:39:20,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 600 -> 625
2017-08-10 09:39:20,389 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_23_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:21,201 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 24.0 (TID 48). 787 bytes result sent to driver
2017-08-10 09:39:21,203 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 24.0 (TID 48) in 1139 ms on localhost (1/2)
2017-08-10 09:39:21,434 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 24.0 (TID 49). 787 bytes result sent to driver
2017-08-10 09:39:21,436 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 24.0 (TID 49) in 1371 ms on localhost (2/2)
2017-08-10 09:39:21,437 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 24.0, whose tasks have all completed, from pool 
2017-08-10 09:39:21,437 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 24 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.373 s
2017-08-10 09:39:21,438 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 24 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.393368 s
2017-08-10 09:39:21,438 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329160000 ms.0 from job set of time 1502329160000 ms
2017-08-10 09:39:21,438 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.438 s for time 1502329160000 ms (execution: 1.416 s)
2017-08-10 09:39:21,438 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 47 from persistence list
2017-08-10 09:39:21,439 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 47
2017-08-10 09:39:21,439 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 46 from persistence list
2017-08-10 09:39:21,440 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 46
2017-08-10 09:39:21,440 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:21,440 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329150000 ms
2017-08-10 09:39:25,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329165000 ms
2017-08-10 09:39:25,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329165000 ms.0 from job set of time 1502329165000 ms
2017-08-10 09:39:25,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:25,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 25 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 25 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 25 (MapPartitionsRDD[51] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:25,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:25,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_25_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 25 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 25.0 with 2 tasks
2017-08-10 09:39:25,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 25.0 (TID 50, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:25,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 25.0 (TID 51, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:25,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 25.0 (TID 51)
2017-08-10 09:39:25,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 25.0 (TID 50)
2017-08-10 09:39:25,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 625 -> 650
2017-08-10 09:39:25,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 625 -> 650
2017-08-10 09:39:25,148 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_24_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:25,978 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 25.0 (TID 50). 787 bytes result sent to driver
2017-08-10 09:39:25,980 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 25.0 (TID 50) in 924 ms on localhost (1/2)
2017-08-10 09:39:26,183 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 25.0 (TID 51). 787 bytes result sent to driver
2017-08-10 09:39:26,184 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 25.0 (TID 51) in 1126 ms on localhost (2/2)
2017-08-10 09:39:26,185 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 25.0, whose tasks have all completed, from pool 
2017-08-10 09:39:26,185 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 25 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.129 s
2017-08-10 09:39:26,185 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 25 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.141576 s
2017-08-10 09:39:26,186 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329165000 ms.0 from job set of time 1502329165000 ms
2017-08-10 09:39:26,186 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.186 s for time 1502329165000 ms (execution: 1.165 s)
2017-08-10 09:39:26,186 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 49 from persistence list
2017-08-10 09:39:26,187 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 49
2017-08-10 09:39:26,188 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 48 from persistence list
2017-08-10 09:39:26,190 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 48
2017-08-10 09:39:26,190 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:26,191 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329155000 ms
2017-08-10 09:39:30,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329170000 ms
2017-08-10 09:39:30,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329170000 ms.0 from job set of time 1502329170000 ms
2017-08-10 09:39:30,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:30,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 26 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 26 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 26 (MapPartitionsRDD[53] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:30,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:30,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:30,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_26_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:30,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 26 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 26 (MapPartitionsRDD[53] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 26.0 with 2 tasks
2017-08-10 09:39:30,035 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 26.0 (TID 52, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:30,036 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 26.0 (TID 53, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:30,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 26.0 (TID 53)
2017-08-10 09:39:30,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 26.0 (TID 52)
2017-08-10 09:39:30,038 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 650 -> 675
2017-08-10 09:39:30,038 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 650 -> 653
2017-08-10 09:39:30,187 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 26.0 (TID 52). 714 bytes result sent to driver
2017-08-10 09:39:30,189 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 26.0 (TID 52) in 155 ms on localhost (1/2)
2017-08-10 09:39:30,293 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_25_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:31,189 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 26.0 (TID 53). 787 bytes result sent to driver
2017-08-10 09:39:31,191 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 26.0 (TID 53) in 1156 ms on localhost (2/2)
2017-08-10 09:39:31,191 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 26.0, whose tasks have all completed, from pool 
2017-08-10 09:39:31,191 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 26 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.157 s
2017-08-10 09:39:31,191 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 26 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.164509 s
2017-08-10 09:39:31,192 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329170000 ms.0 from job set of time 1502329170000 ms
2017-08-10 09:39:31,192 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.192 s for time 1502329170000 ms (execution: 1.176 s)
2017-08-10 09:39:31,192 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 51 from persistence list
2017-08-10 09:39:31,193 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 51
2017-08-10 09:39:31,193 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 50 from persistence list
2017-08-10 09:39:31,193 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 50
2017-08-10 09:39:31,193 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:31,193 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329160000 ms
2017-08-10 09:39:35,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329175000 ms
2017-08-10 09:39:35,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329175000 ms.0 from job set of time 1502329175000 ms
2017-08-10 09:39:35,034 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:35,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 27 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 27 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 27 (MapPartitionsRDD[55] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:35,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:35,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:35,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_27_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:35,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 27 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 27.0 with 2 tasks
2017-08-10 09:39:35,047 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 27.0 (TID 54, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:35,048 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 27.0 (TID 55, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:35,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 27.0 (TID 54)
2017-08-10 09:39:35,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 27.0 (TID 55)
2017-08-10 09:39:35,050 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:39:35,050 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 675 -> 698
2017-08-10 09:39:35,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 27.0 (TID 54). 714 bytes result sent to driver
2017-08-10 09:39:35,058 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 27.0 (TID 54) in 12 ms on localhost (1/2)
2017-08-10 09:39:35,488 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_26_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:35,736 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 27.0 (TID 55). 787 bytes result sent to driver
2017-08-10 09:39:35,739 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 27.0 (TID 55) in 692 ms on localhost (2/2)
2017-08-10 09:39:35,739 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 27.0, whose tasks have all completed, from pool 
2017-08-10 09:39:35,739 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 27 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.693 s
2017-08-10 09:39:35,740 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 27 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.705187 s
2017-08-10 09:39:35,741 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329175000 ms.0 from job set of time 1502329175000 ms
2017-08-10 09:39:35,741 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 53 from persistence list
2017-08-10 09:39:35,741 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.741 s for time 1502329175000 ms (execution: 0.719 s)
2017-08-10 09:39:35,741 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 53
2017-08-10 09:39:35,742 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 52 from persistence list
2017-08-10 09:39:35,742 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 52
2017-08-10 09:39:35,742 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:35,742 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329165000 ms
2017-08-10 09:39:40,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329180000 ms
2017-08-10 09:39:40,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329180000 ms.0 from job set of time 1502329180000 ms
2017-08-10 09:39:40,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 28 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 28 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 28 (MapPartitionsRDD[57] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:40,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:40,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:40,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_28_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 28 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:40,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 28.0 with 2 tasks
2017-08-10 09:39:40,032 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 28.0 (TID 56, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:40,033 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 28.0 (TID 57, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:40,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 28.0 (TID 57)
2017-08-10 09:39:40,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 28.0 (TID 56)
2017-08-10 09:39:40,034 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:39:40,034 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:39:40,037 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 28.0 (TID 56). 714 bytes result sent to driver
2017-08-10 09:39:40,037 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 28.0 (TID 57). 714 bytes result sent to driver
2017-08-10 09:39:40,039 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 28.0 (TID 57) in 7 ms on localhost (1/2)
2017-08-10 09:39:40,039 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 28.0 (TID 56) in 8 ms on localhost (2/2)
2017-08-10 09:39:40,039 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 28.0, whose tasks have all completed, from pool 
2017-08-10 09:39:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 28 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.008 s
2017-08-10 09:39:40,040 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 28 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.016654 s
2017-08-10 09:39:40,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329180000 ms.0 from job set of time 1502329180000 ms
2017-08-10 09:39:40,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.040 s for time 1502329180000 ms (execution: 0.026 s)
2017-08-10 09:39:40,040 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 55 from persistence list
2017-08-10 09:39:40,041 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 55
2017-08-10 09:39:40,041 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 54 from persistence list
2017-08-10 09:39:40,041 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 54
2017-08-10 09:39:40,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:40,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329170000 ms
2017-08-10 09:39:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329185000 ms
2017-08-10 09:39:45,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329185000 ms.0 from job set of time 1502329185000 ms
2017-08-10 09:39:45,022 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:45,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 29 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:45,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 29 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:45,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:45,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:45,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 29 (MapPartitionsRDD[59] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:45,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:45,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:45,028 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_29_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:45,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 29 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:45,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 29 (MapPartitionsRDD[59] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:45,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 29.0 with 2 tasks
2017-08-10 09:39:45,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 29.0 (TID 58, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:45,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 29.0 (TID 59, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:45,031 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 29.0 (TID 59)
2017-08-10 09:39:45,031 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 29.0 (TID 58)
2017-08-10 09:39:45,032 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:39:45,032 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:39:45,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 29.0 (TID 59). 714 bytes result sent to driver
2017-08-10 09:39:45,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 29.0 (TID 58). 714 bytes result sent to driver
2017-08-10 09:39:45,038 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 29.0 (TID 58) in 9 ms on localhost (1/2)
2017-08-10 09:39:45,038 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 29.0 (TID 59) in 8 ms on localhost (2/2)
2017-08-10 09:39:45,038 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 29.0, whose tasks have all completed, from pool 
2017-08-10 09:39:45,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 29 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.009 s
2017-08-10 09:39:45,039 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 29 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.016194 s
2017-08-10 09:39:45,039 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329185000 ms.0 from job set of time 1502329185000 ms
2017-08-10 09:39:45,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.039 s for time 1502329185000 ms (execution: 0.025 s)
2017-08-10 09:39:45,040 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 57 from persistence list
2017-08-10 09:39:45,040 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 57
2017-08-10 09:39:45,040 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 56 from persistence list
2017-08-10 09:39:45,040 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 56
2017-08-10 09:39:45,040 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:45,040 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329175000 ms
2017-08-10 09:39:50,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329190000 ms
2017-08-10 09:39:50,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329190000 ms.0 from job set of time 1502329190000 ms
2017-08-10 09:39:50,020 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:50,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 30 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:50,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 30 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:50,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:50,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:50,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 30 (MapPartitionsRDD[61] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:50,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:50,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:50,040 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_30_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 30 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 30.0 with 2 tasks
2017-08-10 09:39:50,042 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 30.0 (TID 60, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:50,042 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 30.0 (TID 61, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:50,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 30.0 (TID 60)
2017-08-10 09:39:50,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 30.0 (TID 61)
2017-08-10 09:39:50,043 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:39:50,043 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:39:50,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 30.0 (TID 60). 714 bytes result sent to driver
2017-08-10 09:39:50,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 30.0 (TID 61). 714 bytes result sent to driver
2017-08-10 09:39:50,048 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 30.0 (TID 61) in 6 ms on localhost (1/2)
2017-08-10 09:39:50,049 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 30.0 (TID 60) in 8 ms on localhost (2/2)
2017-08-10 09:39:50,049 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 30.0, whose tasks have all completed, from pool 
2017-08-10 09:39:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 30 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.008 s
2017-08-10 09:39:50,049 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 30 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.028636 s
2017-08-10 09:39:50,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329190000 ms.0 from job set of time 1502329190000 ms
2017-08-10 09:39:50,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.049 s for time 1502329190000 ms (execution: 0.035 s)
2017-08-10 09:39:50,050 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 59 from persistence list
2017-08-10 09:39:50,050 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 59
2017-08-10 09:39:50,050 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 58 from persistence list
2017-08-10 09:39:50,050 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 58
2017-08-10 09:39:50,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:50,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329180000 ms
2017-08-10 09:39:55,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329195000 ms
2017-08-10 09:39:55,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329195000 ms.0 from job set of time 1502329195000 ms
2017-08-10 09:39:55,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 31 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 31 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 31 (MapPartitionsRDD[63] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:55,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:55,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:55,029 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_31_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 31 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:55,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:55,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 31.0 with 2 tasks
2017-08-10 09:39:55,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 31.0 (TID 62, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:55,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 31.0 (TID 63, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:55,032 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 31.0 (TID 63)
2017-08-10 09:39:55,032 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 31.0 (TID 62)
2017-08-10 09:39:55,033 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:39:55,033 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:39:55,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 31.0 (TID 63). 714 bytes result sent to driver
2017-08-10 09:39:55,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 31.0 (TID 62). 714 bytes result sent to driver
2017-08-10 09:39:55,038 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 31.0 (TID 62) in 8 ms on localhost (1/2)
2017-08-10 09:39:55,039 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 31.0 (TID 63) in 8 ms on localhost (2/2)
2017-08-10 09:39:55,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 31.0, whose tasks have all completed, from pool 
2017-08-10 09:39:55,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 31 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.010 s
2017-08-10 09:39:55,040 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 31 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.017033 s
2017-08-10 09:39:55,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329195000 ms.0 from job set of time 1502329195000 ms
2017-08-10 09:39:55,042 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1502329195000 ms (execution: 0.027 s)
2017-08-10 09:39:55,042 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 61 from persistence list
2017-08-10 09:39:55,042 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 61
2017-08-10 09:39:55,042 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 60 from persistence list
2017-08-10 09:39:55,043 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 60
2017-08-10 09:39:55,043 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:55,043 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329185000 ms
2017-08-10 09:40:00,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329200000 ms
2017-08-10 09:40:00,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329200000 ms.0 from job set of time 1502329200000 ms
2017-08-10 09:40:00,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:40:00,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 32 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:40:00,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 32 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:40:00,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:40:00,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:40:00,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 32 (MapPartitionsRDD[65] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:40:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:40:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:40:00,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_32_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 32 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:40:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:40:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 32.0 with 2 tasks
2017-08-10 09:40:00,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 32.0 (TID 64, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:40:00,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 32.0 (TID 65, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:40:00,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 32.0 (TID 64)
2017-08-10 09:40:00,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 32.0 (TID 65)
2017-08-10 09:40:00,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:40:00,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:40:00,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 32.0 (TID 65). 714 bytes result sent to driver
2017-08-10 09:40:00,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 32.0 (TID 64). 714 bytes result sent to driver
2017-08-10 09:40:00,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 32.0 (TID 65) in 9 ms on localhost (1/2)
2017-08-10 09:40:00,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 32.0 (TID 64) in 11 ms on localhost (2/2)
2017-08-10 09:40:00,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 32.0, whose tasks have all completed, from pool 
2017-08-10 09:40:00,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 32 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.011 s
2017-08-10 09:40:00,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 32 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.024169 s
2017-08-10 09:40:00,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329200000 ms.0 from job set of time 1502329200000 ms
2017-08-10 09:40:00,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 63 from persistence list
2017-08-10 09:40:00,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1502329200000 ms (execution: 0.047 s)
2017-08-10 09:40:00,076 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 63
2017-08-10 09:40:00,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 62 from persistence list
2017-08-10 09:40:00,076 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 62
2017-08-10 09:40:00,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:40:00,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329190000 ms
2017-08-10 09:40:00,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_32_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:00,079 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_27_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:00,081 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_28_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:00,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_29_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:00,086 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_30_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:00,087 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_31_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:05,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329205000 ms
2017-08-10 09:40:05,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329205000 ms.0 from job set of time 1502329205000 ms
2017-08-10 09:40:05,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:40:05,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 33 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:40:05,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 33 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:40:05,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:40:05,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:40:05,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 33 (MapPartitionsRDD[67] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:40:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:40:05,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:40:05,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_33_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 33 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:40:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 33 (MapPartitionsRDD[67] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:40:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 33.0 with 2 tasks
2017-08-10 09:40:05,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 33.0 (TID 66, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:40:05,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 33.0 (TID 67, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:40:05,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 33.0 (TID 66)
2017-08-10 09:40:05,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 33.0 (TID 67)
2017-08-10 09:40:05,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:40:05,063 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:40:05,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 33.0 (TID 66). 714 bytes result sent to driver
2017-08-10 09:40:05,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 33.0 (TID 67). 714 bytes result sent to driver
2017-08-10 09:40:05,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 33.0 (TID 66) in 14 ms on localhost (1/2)
2017-08-10 09:40:05,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 33.0 (TID 67) in 12 ms on localhost (2/2)
2017-08-10 09:40:05,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 33.0, whose tasks have all completed, from pool 
2017-08-10 09:40:05,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 33 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.015 s
2017-08-10 09:40:05,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 33 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.033046 s
2017-08-10 09:40:05,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329205000 ms.0 from job set of time 1502329205000 ms
2017-08-10 09:40:05,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1502329205000 ms (execution: 0.053 s)
2017-08-10 09:40:05,074 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 65 from persistence list
2017-08-10 09:40:05,074 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 65
2017-08-10 09:40:05,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 64 from persistence list
2017-08-10 09:40:05,075 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 64
2017-08-10 09:40:05,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:40:05,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329195000 ms
2017-08-11 18:42:57,124 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-11 18:42:57,718 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-11 18:42:58,205 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-11 18:42:58,207 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-11 18:42:58,209 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-11 18:42:58,210 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-11 18:42:58,212 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-11 18:43:00,043 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 55891.
2017-08-11 18:43:00,095 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-11 18:43:00,126 [main] ERROR [org.apache.spark.SparkContext] - Error initializing SparkContext.
java.lang.IllegalArgumentException: System memory 259522560 must be at least 471859200. Please increase heap size using the --driver-memory option or spark.driver.memory in Spark configuration.
	at org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:212) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:194) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:308) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:165) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:259) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:423) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:836) [spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84) [spark-streaming_2.11-2.0.0.jar:2.0.0]
	at cn.datapark.process.education.process.streamingProcessNew$.main(streamingProcessNew.scala:36) [classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew.main(streamingProcessNew.scala) [classes/:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_111]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_111]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_111]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_111]
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144) [idea_rt.jar:na]
2017-08-11 18:43:00,159 [main] INFO  [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2017-08-11 18:43:00,162 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2017-08-11 18:43:44,826 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-11 18:43:45,056 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-11 18:43:45,338 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-11 18:43:45,338 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-11 18:43:45,339 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-11 18:43:45,339 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-11 18:43:45,340 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-11 18:43:46,085 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 55996.
2017-08-11 18:43:46,105 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-11 18:43:46,146 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-11 18:43:46,164 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-65c186db-1184-4f49-879d-dbcc04c676b3
2017-08-11 18:43:46,202 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-11 18:43:46,272 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-11 18:43:46,449 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2583ms
2017-08-11 18:43:46,614 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-11 18:43:46,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@17ea53a{/jobs,null,AVAILABLE}
2017-08-11 18:43:46,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@cb0951{/jobs/json,null,AVAILABLE}
2017-08-11 18:43:46,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1f22f18{/jobs/job,null,AVAILABLE}
2017-08-11 18:43:46,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7f474e{/jobs/job/json,null,AVAILABLE}
2017-08-11 18:43:46,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7471ce{/stages,null,AVAILABLE}
2017-08-11 18:43:46,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@276e50{/stages/json,null,AVAILABLE}
2017-08-11 18:43:46,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e70ea3{/stages/stage,null,AVAILABLE}
2017-08-11 18:43:46,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fe7086{/stages/stage/json,null,AVAILABLE}
2017-08-11 18:43:46,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15d8c96{/stages/pool,null,AVAILABLE}
2017-08-11 18:43:46,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/stages/pool/json,null,AVAILABLE}
2017-08-11 18:43:46,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/storage,null,AVAILABLE}
2017-08-11 18:43:46,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/storage/json,null,AVAILABLE}
2017-08-11 18:43:46,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/storage/rdd,null,AVAILABLE}
2017-08-11 18:43:46,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/storage/rdd/json,null,AVAILABLE}
2017-08-11 18:43:46,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/environment,null,AVAILABLE}
2017-08-11 18:43:46,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/environment/json,null,AVAILABLE}
2017-08-11 18:43:46,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/executors,null,AVAILABLE}
2017-08-11 18:43:46,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/executors/json,null,AVAILABLE}
2017-08-11 18:43:46,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/executors/threadDump,null,AVAILABLE}
2017-08-11 18:43:46,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/executors/threadDump/json,null,AVAILABLE}
2017-08-11 18:43:46,642 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/static,null,AVAILABLE}
2017-08-11 18:43:46,642 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/,null,AVAILABLE}
2017-08-11 18:43:46,643 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/api,null,AVAILABLE}
2017-08-11 18:43:46,643 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/stages/stage/kill,null,AVAILABLE}
2017-08-11 18:43:46,650 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@f92cc7{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 18:43:46,651 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2785ms
2017-08-11 18:43:46,651 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-11 18:43:46,679 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-11 18:43:46,870 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-11 18:43:46,919 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56017.
2017-08-11 18:43:46,920 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:56017
2017-08-11 18:43:46,939 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 56017)
2017-08-11 18:43:46,941 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:56017 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 56017)
2017-08-11 18:43:46,954 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 56017)
2017-08-11 18:43:47,231 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@109fac1{/metrics/json,null,AVAILABLE}
2017-08-11 18:43:48,242 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-08-11 18:43:48,242 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:43:48,243 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-11 18:43:48,243 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@1acb27a
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@db392f
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-08-11 18:43:48,245 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@1b2c305
2017-08-11 18:43:48,301 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1502448230000
2017-08-11 18:43:48,302 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1502448230000 ms
2017-08-11 18:43:48,303 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-11 18:43:48,305 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cf395{/streaming,null,AVAILABLE}
2017-08-11 18:43:48,305 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6aa04a{/streaming/json,null,AVAILABLE}
2017-08-11 18:43:48,306 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch,null,AVAILABLE}
2017-08-11 18:43:48,306 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@814f43{/streaming/batch/json,null,AVAILABLE}
2017-08-11 18:43:48,308 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1a00961{/static/streaming,null,AVAILABLE}
2017-08-11 18:43:48,310 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-11 18:43:50,211 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448230000 ms
2017-08-11 18:43:50,216 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448230000 ms.0 from job set of time 1502448230000 ms
2017-08-11 18:43:50,266 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:43:50,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:43:50,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:43:50,302 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:43:50,304 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:43:50,322 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:43:50,481 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-11 18:43:50,491 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:43:50,804 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:43:50,807 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:56017 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:43:50,829 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:43:50,834 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49)
2017-08-11 18:43:50,837 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-11 18:43:50,907 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:43:50,911 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:43:50,924 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-11 18:43:50,924 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-11 18:43:50,974 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 0 -> 25
2017-08-11 18:43:50,974 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 0 -> 25
2017-08-11 18:43:51,196 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-08-11 18:43:51,525 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-08-11 18:43:55,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448235000 ms
2017-08-11 18:44:00,024 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448240000 ms
2017-08-11 18:44:02,109 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-08-11 18:44:02,129 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 11217 ms on localhost (1/2)
2017-08-11 18:44:02,261 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-08-11 18:44:02,269 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 11394 ms on localhost (2/2)
2017-08-11 18:44:02,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcessNew.scala:51) finished in 11.421 s
2017-08-11 18:44:02,271 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-08-11 18:44:02,279 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcessNew.scala:51, took 12.012820 s
2017-08-11 18:44:02,311 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:44:02,313 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448230000 ms.0 from job set of time 1502448230000 ms
2017-08-11 18:44:02,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:44:02,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:44:02,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:44:02,314 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:44:02,315 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:44:02,319 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 12.300 s for time 1502448230000 ms (execution: 12.086 s)
2017-08-11 18:44:02,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:44:02,320 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448235000 ms.0 from job set of time 1502448235000 ms
2017-08-11 18:44:02,326 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:44:02,326 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.111:56017 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:44:02,327 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:44:02,327 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49)
2017-08-11 18:44:02,327 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-08-11 18:44:02,330 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:44:02,334 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:44:02,336 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:44:02,336 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-08-11 18:44:02,336 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-08-11 18:44:02,339 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 25 -> 50
2017-08-11 18:44:02,340 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 25 -> 50
2017-08-11 18:44:02,341 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 18:44:02,434 [Executor task launch worker-0] ERROR [org.apache.spark.executor.Executor] - Exception in task 0.0 in stage 1.0 (TID 2)
java.util.regex.PatternSyntaxException: Illegal character range near index 464
55 </p>|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>| HUNT:|<a href="http://socialbeta.com.*>.*?</a>|<h2.*.*?</h2>|<strong>[\u4e00-\u9fa5]{3}.*?</strong>|<blockquote>.*?</blockquote>|<p>.*?</p>|<p><strong>.*?</p>|<p><strong>.*?</p>|<p style="white-space: normal;">.*.*?</p>|<p style="text-align: justify;">.*.*?</p>|<strong>[A-Za-Z]*[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*</a> SocialBeta .*.*?</p>|<p><a href=.*></a> SocialBeta .*</p>|<p hiragino="".*>brandfilm 20:46 </p>|<strong> SocialBeta  2017 </strong>|<p style="text-align: justify;">.*</p>|<p style="margin: 0px;.*>.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta</strong>.*?</p>|<p>.*.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.range(Pattern.java:2655) ~[na:1.8.0_111]
	at java.util.regex.Pattern.clazz(Pattern.java:2562) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2063) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]
2017-08-11 18:44:02,498 [task-result-getter-2] WARN  [org.apache.spark.scheduler.TaskSetManager] - Lost task 0.0 in stage 1.0 (TID 2, localhost): java.util.regex.PatternSyntaxException: Illegal character range near index 464
55 </p>|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>| HUNT:|<a href="http://socialbeta.com.*>.*?</a>|<h2.*.*?</h2>|<strong>[\u4e00-\u9fa5]{3}.*?</strong>|<blockquote>.*?</blockquote>|<p>.*?</p>|<p><strong>.*?</p>|<p><strong>.*?</p>|<p style="white-space: normal;">.*.*?</p>|<p style="text-align: justify;">.*.*?</p>|<strong>[A-Za-Z]*[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*</a> SocialBeta .*.*?</p>|<p><a href=.*></a> SocialBeta .*</p>|<p hiragino="".*>brandfilm 20:46 </p>|<strong> SocialBeta  2017 </strong>|<p style="text-align: justify;">.*</p>|<p style="margin: 0px;.*>.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta</strong>.*?</p>|<p>.*.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.range(Pattern.java:2655)
	at java.util.regex.Pattern.clazz(Pattern.java:2562)
	at java.util.regex.Pattern.sequence(Pattern.java:2063)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2017-08-11 18:44:02,503 [task-result-getter-2] ERROR [org.apache.spark.scheduler.TaskSetManager] - Task 0 in stage 1.0 failed 1 times; aborting job
2017-08-11 18:44:02,507 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Cancelling stage 1
2017-08-11 18:44:02,510 [dispatcher-event-loop-2] INFO  [org.apache.spark.executor.Executor] - Executor is trying to kill task 1.0 in stage 1.0 (TID 3)
2017-08-11 18:44:02,510 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Stage 1 was cancelled
2017-08-11 18:44:02,511 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcessNew.scala:51) failed in 0.182 s
2017-08-11 18:44:02,513 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 failed: foreachPartition at streamingProcessNew.scala:51, took 0.200710 s
2017-08-11 18:44:02,516 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448235000 ms.0 from job set of time 1502448235000 ms
2017-08-11 18:44:02,517 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 7.514 s for time 1502448235000 ms (execution: 0.195 s)
2017-08-11 18:44:02,519 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-08-11 18:44:02,526 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448240000 ms.0 from job set of time 1502448240000 ms
2017-08-11 18:44:02,528 [JobScheduler] ERROR [org.apache.spark.streaming.scheduler.JobScheduler] - Error running job streaming job 1502448235000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 2, localhost): java.util.regex.PatternSyntaxException: Illegal character range near index 464
55 </p>|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>| HUNT:|<a href="http://socialbeta.com.*>.*?</a>|<h2.*.*?</h2>|<strong>[\u4e00-\u9fa5]{3}.*?</strong>|<blockquote>.*?</blockquote>|<p>.*?</p>|<p><strong>.*?</p>|<p><strong>.*?</p>|<p style="white-space: normal;">.*.*?</p>|<p style="text-align: justify;">.*.*?</p>|<strong>[A-Za-Z]*[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*</a> SocialBeta .*.*?</p>|<p><a href=.*></a> SocialBeta .*</p>|<p hiragino="".*>brandfilm 20:46 </p>|<strong> SocialBeta  2017 </strong>|<p style="text-align: justify;">.*</p>|<p style="margin: 0px;.*>.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta</strong>.*?</p>|<p>.*.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.range(Pattern.java:2655)
	at java.util.regex.Pattern.clazz(Pattern.java:2562)
	at java.util.regex.Pattern.sequence(Pattern.java:2063)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[scala-library.jar:na]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[scala-library.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.Option.foreach(Option.scala:257) ~[scala-library.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1911) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:881) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:881) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at scala.util.Try$.apply(Try.scala:192) ~[scala-library.jar:na]
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) ~[scala-library.jar:na]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:244) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_111]
Caused by: java.util.regex.PatternSyntaxException: Illegal character range near index 464
55 </p>|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>| HUNT:|<a href="http://socialbeta.com.*>.*?</a>|<h2.*.*?</h2>|<strong>[\u4e00-\u9fa5]{3}.*?</strong>|<blockquote>.*?</blockquote>|<p>.*?</p>|<p><strong>.*?</p>|<p><strong>.*?</p>|<p style="white-space: normal;">.*.*?</p>|<p style="text-align: justify;">.*.*?</p>|<strong>[A-Za-Z]*[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*</a> SocialBeta .*.*?</p>|<p><a href=.*></a> SocialBeta .*</p>|<p hiragino="".*>brandfilm 20:46 </p>|<strong> SocialBeta  2017 </strong>|<p style="text-align: justify;">.*</p>|<p style="margin: 0px;.*>.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta</strong>.*?</p>|<p>.*.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.range(Pattern.java:2655) ~[na:1.8.0_111]
	at java.util.regex.Pattern.clazz(Pattern.java:2562) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2063) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	... 3 common frames omitted
2017-08-11 18:44:02,530 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:44:02,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:44:02,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:44:02,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:44:02,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:44:02,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:44:02,539 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:44:02,542 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-08-11 18:44:02,543 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:44:02,544 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 18:44:02,545 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:44:02,546 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on 192.168.31.111:56017 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:44:02,546 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:44:02,547 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:49)
2017-08-11 18:44:02,547 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2017-08-11 18:44:02,554 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:44:02,555 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:44:02,556 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2017-08-11 18:44:02,556 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2017-08-11 18:44:02,559 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 50 -> 75
2017-08-11 18:44:02,559 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 50 -> 75
2017-08-11 18:44:02,566 [Thread-0] INFO  [org.apache.spark.streaming.StreamingContext] - Invoking stop(stopGracefully=false) from shutdown hook
2017-08-11 18:44:02,566 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-08-11 18:44:02,567 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-08-11 18:44:02,570 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Stopping JobGenerator immediately
2017-08-11 18:44:02,571 [Thread-0] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Stopped timer for JobGenerator after time 1502448240000
2017-08-11 18:44:02,575 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Stopped JobGenerator
2017-08-11 18:44:02,700 [Executor task launch worker-2] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-11 18:44:02,727 [Executor task launch worker-0] ERROR [org.apache.spark.executor.Executor] - Exception in task 0.0 in stage 2.0 (TID 4)
java.util.regex.PatternSyntaxException: Illegal character range near index 464
55 </p>|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>| HUNT:|<a href="http://socialbeta.com.*>.*?</a>|<h2.*.*?</h2>|<strong>[\u4e00-\u9fa5]{3}.*?</strong>|<blockquote>.*?</blockquote>|<p>.*?</p>|<p><strong>.*?</p>|<p><strong>.*?</p>|<p style="white-space: normal;">.*.*?</p>|<p style="text-align: justify;">.*.*?</p>|<strong>[A-Za-Z]*[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*</a> SocialBeta .*.*?</p>|<p><a href=.*></a> SocialBeta .*</p>|<p hiragino="".*>brandfilm 20:46 </p>|<strong> SocialBeta  2017 </strong>|<p style="text-align: justify;">.*</p>|<p style="margin: 0px;.*>.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta</strong>.*?</p>|<p>.*.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.range(Pattern.java:2655) ~[na:1.8.0_111]
	at java.util.regex.Pattern.clazz(Pattern.java:2562) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2063) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]
2017-08-11 18:44:02,731 [task-result-getter-3] WARN  [org.apache.spark.scheduler.TaskSetManager] - Lost task 0.0 in stage 2.0 (TID 4, localhost): java.util.regex.PatternSyntaxException: Illegal character range near index 464
55 </p>|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>| HUNT:|<a href="http://socialbeta.com.*>.*?</a>|<h2.*.*?</h2>|<strong>[\u4e00-\u9fa5]{3}.*?</strong>|<blockquote>.*?</blockquote>|<p>.*?</p>|<p><strong>.*?</p>|<p><strong>.*?</p>|<p style="white-space: normal;">.*.*?</p>|<p style="text-align: justify;">.*.*?</p>|<strong>[A-Za-Z]*[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*</a> SocialBeta .*.*?</p>|<p><a href=.*></a> SocialBeta .*</p>|<p hiragino="".*>brandfilm 20:46 </p>|<strong> SocialBeta  2017 </strong>|<p style="text-align: justify;">.*</p>|<p style="margin: 0px;.*>.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta</strong>.*?</p>|<p>.*.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.range(Pattern.java:2655)
	at java.util.regex.Pattern.clazz(Pattern.java:2562)
	at java.util.regex.Pattern.sequence(Pattern.java:2063)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2017-08-11 18:44:02,735 [task-result-getter-3] ERROR [org.apache.spark.scheduler.TaskSetManager] - Task 0 in stage 2.0 failed 1 times; aborting job
2017-08-11 18:44:02,735 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Cancelling stage 2
2017-08-11 18:44:02,735 [dispatcher-event-loop-2] INFO  [org.apache.spark.executor.Executor] - Executor is trying to kill task 1.0 in stage 2.0 (TID 5)
2017-08-11 18:44:02,735 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Stage 2 was cancelled
2017-08-11 18:44:02,736 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (foreachPartition at streamingProcessNew.scala:51) failed in 0.187 s
2017-08-11 18:44:02,736 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2 failed: foreachPartition at streamingProcessNew.scala:51, took 0.201530 s
2017-08-11 18:44:02,737 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448240000 ms.0 from job set of time 1502448240000 ms
2017-08-11 18:44:02,737 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.736 s for time 1502448240000 ms (execution: 0.210 s)
2017-08-11 18:44:02,738 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Stopped JobScheduler
2017-08-11 18:44:02,745 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@5cf395{/streaming,null,UNAVAILABLE}
2017-08-11 18:44:02,747 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch,null,UNAVAILABLE}
2017-08-11 18:44:02,749 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1a00961{/static/streaming,null,UNAVAILABLE}
2017-08-11 18:44:02,750 [Thread-0] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext stopped successfully
2017-08-11 18:44:02,750 [Thread-0] INFO  [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2017-08-11 18:44:02,776 [Thread-0] INFO  [org.spark_project.jetty.server.ServerConnector] - Stopped ServerConnector@f92cc7{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 18:44:02,776 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1005f6c{/stages/stage/kill,null,UNAVAILABLE}
2017-08-11 18:44:02,776 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1cec031{/api,null,UNAVAILABLE}
2017-08-11 18:44:02,776 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@13cb11{/,null,UNAVAILABLE}
2017-08-11 18:44:02,776 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@beebb7{/static,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@320ade{/executors/threadDump/json,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@faea88{/executors/threadDump,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@158ed3c{/executors/json,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1366c9b{/executors,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@185fa6b{/environment/json,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@51218e{/environment,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1be7cd5{/storage/rdd/json,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@d6972f{/storage/rdd,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1648ee9{/storage/json,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@d3b1f5{/storage,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1b3bc3{/stages/pool/json,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@15d8c96{/stages/pool,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@fe7086{/stages/stage/json,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@e70ea3{/stages/stage,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@276e50{/stages/json,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@7471ce{/stages,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@7f474e{/jobs/job/json,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1f22f18{/jobs/job,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@cb0951{/jobs/json,null,UNAVAILABLE}
2017-08-11 18:44:02,781 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@17ea53a{/jobs,null,UNAVAILABLE}
2017-08-11 18:44:02,783 [Thread-0] INFO  [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://192.168.31.111:4040
2017-08-11 18:44:02,817 [BoneCP-connection-watch-thread] ERROR [com.jolbox.bonecp.CloseThreadMonitor] - Connection obtained from thread [Executor task launch worker-0] was never closed. 
Stack trace of location where connection was obtained follows:
 java.lang.Thread.getStackTrace(Thread.java:1556)
 com.jolbox.bonecp.BoneCP.captureStackTrace(BoneCP.java:572)
 com.jolbox.bonecp.BoneCP.watchConnection(BoneCP.java:561)
 com.jolbox.bonecp.AbstractConnectionStrategy.postConnection(AbstractConnectionStrategy.java:79)
 com.jolbox.bonecp.AbstractConnectionStrategy.getConnection(AbstractConnectionStrategy.java:92)
 com.jolbox.bonecp.BoneCP.getConnection(BoneCP.java:553)
 cn.datapark.process.education.Util.ConnectionPool$.getConnection(ConnectionPool.scala:32)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:53)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
 org.apache.spark.scheduler.Task.run(Task.scala:85)
 org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
 java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 java.lang.Thread.run(Thread.java:745)

2017-08-11 18:44:02,817 [BoneCP-connection-watch-thread] ERROR [com.jolbox.bonecp.CloseThreadMonitor] - Connection obtained from thread [Executor task launch worker-0] was never closed. 
Stack trace of location where connection was obtained follows:
 java.lang.Thread.getStackTrace(Thread.java:1556)
 com.jolbox.bonecp.BoneCP.captureStackTrace(BoneCP.java:572)
 com.jolbox.bonecp.BoneCP.watchConnection(BoneCP.java:561)
 com.jolbox.bonecp.AbstractConnectionStrategy.postConnection(AbstractConnectionStrategy.java:79)
 com.jolbox.bonecp.AbstractConnectionStrategy.getConnection(AbstractConnectionStrategy.java:92)
 com.jolbox.bonecp.BoneCP.getConnection(BoneCP.java:553)
 cn.datapark.process.education.Util.ConnectionPool$.getConnection(ConnectionPool.scala:32)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:53)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
 org.apache.spark.scheduler.Task.run(Task.scala:85)
 org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
 java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 java.lang.Thread.run(Thread.java:745)

2017-08-11 18:44:02,821 [dispatcher-event-loop-0] INFO  [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2017-08-11 18:44:02,840 [Thread-0] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2017-08-11 18:44:02,841 [Thread-0] INFO  [org.apache.spark.storage.BlockManager] - BlockManager stopped
2017-08-11 18:44:02,844 [Thread-0] INFO  [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2017-08-11 18:44:02,846 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2017-08-11 18:44:02,849 [Thread-0] INFO  [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2017-08-11 18:44:02,849 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2017-08-11 18:44:02,850 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\DN\AppData\Local\Temp\spark-cd78a958-096b-4d96-97cf-19e1da2047de
2017-08-11 18:50:48,787 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-11 18:50:48,992 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-11 18:50:49,072 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-11 18:50:49,072 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-11 18:50:49,073 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-11 18:50:49,073 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-11 18:50:49,074 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-11 18:50:49,808 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 56665.
2017-08-11 18:50:49,829 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-11 18:50:49,848 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-11 18:50:49,863 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-2b95574b-c46e-4568-ab4e-685e21bc3d36
2017-08-11 18:50:49,878 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-11 18:50:49,921 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-11 18:50:50,008 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2015ms
2017-08-11 18:50:50,114 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-11 18:50:50,135 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1f22f18{/jobs,null,AVAILABLE}
2017-08-11 18:50:50,135 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7f474e{/jobs/json,null,AVAILABLE}
2017-08-11 18:50:50,135 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7471ce{/jobs/job,null,AVAILABLE}
2017-08-11 18:50:50,135 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@276e50{/jobs/job/json,null,AVAILABLE}
2017-08-11 18:50:50,136 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e70ea3{/stages,null,AVAILABLE}
2017-08-11 18:50:50,136 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fe7086{/stages/json,null,AVAILABLE}
2017-08-11 18:50:50,136 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15d8c96{/stages/stage,null,AVAILABLE}
2017-08-11 18:50:50,136 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/stages/stage/json,null,AVAILABLE}
2017-08-11 18:50:50,136 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/stages/pool,null,AVAILABLE}
2017-08-11 18:50:50,137 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/stages/pool/json,null,AVAILABLE}
2017-08-11 18:50:50,137 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/storage,null,AVAILABLE}
2017-08-11 18:50:50,137 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/storage/json,null,AVAILABLE}
2017-08-11 18:50:50,137 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/storage/rdd,null,AVAILABLE}
2017-08-11 18:50:50,137 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/storage/rdd/json,null,AVAILABLE}
2017-08-11 18:50:50,137 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/environment,null,AVAILABLE}
2017-08-11 18:50:50,138 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/environment/json,null,AVAILABLE}
2017-08-11 18:50:50,138 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/executors,null,AVAILABLE}
2017-08-11 18:50:50,138 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/executors/json,null,AVAILABLE}
2017-08-11 18:50:50,138 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/executors/threadDump,null,AVAILABLE}
2017-08-11 18:50:50,138 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/executors/threadDump/json,null,AVAILABLE}
2017-08-11 18:50:50,144 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/static,null,AVAILABLE}
2017-08-11 18:50:50,144 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/,null,AVAILABLE}
2017-08-11 18:50:50,144 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0fba8{/api,null,AVAILABLE}
2017-08-11 18:50:50,145 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5de5a4{/stages/stage/kill,null,AVAILABLE}
2017-08-11 18:50:50,152 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@1139eab{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 18:50:50,153 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2160ms
2017-08-11 18:50:50,153 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-11 18:50:50,154 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-11 18:50:50,232 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-11 18:50:50,265 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56688.
2017-08-11 18:50:50,266 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:56688
2017-08-11 18:50:50,267 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 56688)
2017-08-11 18:50:50,269 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:56688 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 56688)
2017-08-11 18:50:50,271 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 56688)
2017-08-11 18:50:50,471 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@109fac1{/metrics/json,null,AVAILABLE}
2017-08-11 18:50:50,955 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-08-11 18:50:50,956 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:50:50,956 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-11 18:50:50,957 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-08-11 18:50:50,957 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@3478cf
2017-08-11 18:50:50,957 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-08-11 18:50:50,957 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:50:50,957 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@ca9c1f
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@332235
2017-08-11 18:50:50,995 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1502448655000
2017-08-11 18:50:50,995 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1502448655000 ms
2017-08-11 18:50:50,997 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-11 18:50:50,999 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1740dae{/streaming,null,AVAILABLE}
2017-08-11 18:50:50,999 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cf395{/streaming/json,null,AVAILABLE}
2017-08-11 18:50:51,000 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@82cd4f{/streaming/batch,null,AVAILABLE}
2017-08-11 18:50:51,000 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch/json,null,AVAILABLE}
2017-08-11 18:50:51,001 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cf9028{/static/streaming,null,AVAILABLE}
2017-08-11 18:50:51,001 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-11 18:50:55,118 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448655000 ms
2017-08-11 18:50:55,121 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448655000 ms.0 from job set of time 1502448655000 ms
2017-08-11 18:50:55,147 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:50:55,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:50:55,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:50:55,160 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:50:55,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:50:55,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:50:55,226 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-11 18:50:55,233 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:50:55,435 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:50:55,437 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:56688 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:50:55,440 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:50:55,444 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49)
2017-08-11 18:50:55,446 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-11 18:50:55,486 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:50:55,490 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:50:55,496 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-11 18:50:55,496 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-11 18:50:55,523 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 0 -> 25
2017-08-11 18:50:55,523 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 0 -> 25
2017-08-11 18:50:55,591 [Executor task launch worker-1] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-08-11 18:50:55,820 [Executor task launch worker-1] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-08-11 18:50:59,392 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-08-11 18:50:59,407 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 3917 ms on localhost (1/2)
2017-08-11 18:50:59,438 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-08-11 18:50:59,443 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 3977 ms on localhost (2/2)
2017-08-11 18:50:59,444 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcessNew.scala:51) finished in 3.987 s
2017-08-11 18:50:59,444 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-08-11 18:50:59,449 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcessNew.scala:51, took 4.301052 s
2017-08-11 18:50:59,453 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448655000 ms.0 from job set of time 1502448655000 ms
2017-08-11 18:50:59,454 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 4.452 s for time 1502448655000 ms (execution: 4.332 s)
2017-08-11 18:50:59,460 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:50:59,464 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 18:51:00,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448660000 ms
2017-08-11 18:51:00,025 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448660000 ms.0 from job set of time 1502448660000 ms
2017-08-11 18:51:00,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:51:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:51:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:51:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:51:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:51:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:51:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:51:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:51:00,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.111:56688 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:51:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:51:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49)
2017-08-11 18:51:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-08-11 18:51:00,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:51:00,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:51:00,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-08-11 18:51:00,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-08-11 18:51:00,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 25 -> 50
2017-08-11 18:51:00,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 25 -> 50
2017-08-11 18:51:00,169 [Executor task launch worker-0] ERROR [org.apache.spark.executor.Executor] - Exception in task 0.0 in stage 1.0 (TID 2)
java.util.regex.PatternSyntaxException: Illegal character range near index 459
55 |<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>| HUNT:|<a href="http://socialbeta.com.*>.*?</a>|<h2.*>.*?</h2>|<strong>[\u4e00-\u9fa5]{3}.*?</strong>|<blockquote>.*?</blockquote>|<p>.*?</p>|<p><strong>.*?</p>|<p><strong>.*?</p>|<p style="white-space: normal;">.*.*?</p>|<p style="text-align: justify;">.*.*?</p>|<strong>[A-Za-Z]*[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*</a> SocialBeta .*.*?</p>|<p><a href=.*></a> SocialBeta .*</p>|<p hiragino="".*>brandfilm 20:46 </p>|<strong> SocialBeta  2017 </strong>|<p style="text-align: justify;">.*</p>|<p style="margin: 0px;.*>.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta</strong>.*?</p>|<p>.*.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.range(Pattern.java:2655) ~[na:1.8.0_111]
	at java.util.regex.Pattern.clazz(Pattern.java:2562) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2063) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]
2017-08-11 18:51:00,191 [task-result-getter-2] WARN  [org.apache.spark.scheduler.TaskSetManager] - Lost task 0.0 in stage 1.0 (TID 2, localhost): java.util.regex.PatternSyntaxException: Illegal character range near index 459
55 |<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>| HUNT:|<a href="http://socialbeta.com.*>.*?</a>|<h2.*>.*?</h2>|<strong>[\u4e00-\u9fa5]{3}.*?</strong>|<blockquote>.*?</blockquote>|<p>.*?</p>|<p><strong>.*?</p>|<p><strong>.*?</p>|<p style="white-space: normal;">.*.*?</p>|<p style="text-align: justify;">.*.*?</p>|<strong>[A-Za-Z]*[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*</a> SocialBeta .*.*?</p>|<p><a href=.*></a> SocialBeta .*</p>|<p hiragino="".*>brandfilm 20:46 </p>|<strong> SocialBeta  2017 </strong>|<p style="text-align: justify;">.*</p>|<p style="margin: 0px;.*>.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta</strong>.*?</p>|<p>.*.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.range(Pattern.java:2655)
	at java.util.regex.Pattern.clazz(Pattern.java:2562)
	at java.util.regex.Pattern.sequence(Pattern.java:2063)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2017-08-11 18:51:00,197 [task-result-getter-2] ERROR [org.apache.spark.scheduler.TaskSetManager] - Task 0 in stage 1.0 failed 1 times; aborting job
2017-08-11 18:51:00,202 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Cancelling stage 1
2017-08-11 18:51:00,204 [dispatcher-event-loop-2] INFO  [org.apache.spark.executor.Executor] - Executor is trying to kill task 1.0 in stage 1.0 (TID 3)
2017-08-11 18:51:00,204 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Stage 1 was cancelled
2017-08-11 18:51:00,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcessNew.scala:51) failed in 0.140 s
2017-08-11 18:51:00,206 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 failed: foreachPartition at streamingProcessNew.scala:51, took 0.157001 s
2017-08-11 18:51:00,207 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448660000 ms.0 from job set of time 1502448660000 ms
2017-08-11 18:51:00,207 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.207 s for time 1502448660000 ms (execution: 0.182 s)
2017-08-11 18:51:00,208 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-08-11 18:51:00,209 [JobScheduler] ERROR [org.apache.spark.streaming.scheduler.JobScheduler] - Error running job streaming job 1502448660000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 2, localhost): java.util.regex.PatternSyntaxException: Illegal character range near index 459
55 |<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>| HUNT:|<a href="http://socialbeta.com.*>.*?</a>|<h2.*>.*?</h2>|<strong>[\u4e00-\u9fa5]{3}.*?</strong>|<blockquote>.*?</blockquote>|<p>.*?</p>|<p><strong>.*?</p>|<p><strong>.*?</p>|<p style="white-space: normal;">.*.*?</p>|<p style="text-align: justify;">.*.*?</p>|<strong>[A-Za-Z]*[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*</a> SocialBeta .*.*?</p>|<p><a href=.*></a> SocialBeta .*</p>|<p hiragino="".*>brandfilm 20:46 </p>|<strong> SocialBeta  2017 </strong>|<p style="text-align: justify;">.*</p>|<p style="margin: 0px;.*>.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta</strong>.*?</p>|<p>.*.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.range(Pattern.java:2655)
	at java.util.regex.Pattern.clazz(Pattern.java:2562)
	at java.util.regex.Pattern.sequence(Pattern.java:2063)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[scala-library.jar:na]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[scala-library.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.Option.foreach(Option.scala:257) ~[scala-library.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1911) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:881) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:881) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at scala.util.Try$.apply(Try.scala:192) ~[scala-library.jar:na]
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) ~[scala-library.jar:na]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:244) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_111]
Caused by: java.util.regex.PatternSyntaxException: Illegal character range near index 459
55 |<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>| HUNT:|<a href="http://socialbeta.com.*>.*?</a>|<h2.*>.*?</h2>|<strong>[\u4e00-\u9fa5]{3}.*?</strong>|<blockquote>.*?</blockquote>|<p>.*?</p>|<p><strong>.*?</p>|<p><strong>.*?</p>|<p style="white-space: normal;">.*.*?</p>|<p style="text-align: justify;">.*.*?</p>|<strong>[A-Za-Z]*[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*</a> SocialBeta .*.*?</p>|<p><a href=.*></a> SocialBeta .*</p>|<p hiragino="".*>brandfilm 20:46 </p>|<strong> SocialBeta  2017 </strong>|<p style="text-align: justify;">.*</p>|<p style="margin: 0px;.*>.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta</strong>.*?</p>|<p>.*.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.range(Pattern.java:2655) ~[na:1.8.0_111]
	at java.util.regex.Pattern.clazz(Pattern.java:2562) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2063) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	... 3 common frames omitted
2017-08-11 18:51:00,219 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-08-11 18:51:00,219 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-08-11 18:51:00,221 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:51:00,222 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 18:51:00,222 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-08-11 18:51:00,224 [Thread-0] INFO  [org.apache.spark.streaming.StreamingContext] - Invoking stop(stopGracefully=false) from shutdown hook
2017-08-11 18:51:00,226 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Stopping JobGenerator immediately
2017-08-11 18:51:00,227 [Thread-0] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Stopped timer for JobGenerator after time 1502448660000
2017-08-11 18:51:00,229 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Stopped JobGenerator
2017-08-11 18:51:00,231 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Stopped JobScheduler
2017-08-11 18:51:00,236 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1740dae{/streaming,null,UNAVAILABLE}
2017-08-11 18:51:00,237 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@82cd4f{/streaming/batch,null,UNAVAILABLE}
2017-08-11 18:51:00,238 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1cf9028{/static/streaming,null,UNAVAILABLE}
2017-08-11 18:51:00,239 [Thread-0] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext stopped successfully
2017-08-11 18:51:00,240 [Thread-0] INFO  [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2017-08-11 18:51:00,244 [Thread-0] INFO  [org.spark_project.jetty.server.ServerConnector] - Stopped ServerConnector@1139eab{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 18:51:00,244 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@5de5a4{/stages/stage/kill,null,UNAVAILABLE}
2017-08-11 18:51:00,244 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@f0fba8{/api,null,UNAVAILABLE}
2017-08-11 18:51:00,244 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1005f6c{/,null,UNAVAILABLE}
2017-08-11 18:51:00,244 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1cec031{/static,null,UNAVAILABLE}
2017-08-11 18:51:00,244 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@13cb11{/executors/threadDump/json,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@beebb7{/executors/threadDump,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@320ade{/executors/json,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@faea88{/executors,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@158ed3c{/environment/json,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1366c9b{/environment,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@185fa6b{/storage/rdd/json,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@51218e{/storage/rdd,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1be7cd5{/storage/json,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@d6972f{/storage,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1648ee9{/stages/pool/json,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@d3b1f5{/stages/pool,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1b3bc3{/stages/stage/json,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@15d8c96{/stages/stage,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@fe7086{/stages/json,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@e70ea3{/stages,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@276e50{/jobs/job/json,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@7471ce{/jobs/job,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@7f474e{/jobs/json,null,UNAVAILABLE}
2017-08-11 18:51:00,247 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1f22f18{/jobs,null,UNAVAILABLE}
2017-08-11 18:51:00,249 [Thread-0] INFO  [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://192.168.31.111:4040
2017-08-11 18:51:00,257 [BoneCP-connection-watch-thread] ERROR [com.jolbox.bonecp.CloseThreadMonitor] - Connection obtained from thread [Executor task launch worker-0] was never closed. 
Stack trace of location where connection was obtained follows:
 java.lang.Thread.getStackTrace(Thread.java:1556)
 com.jolbox.bonecp.BoneCP.captureStackTrace(BoneCP.java:572)
 com.jolbox.bonecp.BoneCP.watchConnection(BoneCP.java:561)
 com.jolbox.bonecp.AbstractConnectionStrategy.postConnection(AbstractConnectionStrategy.java:79)
 com.jolbox.bonecp.AbstractConnectionStrategy.getConnection(AbstractConnectionStrategy.java:92)
 com.jolbox.bonecp.BoneCP.getConnection(BoneCP.java:553)
 cn.datapark.process.education.Util.ConnectionPool$.getConnection(ConnectionPool.scala:32)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:53)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
 org.apache.spark.scheduler.Task.run(Task.scala:85)
 org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
 java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 java.lang.Thread.run(Thread.java:745)

2017-08-11 18:51:00,260 [dispatcher-event-loop-2] INFO  [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2017-08-11 18:51:00,270 [Thread-0] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2017-08-11 18:51:00,270 [Thread-0] INFO  [org.apache.spark.storage.BlockManager] - BlockManager stopped
2017-08-11 18:51:00,273 [Thread-0] INFO  [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2017-08-11 18:51:00,276 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2017-08-11 18:51:00,281 [Thread-0] INFO  [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2017-08-11 18:51:00,282 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2017-08-11 18:51:00,283 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\DN\AppData\Local\Temp\spark-e13cad22-6798-4772-ba3f-8a53b9a35d3e
2017-08-11 18:54:53,342 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-11 18:54:53,556 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-11 18:54:53,638 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-11 18:54:53,639 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-11 18:54:53,639 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-11 18:54:53,640 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-11 18:54:53,640 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-11 18:54:54,380 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 57088.
2017-08-11 18:54:54,400 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-11 18:54:54,416 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-11 18:54:54,430 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-e96ec91c-7cc3-4663-adaa-96348eff5153
2017-08-11 18:54:54,443 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-11 18:54:54,484 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-11 18:54:54,565 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2001ms
2017-08-11 18:54:54,653 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-11 18:54:54,670 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1f22f18{/jobs,null,AVAILABLE}
2017-08-11 18:54:54,671 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7f474e{/jobs/json,null,AVAILABLE}
2017-08-11 18:54:54,671 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7471ce{/jobs/job,null,AVAILABLE}
2017-08-11 18:54:54,671 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@276e50{/jobs/job/json,null,AVAILABLE}
2017-08-11 18:54:54,671 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e70ea3{/stages,null,AVAILABLE}
2017-08-11 18:54:54,672 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fe7086{/stages/json,null,AVAILABLE}
2017-08-11 18:54:54,672 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15d8c96{/stages/stage,null,AVAILABLE}
2017-08-11 18:54:54,672 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/stages/stage/json,null,AVAILABLE}
2017-08-11 18:54:54,672 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/stages/pool,null,AVAILABLE}
2017-08-11 18:54:54,672 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/stages/pool/json,null,AVAILABLE}
2017-08-11 18:54:54,673 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/storage,null,AVAILABLE}
2017-08-11 18:54:54,673 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/storage/json,null,AVAILABLE}
2017-08-11 18:54:54,673 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/storage/rdd,null,AVAILABLE}
2017-08-11 18:54:54,673 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/storage/rdd/json,null,AVAILABLE}
2017-08-11 18:54:54,673 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/environment,null,AVAILABLE}
2017-08-11 18:54:54,674 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/environment/json,null,AVAILABLE}
2017-08-11 18:54:54,674 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/executors,null,AVAILABLE}
2017-08-11 18:54:54,674 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/executors/json,null,AVAILABLE}
2017-08-11 18:54:54,674 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/executors/threadDump,null,AVAILABLE}
2017-08-11 18:54:54,674 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/executors/threadDump/json,null,AVAILABLE}
2017-08-11 18:54:54,679 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/static,null,AVAILABLE}
2017-08-11 18:54:54,680 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/,null,AVAILABLE}
2017-08-11 18:54:54,680 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0fba8{/api,null,AVAILABLE}
2017-08-11 18:54:54,680 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5de5a4{/stages/stage/kill,null,AVAILABLE}
2017-08-11 18:54:54,690 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@1139eab{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 18:54:54,690 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2126ms
2017-08-11 18:54:54,690 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-11 18:54:54,691 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-11 18:54:54,766 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-11 18:54:54,809 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57111.
2017-08-11 18:54:54,809 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:57111
2017-08-11 18:54:54,810 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 57111)
2017-08-11 18:54:54,814 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:57111 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 57111)
2017-08-11 18:54:54,816 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 57111)
2017-08-11 18:54:54,999 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@109fac1{/metrics/json,null,AVAILABLE}
2017-08-11 18:54:55,482 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-08-11 18:54:55,482 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:54:55,483 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@e18910
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@850a5b
2017-08-11 18:54:55,485 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-08-11 18:54:55,485 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:54:55,485 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-11 18:54:55,485 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-08-11 18:54:55,485 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5dc86c
2017-08-11 18:54:55,522 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1502448900000
2017-08-11 18:54:55,523 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1502448900000 ms
2017-08-11 18:54:55,523 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-11 18:54:55,525 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1740dae{/streaming,null,AVAILABLE}
2017-08-11 18:54:55,525 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cf395{/streaming/json,null,AVAILABLE}
2017-08-11 18:54:55,525 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@82cd4f{/streaming/batch,null,AVAILABLE}
2017-08-11 18:54:55,526 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch/json,null,AVAILABLE}
2017-08-11 18:54:55,527 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cf9028{/static/streaming,null,AVAILABLE}
2017-08-11 18:54:55,527 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-11 18:55:00,162 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448900000 ms
2017-08-11 18:55:00,166 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448900000 ms.0 from job set of time 1502448900000 ms
2017-08-11 18:55:00,192 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:55:00,204 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:55:00,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:55:00,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:55:00,207 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:55:00,214 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:55:00,273 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-11 18:55:00,280 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:55:00,463 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:55:00,465 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:57111 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:55:00,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:55:00,471 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49)
2017-08-11 18:55:00,474 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-11 18:55:00,515 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:55:00,518 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:55:00,524 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-11 18:55:00,524 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-11 18:55:00,549 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 0 -> 25
2017-08-11 18:55:00,549 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 0 -> 25
2017-08-11 18:55:00,639 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-08-11 18:55:00,866 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-08-11 18:55:04,285 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-08-11 18:55:04,301 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 3782 ms on localhost (1/2)
2017-08-11 18:55:04,776 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-08-11 18:55:04,781 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 4287 ms on localhost (2/2)
2017-08-11 18:55:04,783 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcessNew.scala:51) finished in 4.298 s
2017-08-11 18:55:04,784 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-08-11 18:55:04,788 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcessNew.scala:51, took 4.596028 s
2017-08-11 18:55:04,794 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448900000 ms.0 from job set of time 1502448900000 ms
2017-08-11 18:55:04,795 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 4.792 s for time 1502448900000 ms (execution: 4.628 s)
2017-08-11 18:55:04,802 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:55:04,806 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 18:55:05,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448905000 ms
2017-08-11 18:55:05,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448905000 ms.0 from job set of time 1502448905000 ms
2017-08-11 18:55:05,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:55:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:55:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:55:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:55:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:55:05,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:55:05,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:55:05,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:55:05,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.111:57111 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:55:05,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:55:05,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49)
2017-08-11 18:55:05,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-08-11 18:55:05,040 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:55:05,041 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:55:05,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-08-11 18:55:05,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-08-11 18:55:05,045 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 25 -> 50
2017-08-11 18:55:05,045 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 25 -> 50
2017-08-11 18:55:06,173 [Executor task launch worker-0] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-11 18:55:06,544 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 787 bytes result sent to driver
2017-08-11 18:55:06,548 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 1508 ms on localhost (1/2)
2017-08-11 18:55:07,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 787 bytes result sent to driver
2017-08-11 18:55:07,054 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 2016 ms on localhost (2/2)
2017-08-11 18:55:07,054 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-08-11 18:55:07,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcessNew.scala:51) finished in 2.016 s
2017-08-11 18:55:07,055 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: foreachPartition at streamingProcessNew.scala:51, took 2.029054 s
2017-08-11 18:55:07,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448905000 ms.0 from job set of time 1502448905000 ms
2017-08-11 18:55:07,056 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.055 s for time 1502448905000 ms (execution: 2.037 s)
2017-08-11 18:55:07,056 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-08-11 18:55:07,061 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-08-11 18:55:07,062 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-08-11 18:55:07,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:55:07,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 18:55:07,062 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-08-11 18:55:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448910000 ms
2017-08-11 18:55:10,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448910000 ms.0 from job set of time 1502448910000 ms
2017-08-11 18:55:10,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:55:10,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:55:10,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:55:10,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:55:10,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:55:10,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:55:10,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:55:10,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:55:10,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on 192.168.31.111:57111 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:55:10,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:55:10,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:49)
2017-08-11 18:55:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2017-08-11 18:55:10,042 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:55:10,043 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:55:10,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2017-08-11 18:55:10,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2017-08-11 18:55:10,049 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 50 -> 75
2017-08-11 18:55:10,049 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 50 -> 75
2017-08-11 18:55:10,168 [Executor task launch worker-1] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-11 18:55:10,345 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on 192.168.31.111:57111 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:55:10,471 [Executor task launch worker-1] ERROR [org.apache.spark.executor.Executor] - Exception in task 1.0 in stage 2.0 (TID 5)
java.util.regex.PatternSyntaxException: Dangling meta character '*' near index 0
*| App |[|].*.*?
^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2123) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:53) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]
2017-08-11 18:55:10,488 [task-result-getter-0] WARN  [org.apache.spark.scheduler.TaskSetManager] - Lost task 1.0 in stage 2.0 (TID 5, localhost): java.util.regex.PatternSyntaxException: Dangling meta character '*' near index 0
*| App |[|].*.*?
^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.sequence(Pattern.java:2123)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:53)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2017-08-11 18:55:10,490 [task-result-getter-0] ERROR [org.apache.spark.scheduler.TaskSetManager] - Task 1 in stage 2.0 failed 1 times; aborting job
2017-08-11 18:55:10,493 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Cancelling stage 2
2017-08-11 18:55:10,496 [dispatcher-event-loop-3] INFO  [org.apache.spark.executor.Executor] - Executor is trying to kill task 0.0 in stage 2.0 (TID 4)
2017-08-11 18:55:10,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Stage 2 was cancelled
2017-08-11 18:55:10,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (foreachPartition at streamingProcessNew.scala:51) failed in 0.458 s
2017-08-11 18:55:10,498 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2 failed: foreachPartition at streamingProcessNew.scala:51, took 0.473196 s
2017-08-11 18:55:10,499 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448910000 ms.0 from job set of time 1502448910000 ms
2017-08-11 18:55:10,499 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.499 s for time 1502448910000 ms (execution: 0.484 s)
2017-08-11 18:55:10,499 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 3 from persistence list
2017-08-11 18:55:10,500 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 3
2017-08-11 18:55:10,500 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2 from persistence list
2017-08-11 18:55:10,501 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2
2017-08-11 18:55:10,501 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:55:10,501 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502448900000 ms
2017-08-11 18:55:10,502 [JobScheduler] ERROR [org.apache.spark.streaming.scheduler.JobScheduler] - Error running job streaming job 1502448910000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 5, localhost): java.util.regex.PatternSyntaxException: Dangling meta character '*' near index 0
*| App |[|].*.*?
^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.sequence(Pattern.java:2123)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:53)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[scala-library.jar:na]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[scala-library.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.Option.foreach(Option.scala:257) ~[scala-library.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1911) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:881) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:881) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at scala.util.Try$.apply(Try.scala:192) ~[scala-library.jar:na]
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) ~[scala-library.jar:na]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:244) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_111]
Caused by: java.util.regex.PatternSyntaxException: Dangling meta character '*' near index 0
*| App |[|].*.*?
^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2123) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:53) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	... 3 common frames omitted
2017-08-11 18:55:10,509 [Thread-0] INFO  [org.apache.spark.streaming.StreamingContext] - Invoking stop(stopGracefully=false) from shutdown hook
2017-08-11 18:55:10,511 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Stopping JobGenerator immediately
2017-08-11 18:55:10,512 [Thread-0] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Stopped timer for JobGenerator after time 1502448910000
2017-08-11 18:55:10,513 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Stopped JobGenerator
2017-08-11 18:55:10,514 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Stopped JobScheduler
2017-08-11 18:55:10,519 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1740dae{/streaming,null,UNAVAILABLE}
2017-08-11 18:55:10,520 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@82cd4f{/streaming/batch,null,UNAVAILABLE}
2017-08-11 18:55:10,521 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1cf9028{/static/streaming,null,UNAVAILABLE}
2017-08-11 18:55:10,521 [Thread-0] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext stopped successfully
2017-08-11 18:55:10,522 [Thread-0] INFO  [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2017-08-11 18:55:10,525 [Thread-0] INFO  [org.spark_project.jetty.server.ServerConnector] - Stopped ServerConnector@1139eab{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@5de5a4{/stages/stage/kill,null,UNAVAILABLE}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@f0fba8{/api,null,UNAVAILABLE}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1005f6c{/,null,UNAVAILABLE}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1cec031{/static,null,UNAVAILABLE}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@13cb11{/executors/threadDump/json,null,UNAVAILABLE}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@beebb7{/executors/threadDump,null,UNAVAILABLE}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@320ade{/executors/json,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@faea88{/executors,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@158ed3c{/environment/json,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1366c9b{/environment,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@185fa6b{/storage/rdd/json,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@51218e{/storage/rdd,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1be7cd5{/storage/json,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@d6972f{/storage,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1648ee9{/stages/pool/json,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@d3b1f5{/stages/pool,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1b3bc3{/stages/stage/json,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@15d8c96{/stages/stage,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@fe7086{/stages/json,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@e70ea3{/stages,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@276e50{/jobs/job/json,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@7471ce{/jobs/job,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@7f474e{/jobs/json,null,UNAVAILABLE}
2017-08-11 18:55:10,529 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1f22f18{/jobs,null,UNAVAILABLE}
2017-08-11 18:55:10,530 [Thread-0] INFO  [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://192.168.31.111:4040
2017-08-11 18:55:10,536 [BoneCP-connection-watch-thread] ERROR [com.jolbox.bonecp.CloseThreadMonitor] - Connection obtained from thread [Executor task launch worker-1] was never closed. 
Stack trace of location where connection was obtained follows:
 java.lang.Thread.getStackTrace(Thread.java:1556)
 com.jolbox.bonecp.BoneCP.captureStackTrace(BoneCP.java:572)
 com.jolbox.bonecp.BoneCP.watchConnection(BoneCP.java:561)
 com.jolbox.bonecp.AbstractConnectionStrategy.postConnection(AbstractConnectionStrategy.java:79)
 com.jolbox.bonecp.AbstractConnectionStrategy.getConnection(AbstractConnectionStrategy.java:92)
 com.jolbox.bonecp.BoneCP.getConnection(BoneCP.java:553)
 cn.datapark.process.education.Util.ConnectionPool$.getConnection(ConnectionPool.scala:32)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:53)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
 org.apache.spark.scheduler.Task.run(Task.scala:85)
 org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
 java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 java.lang.Thread.run(Thread.java:745)

2017-08-11 18:55:10,540 [dispatcher-event-loop-3] INFO  [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2017-08-11 18:55:10,549 [Thread-0] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2017-08-11 18:55:10,550 [Thread-0] INFO  [org.apache.spark.storage.BlockManager] - BlockManager stopped
2017-08-11 18:55:10,551 [Thread-0] INFO  [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2017-08-11 18:55:10,553 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2017-08-11 18:55:10,555 [Thread-0] INFO  [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2017-08-11 18:55:10,556 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2017-08-11 18:55:10,557 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\DN\AppData\Local\Temp\spark-a1e32a3a-2551-4600-b6b3-486ba4bf8dcc
2017-08-11 19:04:35,297 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-11 19:04:35,500 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-11 19:04:35,585 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-11 19:04:35,586 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-11 19:04:35,586 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-11 19:04:35,587 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-11 19:04:35,587 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-11 19:04:36,317 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 58009.
2017-08-11 19:04:36,337 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-11 19:04:36,355 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-11 19:04:36,369 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-a3590255-845b-4aa8-9d7d-97e95eae7dbc
2017-08-11 19:04:36,382 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-11 19:04:36,424 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-11 19:04:36,507 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2205ms
2017-08-11 19:04:36,606 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-11 19:04:36,623 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1f22f18{/jobs,null,AVAILABLE}
2017-08-11 19:04:36,624 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7f474e{/jobs/json,null,AVAILABLE}
2017-08-11 19:04:36,624 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7471ce{/jobs/job,null,AVAILABLE}
2017-08-11 19:04:36,624 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@276e50{/jobs/job/json,null,AVAILABLE}
2017-08-11 19:04:36,624 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e70ea3{/stages,null,AVAILABLE}
2017-08-11 19:04:36,625 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fe7086{/stages/json,null,AVAILABLE}
2017-08-11 19:04:36,625 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15d8c96{/stages/stage,null,AVAILABLE}
2017-08-11 19:04:36,625 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/stages/stage/json,null,AVAILABLE}
2017-08-11 19:04:36,625 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/stages/pool,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/stages/pool/json,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/storage,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/storage/json,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/storage/rdd,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/storage/rdd/json,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/environment,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/environment/json,null,AVAILABLE}
2017-08-11 19:04:36,627 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/executors,null,AVAILABLE}
2017-08-11 19:04:36,627 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/executors/json,null,AVAILABLE}
2017-08-11 19:04:36,627 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/executors/threadDump,null,AVAILABLE}
2017-08-11 19:04:36,628 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/executors/threadDump/json,null,AVAILABLE}
2017-08-11 19:04:36,636 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/static,null,AVAILABLE}
2017-08-11 19:04:36,636 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/,null,AVAILABLE}
2017-08-11 19:04:36,637 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0fba8{/api,null,AVAILABLE}
2017-08-11 19:04:36,637 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5de5a4{/stages/stage/kill,null,AVAILABLE}
2017-08-11 19:04:36,644 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@1139eab{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 19:04:36,644 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2343ms
2017-08-11 19:04:36,644 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-11 19:04:36,646 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-11 19:04:36,731 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-11 19:04:36,761 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58030.
2017-08-11 19:04:36,762 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:58030
2017-08-11 19:04:36,764 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 58030)
2017-08-11 19:04:36,767 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:58030 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 58030)
2017-08-11 19:04:36,769 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 58030)
2017-08-11 19:04:36,976 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@109fac1{/metrics/json,null,AVAILABLE}
2017-08-11 19:04:37,471 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-08-11 19:04:37,472 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-11 19:04:37,472 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-11 19:04:37,474 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@1ae3817
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@14bf9cb
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-08-11 19:04:37,476 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@154106c
2017-08-11 19:04:37,516 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1502449480000
2017-08-11 19:04:37,517 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1502449480000 ms
2017-08-11 19:04:37,518 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-11 19:04:37,519 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1740dae{/streaming,null,AVAILABLE}
2017-08-11 19:04:37,520 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cf395{/streaming/json,null,AVAILABLE}
2017-08-11 19:04:37,520 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@82cd4f{/streaming/batch,null,AVAILABLE}
2017-08-11 19:04:37,521 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch/json,null,AVAILABLE}
2017-08-11 19:04:37,522 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cf9028{/static/streaming,null,AVAILABLE}
2017-08-11 19:04:37,522 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-11 19:04:40,211 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449480000 ms
2017-08-11 19:04:40,216 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449480000 ms.0 from job set of time 1502449480000 ms
2017-08-11 19:04:40,253 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:04:40,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:04:40,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:04:40,266 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:04:40,268 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:04:40,275 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:04:40,339 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-11 19:04:40,348 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:04:40,554 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:04:40,556 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:04:40,560 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:04:40,563 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49)
2017-08-11 19:04:40,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-11 19:04:40,608 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 19:04:40,612 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 19:04:40,620 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-11 19:04:40,620 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-11 19:04:40,646 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 0 -> 25
2017-08-11 19:04:40,646 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 0 -> 25
2017-08-11 19:04:40,714 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-08-11 19:04:40,980 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-08-11 19:04:44,575 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-08-11 19:04:44,594 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 3981 ms on localhost (1/2)
2017-08-11 19:04:44,677 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-08-11 19:04:44,681 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 4092 ms on localhost (2/2)
2017-08-11 19:04:44,682 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcessNew.scala:51) finished in 4.104 s
2017-08-11 19:04:44,682 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-08-11 19:04:44,687 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcessNew.scala:51, took 4.433350 s
2017-08-11 19:04:44,692 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449480000 ms.0 from job set of time 1502449480000 ms
2017-08-11 19:04:44,693 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 4.690 s for time 1502449480000 ms (execution: 4.477 s)
2017-08-11 19:04:44,700 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:04:44,704 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 19:04:45,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449485000 ms
2017-08-11 19:04:45,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449485000 ms.0 from job set of time 1502449485000 ms
2017-08-11 19:04:45,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:04:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:04:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:04:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:04:45,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:04:45,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:04:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:04:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:04:45,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:04:45,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:04:45,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49)
2017-08-11 19:04:45,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-08-11 19:04:45,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 19:04:45,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 19:04:45,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-08-11 19:04:45,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-08-11 19:04:45,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 25 -> 50
2017-08-11 19:04:45,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 25 -> 50
2017-08-11 19:04:46,713 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 787 bytes result sent to driver
2017-08-11 19:04:46,720 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 1655 ms on localhost (1/2)
2017-08-11 19:04:47,594 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 787 bytes result sent to driver
2017-08-11 19:04:47,597 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 2535 ms on localhost (2/2)
2017-08-11 19:04:47,597 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-08-11 19:04:47,598 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcessNew.scala:51) finished in 2.535 s
2017-08-11 19:04:47,598 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: foreachPartition at streamingProcessNew.scala:51, took 2.549610 s
2017-08-11 19:04:47,599 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449485000 ms.0 from job set of time 1502449485000 ms
2017-08-11 19:04:47,600 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.599 s for time 1502449485000 ms (execution: 2.575 s)
2017-08-11 19:04:47,602 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-08-11 19:04:47,611 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-08-11 19:04:47,611 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:04:47,611 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 19:04:47,611 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-08-11 19:04:47,612 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-08-11 19:04:50,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449490000 ms
2017-08-11 19:04:50,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449490000 ms.0 from job set of time 1502449490000 ms
2017-08-11 19:04:50,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:04:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:04:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:04:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:04:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:04:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:04:50,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:04:50,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:04:50,052 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:04:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:04:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:49)
2017-08-11 19:04:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2017-08-11 19:04:50,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 19:04:50,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 19:04:50,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2017-08-11 19:04:50,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2017-08-11 19:04:50,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 50 -> 75
2017-08-11 19:04:50,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 50 -> 75
2017-08-11 19:04:50,473 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:04:51,386 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2.0 (TID 5). 787 bytes result sent to driver
2017-08-11 19:04:51,389 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2.0 (TID 5) in 1331 ms on localhost (1/2)
2017-08-11 19:04:51,564 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 787 bytes result sent to driver
2017-08-11 19:04:51,567 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 1512 ms on localhost (2/2)
2017-08-11 19:04:51,568 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (foreachPartition at streamingProcessNew.scala:51) finished in 1.512 s
2017-08-11 19:04:51,568 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-08-11 19:04:51,568 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: foreachPartition at streamingProcessNew.scala:51, took 1.530642 s
2017-08-11 19:04:51,569 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449490000 ms.0 from job set of time 1502449490000 ms
2017-08-11 19:04:51,569 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.569 s for time 1502449490000 ms (execution: 1.546 s)
2017-08-11 19:04:51,569 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 3 from persistence list
2017-08-11 19:04:51,570 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 3
2017-08-11 19:04:51,570 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2 from persistence list
2017-08-11 19:04:51,570 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2
2017-08-11 19:04:51,570 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:04:51,570 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449480000 ms
2017-08-11 19:04:55,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449495000 ms
2017-08-11 19:04:55,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449495000 ms.0 from job set of time 1502449495000 ms
2017-08-11 19:04:55,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:04:55,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:04:55,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 3 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:04:55,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:04:55,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:04:55,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:04:55,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:04:55,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:04:55,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:04:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:04:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcessNew.scala:49)
2017-08-11 19:04:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 3.0 with 2 tasks
2017-08-11 19:04:55,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 3.0 (TID 6, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:04:55,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 3.0 (TID 7, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:04:55,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 3.0 (TID 7)
2017-08-11 19:04:55,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 3.0 (TID 6)
2017-08-11 19:04:55,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 75 -> 100
2017-08-11 19:04:55,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 75 -> 100
2017-08-11 19:04:55,394 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:04:55,689 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 3.0 (TID 7). 787 bytes result sent to driver
2017-08-11 19:04:55,692 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 3.0 (TID 7) in 631 ms on localhost (1/2)
2017-08-11 19:04:55,856 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 3.0 (TID 6). 787 bytes result sent to driver
2017-08-11 19:04:55,858 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 3.0 (TID 6) in 799 ms on localhost (2/2)
2017-08-11 19:04:55,858 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-08-11 19:04:55,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 3 (foreachPartition at streamingProcessNew.scala:51) finished in 0.799 s
2017-08-11 19:04:55,859 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: foreachPartition at streamingProcessNew.scala:51, took 0.816545 s
2017-08-11 19:04:55,859 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449495000 ms.0 from job set of time 1502449495000 ms
2017-08-11 19:04:55,860 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.859 s for time 1502449495000 ms (execution: 0.836 s)
2017-08-11 19:04:55,860 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 5 from persistence list
2017-08-11 19:04:55,860 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 5
2017-08-11 19:04:55,860 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 4 from persistence list
2017-08-11 19:04:55,860 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 4
2017-08-11 19:04:55,860 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:04:55,860 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449485000 ms
2017-08-11 19:05:00,026 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449500000 ms
2017-08-11 19:05:00,027 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449500000 ms.0 from job set of time 1502449500000 ms
2017-08-11 19:05:00,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 4 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:00,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:00,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:00,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:00,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 2 tasks
2017-08-11 19:05:00,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 8, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:00,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 4.0 (TID 9, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:00,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 4.0 (TID 9)
2017-08-11 19:05:00,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 8)
2017-08-11 19:05:00,066 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 100 -> 125
2017-08-11 19:05:00,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 100 -> 125
2017-08-11 19:05:01,924 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 4.0 (TID 9). 714 bytes result sent to driver
2017-08-11 19:05:01,926 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 4.0 (TID 9) in 1865 ms on localhost (1/2)
2017-08-11 19:05:04,754 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_3_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:05,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449505000 ms
2017-08-11 19:05:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449510000 ms
2017-08-11 19:05:11,150 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 8). 787 bytes result sent to driver
2017-08-11 19:05:11,152 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 8) in 11093 ms on localhost (2/2)
2017-08-11 19:05:11,152 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-08-11 19:05:11,152 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (foreachPartition at streamingProcessNew.scala:51) finished in 11.093 s
2017-08-11 19:05:11,153 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 4 finished: foreachPartition at streamingProcessNew.scala:51, took 11.105778 s
2017-08-11 19:05:11,154 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449500000 ms.0 from job set of time 1502449500000 ms
2017-08-11 19:05:11,154 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 11.154 s for time 1502449500000 ms (execution: 11.127 s)
2017-08-11 19:05:11,154 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 7 from persistence list
2017-08-11 19:05:11,154 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449505000 ms.0 from job set of time 1502449505000 ms
2017-08-11 19:05:11,154 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 7
2017-08-11 19:05:11,154 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 6 from persistence list
2017-08-11 19:05:11,155 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 6
2017-08-11 19:05:11,155 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:05:11,155 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449490000 ms
2017-08-11 19:05:11,161 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:11,161 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 5 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:11,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 5 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:11,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:11,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:11,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:11,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:11,166 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:11,167 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:11,168 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:11,168 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:11,168 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 5.0 with 2 tasks
2017-08-11 19:05:11,170 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 5.0 (TID 10, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:11,171 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 5.0 (TID 11, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:11,171 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 5.0 (TID 11)
2017-08-11 19:05:11,171 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 5.0 (TID 10)
2017-08-11 19:05:11,172 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 125 -> 150
2017-08-11 19:05:11,172 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 125 -> 150
2017-08-11 19:05:12,190 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_4_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:12,223 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 5.0 (TID 11). 787 bytes result sent to driver
2017-08-11 19:05:12,226 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 5.0 (TID 11) in 1056 ms on localhost (1/2)
2017-08-11 19:05:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449515000 ms
2017-08-11 19:05:16,764 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 5.0 (TID 10). 787 bytes result sent to driver
2017-08-11 19:05:16,768 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 5.0 (TID 10) in 5600 ms on localhost (2/2)
2017-08-11 19:05:16,769 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-08-11 19:05:16,769 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 5 (foreachPartition at streamingProcessNew.scala:51) finished in 5.601 s
2017-08-11 19:05:16,770 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 5 finished: foreachPartition at streamingProcessNew.scala:51, took 5.608559 s
2017-08-11 19:05:16,770 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449505000 ms.0 from job set of time 1502449505000 ms
2017-08-11 19:05:16,771 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 11.770 s for time 1502449505000 ms (execution: 5.616 s)
2017-08-11 19:05:16,771 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449510000 ms.0 from job set of time 1502449510000 ms
2017-08-11 19:05:16,771 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 9 from persistence list
2017-08-11 19:05:16,772 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 9
2017-08-11 19:05:16,772 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 8 from persistence list
2017-08-11 19:05:16,772 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 8
2017-08-11 19:05:16,772 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:05:16,773 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449495000 ms
2017-08-11 19:05:16,778 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:16,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 6 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:16,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 6 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:16,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:16,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:16,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:16,780 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:16,786 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:16,789 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_6_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:16,789 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:16,790 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:16,790 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 6.0 with 2 tasks
2017-08-11 19:05:16,791 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 6.0 (TID 12, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:16,792 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 6.0 (TID 13, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:16,792 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 6.0 (TID 12)
2017-08-11 19:05:16,792 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 6.0 (TID 13)
2017-08-11 19:05:16,794 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 150 -> 175
2017-08-11 19:05:16,794 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 150 -> 175
2017-08-11 19:05:17,521 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_5_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:20,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449520000 ms
2017-08-11 19:05:23,762 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 6.0 (TID 13). 787 bytes result sent to driver
2017-08-11 19:05:23,763 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 6.0 (TID 13) in 6972 ms on localhost (1/2)
2017-08-11 19:05:25,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449525000 ms
2017-08-11 19:05:27,903 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 6.0 (TID 12). 787 bytes result sent to driver
2017-08-11 19:05:27,905 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 6.0 (TID 12) in 11115 ms on localhost (2/2)
2017-08-11 19:05:27,905 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 6 (foreachPartition at streamingProcessNew.scala:51) finished in 11.115 s
2017-08-11 19:05:27,905 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2017-08-11 19:05:27,905 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 6 finished: foreachPartition at streamingProcessNew.scala:51, took 11.126572 s
2017-08-11 19:05:27,906 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449510000 ms.0 from job set of time 1502449510000 ms
2017-08-11 19:05:27,906 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 17.906 s for time 1502449510000 ms (execution: 11.135 s)
2017-08-11 19:05:27,906 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 11 from persistence list
2017-08-11 19:05:27,906 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449515000 ms.0 from job set of time 1502449515000 ms
2017-08-11 19:05:27,906 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 11
2017-08-11 19:05:27,906 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 10 from persistence list
2017-08-11 19:05:27,907 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 10
2017-08-11 19:05:27,907 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:05:27,907 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449500000 ms
2017-08-11 19:05:27,913 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:27,913 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 7 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:27,913 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 7 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:27,913 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:27,914 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:27,914 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:27,915 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:27,919 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:27,920 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_7_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:27,920 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:27,920 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:27,920 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 7.0 with 2 tasks
2017-08-11 19:05:27,922 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 7.0 (TID 14, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:27,922 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 7.0 (TID 15, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:27,923 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 7.0 (TID 15)
2017-08-11 19:05:27,923 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 7.0 (TID 14)
2017-08-11 19:05:27,924 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 175 -> 200
2017-08-11 19:05:27,924 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 175 -> 200
2017-08-11 19:05:28,759 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_6_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:30,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449530000 ms
2017-08-11 19:05:34,000 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 7.0 (TID 14). 787 bytes result sent to driver
2017-08-11 19:05:34,002 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 7.0 (TID 14) in 6081 ms on localhost (1/2)
2017-08-11 19:05:35,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449535000 ms
2017-08-11 19:05:36,173 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 7.0 (TID 15). 787 bytes result sent to driver
2017-08-11 19:05:36,176 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 7.0 (TID 15) in 8254 ms on localhost (2/2)
2017-08-11 19:05:36,176 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 7 (foreachPartition at streamingProcessNew.scala:51) finished in 8.255 s
2017-08-11 19:05:36,177 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2017-08-11 19:05:36,177 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 7 finished: foreachPartition at streamingProcessNew.scala:51, took 8.263509 s
2017-08-11 19:05:36,177 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449515000 ms.0 from job set of time 1502449515000 ms
2017-08-11 19:05:36,178 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 21.177 s for time 1502449515000 ms (execution: 8.271 s)
2017-08-11 19:05:36,178 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 13 from persistence list
2017-08-11 19:05:36,178 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449520000 ms.0 from job set of time 1502449520000 ms
2017-08-11 19:05:36,178 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 12 from persistence list
2017-08-11 19:05:36,178 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 12
2017-08-11 19:05:36,178 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 13
2017-08-11 19:05:36,179 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:05:36,179 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449505000 ms
2017-08-11 19:05:36,186 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:36,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 8 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:36,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 8 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:36,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:36,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:36,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:36,189 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:36,193 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:36,194 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_8_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:36,194 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:36,194 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:36,195 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 8.0 with 2 tasks
2017-08-11 19:05:36,196 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 8.0 (TID 16, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:36,197 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 8.0 (TID 17, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:36,197 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 8.0 (TID 17)
2017-08-11 19:05:36,197 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 8.0 (TID 16)
2017-08-11 19:05:36,199 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 200 -> 225
2017-08-11 19:05:36,199 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 200 -> 225
2017-08-11 19:05:37,943 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_7_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449540000 ms
2017-08-11 19:05:41,880 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 8.0 (TID 16). 787 bytes result sent to driver
2017-08-11 19:05:41,882 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 8.0 (TID 16) in 5687 ms on localhost (1/2)
2017-08-11 19:05:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449545000 ms
2017-08-11 19:05:47,277 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 8.0 (TID 17). 787 bytes result sent to driver
2017-08-11 19:05:47,278 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 8.0 (TID 17) in 11082 ms on localhost (2/2)
2017-08-11 19:05:47,279 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2017-08-11 19:05:47,279 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 8 (foreachPartition at streamingProcessNew.scala:51) finished in 11.084 s
2017-08-11 19:05:47,279 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 8 finished: foreachPartition at streamingProcessNew.scala:51, took 11.092670 s
2017-08-11 19:05:47,280 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449520000 ms.0 from job set of time 1502449520000 ms
2017-08-11 19:05:47,280 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 27.280 s for time 1502449520000 ms (execution: 11.102 s)
2017-08-11 19:05:47,280 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 15 from persistence list
2017-08-11 19:05:47,280 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449525000 ms.0 from job set of time 1502449525000 ms
2017-08-11 19:05:47,280 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 15
2017-08-11 19:05:47,280 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 14 from persistence list
2017-08-11 19:05:47,281 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 14
2017-08-11 19:05:47,281 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:05:47,281 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449510000 ms
2017-08-11 19:05:47,286 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:47,287 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 9 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:47,287 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 9 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:47,287 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:47,287 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:47,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:47,289 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:47,292 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:47,293 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_9_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:47,293 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:47,293 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:47,293 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 9.0 with 2 tasks
2017-08-11 19:05:47,294 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 9.0 (TID 18, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:47,295 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 9.0 (TID 19, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:47,295 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 9.0 (TID 18)
2017-08-11 19:05:47,295 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 9.0 (TID 19)
2017-08-11 19:05:47,297 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 225 -> 250
2017-08-11 19:05:47,297 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 225 -> 250
2017-08-11 19:05:47,992 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_8_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:50,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449550000 ms
2017-08-11 19:05:53,405 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 9.0 (TID 18). 787 bytes result sent to driver
2017-08-11 19:05:53,408 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 9.0 (TID 18) in 6114 ms on localhost (1/2)
2017-08-11 19:05:53,688 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 9.0 (TID 19). 787 bytes result sent to driver
2017-08-11 19:05:53,690 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 9.0 (TID 19) in 6395 ms on localhost (2/2)
2017-08-11 19:05:53,690 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2017-08-11 19:05:53,690 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 9 (foreachPartition at streamingProcessNew.scala:51) finished in 6.396 s
2017-08-11 19:05:53,690 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 9 finished: foreachPartition at streamingProcessNew.scala:51, took 6.403461 s
2017-08-11 19:05:53,690 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449525000 ms.0 from job set of time 1502449525000 ms
2017-08-11 19:05:53,691 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 28.690 s for time 1502449525000 ms (execution: 6.410 s)
2017-08-11 19:05:53,691 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 17 from persistence list
2017-08-11 19:05:53,691 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449530000 ms.0 from job set of time 1502449530000 ms
2017-08-11 19:05:53,691 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 17
2017-08-11 19:05:53,691 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 16 from persistence list
2017-08-11 19:05:53,691 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 16
2017-08-11 19:05:53,691 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:05:53,691 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449515000 ms
2017-08-11 19:05:53,696 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:53,697 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 10 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:53,697 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 10 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:53,697 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:53,697 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:53,698 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:53,699 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:53,702 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:53,703 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_10_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:53,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:53,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:53,704 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 10.0 with 2 tasks
2017-08-11 19:05:53,704 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 10.0 (TID 20, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:53,705 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 10.0 (TID 21, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:53,705 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 10.0 (TID 20)
2017-08-11 19:05:53,705 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 10.0 (TID 21)
2017-08-11 19:05:53,706 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 250 -> 275
2017-08-11 19:05:53,706 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 250 -> 275
2017-08-11 19:05:54,765 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_9_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:55,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449555000 ms
2017-08-11 19:05:59,491 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 10.0 (TID 20). 787 bytes result sent to driver
2017-08-11 19:05:59,493 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 10.0 (TID 20) in 5789 ms on localhost (1/2)
2017-08-11 19:06:00,025 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449560000 ms
2017-08-11 19:06:00,514 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 10.0 (TID 21). 787 bytes result sent to driver
2017-08-11 19:06:00,518 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 10.0 (TID 21) in 6813 ms on localhost (2/2)
2017-08-11 19:06:00,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 10 (foreachPartition at streamingProcessNew.scala:51) finished in 6.814 s
2017-08-11 19:06:00,519 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2017-08-11 19:06:00,519 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 10 finished: foreachPartition at streamingProcessNew.scala:51, took 6.822007 s
2017-08-11 19:06:00,519 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449530000 ms.0 from job set of time 1502449530000 ms
2017-08-11 19:06:00,520 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 30.519 s for time 1502449530000 ms (execution: 6.828 s)
2017-08-11 19:06:00,520 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 19 from persistence list
2017-08-11 19:06:00,520 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449535000 ms.0 from job set of time 1502449535000 ms
2017-08-11 19:06:00,520 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 18 from persistence list
2017-08-11 19:06:00,520 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 19
2017-08-11 19:06:00,521 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 18
2017-08-11 19:06:00,521 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:00,521 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449520000 ms
2017-08-11 19:06:00,525 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:00,526 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 11 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:00,526 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 11 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:00,526 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:00,526 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:00,526 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:00,528 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:00,531 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:00,532 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_11_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:00,533 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 11 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:00,533 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:00,534 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 11.0 with 2 tasks
2017-08-11 19:06:00,535 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 11.0 (TID 22, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:00,536 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 11.0 (TID 23, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:00,536 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 11.0 (TID 23)
2017-08-11 19:06:00,536 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 11.0 (TID 22)
2017-08-11 19:06:00,537 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 275 -> 300
2017-08-11 19:06:00,537 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 275 -> 300
2017-08-11 19:06:02,585 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_10_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:05,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449565000 ms
2017-08-11 19:06:07,539 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 11.0 (TID 23). 787 bytes result sent to driver
2017-08-11 19:06:07,541 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 11.0 (TID 23) in 7006 ms on localhost (1/2)
2017-08-11 19:06:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449570000 ms
2017-08-11 19:06:10,759 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 11.0 (TID 22). 787 bytes result sent to driver
2017-08-11 19:06:10,761 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 11.0 (TID 22) in 10226 ms on localhost (2/2)
2017-08-11 19:06:10,761 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2017-08-11 19:06:10,761 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 11 (foreachPartition at streamingProcessNew.scala:51) finished in 10.227 s
2017-08-11 19:06:10,761 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 11 finished: foreachPartition at streamingProcessNew.scala:51, took 10.235443 s
2017-08-11 19:06:10,762 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449535000 ms.0 from job set of time 1502449535000 ms
2017-08-11 19:06:10,762 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 35.762 s for time 1502449535000 ms (execution: 10.242 s)
2017-08-11 19:06:10,762 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 21 from persistence list
2017-08-11 19:06:10,762 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449540000 ms.0 from job set of time 1502449540000 ms
2017-08-11 19:06:10,762 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 21
2017-08-11 19:06:10,762 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 20 from persistence list
2017-08-11 19:06:10,763 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 20
2017-08-11 19:06:10,763 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:10,763 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449525000 ms
2017-08-11 19:06:10,767 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:10,768 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 12 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:10,768 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 12 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:10,768 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:10,768 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:10,769 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:10,770 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:10,773 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:10,773 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_12_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:10,774 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:10,774 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:10,774 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 12.0 with 2 tasks
2017-08-11 19:06:10,775 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 12.0 (TID 24, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:10,776 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 12.0 (TID 25, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:10,776 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 12.0 (TID 25)
2017-08-11 19:06:10,776 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 12.0 (TID 24)
2017-08-11 19:06:10,777 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 300 -> 325
2017-08-11 19:06:10,777 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 300 -> 325
2017-08-11 19:06:11,739 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_11_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:13,388 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 12.0 (TID 24). 787 bytes result sent to driver
2017-08-11 19:06:13,390 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 12.0 (TID 24) in 2615 ms on localhost (1/2)
2017-08-11 19:06:15,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449575000 ms
2017-08-11 19:06:16,993 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 12.0 (TID 25). 700 bytes result sent to driver
2017-08-11 19:06:16,995 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 12.0 (TID 25) in 6220 ms on localhost (2/2)
2017-08-11 19:06:16,995 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 12 (foreachPartition at streamingProcessNew.scala:51) finished in 6.221 s
2017-08-11 19:06:16,995 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2017-08-11 19:06:16,996 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 12 finished: foreachPartition at streamingProcessNew.scala:51, took 6.227541 s
2017-08-11 19:06:16,996 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449540000 ms.0 from job set of time 1502449540000 ms
2017-08-11 19:06:16,996 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 36.996 s for time 1502449540000 ms (execution: 6.234 s)
2017-08-11 19:06:16,996 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 23 from persistence list
2017-08-11 19:06:16,997 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 22 from persistence list
2017-08-11 19:06:16,997 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:16,997 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 22
2017-08-11 19:06:16,997 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449530000 ms
2017-08-11 19:06:16,996 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449545000 ms.0 from job set of time 1502449545000 ms
2017-08-11 19:06:16,997 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 23
2017-08-11 19:06:17,005 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:17,005 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 13 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:17,005 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 13 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:17,005 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:17,006 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:17,006 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:17,007 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:17,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:17,021 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_13_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:17,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 13 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:17,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:17,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 13.0 with 2 tasks
2017-08-11 19:06:17,023 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 13.0 (TID 26, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:17,023 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 13.0 (TID 27, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:17,024 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 13.0 (TID 26)
2017-08-11 19:06:17,024 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 13.0 (TID 27)
2017-08-11 19:06:17,025 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 325 -> 350
2017-08-11 19:06:17,025 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 325 -> 350
2017-08-11 19:06:17,557 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_12_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:18,210 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 13.0 (TID 26). 787 bytes result sent to driver
2017-08-11 19:06:18,212 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 13.0 (TID 26) in 1190 ms on localhost (1/2)
2017-08-11 19:06:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449580000 ms
2017-08-11 19:06:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449585000 ms
2017-08-11 19:06:25,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 13.0 (TID 27). 787 bytes result sent to driver
2017-08-11 19:06:25,060 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 13.0 (TID 27) in 8037 ms on localhost (2/2)
2017-08-11 19:06:25,060 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2017-08-11 19:06:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 13 (foreachPartition at streamingProcessNew.scala:51) finished in 8.038 s
2017-08-11 19:06:25,061 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 13 finished: foreachPartition at streamingProcessNew.scala:51, took 8.055387 s
2017-08-11 19:06:25,061 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449545000 ms.0 from job set of time 1502449545000 ms
2017-08-11 19:06:25,062 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 25 from persistence list
2017-08-11 19:06:25,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 40.061 s for time 1502449545000 ms (execution: 8.065 s)
2017-08-11 19:06:25,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449550000 ms.0 from job set of time 1502449550000 ms
2017-08-11 19:06:25,062 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 25
2017-08-11 19:06:25,062 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 24 from persistence list
2017-08-11 19:06:25,062 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 24
2017-08-11 19:06:25,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:25,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449535000 ms
2017-08-11 19:06:25,069 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:25,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 14 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:25,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 14 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:25,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:25,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:25,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:25,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:25,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:25,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_14_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:25,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:25,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:25,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 14.0 with 2 tasks
2017-08-11 19:06:25,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 14.0 (TID 28, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:25,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 14.0 (TID 29, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:25,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 14.0 (TID 28)
2017-08-11 19:06:25,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 14.0 (TID 29)
2017-08-11 19:06:25,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 350 -> 375
2017-08-11 19:06:25,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 350 -> 375
2017-08-11 19:06:25,912 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_13_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:26,741 [Executor task launch worker-0] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-11 19:06:27,436 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 14.0 (TID 28). 787 bytes result sent to driver
2017-08-11 19:06:27,444 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 14.0 (TID 28) in 2368 ms on localhost (1/2)
2017-08-11 19:06:27,714 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 14.0 (TID 29). 787 bytes result sent to driver
2017-08-11 19:06:27,715 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 14.0 (TID 29) in 2638 ms on localhost (2/2)
2017-08-11 19:06:27,715 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 14 (foreachPartition at streamingProcessNew.scala:51) finished in 2.639 s
2017-08-11 19:06:27,716 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2017-08-11 19:06:27,716 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 14 finished: foreachPartition at streamingProcessNew.scala:51, took 2.646759 s
2017-08-11 19:06:27,716 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449550000 ms.0 from job set of time 1502449550000 ms
2017-08-11 19:06:27,716 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 37.716 s for time 1502449550000 ms (execution: 2.654 s)
2017-08-11 19:06:27,716 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 27 from persistence list
2017-08-11 19:06:27,716 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449555000 ms.0 from job set of time 1502449555000 ms
2017-08-11 19:06:27,717 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 27
2017-08-11 19:06:27,717 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 26 from persistence list
2017-08-11 19:06:27,717 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 26
2017-08-11 19:06:27,717 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:27,717 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449540000 ms
2017-08-11 19:06:27,723 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:27,724 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 15 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:27,724 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 15 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:27,724 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:27,724 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:27,725 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:27,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:27,729 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:27,729 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_15_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:27,730 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 15 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:27,730 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:27,730 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 15.0 with 2 tasks
2017-08-11 19:06:27,731 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 15.0 (TID 30, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:27,732 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 15.0 (TID 31, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:27,732 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 15.0 (TID 31)
2017-08-11 19:06:27,732 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 15.0 (TID 30)
2017-08-11 19:06:27,733 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 375 -> 400
2017-08-11 19:06:27,733 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 375 -> 400
2017-08-11 19:06:28,696 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_14_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:29,348 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 15.0 (TID 31). 787 bytes result sent to driver
2017-08-11 19:06:29,350 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 15.0 (TID 31) in 1619 ms on localhost (1/2)
2017-08-11 19:06:30,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449590000 ms
2017-08-11 19:06:30,152 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 15.0 (TID 30). 787 bytes result sent to driver
2017-08-11 19:06:30,154 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 15.0 (TID 30) in 2424 ms on localhost (2/2)
2017-08-11 19:06:30,155 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 15 (foreachPartition at streamingProcessNew.scala:51) finished in 2.425 s
2017-08-11 19:06:30,155 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2017-08-11 19:06:30,155 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 15 finished: foreachPartition at streamingProcessNew.scala:51, took 2.431826 s
2017-08-11 19:06:30,156 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449555000 ms.0 from job set of time 1502449555000 ms
2017-08-11 19:06:30,156 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 35.156 s for time 1502449555000 ms (execution: 2.440 s)
2017-08-11 19:06:30,156 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449560000 ms.0 from job set of time 1502449560000 ms
2017-08-11 19:06:30,156 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 29 from persistence list
2017-08-11 19:06:30,157 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 28 from persistence list
2017-08-11 19:06:30,157 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 29
2017-08-11 19:06:30,158 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 28
2017-08-11 19:06:30,158 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:30,158 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449545000 ms
2017-08-11 19:06:30,163 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:30,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 16 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:30,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 16 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:30,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:30,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:30,164 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:30,165 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:30,171 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:30,172 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_16_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:30,172 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 16 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:30,172 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:30,173 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 16.0 with 2 tasks
2017-08-11 19:06:30,174 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 16.0 (TID 32, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:30,174 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 16.0 (TID 33, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:30,175 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 16.0 (TID 33)
2017-08-11 19:06:30,175 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 16.0 (TID 32)
2017-08-11 19:06:30,176 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 400 -> 425
2017-08-11 19:06:30,176 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 400 -> 425
2017-08-11 19:06:30,417 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_15_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:33,270 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 16.0 (TID 32). 787 bytes result sent to driver
2017-08-11 19:06:33,272 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 16.0 (TID 32) in 3099 ms on localhost (1/2)
2017-08-11 19:06:34,277 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 16.0 (TID 33). 787 bytes result sent to driver
2017-08-11 19:06:34,279 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 16.0 (TID 33) in 4105 ms on localhost (2/2)
2017-08-11 19:06:34,279 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2017-08-11 19:06:34,279 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 16 (foreachPartition at streamingProcessNew.scala:51) finished in 4.106 s
2017-08-11 19:06:34,279 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 16 finished: foreachPartition at streamingProcessNew.scala:51, took 4.116102 s
2017-08-11 19:06:34,279 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449560000 ms.0 from job set of time 1502449560000 ms
2017-08-11 19:06:34,280 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 34.279 s for time 1502449560000 ms (execution: 4.123 s)
2017-08-11 19:06:34,280 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 31 from persistence list
2017-08-11 19:06:34,280 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449565000 ms.0 from job set of time 1502449565000 ms
2017-08-11 19:06:34,280 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 31
2017-08-11 19:06:34,280 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 30 from persistence list
2017-08-11 19:06:34,281 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 30
2017-08-11 19:06:34,281 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:34,281 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449550000 ms
2017-08-11 19:06:34,287 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:34,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 17 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:34,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 17 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:34,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:34,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:34,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:34,289 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:34,293 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:34,293 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_17_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:34,294 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 17 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:34,294 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:34,294 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 17.0 with 2 tasks
2017-08-11 19:06:34,295 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 17.0 (TID 34, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:34,296 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 17.0 (TID 35, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:34,296 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 17.0 (TID 35)
2017-08-11 19:06:34,296 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 17.0 (TID 34)
2017-08-11 19:06:34,297 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 425 -> 450
2017-08-11 19:06:34,297 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 425 -> 450
2017-08-11 19:06:34,877 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_16_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:35,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449595000 ms
2017-08-11 19:06:36,089 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 17.0 (TID 35). 787 bytes result sent to driver
2017-08-11 19:06:36,090 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 17.0 (TID 35) in 1795 ms on localhost (1/2)
2017-08-11 19:06:36,644 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 17.0 (TID 34). 787 bytes result sent to driver
2017-08-11 19:06:36,647 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 17.0 (TID 34) in 2352 ms on localhost (2/2)
2017-08-11 19:06:36,647 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 17 (foreachPartition at streamingProcessNew.scala:51) finished in 2.353 s
2017-08-11 19:06:36,647 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2017-08-11 19:06:36,647 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 17 finished: foreachPartition at streamingProcessNew.scala:51, took 2.359955 s
2017-08-11 19:06:36,648 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449565000 ms.0 from job set of time 1502449565000 ms
2017-08-11 19:06:36,648 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 31.648 s for time 1502449565000 ms (execution: 2.368 s)
2017-08-11 19:06:36,648 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 33 from persistence list
2017-08-11 19:06:36,648 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449570000 ms.0 from job set of time 1502449570000 ms
2017-08-11 19:06:36,648 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 33
2017-08-11 19:06:36,648 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 32 from persistence list
2017-08-11 19:06:36,649 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 32
2017-08-11 19:06:36,649 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:36,649 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449555000 ms
2017-08-11 19:06:36,654 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:36,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 18 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:36,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 18 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:36,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:36,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:36,656 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:36,657 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:36,659 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:36,660 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_18_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:36,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 18 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:36,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:36,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 18.0 with 2 tasks
2017-08-11 19:06:36,663 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 18.0 (TID 36, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:36,663 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 18.0 (TID 37, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:36,663 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 18.0 (TID 36)
2017-08-11 19:06:36,663 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 18.0 (TID 37)
2017-08-11 19:06:36,665 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 450 -> 475
2017-08-11 19:06:36,665 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 450 -> 475
2017-08-11 19:06:37,313 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_17_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:38,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 18.0 (TID 36). 787 bytes result sent to driver
2017-08-11 19:06:38,046 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 18.0 (TID 36) in 1384 ms on localhost (1/2)
2017-08-11 19:06:38,431 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 18.0 (TID 37). 787 bytes result sent to driver
2017-08-11 19:06:38,433 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 18.0 (TID 37) in 1770 ms on localhost (2/2)
2017-08-11 19:06:38,433 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2017-08-11 19:06:38,433 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 18 (foreachPartition at streamingProcessNew.scala:51) finished in 1.772 s
2017-08-11 19:06:38,434 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 18 finished: foreachPartition at streamingProcessNew.scala:51, took 1.779029 s
2017-08-11 19:06:38,434 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449570000 ms.0 from job set of time 1502449570000 ms
2017-08-11 19:06:38,434 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 28.434 s for time 1502449570000 ms (execution: 1.786 s)
2017-08-11 19:06:38,434 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 35 from persistence list
2017-08-11 19:06:38,434 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449575000 ms.0 from job set of time 1502449575000 ms
2017-08-11 19:06:38,435 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 35
2017-08-11 19:06:38,435 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 34 from persistence list
2017-08-11 19:06:38,435 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 34
2017-08-11 19:06:38,435 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:38,435 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449560000 ms
2017-08-11 19:06:38,440 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:38,441 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 19 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:38,441 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 19 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:38,441 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:38,441 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:38,441 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 19 (MapPartitionsRDD[39] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:38,442 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:38,445 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:38,447 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_19_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:38,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 19 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:38,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:38,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 19.0 with 2 tasks
2017-08-11 19:06:38,450 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 19.0 (TID 38, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:38,451 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 19.0 (TID 39, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:38,451 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 19.0 (TID 38)
2017-08-11 19:06:38,451 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 19.0 (TID 39)
2017-08-11 19:06:38,453 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 475 -> 500
2017-08-11 19:06:38,453 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 475 -> 500
2017-08-11 19:06:38,579 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_18_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:39,727 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 19.0 (TID 38). 787 bytes result sent to driver
2017-08-11 19:06:39,729 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 19.0 (TID 38) in 1279 ms on localhost (1/2)
2017-08-11 19:06:40,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449600000 ms
2017-08-11 19:06:40,403 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 19.0 (TID 39). 787 bytes result sent to driver
2017-08-11 19:06:40,404 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 19.0 (TID 39) in 1953 ms on localhost (2/2)
2017-08-11 19:06:40,405 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 19.0, whose tasks have all completed, from pool 
2017-08-11 19:06:40,405 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 19 (foreachPartition at streamingProcessNew.scala:51) finished in 1.956 s
2017-08-11 19:06:40,405 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 19 finished: foreachPartition at streamingProcessNew.scala:51, took 1.964646 s
2017-08-11 19:06:40,405 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449575000 ms.0 from job set of time 1502449575000 ms
2017-08-11 19:06:40,406 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 37 from persistence list
2017-08-11 19:06:40,406 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 25.405 s for time 1502449575000 ms (execution: 1.971 s)
2017-08-11 19:06:40,406 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 37
2017-08-11 19:06:40,406 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449580000 ms.0 from job set of time 1502449580000 ms
2017-08-11 19:06:40,406 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 36 from persistence list
2017-08-11 19:06:40,406 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 36
2017-08-11 19:06:40,406 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:40,407 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449565000 ms
2017-08-11 19:06:40,411 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:40,412 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 20 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:40,412 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 20 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:40,412 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:40,412 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:40,413 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 20 (MapPartitionsRDD[41] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:40,414 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:40,416 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:40,417 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_20_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:40,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 20 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:40,418 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:40,418 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 20.0 with 2 tasks
2017-08-11 19:06:40,419 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 20.0 (TID 40, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:40,419 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 20.0 (TID 41, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:40,420 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 20.0 (TID 40)
2017-08-11 19:06:40,420 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 20.0 (TID 41)
2017-08-11 19:06:40,421 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 500 -> 525
2017-08-11 19:06:40,421 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 500 -> 525
2017-08-11 19:06:40,850 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_19_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:40,969 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 20.0 (TID 40). 787 bytes result sent to driver
2017-08-11 19:06:40,970 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 20.0 (TID 40) in 552 ms on localhost (1/2)
2017-08-11 19:06:42,094 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 20.0 (TID 41). 787 bytes result sent to driver
2017-08-11 19:06:42,096 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 20.0 (TID 41) in 1677 ms on localhost (2/2)
2017-08-11 19:06:42,096 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 20.0, whose tasks have all completed, from pool 
2017-08-11 19:06:42,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 20 (foreachPartition at streamingProcessNew.scala:51) finished in 1.678 s
2017-08-11 19:06:42,096 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 20 finished: foreachPartition at streamingProcessNew.scala:51, took 1.684719 s
2017-08-11 19:06:42,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449580000 ms.0 from job set of time 1502449580000 ms
2017-08-11 19:06:42,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 22.097 s for time 1502449580000 ms (execution: 1.691 s)
2017-08-11 19:06:42,097 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 39 from persistence list
2017-08-11 19:06:42,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449585000 ms.0 from job set of time 1502449585000 ms
2017-08-11 19:06:42,097 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 39
2017-08-11 19:06:42,097 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 38 from persistence list
2017-08-11 19:06:42,098 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 38
2017-08-11 19:06:42,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:42,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449570000 ms
2017-08-11 19:06:42,104 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:42,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 21 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:42,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 21 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:42,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:42,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:42,106 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 21 (MapPartitionsRDD[43] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:42,107 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:42,109 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:42,110 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_21_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:42,111 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 21 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:42,111 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:42,111 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 21.0 with 2 tasks
2017-08-11 19:06:42,112 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 21.0 (TID 42, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:42,112 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 21.0 (TID 43, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:42,113 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 21.0 (TID 43)
2017-08-11 19:06:42,113 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 21.0 (TID 42)
2017-08-11 19:06:42,114 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 525 -> 550
2017-08-11 19:06:42,114 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 525 -> 550
2017-08-11 19:06:42,332 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_20_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:42,729 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 21.0 (TID 43). 787 bytes result sent to driver
2017-08-11 19:06:42,731 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 21.0 (TID 43) in 619 ms on localhost (1/2)
2017-08-11 19:06:43,192 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 21.0 (TID 42). 787 bytes result sent to driver
2017-08-11 19:06:43,193 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 21.0 (TID 42) in 1082 ms on localhost (2/2)
2017-08-11 19:06:43,193 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 21.0, whose tasks have all completed, from pool 
2017-08-11 19:06:43,193 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 21 (foreachPartition at streamingProcessNew.scala:51) finished in 1.082 s
2017-08-11 19:06:43,193 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 21 finished: foreachPartition at streamingProcessNew.scala:51, took 1.088578 s
2017-08-11 19:06:43,194 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449585000 ms.0 from job set of time 1502449585000 ms
2017-08-11 19:06:43,194 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 18.194 s for time 1502449585000 ms (execution: 1.097 s)
2017-08-11 19:06:43,194 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 41 from persistence list
2017-08-11 19:06:43,194 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449590000 ms.0 from job set of time 1502449590000 ms
2017-08-11 19:06:43,194 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 41
2017-08-11 19:06:43,194 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 40 from persistence list
2017-08-11 19:06:43,195 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 40
2017-08-11 19:06:43,195 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:43,195 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449575000 ms
2017-08-11 19:06:43,200 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:43,200 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 22 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:43,201 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 22 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:43,201 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:43,201 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:43,201 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 22 (MapPartitionsRDD[45] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:43,202 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:43,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:43,206 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_22_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:43,206 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 22 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:43,206 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:43,206 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 22.0 with 2 tasks
2017-08-11 19:06:43,207 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 22.0 (TID 44, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:43,208 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 22.0 (TID 45, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:43,208 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 22.0 (TID 44)
2017-08-11 19:06:43,208 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 22.0 (TID 45)
2017-08-11 19:06:43,209 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 550 -> 575
2017-08-11 19:06:43,210 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 550 -> 575
2017-08-11 19:06:43,477 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_21_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:44,153 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 22.0 (TID 45). 787 bytes result sent to driver
2017-08-11 19:06:44,155 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 22.0 (TID 45) in 946 ms on localhost (1/2)
2017-08-11 19:06:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449605000 ms
2017-08-11 19:06:47,650 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 22.0 (TID 44). 787 bytes result sent to driver
2017-08-11 19:06:47,651 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 22.0 (TID 44) in 4444 ms on localhost (2/2)
2017-08-11 19:06:47,651 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 22.0, whose tasks have all completed, from pool 
2017-08-11 19:06:47,652 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 22 (foreachPartition at streamingProcessNew.scala:51) finished in 4.445 s
2017-08-11 19:06:47,652 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 22 finished: foreachPartition at streamingProcessNew.scala:51, took 4.451541 s
2017-08-11 19:06:47,652 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449590000 ms.0 from job set of time 1502449590000 ms
2017-08-11 19:06:47,653 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 17.652 s for time 1502449590000 ms (execution: 4.458 s)
2017-08-11 19:06:47,653 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 43 from persistence list
2017-08-11 19:06:47,653 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449595000 ms.0 from job set of time 1502449595000 ms
2017-08-11 19:06:47,653 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 43
2017-08-11 19:06:47,653 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 42 from persistence list
2017-08-11 19:06:47,653 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 42
2017-08-11 19:06:47,653 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:47,653 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449580000 ms
2017-08-11 19:06:47,659 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:47,660 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 23 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:47,660 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 23 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:47,660 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:47,660 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:47,660 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 23 (MapPartitionsRDD[47] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:47,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:47,664 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:47,665 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_23_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:47,665 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 23 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:47,665 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 23 (MapPartitionsRDD[47] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:47,666 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 23.0 with 2 tasks
2017-08-11 19:06:47,667 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 23.0 (TID 46, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:47,667 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 23.0 (TID 47, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:47,667 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 23.0 (TID 46)
2017-08-11 19:06:47,667 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 23.0 (TID 47)
2017-08-11 19:06:47,669 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 575 -> 600
2017-08-11 19:06:47,669 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 575 -> 600
2017-08-11 19:06:48,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_22_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:50,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449610000 ms
2017-08-11 19:06:53,006 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 23.0 (TID 46). 787 bytes result sent to driver
2017-08-11 19:06:53,008 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 23.0 (TID 46) in 5342 ms on localhost (1/2)
2017-08-11 19:06:54,348 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 23.0 (TID 47). 787 bytes result sent to driver
2017-08-11 19:06:54,351 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 23.0 (TID 47) in 6684 ms on localhost (2/2)
2017-08-11 19:06:54,351 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 23 (foreachPartition at streamingProcessNew.scala:51) finished in 6.685 s
2017-08-11 19:06:54,351 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 23.0, whose tasks have all completed, from pool 
2017-08-11 19:06:54,351 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 23 finished: foreachPartition at streamingProcessNew.scala:51, took 6.691522 s
2017-08-11 19:06:54,352 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449595000 ms.0 from job set of time 1502449595000 ms
2017-08-11 19:06:54,352 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 19.352 s for time 1502449595000 ms (execution: 6.699 s)
2017-08-11 19:06:54,352 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 45 from persistence list
2017-08-11 19:06:54,353 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449600000 ms.0 from job set of time 1502449600000 ms
2017-08-11 19:06:54,353 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 45
2017-08-11 19:06:54,353 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 44 from persistence list
2017-08-11 19:06:54,354 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 44
2017-08-11 19:06:54,354 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:54,354 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449585000 ms
2017-08-11 19:06:54,362 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:54,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 24 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:54,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 24 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:54,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:54,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:54,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 24 (MapPartitionsRDD[49] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:54,366 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:54,370 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:54,371 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_24_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:54,371 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 24 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:54,372 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:54,372 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 24.0 with 2 tasks
2017-08-11 19:06:54,374 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 24.0 (TID 48, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:54,374 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 24.0 (TID 49, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:54,375 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 24.0 (TID 48)
2017-08-11 19:06:54,375 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 24.0 (TID 49)
2017-08-11 19:06:54,376 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 600 -> 625
2017-08-11 19:06:54,376 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 600 -> 625
2017-08-11 19:06:55,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449615000 ms
2017-08-11 19:06:55,387 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_23_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:57,945 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 24.0 (TID 49). 787 bytes result sent to driver
2017-08-11 19:06:57,947 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 24.0 (TID 49) in 3573 ms on localhost (1/2)
2017-08-11 19:07:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449620000 ms
2017-08-11 19:07:05,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449625000 ms
2017-08-11 19:07:05,630 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 24.0 (TID 48). 874 bytes result sent to driver
2017-08-11 19:07:05,633 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 24.0 (TID 48) in 11260 ms on localhost (2/2)
2017-08-11 19:07:05,633 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 24.0, whose tasks have all completed, from pool 
2017-08-11 19:07:05,633 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 24 (foreachPartition at streamingProcessNew.scala:51) finished in 11.261 s
2017-08-11 19:07:05,634 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 24 finished: foreachPartition at streamingProcessNew.scala:51, took 11.270865 s
2017-08-11 19:07:05,634 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449600000 ms.0 from job set of time 1502449600000 ms
2017-08-11 19:07:05,634 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 25.634 s for time 1502449600000 ms (execution: 11.282 s)
2017-08-11 19:07:05,634 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 47 from persistence list
2017-08-11 19:07:05,634 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449605000 ms.0 from job set of time 1502449605000 ms
2017-08-11 19:07:05,635 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 47
2017-08-11 19:07:05,635 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 46 from persistence list
2017-08-11 19:07:05,635 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:05,635 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449590000 ms
2017-08-11 19:07:05,635 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 46
2017-08-11 19:07:05,641 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:05,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 25 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:05,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 25 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:05,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:05,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:05,643 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 25 (MapPartitionsRDD[51] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:05,644 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:05,650 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:05,651 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_25_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:05,651 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 25 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:05,651 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:05,651 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 25.0 with 2 tasks
2017-08-11 19:07:05,653 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 25.0 (TID 50, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:05,653 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 25.0 (TID 51, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:05,654 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 25.0 (TID 51)
2017-08-11 19:07:05,654 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 25.0 (TID 50)
2017-08-11 19:07:05,656 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 625 -> 650
2017-08-11 19:07:05,656 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 625 -> 650
2017-08-11 19:07:10,006 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_24_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:10,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449630000 ms
2017-08-11 19:07:11,510 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 25.0 (TID 51). 787 bytes result sent to driver
2017-08-11 19:07:11,514 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 25.0 (TID 51) in 5861 ms on localhost (1/2)
2017-08-11 19:07:11,522 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 25.0 (TID 50). 874 bytes result sent to driver
2017-08-11 19:07:11,526 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 25.0 (TID 50) in 5874 ms on localhost (2/2)
2017-08-11 19:07:11,526 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 25 (foreachPartition at streamingProcessNew.scala:51) finished in 5.874 s
2017-08-11 19:07:11,526 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 25.0, whose tasks have all completed, from pool 
2017-08-11 19:07:11,527 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 25 finished: foreachPartition at streamingProcessNew.scala:51, took 5.884944 s
2017-08-11 19:07:11,527 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449605000 ms.0 from job set of time 1502449605000 ms
2017-08-11 19:07:11,527 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 26.527 s for time 1502449605000 ms (execution: 5.893 s)
2017-08-11 19:07:11,527 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 49 from persistence list
2017-08-11 19:07:11,527 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449610000 ms.0 from job set of time 1502449610000 ms
2017-08-11 19:07:11,528 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 48 from persistence list
2017-08-11 19:07:11,528 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 49
2017-08-11 19:07:11,528 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 48
2017-08-11 19:07:11,528 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:11,528 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449595000 ms
2017-08-11 19:07:11,536 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:11,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 26 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:11,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 26 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:11,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:11,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:11,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 26 (MapPartitionsRDD[53] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:11,538 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:11,542 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:11,543 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_26_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:11,543 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 26 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:11,544 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 26 (MapPartitionsRDD[53] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:11,544 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 26.0 with 2 tasks
2017-08-11 19:07:11,545 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 26.0 (TID 52, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:11,546 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 26.0 (TID 53, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:11,546 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 26.0 (TID 53)
2017-08-11 19:07:11,547 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 26.0 (TID 52)
2017-08-11 19:07:11,548 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 650 -> 675
2017-08-11 19:07:11,549 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 650 -> 653
2017-08-11 19:07:11,679 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 26.0 (TID 52). 714 bytes result sent to driver
2017-08-11 19:07:11,681 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 26.0 (TID 52) in 137 ms on localhost (1/2)
2017-08-11 19:07:15,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449635000 ms
2017-08-11 19:07:19,552 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_25_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449640000 ms
2017-08-11 19:07:22,498 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 26.0 (TID 53). 787 bytes result sent to driver
2017-08-11 19:07:22,500 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 26.0 (TID 53) in 10955 ms on localhost (2/2)
2017-08-11 19:07:22,501 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 26.0, whose tasks have all completed, from pool 
2017-08-11 19:07:22,501 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 26 (foreachPartition at streamingProcessNew.scala:51) finished in 10.957 s
2017-08-11 19:07:22,501 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 26 finished: foreachPartition at streamingProcessNew.scala:51, took 10.964820 s
2017-08-11 19:07:22,502 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449610000 ms.0 from job set of time 1502449610000 ms
2017-08-11 19:07:22,502 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 32.502 s for time 1502449610000 ms (execution: 10.975 s)
2017-08-11 19:07:22,502 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 51 from persistence list
2017-08-11 19:07:22,502 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449615000 ms.0 from job set of time 1502449615000 ms
2017-08-11 19:07:22,502 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 51
2017-08-11 19:07:22,502 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 50 from persistence list
2017-08-11 19:07:22,503 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 50
2017-08-11 19:07:22,503 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:22,503 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449600000 ms
2017-08-11 19:07:22,509 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:22,509 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 27 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:22,509 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 27 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:22,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:22,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:22,578 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 27 (MapPartitionsRDD[55] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:22,582 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:22,592 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27_piece0 stored as bytes in memory (estimated size 2.2 KB, free 413.9 MB)
2017-08-11 19:07:22,594 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_27_piece0 in memory on 192.168.31.111:58030 (size: 2.2 KB, free: 413.9 MB)
2017-08-11 19:07:22,595 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 27 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:22,596 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:22,596 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 27.0 with 2 tasks
2017-08-11 19:07:22,600 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 27.0 (TID 54, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:22,602 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 27.0 (TID 55, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:22,603 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 27.0 (TID 54)
2017-08-11 19:07:22,603 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 27.0 (TID 55)
2017-08-11 19:07:22,607 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:22,608 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 675 -> 698
2017-08-11 19:07:22,611 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 27.0 (TID 54). 714 bytes result sent to driver
2017-08-11 19:07:22,613 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 27.0 (TID 54) in 15 ms on localhost (1/2)
2017-08-11 19:07:25,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449645000 ms
2017-08-11 19:07:26,269 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 27.0 (TID 55). 714 bytes result sent to driver
2017-08-11 19:07:26,275 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 27.0 (TID 55) in 3675 ms on localhost (2/2)
2017-08-11 19:07:26,276 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 27.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 27 (foreachPartition at streamingProcessNew.scala:51) finished in 3.679 s
2017-08-11 19:07:26,277 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 27 finished: foreachPartition at streamingProcessNew.scala:51, took 3.767312 s
2017-08-11 19:07:26,278 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449615000 ms.0 from job set of time 1502449615000 ms
2017-08-11 19:07:26,279 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 31.278 s for time 1502449615000 ms (execution: 3.776 s)
2017-08-11 19:07:26,279 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 53 from persistence list
2017-08-11 19:07:26,279 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449620000 ms.0 from job set of time 1502449620000 ms
2017-08-11 19:07:26,280 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 53
2017-08-11 19:07:26,280 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 52 from persistence list
2017-08-11 19:07:26,281 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 52
2017-08-11 19:07:26,281 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,281 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449605000 ms
2017-08-11 19:07:26,298 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:26,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 28 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:26,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 28 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:26,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:26,302 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:26,303 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 28 (MapPartitionsRDD[57] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:26,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:26,314 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:26,316 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_28_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,318 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 28 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:26,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:26,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 28.0 with 2 tasks
2017-08-11 19:07:26,323 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 28.0 (TID 56, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:26,325 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 28.0 (TID 57, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:26,325 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 28.0 (TID 56)
2017-08-11 19:07:26,325 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 28.0 (TID 57)
2017-08-11 19:07:26,328 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:26,328 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:26,331 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 28.0 (TID 56). 714 bytes result sent to driver
2017-08-11 19:07:26,331 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 28.0 (TID 57). 714 bytes result sent to driver
2017-08-11 19:07:26,333 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 28.0 (TID 57) in 10 ms on localhost (1/2)
2017-08-11 19:07:26,334 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 28.0 (TID 56) in 14 ms on localhost (2/2)
2017-08-11 19:07:26,334 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 28.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,334 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 28 (foreachPartition at streamingProcessNew.scala:51) finished in 0.014 s
2017-08-11 19:07:26,334 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 28 finished: foreachPartition at streamingProcessNew.scala:51, took 0.035879 s
2017-08-11 19:07:26,335 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449620000 ms.0 from job set of time 1502449620000 ms
2017-08-11 19:07:26,335 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 26.335 s for time 1502449620000 ms (execution: 0.056 s)
2017-08-11 19:07:26,335 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 55 from persistence list
2017-08-11 19:07:26,335 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449625000 ms.0 from job set of time 1502449625000 ms
2017-08-11 19:07:26,335 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 55
2017-08-11 19:07:26,336 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 54 from persistence list
2017-08-11 19:07:26,336 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 54
2017-08-11 19:07:26,336 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,336 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449610000 ms
2017-08-11 19:07:26,341 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:26,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 29 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:26,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 29 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:26,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:26,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:26,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 29 (MapPartitionsRDD[59] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:26,344 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:26,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:26,347 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_29_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,348 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 29 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:26,348 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 29 (MapPartitionsRDD[59] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:26,348 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 29.0 with 2 tasks
2017-08-11 19:07:26,349 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 29.0 (TID 58, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:26,350 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 29.0 (TID 59, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:26,350 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 29.0 (TID 59)
2017-08-11 19:07:26,350 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 29.0 (TID 58)
2017-08-11 19:07:26,351 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:26,351 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:26,355 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 29.0 (TID 58). 714 bytes result sent to driver
2017-08-11 19:07:26,355 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 29.0 (TID 59). 714 bytes result sent to driver
2017-08-11 19:07:26,357 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 29.0 (TID 58) in 9 ms on localhost (1/2)
2017-08-11 19:07:26,357 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 29.0 (TID 59) in 8 ms on localhost (2/2)
2017-08-11 19:07:26,357 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 29.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,357 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 29 (foreachPartition at streamingProcessNew.scala:51) finished in 0.009 s
2017-08-11 19:07:26,358 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 29 finished: foreachPartition at streamingProcessNew.scala:51, took 0.016259 s
2017-08-11 19:07:26,358 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449625000 ms.0 from job set of time 1502449625000 ms
2017-08-11 19:07:26,358 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 21.358 s for time 1502449625000 ms (execution: 0.023 s)
2017-08-11 19:07:26,358 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449630000 ms.0 from job set of time 1502449630000 ms
2017-08-11 19:07:26,360 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 57 from persistence list
2017-08-11 19:07:26,360 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 56 from persistence list
2017-08-11 19:07:26,361 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 57
2017-08-11 19:07:26,361 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,361 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449615000 ms
2017-08-11 19:07:26,362 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 56
2017-08-11 19:07:26,364 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:26,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 30 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:26,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 30 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:26,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:26,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:26,366 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 30 (MapPartitionsRDD[61] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:26,367 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:26,374 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:26,375 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_26_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,375 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_30_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,376 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 30 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:26,376 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_27_piece0 on 192.168.31.111:58030 in memory (size: 2.2 KB, free: 413.9 MB)
2017-08-11 19:07:26,376 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:26,376 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 30.0 with 2 tasks
2017-08-11 19:07:26,378 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 30.0 (TID 60, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:26,378 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_28_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,379 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 30.0 (TID 61, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:26,379 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 30.0 (TID 60)
2017-08-11 19:07:26,379 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 30.0 (TID 61)
2017-08-11 19:07:26,380 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_29_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,380 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:26,380 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:26,384 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 30.0 (TID 61). 714 bytes result sent to driver
2017-08-11 19:07:26,384 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 30.0 (TID 60). 714 bytes result sent to driver
2017-08-11 19:07:26,386 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 30.0 (TID 61) in 8 ms on localhost (1/2)
2017-08-11 19:07:26,386 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 30.0 (TID 60) in 9 ms on localhost (2/2)
2017-08-11 19:07:26,386 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 30.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 30 (foreachPartition at streamingProcessNew.scala:51) finished in 0.009 s
2017-08-11 19:07:26,386 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 30 finished: foreachPartition at streamingProcessNew.scala:51, took 0.021912 s
2017-08-11 19:07:26,387 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449630000 ms.0 from job set of time 1502449630000 ms
2017-08-11 19:07:26,387 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 16.387 s for time 1502449630000 ms (execution: 0.029 s)
2017-08-11 19:07:26,387 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 59 from persistence list
2017-08-11 19:07:26,387 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449635000 ms.0 from job set of time 1502449635000 ms
2017-08-11 19:07:26,387 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 59
2017-08-11 19:07:26,388 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 58 from persistence list
2017-08-11 19:07:26,388 [block-manager-slave-async-thread-pool-6] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 58
2017-08-11 19:07:26,388 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,388 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449620000 ms
2017-08-11 19:07:26,393 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:26,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 31 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:26,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 31 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:26,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:26,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:26,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 31 (MapPartitionsRDD[63] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:26,395 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:26,398 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:26,399 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_31_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 31 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:26,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:26,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 31.0 with 2 tasks
2017-08-11 19:07:26,400 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 31.0 (TID 62, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:26,401 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 31.0 (TID 63, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:26,401 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 31.0 (TID 63)
2017-08-11 19:07:26,401 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 31.0 (TID 62)
2017-08-11 19:07:26,402 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:26,402 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:26,405 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 31.0 (TID 63). 714 bytes result sent to driver
2017-08-11 19:07:26,405 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 31.0 (TID 62). 714 bytes result sent to driver
2017-08-11 19:07:26,407 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 31.0 (TID 63) in 6 ms on localhost (1/2)
2017-08-11 19:07:26,407 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 31.0 (TID 62) in 7 ms on localhost (2/2)
2017-08-11 19:07:26,407 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 31.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,408 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 31 (foreachPartition at streamingProcessNew.scala:51) finished in 0.009 s
2017-08-11 19:07:26,408 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 31 finished: foreachPartition at streamingProcessNew.scala:51, took 0.014434 s
2017-08-11 19:07:26,408 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449635000 ms.0 from job set of time 1502449635000 ms
2017-08-11 19:07:26,408 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 11.408 s for time 1502449635000 ms (execution: 0.021 s)
2017-08-11 19:07:26,409 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449640000 ms.0 from job set of time 1502449640000 ms
2017-08-11 19:07:26,409 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 61 from persistence list
2017-08-11 19:07:26,409 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 60 from persistence list
2017-08-11 19:07:26,409 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 61
2017-08-11 19:07:26,409 [block-manager-slave-async-thread-pool-7] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 60
2017-08-11 19:07:26,409 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,410 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449625000 ms
2017-08-11 19:07:26,414 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:26,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 32 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:26,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 32 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:26,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:26,416 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:26,416 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 32 (MapPartitionsRDD[65] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:26,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:26,419 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:26,419 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_32_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 32 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:26,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:26,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 32.0 with 2 tasks
2017-08-11 19:07:26,421 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 32.0 (TID 64, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:26,421 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 32.0 (TID 65, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:26,422 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 32.0 (TID 65)
2017-08-11 19:07:26,422 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 32.0 (TID 64)
2017-08-11 19:07:26,423 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:26,423 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:26,426 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 32.0 (TID 64). 714 bytes result sent to driver
2017-08-11 19:07:26,426 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 32.0 (TID 65). 714 bytes result sent to driver
2017-08-11 19:07:26,427 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 32.0 (TID 65) in 6 ms on localhost (1/2)
2017-08-11 19:07:26,428 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 32.0 (TID 64) in 8 ms on localhost (2/2)
2017-08-11 19:07:26,428 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 32.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,428 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 32 (foreachPartition at streamingProcessNew.scala:51) finished in 0.008 s
2017-08-11 19:07:26,428 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 32 finished: foreachPartition at streamingProcessNew.scala:51, took 0.013367 s
2017-08-11 19:07:26,429 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449640000 ms.0 from job set of time 1502449640000 ms
2017-08-11 19:07:26,429 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 6.428 s for time 1502449640000 ms (execution: 0.019 s)
2017-08-11 19:07:26,429 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 63 from persistence list
2017-08-11 19:07:26,429 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449645000 ms.0 from job set of time 1502449645000 ms
2017-08-11 19:07:26,429 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 63
2017-08-11 19:07:26,429 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 62 from persistence list
2017-08-11 19:07:26,429 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 62
2017-08-11 19:07:26,429 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,429 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449630000 ms
2017-08-11 19:07:26,434 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:26,434 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 33 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:26,434 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 33 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:26,434 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:26,434 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:26,435 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 33 (MapPartitionsRDD[67] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:26,435 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:26,438 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33_piece0 stored as bytes in memory (estimated size 2.2 KB, free 413.9 MB)
2017-08-11 19:07:26,438 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_33_piece0 in memory on 192.168.31.111:58030 (size: 2.2 KB, free: 413.9 MB)
2017-08-11 19:07:26,439 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 33 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:26,439 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 33 (MapPartitionsRDD[67] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:26,439 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 33.0 with 2 tasks
2017-08-11 19:07:26,440 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 33.0 (TID 66, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:26,441 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 33.0 (TID 67, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:26,441 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 33.0 (TID 67)
2017-08-11 19:07:26,441 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 33.0 (TID 66)
2017-08-11 19:07:26,442 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:26,442 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:26,445 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 33.0 (TID 67). 714 bytes result sent to driver
2017-08-11 19:07:26,445 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 33.0 (TID 66). 714 bytes result sent to driver
2017-08-11 19:07:26,446 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 33.0 (TID 66) in 6 ms on localhost (1/2)
2017-08-11 19:07:26,446 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 33.0 (TID 67) in 6 ms on localhost (2/2)
2017-08-11 19:07:26,446 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 33.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,447 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 33 (foreachPartition at streamingProcessNew.scala:51) finished in 0.006 s
2017-08-11 19:07:26,447 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 33 finished: foreachPartition at streamingProcessNew.scala:51, took 0.012780 s
2017-08-11 19:07:26,447 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449645000 ms.0 from job set of time 1502449645000 ms
2017-08-11 19:07:26,447 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 65 from persistence list
2017-08-11 19:07:26,447 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.447 s for time 1502449645000 ms (execution: 0.018 s)
2017-08-11 19:07:26,448 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 65
2017-08-11 19:07:26,448 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 64 from persistence list
2017-08-11 19:07:26,448 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 64
2017-08-11 19:07:26,448 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,448 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449635000 ms
2017-08-11 19:07:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449650000 ms
2017-08-11 19:07:30,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449650000 ms.0 from job set of time 1502449650000 ms
2017-08-11 19:07:30,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:30,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 34 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 34 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:30,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 34 (MapPartitionsRDD[69] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_34 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_34_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:30,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_34_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 34 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 34 (MapPartitionsRDD[69] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 34.0 with 2 tasks
2017-08-11 19:07:30,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 34.0 (TID 68, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:30,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 34.0 (TID 69, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:30,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 34.0 (TID 68)
2017-08-11 19:07:30,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 34.0 (TID 69)
2017-08-11 19:07:30,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:30,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:30,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 34.0 (TID 68). 714 bytes result sent to driver
2017-08-11 19:07:30,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 34.0 (TID 69). 714 bytes result sent to driver
2017-08-11 19:07:30,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 34.0 (TID 68) in 11 ms on localhost (1/2)
2017-08-11 19:07:30,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 34.0 (TID 69) in 11 ms on localhost (2/2)
2017-08-11 19:07:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 34 (foreachPartition at streamingProcessNew.scala:51) finished in 0.012 s
2017-08-11 19:07:30,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 34.0, whose tasks have all completed, from pool 
2017-08-11 19:07:30,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 34 finished: foreachPartition at streamingProcessNew.scala:51, took 0.025855 s
2017-08-11 19:07:30,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449650000 ms.0 from job set of time 1502449650000 ms
2017-08-11 19:07:30,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1502449650000 ms (execution: 0.045 s)
2017-08-11 19:07:30,066 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 67 from persistence list
2017-08-11 19:07:30,067 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 67
2017-08-11 19:07:30,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 66 from persistence list
2017-08-11 19:07:30,067 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 66
2017-08-11 19:07:30,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:30,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449640000 ms
2017-08-11 19:07:35,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449655000 ms
2017-08-11 19:07:35,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449655000 ms.0 from job set of time 1502449655000 ms
2017-08-11 19:07:35,020 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:35,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 35 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:35,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 35 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:35,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:35,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:35,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 35 (MapPartitionsRDD[71] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:35,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_35 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:35,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_35_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:35,026 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_35_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:35,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 35 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:35,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 35 (MapPartitionsRDD[71] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:35,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 35.0 with 2 tasks
2017-08-11 19:07:35,028 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 35.0 (TID 70, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:35,028 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 35.0 (TID 71, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:35,028 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 35.0 (TID 70)
2017-08-11 19:07:35,028 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 35.0 (TID 71)
2017-08-11 19:07:35,029 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:35,029 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:35,032 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 35.0 (TID 71). 714 bytes result sent to driver
2017-08-11 19:07:35,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 35.0 (TID 70). 714 bytes result sent to driver
2017-08-11 19:07:35,034 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 35.0 (TID 71) in 6 ms on localhost (1/2)
2017-08-11 19:07:35,035 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 35.0 (TID 70) in 8 ms on localhost (2/2)
2017-08-11 19:07:35,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 35 (foreachPartition at streamingProcessNew.scala:51) finished in 0.009 s
2017-08-11 19:07:35,035 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 35.0, whose tasks have all completed, from pool 
2017-08-11 19:07:35,035 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 35 finished: foreachPartition at streamingProcessNew.scala:51, took 0.015128 s
2017-08-11 19:07:35,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449655000 ms.0 from job set of time 1502449655000 ms
2017-08-11 19:07:35,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.036 s for time 1502449655000 ms (execution: 0.022 s)
2017-08-11 19:07:35,036 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 69 from persistence list
2017-08-11 19:07:35,036 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 69
2017-08-11 19:07:35,036 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 68 from persistence list
2017-08-11 19:07:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:35,037 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 68
2017-08-11 19:07:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449645000 ms
2017-08-11 19:07:40,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449660000 ms
2017-08-11 19:07:40,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449660000 ms.0 from job set of time 1502449660000 ms
2017-08-11 19:07:40,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:40,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 36 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 36 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 36 (MapPartitionsRDD[73] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:40,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_36 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_36_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:40,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_36_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:40,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 36 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 36 (MapPartitionsRDD[73] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 36.0 with 2 tasks
2017-08-11 19:07:40,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 36.0 (TID 72, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:40,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 36.0 (TID 73, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:40,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 36.0 (TID 73)
2017-08-11 19:07:40,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 36.0 (TID 72)
2017-08-11 19:07:40,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:40,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:40,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 36.0 (TID 73). 714 bytes result sent to driver
2017-08-11 19:07:40,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 36.0 (TID 72). 714 bytes result sent to driver
2017-08-11 19:07:40,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 36.0 (TID 73) in 17 ms on localhost (1/2)
2017-08-11 19:07:40,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 36.0 (TID 72) in 21 ms on localhost (2/2)
2017-08-11 19:07:40,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 36.0, whose tasks have all completed, from pool 
2017-08-11 19:07:40,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 36 (foreachPartition at streamingProcessNew.scala:51) finished in 0.021 s
2017-08-11 19:07:40,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 36 finished: foreachPartition at streamingProcessNew.scala:51, took 0.039014 s
2017-08-11 19:07:40,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449660000 ms.0 from job set of time 1502449660000 ms
2017-08-11 19:07:40,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1502449660000 ms (execution: 0.060 s)
2017-08-11 19:07:40,081 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 71 from persistence list
2017-08-11 19:07:40,082 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 71
2017-08-11 19:07:40,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 70 from persistence list
2017-08-11 19:07:40,083 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 70
2017-08-11 19:07:40,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:40,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449650000 ms
2017-08-11 19:07:45,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449665000 ms
2017-08-11 19:07:45,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449665000 ms.0 from job set of time 1502449665000 ms
2017-08-11 19:07:45,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:45,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 37 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:45,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 37 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:45,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:45,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:45,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 37 (MapPartitionsRDD[75] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:45,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_37 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:45,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_37_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:45,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_37_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 37 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 37 (MapPartitionsRDD[75] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 37.0 with 2 tasks
2017-08-11 19:07:45,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 37.0 (TID 74, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:45,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 37.0 (TID 75, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:45,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 37.0 (TID 75)
2017-08-11 19:07:45,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 37.0 (TID 74)
2017-08-11 19:07:45,056 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:45,056 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:45,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 37.0 (TID 74). 714 bytes result sent to driver
2017-08-11 19:07:45,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 37.0 (TID 75). 714 bytes result sent to driver
2017-08-11 19:07:45,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 37.0 (TID 74) in 9 ms on localhost (1/2)
2017-08-11 19:07:45,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 37.0 (TID 75) in 9 ms on localhost (2/2)
2017-08-11 19:07:45,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 37.0, whose tasks have all completed, from pool 
2017-08-11 19:07:45,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 37 (foreachPartition at streamingProcessNew.scala:51) finished in 0.010 s
2017-08-11 19:07:45,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 37 finished: foreachPartition at streamingProcessNew.scala:51, took 0.021704 s
2017-08-11 19:07:45,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449665000 ms.0 from job set of time 1502449665000 ms
2017-08-11 19:07:45,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1502449665000 ms (execution: 0.041 s)
2017-08-11 19:07:45,064 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 73 from persistence list
2017-08-11 19:07:45,065 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 73
2017-08-11 19:07:45,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 72 from persistence list
2017-08-11 19:07:45,066 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 72
2017-08-11 19:07:45,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:45,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449655000 ms
2017-08-11 19:07:50,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449670000 ms
2017-08-11 19:07:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449670000 ms.0 from job set of time 1502449670000 ms
2017-08-11 19:07:50,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 38 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 38 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 38 (MapPartitionsRDD[77] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_38 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_38_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:50,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_38_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 38 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 38 (MapPartitionsRDD[77] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 38.0 with 2 tasks
2017-08-11 19:07:50,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 38.0 (TID 76, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:50,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 38.0 (TID 77, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:50,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 38.0 (TID 76)
2017-08-11 19:07:50,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 38.0 (TID 77)
2017-08-11 19:07:50,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:50,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:50,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 38.0 (TID 77). 714 bytes result sent to driver
2017-08-11 19:07:50,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 38.0 (TID 76). 714 bytes result sent to driver
2017-08-11 19:07:50,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 38.0 (TID 76) in 11 ms on localhost (1/2)
2017-08-11 19:07:50,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 38.0 (TID 77) in 10 ms on localhost (2/2)
2017-08-11 19:07:50,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 38.0, whose tasks have all completed, from pool 
2017-08-11 19:07:50,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 38 (foreachPartition at streamingProcessNew.scala:51) finished in 0.011 s
2017-08-11 19:07:50,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 38 finished: foreachPartition at streamingProcessNew.scala:51, took 0.028136 s
2017-08-11 19:07:50,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449670000 ms.0 from job set of time 1502449670000 ms
2017-08-11 19:07:50,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1502449670000 ms (execution: 0.050 s)
2017-08-11 19:07:50,071 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 75 from persistence list
2017-08-11 19:07:50,071 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 75
2017-08-11 19:07:50,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 74 from persistence list
2017-08-11 19:07:50,072 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 74
2017-08-11 19:07:50,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:50,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449660000 ms
2017-08-11 19:07:55,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449675000 ms
2017-08-11 19:07:55,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449675000 ms.0 from job set of time 1502449675000 ms
2017-08-11 19:07:55,024 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:55,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 39 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:55,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 39 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:55,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:55,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:55,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 39 (MapPartitionsRDD[79] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:55,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_39 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:55,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_39_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:55,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_39_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:55,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 39 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:55,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 39 (MapPartitionsRDD[79] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:55,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 39.0 with 2 tasks
2017-08-11 19:07:55,033 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 39.0 (TID 78, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:55,034 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 39.0 (TID 79, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:55,034 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 39.0 (TID 78)
2017-08-11 19:07:55,034 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 39.0 (TID 79)
2017-08-11 19:07:55,035 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:55,036 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:55,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 39.0 (TID 79). 714 bytes result sent to driver
2017-08-11 19:07:55,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 39.0 (TID 78). 714 bytes result sent to driver
2017-08-11 19:07:55,040 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 39.0 (TID 78) in 8 ms on localhost (1/2)
2017-08-11 19:07:55,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 39.0 (TID 79) in 7 ms on localhost (2/2)
2017-08-11 19:07:55,041 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 39.0, whose tasks have all completed, from pool 
2017-08-11 19:07:55,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 39 (foreachPartition at streamingProcessNew.scala:51) finished in 0.009 s
2017-08-11 19:07:55,041 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 39 finished: foreachPartition at streamingProcessNew.scala:51, took 0.016317 s
2017-08-11 19:07:55,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449675000 ms.0 from job set of time 1502449675000 ms
2017-08-11 19:07:55,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1502449675000 ms (execution: 0.025 s)
2017-08-11 19:07:55,041 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 77 from persistence list
2017-08-11 19:07:55,042 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 77
2017-08-11 19:07:55,042 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 76 from persistence list
2017-08-11 19:07:55,042 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 76
2017-08-11 19:07:55,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:55,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449665000 ms
2017-08-11 19:08:00,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449680000 ms
2017-08-11 19:08:00,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449680000 ms.0 from job set of time 1502449680000 ms
2017-08-11 19:08:00,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:08:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 40 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:08:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 40 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:08:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:08:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:08:00,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 40 (MapPartitionsRDD[81] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:08:00,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_40 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:08:00,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_40_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:08:00,039 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_40_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:08:00,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 40 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:08:00,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 40 (MapPartitionsRDD[81] at map at streamingProcessNew.scala:49)
2017-08-11 19:08:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 40.0 with 2 tasks
2017-08-11 19:08:00,043 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 40.0 (TID 80, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:08:00,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 40.0 (TID 81, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:08:00,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 40.0 (TID 81)
2017-08-11 19:08:00,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 40.0 (TID 80)
2017-08-11 19:08:00,046 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:08:00,046 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:08:00,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 40.0 (TID 80). 714 bytes result sent to driver
2017-08-11 19:08:00,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 40.0 (TID 81). 714 bytes result sent to driver
2017-08-11 19:08:00,052 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 40.0 (TID 80) in 10 ms on localhost (1/2)
2017-08-11 19:08:00,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 40.0 (TID 81) in 9 ms on localhost (2/2)
2017-08-11 19:08:00,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 40.0, whose tasks have all completed, from pool 
2017-08-11 19:08:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 40 (foreachPartition at streamingProcessNew.scala:51) finished in 0.011 s
2017-08-11 19:08:00,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 40 finished: foreachPartition at streamingProcessNew.scala:51, took 0.021904 s
2017-08-11 19:08:00,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449680000 ms.0 from job set of time 1502449680000 ms
2017-08-11 19:08:00,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.053 s for time 1502449680000 ms (execution: 0.034 s)
2017-08-11 19:08:00,053 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 79 from persistence list
2017-08-11 19:08:00,053 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 79
2017-08-11 19:08:00,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 78 from persistence list
2017-08-11 19:08:00,054 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 78
2017-08-11 19:08:00,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:08:00,055 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449670000 ms
2017-08-11 19:08:05,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449685000 ms
2017-08-11 19:08:05,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449685000 ms.0 from job set of time 1502449685000 ms
2017-08-11 19:08:05,035 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:08:05,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 41 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:08:05,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 41 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:08:05,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:08:05,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:08:05,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 41 (MapPartitionsRDD[83] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:08:05,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_41 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:08:05,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_41_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.8 MB)
2017-08-11 19:08:05,044 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_41_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:08:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 41 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:08:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 41 (MapPartitionsRDD[83] at map at streamingProcessNew.scala:49)
2017-08-11 19:08:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 41.0 with 2 tasks
2017-08-11 19:08:05,046 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 41.0 (TID 82, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:08:05,047 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 41.0 (TID 83, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:08:05,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 41.0 (TID 83)
2017-08-11 19:08:05,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 41.0 (TID 82)
2017-08-11 19:08:05,049 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:08:05,050 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:08:05,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 41.0 (TID 82). 714 bytes result sent to driver
2017-08-11 19:08:05,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 41.0 (TID 83). 801 bytes result sent to driver
2017-08-11 19:08:05,056 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 41.0 (TID 83) in 9 ms on localhost (1/2)
2017-08-11 19:08:05,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 41.0 (TID 82) in 12 ms on localhost (2/2)
2017-08-11 19:08:05,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 41.0, whose tasks have all completed, from pool 
2017-08-11 19:08:05,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 41 (foreachPartition at streamingProcessNew.scala:51) finished in 0.012 s
2017-08-11 19:08:05,057 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 41 finished: foreachPartition at streamingProcessNew.scala:51, took 0.021698 s
2017-08-11 19:08:05,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449685000 ms.0 from job set of time 1502449685000 ms
2017-08-11 19:08:05,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.058 s for time 1502449685000 ms (execution: 0.035 s)
2017-08-11 19:08:05,058 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 81 from persistence list
2017-08-11 19:08:05,058 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 81
2017-08-11 19:08:05,058 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 80 from persistence list
2017-08-11 19:08:05,058 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 80
2017-08-11 19:08:05,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:08:05,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449675000 ms
2017-08-11 19:08:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449690000 ms
2017-08-11 19:08:10,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449690000 ms.0 from job set of time 1502449690000 ms
2017-08-11 19:08:10,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:08:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 42 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:08:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 42 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:08:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:08:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:08:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 42 (MapPartitionsRDD[85] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:08:10,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_42 stored as values in memory (estimated size 3.9 KB, free 413.8 MB)
2017-08-11 19:08:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_42_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.8 MB)
2017-08-11 19:08:10,047 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_42_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:08:10,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 42 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:08:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 42 (MapPartitionsRDD[85] at map at streamingProcessNew.scala:49)
2017-08-11 19:08:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 42.0 with 2 tasks
2017-08-11 19:08:10,050 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 42.0 (TID 84, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:08:10,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 42.0 (TID 85, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:08:10,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 42.0 (TID 84)
2017-08-11 19:08:10,053 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:08:10,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 42.0 (TID 85)
2017-08-11 19:08:10,056 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:08:10,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 42.0 (TID 84). 714 bytes result sent to driver
2017-08-11 19:08:10,060 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 42.0 (TID 84) in 11 ms on localhost (1/2)
2017-08-11 19:08:10,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 42.0 (TID 85). 714 bytes result sent to driver
2017-08-11 19:08:10,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 42.0 (TID 85) in 14 ms on localhost (2/2)
2017-08-11 19:08:10,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 42.0, whose tasks have all completed, from pool 
2017-08-11 19:08:10,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 42 (foreachPartition at streamingProcessNew.scala:51) finished in 0.016 s
2017-08-11 19:08:10,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 42 finished: foreachPartition at streamingProcessNew.scala:51, took 0.025798 s
2017-08-11 19:08:10,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449690000 ms.0 from job set of time 1502449690000 ms
2017-08-11 19:08:10,066 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 83 from persistence list
2017-08-11 19:08:10,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1502449690000 ms (execution: 0.045 s)
2017-08-11 19:08:10,066 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 83
2017-08-11 19:08:10,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 82 from persistence list
2017-08-11 19:08:10,067 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 82
2017-08-11 19:08:10,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:08:10,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449680000 ms
2017-08-11 19:08:15,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449695000 ms
2017-08-11 19:08:15,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449695000 ms.0 from job set of time 1502449695000 ms
2017-08-11 19:08:15,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:08:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 43 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:08:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 43 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:08:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:08:15,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:08:15,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 43 (MapPartitionsRDD[87] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:08:15,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_43 stored as values in memory (estimated size 3.9 KB, free 413.8 MB)
2017-08-11 19:08:15,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_43_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.8 MB)
2017-08-11 19:08:15,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_43_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:08:15,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 43 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:08:15,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 43 (MapPartitionsRDD[87] at map at streamingProcessNew.scala:49)
2017-08-11 19:08:15,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 43.0 with 2 tasks
2017-08-11 19:08:15,041 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 43.0 (TID 86, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:08:15,041 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 43.0 (TID 87, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:08:15,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 43.0 (TID 87)
2017-08-11 19:08:15,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 43.0 (TID 86)
2017-08-11 19:08:15,043 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:08:15,045 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:08:15,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 43.0 (TID 87). 714 bytes result sent to driver
2017-08-11 19:08:15,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 43.0 (TID 86). 714 bytes result sent to driver
2017-08-11 19:08:15,049 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 43.0 (TID 87) in 8 ms on localhost (1/2)
2017-08-11 19:08:15,049 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 43.0 (TID 86) in 10 ms on localhost (2/2)
2017-08-11 19:08:15,050 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 43.0, whose tasks have all completed, from pool 
2017-08-11 19:08:15,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 43 (foreachPartition at streamingProcessNew.scala:51) finished in 0.011 s
2017-08-11 19:08:15,050 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 43 finished: foreachPartition at streamingProcessNew.scala:51, took 0.020363 s
2017-08-11 19:08:15,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449695000 ms.0 from job set of time 1502449695000 ms
2017-08-11 19:08:15,051 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.050 s for time 1502449695000 ms (execution: 0.031 s)
2017-08-11 19:08:15,051 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 85 from persistence list
2017-08-11 19:08:15,051 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 85
2017-08-11 19:08:15,051 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 84 from persistence list
2017-08-11 19:08:15,052 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 84
2017-08-11 19:08:15,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:08:15,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449685000 ms
2017-08-11 19:08:20,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449700000 ms
2017-08-11 19:08:20,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449700000 ms.0 from job set of time 1502449700000 ms
2017-08-11 19:08:20,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:08:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 44 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:08:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 44 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:08:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:08:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:08:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 44 (MapPartitionsRDD[89] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:08:20,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_44 stored as values in memory (estimated size 3.9 KB, free 413.8 MB)
2017-08-11 19:08:20,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_44_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.8 MB)
2017-08-11 19:08:20,033 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_44_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:08:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 44 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:08:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 44 (MapPartitionsRDD[89] at map at streamingProcessNew.scala:49)
2017-08-11 19:08:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 44.0 with 2 tasks
2017-08-11 19:08:20,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 44.0 (TID 88, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:08:20,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 44.0 (TID 89, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:08:20,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 44.0 (TID 89)
2017-08-11 19:08:20,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 44.0 (TID 88)
2017-08-11 19:08:20,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:08:20,040 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:08:20,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 44.0 (TID 88). 714 bytes result sent to driver
2017-08-11 19:08:20,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 44.0 (TID 89). 714 bytes result sent to driver
2017-08-11 19:08:20,046 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 44.0 (TID 88) in 9 ms on localhost (1/2)
2017-08-11 19:08:20,047 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 44.0 (TID 89) in 8 ms on localhost (2/2)
2017-08-11 19:08:20,047 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 44.0, whose tasks have all completed, from pool 
2017-08-11 19:08:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 44 (foreachPartition at streamingProcessNew.scala:51) finished in 0.010 s
2017-08-11 19:08:20,047 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 44 finished: foreachPartition at streamingProcessNew.scala:51, took 0.020885 s
2017-08-11 19:08:20,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449700000 ms.0 from job set of time 1502449700000 ms
2017-08-11 19:08:20,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1502449700000 ms (execution: 0.029 s)
2017-08-11 19:08:20,048 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 87 from persistence list
2017-08-11 19:08:20,049 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 87
2017-08-11 19:08:20,050 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 86 from persistence list
2017-08-11 19:08:20,050 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 86
2017-08-11 19:08:20,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:08:20,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449690000 ms
