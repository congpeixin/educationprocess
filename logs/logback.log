2017-08-09 15:28:52,167 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 154.0 with 2 tasks
2017-08-09 15:28:52,176 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 154.0 (TID 308, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,178 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 154.0 (TID 309, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,178 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 154.0 (TID 308)
2017-08-09 15:28:52,178 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 154.0 (TID 309)
2017-08-09 15:28:52,180 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,180 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,182 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 154.0 (TID 308). 714 bytes result sent to driver
2017-08-09 15:28:52,182 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 154.0 (TID 309). 714 bytes result sent to driver
2017-08-09 15:28:52,183 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 154.0 (TID 308) in 7 ms on localhost (1/2)
2017-08-09 15:28:52,183 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 154.0 (TID 309) in 6 ms on localhost (2/2)
2017-08-09 15:28:52,183 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 154.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,183 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 154 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,183 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 154 finished: foreachPartition at streamingProcess.scala:50, took 0.021861 s
2017-08-09 15:28:52,184 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263655000 ms.0 from job set of time 1502263655000 ms
2017-08-09 15:28:52,184 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 77.184 s for time 1502263655000 ms (execution: 0.032 s)
2017-08-09 15:28:52,184 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 307 from persistence list
2017-08-09 15:28:52,184 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263660000 ms.0 from job set of time 1502263660000 ms
2017-08-09 15:28:52,184 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 307
2017-08-09 15:28:52,184 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 306 from persistence list
2017-08-09 15:28:52,184 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 306
2017-08-09 15:28:52,184 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,185 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263645000 ms
2017-08-09 15:28:52,195 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,195 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 155 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,195 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 155 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,196 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,196 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,196 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 155 (MapPartitionsRDD[311] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_155 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-09 15:28:52,200 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_155_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-09 15:28:52,200 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_155_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:28:52,200 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 155 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,200 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 155 (MapPartitionsRDD[311] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,200 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 155.0 with 2 tasks
2017-08-09 15:28:52,201 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 155.0 (TID 310, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,201 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 155.0 (TID 311, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,201 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 155.0 (TID 310)
2017-08-09 15:28:52,201 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 155.0 (TID 311)
2017-08-09 15:28:52,203 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,203 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,206 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 155.0 (TID 310). 714 bytes result sent to driver
2017-08-09 15:28:52,206 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 155.0 (TID 311). 714 bytes result sent to driver
2017-08-09 15:28:52,207 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 155.0 (TID 311) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,207 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 155.0 (TID 310) in 6 ms on localhost (2/2)
2017-08-09 15:28:52,207 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 155.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,207 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 155 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-09 15:28:52,207 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 155 finished: foreachPartition at streamingProcess.scala:50, took 0.011885 s
2017-08-09 15:28:52,207 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263660000 ms.0 from job set of time 1502263660000 ms
2017-08-09 15:28:52,207 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 72.207 s for time 1502263660000 ms (execution: 0.023 s)
2017-08-09 15:28:52,207 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 309 from persistence list
2017-08-09 15:28:52,207 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263665000 ms.0 from job set of time 1502263665000 ms
2017-08-09 15:28:52,208 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 308 from persistence list
2017-08-09 15:28:52,208 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 309
2017-08-09 15:28:52,208 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 308
2017-08-09 15:28:52,208 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,208 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263650000 ms
2017-08-09 15:28:52,217 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,217 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 156 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,217 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 156 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,217 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,217 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,217 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 156 (MapPartitionsRDD[313] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,221 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_156 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-09 15:28:52,224 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_156_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-09 15:28:52,224 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_156_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:28:52,225 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 156 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,225 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 156 (MapPartitionsRDD[313] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,225 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 156.0 with 2 tasks
2017-08-09 15:28:52,226 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 156.0 (TID 312, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,226 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 156.0 (TID 313, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,226 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 156.0 (TID 312)
2017-08-09 15:28:52,226 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 156.0 (TID 313)
2017-08-09 15:28:52,236 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_141_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:28:52,236 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,236 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,238 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_142_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,239 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_143_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,240 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 156.0 (TID 313). 787 bytes result sent to driver
2017-08-09 15:28:52,240 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 156.0 (TID 312). 787 bytes result sent to driver
2017-08-09 15:28:52,242 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 156.0 (TID 312) in 17 ms on localhost (1/2)
2017-08-09 15:28:52,242 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_144_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,242 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 156.0 (TID 313) in 16 ms on localhost (2/2)
2017-08-09 15:28:52,242 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 156.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 156 (foreachPartition at streamingProcess.scala:50) finished in 0.017 s
2017-08-09 15:28:52,242 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 156 finished: foreachPartition at streamingProcess.scala:50, took 0.025630 s
2017-08-09 15:28:52,243 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263665000 ms.0 from job set of time 1502263665000 ms
2017-08-09 15:28:52,243 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 67.243 s for time 1502263665000 ms (execution: 0.036 s)
2017-08-09 15:28:52,243 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263670000 ms.0 from job set of time 1502263670000 ms
2017-08-09 15:28:52,243 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 311 from persistence list
2017-08-09 15:28:52,245 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_145_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,245 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 311
2017-08-09 15:28:52,245 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 310 from persistence list
2017-08-09 15:28:52,245 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 310
2017-08-09 15:28:52,245 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,245 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263655000 ms
2017-08-09 15:28:52,246 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_146_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,246 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_147_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,247 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_148_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,248 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_149_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,249 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_150_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,250 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_151_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,250 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_152_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,251 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_153_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,251 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_154_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:28:52,252 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_155_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:28:52,256 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,257 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 157 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,257 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 157 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,257 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,257 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,257 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 157 (MapPartitionsRDD[315] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_157 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-09 15:28:52,262 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_157_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-09 15:28:52,262 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_157_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:28:52,263 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 157 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,263 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 157 (MapPartitionsRDD[315] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,263 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 157.0 with 2 tasks
2017-08-09 15:28:52,263 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 157.0 (TID 314, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,263 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 157.0 (TID 315, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,264 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 157.0 (TID 314)
2017-08-09 15:28:52,264 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 157.0 (TID 315)
2017-08-09 15:28:52,266 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,266 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,269 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 157.0 (TID 315). 714 bytes result sent to driver
2017-08-09 15:28:52,269 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 157.0 (TID 314). 714 bytes result sent to driver
2017-08-09 15:28:52,272 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 157.0 (TID 314) in 9 ms on localhost (1/2)
2017-08-09 15:28:52,272 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 157.0 (TID 315) in 9 ms on localhost (2/2)
2017-08-09 15:28:52,272 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 157.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,272 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 157 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-09 15:28:52,272 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 157 finished: foreachPartition at streamingProcess.scala:50, took 0.016333 s
2017-08-09 15:28:52,273 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263670000 ms.0 from job set of time 1502263670000 ms
2017-08-09 15:28:52,273 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 62.273 s for time 1502263670000 ms (execution: 0.030 s)
2017-08-09 15:28:52,273 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 313 from persistence list
2017-08-09 15:28:52,273 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263675000 ms.0 from job set of time 1502263675000 ms
2017-08-09 15:28:52,273 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 313
2017-08-09 15:28:52,273 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 312 from persistence list
2017-08-09 15:28:52,273 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 312
2017-08-09 15:28:52,273 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,273 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263660000 ms
2017-08-09 15:28:52,285 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 158 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 158 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 158 (MapPartitionsRDD[317] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,292 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_158 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-09 15:28:52,294 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_158_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-09 15:28:52,295 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_158_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,296 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 158 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,296 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 158 (MapPartitionsRDD[317] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,296 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 158.0 with 2 tasks
2017-08-09 15:28:52,297 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 158.0 (TID 316, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,297 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 158.0 (TID 317, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,297 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 158.0 (TID 317)
2017-08-09 15:28:52,297 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 158.0 (TID 316)
2017-08-09 15:28:52,299 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,299 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,302 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 158.0 (TID 317). 714 bytes result sent to driver
2017-08-09 15:28:52,302 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 158.0 (TID 316). 714 bytes result sent to driver
2017-08-09 15:28:52,303 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 158.0 (TID 317) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,303 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 158.0 (TID 316) in 7 ms on localhost (2/2)
2017-08-09 15:28:52,303 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 158.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,304 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 158 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,304 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 158 finished: foreachPartition at streamingProcess.scala:50, took 0.018555 s
2017-08-09 15:28:52,305 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263675000 ms.0 from job set of time 1502263675000 ms
2017-08-09 15:28:52,306 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 57.305 s for time 1502263675000 ms (execution: 0.032 s)
2017-08-09 15:28:52,306 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263680000 ms.0 from job set of time 1502263680000 ms
2017-08-09 15:28:52,306 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 315 from persistence list
2017-08-09 15:28:52,307 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 315
2017-08-09 15:28:52,307 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 314 from persistence list
2017-08-09 15:28:52,308 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 314
2017-08-09 15:28:52,308 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,308 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263665000 ms
2017-08-09 15:28:52,320 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,320 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 159 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,320 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 159 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,320 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,321 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,321 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 159 (MapPartitionsRDD[319] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,324 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_159 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-09 15:28:52,327 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_159_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-09 15:28:52,328 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_159_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,328 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 159 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,328 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 159 (MapPartitionsRDD[319] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,328 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 159.0 with 2 tasks
2017-08-09 15:28:52,329 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 159.0 (TID 318, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,329 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 159.0 (TID 319, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,329 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 159.0 (TID 319)
2017-08-09 15:28:52,329 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 159.0 (TID 318)
2017-08-09 15:28:52,331 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,331 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,334 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 159.0 (TID 319). 714 bytes result sent to driver
2017-08-09 15:28:52,334 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 159.0 (TID 318). 714 bytes result sent to driver
2017-08-09 15:28:52,335 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 159.0 (TID 319) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,335 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 159.0 (TID 318) in 7 ms on localhost (2/2)
2017-08-09 15:28:52,335 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 159.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 159 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,335 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 159 finished: foreachPartition at streamingProcess.scala:50, took 0.015267 s
2017-08-09 15:28:52,335 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263680000 ms.0 from job set of time 1502263680000 ms
2017-08-09 15:28:52,336 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 52.335 s for time 1502263680000 ms (execution: 0.029 s)
2017-08-09 15:28:52,336 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263685000 ms.0 from job set of time 1502263685000 ms
2017-08-09 15:28:52,336 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 317 from persistence list
2017-08-09 15:28:52,336 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 317
2017-08-09 15:28:52,336 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 316 from persistence list
2017-08-09 15:28:52,336 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 316
2017-08-09 15:28:52,336 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,336 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263670000 ms
2017-08-09 15:28:52,347 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 160 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 160 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 160 (MapPartitionsRDD[321] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,350 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_160 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-09 15:28:52,352 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_160_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-09 15:28:52,352 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_160_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,352 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 160 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,352 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 160 (MapPartitionsRDD[321] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,352 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 160.0 with 2 tasks
2017-08-09 15:28:52,353 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 160.0 (TID 320, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,353 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 160.0 (TID 321, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,353 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 160.0 (TID 320)
2017-08-09 15:28:52,353 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 160.0 (TID 321)
2017-08-09 15:28:52,355 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,355 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,359 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 160.0 (TID 321). 714 bytes result sent to driver
2017-08-09 15:28:52,359 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 160.0 (TID 320). 714 bytes result sent to driver
2017-08-09 15:28:52,360 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 160.0 (TID 321) in 7 ms on localhost (1/2)
2017-08-09 15:28:52,360 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 160.0 (TID 320) in 8 ms on localhost (2/2)
2017-08-09 15:28:52,360 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 160.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,360 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 160 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-09 15:28:52,361 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 160 finished: foreachPartition at streamingProcess.scala:50, took 0.013593 s
2017-08-09 15:28:52,361 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263685000 ms.0 from job set of time 1502263685000 ms
2017-08-09 15:28:52,361 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 47.361 s for time 1502263685000 ms (execution: 0.025 s)
2017-08-09 15:28:52,361 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263690000 ms.0 from job set of time 1502263690000 ms
2017-08-09 15:28:52,361 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 319 from persistence list
2017-08-09 15:28:52,361 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 319
2017-08-09 15:28:52,361 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 318 from persistence list
2017-08-09 15:28:52,361 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 318
2017-08-09 15:28:52,361 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,361 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263675000 ms
2017-08-09 15:28:52,370 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,370 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 161 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,370 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 161 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,370 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,370 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,371 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 161 (MapPartitionsRDD[323] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,373 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_161 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-09 15:28:52,375 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_161_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-09 15:28:52,375 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_161_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:28:52,375 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 161 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,376 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 161 (MapPartitionsRDD[323] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,376 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 161.0 with 2 tasks
2017-08-09 15:28:52,376 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 161.0 (TID 322, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,377 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 161.0 (TID 323, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,377 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 161.0 (TID 323)
2017-08-09 15:28:52,377 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 161.0 (TID 322)
2017-08-09 15:28:52,378 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,378 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,381 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 161.0 (TID 322). 714 bytes result sent to driver
2017-08-09 15:28:52,381 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 161.0 (TID 323). 714 bytes result sent to driver
2017-08-09 15:28:52,382 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 161.0 (TID 323) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,383 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 161.0 (TID 322) in 7 ms on localhost (2/2)
2017-08-09 15:28:52,383 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 161.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,383 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 161 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,383 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 161 finished: foreachPartition at streamingProcess.scala:50, took 0.013000 s
2017-08-09 15:28:52,383 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263690000 ms.0 from job set of time 1502263690000 ms
2017-08-09 15:28:52,383 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 42.383 s for time 1502263690000 ms (execution: 0.022 s)
2017-08-09 15:28:52,383 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 321 from persistence list
2017-08-09 15:28:52,383 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263695000 ms.0 from job set of time 1502263695000 ms
2017-08-09 15:28:52,384 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 321
2017-08-09 15:28:52,384 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 320 from persistence list
2017-08-09 15:28:52,384 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 320
2017-08-09 15:28:52,384 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,384 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263680000 ms
2017-08-09 15:28:52,393 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,393 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 162 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 162 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 162 (MapPartitionsRDD[325] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,396 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_162 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-09 15:28:52,398 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_162_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-09 15:28:52,399 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_162_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 162 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,400 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 162 (MapPartitionsRDD[325] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,400 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 162.0 with 2 tasks
2017-08-09 15:28:52,400 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 162.0 (TID 324, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,400 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 162.0 (TID 325, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,401 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 162.0 (TID 324)
2017-08-09 15:28:52,401 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 162.0 (TID 325)
2017-08-09 15:28:52,402 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,402 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,405 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 162.0 (TID 324). 714 bytes result sent to driver
2017-08-09 15:28:52,405 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 162.0 (TID 325). 714 bytes result sent to driver
2017-08-09 15:28:52,406 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 162.0 (TID 325) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,406 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 162.0 (TID 324) in 6 ms on localhost (2/2)
2017-08-09 15:28:52,407 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 162.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 162 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,407 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 162 finished: foreachPartition at streamingProcess.scala:50, took 0.013489 s
2017-08-09 15:28:52,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263695000 ms.0 from job set of time 1502263695000 ms
2017-08-09 15:28:52,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 37.407 s for time 1502263695000 ms (execution: 0.024 s)
2017-08-09 15:28:52,407 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 323 from persistence list
2017-08-09 15:28:52,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263700000 ms.0 from job set of time 1502263700000 ms
2017-08-09 15:28:52,407 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 323
2017-08-09 15:28:52,407 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 322 from persistence list
2017-08-09 15:28:52,408 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 322
2017-08-09 15:28:52,408 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,408 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263685000 ms
2017-08-09 15:28:52,416 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 163 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 163 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 163 (MapPartitionsRDD[327] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_163 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-09 15:28:52,422 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_163_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-09 15:28:52,422 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_163_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,423 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 163 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,423 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 163 (MapPartitionsRDD[327] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,423 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 163.0 with 2 tasks
2017-08-09 15:28:52,423 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 163.0 (TID 326, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,424 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 163.0 (TID 327, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,424 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 163.0 (TID 327)
2017-08-09 15:28:52,424 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 163.0 (TID 326)
2017-08-09 15:28:52,426 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,426 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,429 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 163.0 (TID 326). 714 bytes result sent to driver
2017-08-09 15:28:52,429 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 163.0 (TID 327). 714 bytes result sent to driver
2017-08-09 15:28:52,430 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 163.0 (TID 326) in 7 ms on localhost (1/2)
2017-08-09 15:28:52,431 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 163.0 (TID 327) in 7 ms on localhost (2/2)
2017-08-09 15:28:52,431 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 163.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,431 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 163 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-09 15:28:52,431 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 163 finished: foreachPartition at streamingProcess.scala:50, took 0.014389 s
2017-08-09 15:28:52,431 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263700000 ms.0 from job set of time 1502263700000 ms
2017-08-09 15:28:52,431 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 32.431 s for time 1502263700000 ms (execution: 0.024 s)
2017-08-09 15:28:52,431 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 325 from persistence list
2017-08-09 15:28:52,431 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263705000 ms.0 from job set of time 1502263705000 ms
2017-08-09 15:28:52,432 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 325
2017-08-09 15:28:52,432 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 324 from persistence list
2017-08-09 15:28:52,432 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 324
2017-08-09 15:28:52,432 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,432 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263690000 ms
2017-08-09 15:28:52,442 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,442 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 164 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,443 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 164 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,443 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,443 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,443 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 164 (MapPartitionsRDD[329] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,446 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_164 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-09 15:28:52,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_164_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-09 15:28:52,448 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_164_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,449 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 164 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,449 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 164 (MapPartitionsRDD[329] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,449 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 164.0 with 2 tasks
2017-08-09 15:28:52,449 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 164.0 (TID 328, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,450 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 164.0 (TID 329, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,450 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 164.0 (TID 329)
2017-08-09 15:28:52,450 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 164.0 (TID 328)
2017-08-09 15:28:52,452 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,452 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,454 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 164.0 (TID 328). 714 bytes result sent to driver
2017-08-09 15:28:52,454 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 164.0 (TID 329). 714 bytes result sent to driver
2017-08-09 15:28:52,456 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 164.0 (TID 328) in 7 ms on localhost (1/2)
2017-08-09 15:28:52,456 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 164.0 (TID 329) in 7 ms on localhost (2/2)
2017-08-09 15:28:52,456 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 164.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,457 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 164 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,457 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 164 finished: foreachPartition at streamingProcess.scala:50, took 0.014502 s
2017-08-09 15:28:52,457 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263705000 ms.0 from job set of time 1502263705000 ms
2017-08-09 15:28:52,457 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 27.457 s for time 1502263705000 ms (execution: 0.026 s)
2017-08-09 15:28:52,457 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 327 from persistence list
2017-08-09 15:28:52,457 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263710000 ms.0 from job set of time 1502263710000 ms
2017-08-09 15:28:52,457 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 327
2017-08-09 15:28:52,458 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 326 from persistence list
2017-08-09 15:28:52,458 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 326
2017-08-09 15:28:52,458 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,458 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263695000 ms
2017-08-09 15:28:52,468 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 165 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 165 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 165 (MapPartitionsRDD[331] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,472 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_165 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-09 15:28:52,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_165_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-09 15:28:52,476 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_165_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:28:52,476 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 165 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,476 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 165 (MapPartitionsRDD[331] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,476 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 165.0 with 2 tasks
2017-08-09 15:28:52,477 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 165.0 (TID 330, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,477 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 165.0 (TID 331, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,477 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 165.0 (TID 330)
2017-08-09 15:28:52,477 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 165.0 (TID 331)
2017-08-09 15:28:52,479 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,479 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,481 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 165.0 (TID 330). 714 bytes result sent to driver
2017-08-09 15:28:52,481 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 165.0 (TID 331). 714 bytes result sent to driver
2017-08-09 15:28:52,483 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 165.0 (TID 330) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,483 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 165.0 (TID 331) in 6 ms on localhost (2/2)
2017-08-09 15:28:52,483 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 165.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,483 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 165 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,483 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 165 finished: foreachPartition at streamingProcess.scala:50, took 0.015302 s
2017-08-09 15:28:52,484 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263710000 ms.0 from job set of time 1502263710000 ms
2017-08-09 15:28:52,484 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 22.484 s for time 1502263710000 ms (execution: 0.027 s)
2017-08-09 15:28:52,484 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 329 from persistence list
2017-08-09 15:28:52,484 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 328 from persistence list
2017-08-09 15:28:52,484 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263715000 ms.0 from job set of time 1502263715000 ms
2017-08-09 15:28:52,484 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 329
2017-08-09 15:28:52,485 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 328
2017-08-09 15:28:52,485 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,485 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263700000 ms
2017-08-09 15:28:52,496 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 166 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 166 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 166 (MapPartitionsRDD[333] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,500 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_166 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-09 15:28:52,502 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_166_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-09 15:28:52,502 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_166_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,502 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 166 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,503 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 166 (MapPartitionsRDD[333] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,503 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 166.0 with 2 tasks
2017-08-09 15:28:52,503 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 166.0 (TID 332, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,504 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 166.0 (TID 333, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,504 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 166.0 (TID 332)
2017-08-09 15:28:52,504 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 166.0 (TID 333)
2017-08-09 15:28:52,506 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,506 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,509 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 166.0 (TID 332). 714 bytes result sent to driver
2017-08-09 15:28:52,509 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 166.0 (TID 333). 714 bytes result sent to driver
2017-08-09 15:28:52,511 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 166.0 (TID 332) in 8 ms on localhost (1/2)
2017-08-09 15:28:52,511 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 166.0 (TID 333) in 8 ms on localhost (2/2)
2017-08-09 15:28:52,511 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 166.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,511 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 166 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-09 15:28:52,511 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 166 finished: foreachPartition at streamingProcess.scala:50, took 0.015525 s
2017-08-09 15:28:52,512 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263715000 ms.0 from job set of time 1502263715000 ms
2017-08-09 15:28:52,512 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 17.512 s for time 1502263715000 ms (execution: 0.028 s)
2017-08-09 15:28:52,512 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 331 from persistence list
2017-08-09 15:28:52,512 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263720000 ms.0 from job set of time 1502263720000 ms
2017-08-09 15:28:52,512 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 331
2017-08-09 15:28:52,512 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 330 from persistence list
2017-08-09 15:28:52,512 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 330
2017-08-09 15:28:52,512 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,513 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263705000 ms
2017-08-09 15:28:52,522 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,523 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 167 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,523 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 167 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,523 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,523 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,523 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 167 (MapPartitionsRDD[335] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,525 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_167 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-09 15:28:52,528 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_167_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-09 15:28:52,528 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_167_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,528 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 167 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,528 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 167 (MapPartitionsRDD[335] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,528 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 167.0 with 2 tasks
2017-08-09 15:28:52,529 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 167.0 (TID 334, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,529 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 167.0 (TID 335, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,529 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 167.0 (TID 334)
2017-08-09 15:28:52,529 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 167.0 (TID 335)
2017-08-09 15:28:52,531 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,531 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,534 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 167.0 (TID 335). 801 bytes result sent to driver
2017-08-09 15:28:52,534 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 167.0 (TID 334). 801 bytes result sent to driver
2017-08-09 15:28:52,535 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 167.0 (TID 335) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,535 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 167.0 (TID 334) in 6 ms on localhost (2/2)
2017-08-09 15:28:52,535 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 167.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,535 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 167 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,535 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 167 finished: foreachPartition at streamingProcess.scala:50, took 0.012776 s
2017-08-09 15:28:52,536 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263720000 ms.0 from job set of time 1502263720000 ms
2017-08-09 15:28:52,536 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 12.536 s for time 1502263720000 ms (execution: 0.024 s)
2017-08-09 15:28:52,536 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 333 from persistence list
2017-08-09 15:28:52,536 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263725000 ms.0 from job set of time 1502263725000 ms
2017-08-09 15:28:52,536 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 333
2017-08-09 15:28:52,536 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 332 from persistence list
2017-08-09 15:28:52,536 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 332
2017-08-09 15:28:52,536 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,536 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263710000 ms
2017-08-09 15:28:52,546 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,546 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 168 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,546 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 168 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,546 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,546 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,546 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 168 (MapPartitionsRDD[337] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,548 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_168 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-09 15:28:52,550 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_168_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-09 15:28:52,550 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_168_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,551 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 168 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,551 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 168 (MapPartitionsRDD[337] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,551 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 168.0 with 2 tasks
2017-08-09 15:28:52,551 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 168.0 (TID 336, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,552 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 168.0 (TID 337, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,552 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 168.0 (TID 337)
2017-08-09 15:28:52,552 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 168.0 (TID 336)
2017-08-09 15:28:52,554 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,554 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,556 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 168.0 (TID 336). 714 bytes result sent to driver
2017-08-09 15:28:52,556 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 168.0 (TID 337). 714 bytes result sent to driver
2017-08-09 15:28:52,557 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 168.0 (TID 336) in 6 ms on localhost (1/2)
2017-08-09 15:28:52,558 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 168.0 (TID 337) in 7 ms on localhost (2/2)
2017-08-09 15:28:52,558 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 168.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,558 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 168 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-09 15:28:52,558 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 168 finished: foreachPartition at streamingProcess.scala:50, took 0.012137 s
2017-08-09 15:28:52,558 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263725000 ms.0 from job set of time 1502263725000 ms
2017-08-09 15:28:52,558 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 7.558 s for time 1502263725000 ms (execution: 0.022 s)
2017-08-09 15:28:52,558 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 335 from persistence list
2017-08-09 15:28:52,558 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263730000 ms.0 from job set of time 1502263730000 ms
2017-08-09 15:28:52,559 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 335
2017-08-09 15:28:52,559 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 334 from persistence list
2017-08-09 15:28:52,559 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 334
2017-08-09 15:28:52,559 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,559 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263715000 ms
2017-08-09 15:28:52,568 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:52,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 169 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:52,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 169 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:52,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:52,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:52,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 169 (MapPartitionsRDD[339] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:52,571 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_169 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-09 15:28:52,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_169_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-09 15:28:52,573 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_169_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:28:52,574 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 169 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:52,574 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 169 (MapPartitionsRDD[339] at map at streamingProcess.scala:48)
2017-08-09 15:28:52,574 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 169.0 with 2 tasks
2017-08-09 15:28:52,575 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 169.0 (TID 338, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:52,575 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 169.0 (TID 339, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:52,575 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 169.0 (TID 338)
2017-08-09 15:28:52,575 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 169.0 (TID 339)
2017-08-09 15:28:52,577 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:52,577 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:52,579 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 169.0 (TID 339). 714 bytes result sent to driver
2017-08-09 15:28:52,579 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 169.0 (TID 338). 714 bytes result sent to driver
2017-08-09 15:28:52,580 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 169.0 (TID 339) in 5 ms on localhost (1/2)
2017-08-09 15:28:52,580 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 169.0 (TID 338) in 6 ms on localhost (2/2)
2017-08-09 15:28:52,580 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 169.0, whose tasks have all completed, from pool 
2017-08-09 15:28:52,580 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 169 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-09 15:28:52,580 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 169 finished: foreachPartition at streamingProcess.scala:50, took 0.011870 s
2017-08-09 15:28:52,581 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263730000 ms.0 from job set of time 1502263730000 ms
2017-08-09 15:28:52,581 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.580 s for time 1502263730000 ms (execution: 0.022 s)
2017-08-09 15:28:52,581 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 337 from persistence list
2017-08-09 15:28:52,581 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 337
2017-08-09 15:28:52,581 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 336 from persistence list
2017-08-09 15:28:52,581 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 336
2017-08-09 15:28:52,581 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:52,581 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263720000 ms
2017-08-09 15:28:55,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263735000 ms
2017-08-09 15:28:55,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263735000 ms.0 from job set of time 1502263735000 ms
2017-08-09 15:28:55,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:28:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 170 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:28:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 170 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:28:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:28:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:28:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 170 (MapPartitionsRDD[341] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:28:55,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_170 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-09 15:28:55,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_170_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-09 15:28:55,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_170_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:28:55,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 170 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:28:55,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 170 (MapPartitionsRDD[341] at map at streamingProcess.scala:48)
2017-08-09 15:28:55,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 170.0 with 2 tasks
2017-08-09 15:28:55,037 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 170.0 (TID 340, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:28:55,037 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 170.0 (TID 341, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:28:55,038 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 170.0 (TID 340)
2017-08-09 15:28:55,038 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 170.0 (TID 341)
2017-08-09 15:28:55,040 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:28:55,040 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:28:55,043 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 170.0 (TID 341). 714 bytes result sent to driver
2017-08-09 15:28:55,043 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 170.0 (TID 340). 714 bytes result sent to driver
2017-08-09 15:28:55,045 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 170.0 (TID 341) in 8 ms on localhost (1/2)
2017-08-09 15:28:55,045 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 170.0 (TID 340) in 9 ms on localhost (2/2)
2017-08-09 15:28:55,045 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 170.0, whose tasks have all completed, from pool 
2017-08-09 15:28:55,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 170 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-09 15:28:55,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 170 finished: foreachPartition at streamingProcess.scala:50, took 0.016644 s
2017-08-09 15:28:55,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263735000 ms.0 from job set of time 1502263735000 ms
2017-08-09 15:28:55,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.046 s for time 1502263735000 ms (execution: 0.030 s)
2017-08-09 15:28:55,046 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 339 from persistence list
2017-08-09 15:28:55,046 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 339
2017-08-09 15:28:55,046 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 338 from persistence list
2017-08-09 15:28:55,046 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 338
2017-08-09 15:28:55,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:28:55,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263725000 ms
2017-08-09 15:29:00,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263740000 ms
2017-08-09 15:29:00,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263740000 ms.0 from job set of time 1502263740000 ms
2017-08-09 15:29:00,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 171 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 171 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:00,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:00,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 171 (MapPartitionsRDD[343] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_171 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-09 15:29:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_171_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-09 15:29:00,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_171_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:29:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 171 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 171 (MapPartitionsRDD[343] at map at streamingProcess.scala:48)
2017-08-09 15:29:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 171.0 with 2 tasks
2017-08-09 15:29:00,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 171.0 (TID 342, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:00,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 171.0 (TID 343, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:00,056 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 171.0 (TID 343)
2017-08-09 15:29:00,056 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 171.0 (TID 342)
2017-08-09 15:29:00,058 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:00,058 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:00,061 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 171.0 (TID 342). 714 bytes result sent to driver
2017-08-09 15:29:00,061 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 171.0 (TID 343). 714 bytes result sent to driver
2017-08-09 15:29:00,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 171.0 (TID 342) in 8 ms on localhost (1/2)
2017-08-09 15:29:00,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 171.0 (TID 343) in 8 ms on localhost (2/2)
2017-08-09 15:29:00,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 171.0, whose tasks have all completed, from pool 
2017-08-09 15:29:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 171 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-09 15:29:00,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 171 finished: foreachPartition at streamingProcess.scala:50, took 0.021332 s
2017-08-09 15:29:00,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263740000 ms.0 from job set of time 1502263740000 ms
2017-08-09 15:29:00,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1502263740000 ms (execution: 0.045 s)
2017-08-09 15:29:00,064 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 341 from persistence list
2017-08-09 15:29:00,064 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 341
2017-08-09 15:29:00,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 340 from persistence list
2017-08-09 15:29:00,064 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 340
2017-08-09 15:29:00,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:00,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263730000 ms
2017-08-09 15:29:05,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263745000 ms
2017-08-09 15:29:05,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263745000 ms.0 from job set of time 1502263745000 ms
2017-08-09 15:29:05,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:05,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 172 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:05,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 172 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:05,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:05,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:05,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 172 (MapPartitionsRDD[345] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:05,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_172 stored as values in memory (estimated size 34.5 KB, free 412.9 MB)
2017-08-09 15:29:05,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_172_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-09 15:29:05,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_172_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:29:05,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_156_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:29:05,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 172 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:05,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 172 (MapPartitionsRDD[345] at map at streamingProcess.scala:48)
2017-08-09 15:29:05,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 172.0 with 2 tasks
2017-08-09 15:29:05,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_157_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:29:05,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 172.0 (TID 344, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:05,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 172.0 (TID 345, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:05,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_158_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:29:05,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 172.0 (TID 344)
2017-08-09 15:29:05,072 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 172.0 (TID 345)
2017-08-09 15:29:05,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_159_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:29:05,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_160_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:29:05,074 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:05,074 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:05,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_161_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:29:05,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_162_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:05,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_163_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:05,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_164_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:05,077 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 172.0 (TID 345). 714 bytes result sent to driver
2017-08-09 15:29:05,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 172.0 (TID 344). 714 bytes result sent to driver
2017-08-09 15:29:05,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_165_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:05,078 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_166_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:05,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 172.0 (TID 344) in 7 ms on localhost (1/2)
2017-08-09 15:29:05,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 172.0 (TID 345) in 8 ms on localhost (2/2)
2017-08-09 15:29:05,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 172.0, whose tasks have all completed, from pool 
2017-08-09 15:29:05,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 172 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-09 15:29:05,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 172 finished: foreachPartition at streamingProcess.scala:50, took 0.033358 s
2017-08-09 15:29:05,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_167_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:05,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263745000 ms.0 from job set of time 1502263745000 ms
2017-08-09 15:29:05,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1502263745000 ms (execution: 0.060 s)
2017-08-09 15:29:05,080 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 343 from persistence list
2017-08-09 15:29:05,080 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 343
2017-08-09 15:29:05,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 342 from persistence list
2017-08-09 15:29:05,080 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_168_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:05,080 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 342
2017-08-09 15:29:05,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:05,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263735000 ms
2017-08-09 15:29:05,081 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_169_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:05,082 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_170_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:29:05,083 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_171_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:29:10,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263750000 ms
2017-08-09 15:29:10,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263750000 ms.0 from job set of time 1502263750000 ms
2017-08-09 15:29:10,035 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:10,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 173 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:10,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 173 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:10,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:10,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:10,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 173 (MapPartitionsRDD[347] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_173 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-09 15:29:10,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_173_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-09 15:29:10,044 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_173_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:29:10,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 173 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:10,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 173 (MapPartitionsRDD[347] at map at streamingProcess.scala:48)
2017-08-09 15:29:10,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 173.0 with 2 tasks
2017-08-09 15:29:10,046 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 173.0 (TID 346, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:10,046 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 173.0 (TID 347, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:10,046 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 173.0 (TID 347)
2017-08-09 15:29:10,046 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 173.0 (TID 346)
2017-08-09 15:29:10,049 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:10,049 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:10,053 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 173.0 (TID 346). 714 bytes result sent to driver
2017-08-09 15:29:10,053 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 173.0 (TID 347). 714 bytes result sent to driver
2017-08-09 15:29:10,055 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 173.0 (TID 346) in 10 ms on localhost (1/2)
2017-08-09 15:29:10,055 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 173.0 (TID 347) in 9 ms on localhost (2/2)
2017-08-09 15:29:10,055 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 173.0, whose tasks have all completed, from pool 
2017-08-09 15:29:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 173 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-09 15:29:10,056 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 173 finished: foreachPartition at streamingProcess.scala:50, took 0.020808 s
2017-08-09 15:29:10,056 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263750000 ms.0 from job set of time 1502263750000 ms
2017-08-09 15:29:10,056 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.056 s for time 1502263750000 ms (execution: 0.038 s)
2017-08-09 15:29:10,056 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 345 from persistence list
2017-08-09 15:29:10,057 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 345
2017-08-09 15:29:10,057 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 344 from persistence list
2017-08-09 15:29:10,057 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 344
2017-08-09 15:29:10,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:10,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263740000 ms
2017-08-09 15:29:15,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263755000 ms
2017-08-09 15:29:15,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263755000 ms.0 from job set of time 1502263755000 ms
2017-08-09 15:29:15,056 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 174 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 174 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:15,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 174 (MapPartitionsRDD[349] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_174 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-09 15:29:15,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_174_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-09 15:29:15,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_174_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 174 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 174 (MapPartitionsRDD[349] at map at streamingProcess.scala:48)
2017-08-09 15:29:15,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 174.0 with 2 tasks
2017-08-09 15:29:15,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 174.0 (TID 348, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:15,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 174.0 (TID 349, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:15,073 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 174.0 (TID 348)
2017-08-09 15:29:15,073 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 174.0 (TID 349)
2017-08-09 15:29:15,077 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:15,077 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:15,082 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 174.0 (TID 348). 714 bytes result sent to driver
2017-08-09 15:29:15,082 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 174.0 (TID 349). 714 bytes result sent to driver
2017-08-09 15:29:15,085 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 174.0 (TID 348) in 15 ms on localhost (1/2)
2017-08-09 15:29:15,085 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 174.0 (TID 349) in 13 ms on localhost (2/2)
2017-08-09 15:29:15,085 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 174.0, whose tasks have all completed, from pool 
2017-08-09 15:29:15,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 174 (foreachPartition at streamingProcess.scala:50) finished in 0.015 s
2017-08-09 15:29:15,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 174 finished: foreachPartition at streamingProcess.scala:50, took 0.029234 s
2017-08-09 15:29:15,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263755000 ms.0 from job set of time 1502263755000 ms
2017-08-09 15:29:15,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.086 s for time 1502263755000 ms (execution: 0.064 s)
2017-08-09 15:29:15,087 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 347 from persistence list
2017-08-09 15:29:15,087 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 347
2017-08-09 15:29:15,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 346 from persistence list
2017-08-09 15:29:15,088 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 346
2017-08-09 15:29:15,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:15,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263745000 ms
2017-08-09 15:29:20,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263760000 ms
2017-08-09 15:29:20,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263760000 ms.0 from job set of time 1502263760000 ms
2017-08-09 15:29:20,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 175 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 175 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 175 (MapPartitionsRDD[351] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:20,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_175 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-09 15:29:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_175_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-09 15:29:20,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_175_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 175 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 175 (MapPartitionsRDD[351] at map at streamingProcess.scala:48)
2017-08-09 15:29:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 175.0 with 2 tasks
2017-08-09 15:29:20,037 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 175.0 (TID 350, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:20,037 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 175.0 (TID 351, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:20,037 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 175.0 (TID 350)
2017-08-09 15:29:20,039 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 175.0 (TID 351)
2017-08-09 15:29:20,041 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:20,041 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:20,044 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 175.0 (TID 351). 714 bytes result sent to driver
2017-08-09 15:29:20,044 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 175.0 (TID 350). 714 bytes result sent to driver
2017-08-09 15:29:20,046 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 175.0 (TID 350) in 10 ms on localhost (1/2)
2017-08-09 15:29:20,046 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 175.0 (TID 351) in 9 ms on localhost (2/2)
2017-08-09 15:29:20,046 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 175.0, whose tasks have all completed, from pool 
2017-08-09 15:29:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 175 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-09 15:29:20,046 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 175 finished: foreachPartition at streamingProcess.scala:50, took 0.016840 s
2017-08-09 15:29:20,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263760000 ms.0 from job set of time 1502263760000 ms
2017-08-09 15:29:20,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.047 s for time 1502263760000 ms (execution: 0.033 s)
2017-08-09 15:29:20,047 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 349 from persistence list
2017-08-09 15:29:20,047 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 349
2017-08-09 15:29:20,047 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 348 from persistence list
2017-08-09 15:29:20,048 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 348
2017-08-09 15:29:20,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:20,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263750000 ms
2017-08-09 15:29:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263765000 ms
2017-08-09 15:29:25,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263765000 ms.0 from job set of time 1502263765000 ms
2017-08-09 15:29:25,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:25,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 176 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:25,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 176 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:25,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:25,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:25,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 176 (MapPartitionsRDD[353] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:25,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_176 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-09 15:29:25,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_176_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-09 15:29:25,033 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_176_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:25,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 176 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:25,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 176 (MapPartitionsRDD[353] at map at streamingProcess.scala:48)
2017-08-09 15:29:25,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 176.0 with 2 tasks
2017-08-09 15:29:25,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 176.0 (TID 352, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:25,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 176.0 (TID 353, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:25,035 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 176.0 (TID 352)
2017-08-09 15:29:25,035 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 176.0 (TID 353)
2017-08-09 15:29:25,037 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:25,037 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:25,042 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 176.0 (TID 352). 714 bytes result sent to driver
2017-08-09 15:29:25,042 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 176.0 (TID 353). 714 bytes result sent to driver
2017-08-09 15:29:25,043 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 176.0 (TID 353) in 8 ms on localhost (1/2)
2017-08-09 15:29:25,044 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 176.0 (TID 352) in 10 ms on localhost (2/2)
2017-08-09 15:29:25,044 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 176.0, whose tasks have all completed, from pool 
2017-08-09 15:29:25,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 176 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-09 15:29:25,044 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 176 finished: foreachPartition at streamingProcess.scala:50, took 0.018065 s
2017-08-09 15:29:25,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263765000 ms.0 from job set of time 1502263765000 ms
2017-08-09 15:29:25,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.044 s for time 1502263765000 ms (execution: 0.031 s)
2017-08-09 15:29:25,045 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 351 from persistence list
2017-08-09 15:29:25,045 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 350 from persistence list
2017-08-09 15:29:25,045 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 351
2017-08-09 15:29:25,045 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 350
2017-08-09 15:29:25,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:25,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263755000 ms
2017-08-09 15:29:30,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263770000 ms
2017-08-09 15:29:30,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263770000 ms.0 from job set of time 1502263770000 ms
2017-08-09 15:29:30,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 177 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 177 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 177 (MapPartitionsRDD[355] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_177 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-09 15:29:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_177_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-09 15:29:30,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_177_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:29:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 177 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 177 (MapPartitionsRDD[355] at map at streamingProcess.scala:48)
2017-08-09 15:29:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 177.0 with 2 tasks
2017-08-09 15:29:30,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 177.0 (TID 354, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:30,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 177.0 (TID 355, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:30,067 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 177.0 (TID 355)
2017-08-09 15:29:30,067 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 177.0 (TID 354)
2017-08-09 15:29:30,070 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:30,070 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:30,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 177.0 (TID 355). 714 bytes result sent to driver
2017-08-09 15:29:30,074 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 177.0 (TID 354). 714 bytes result sent to driver
2017-08-09 15:29:30,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 177.0 (TID 355) in 12 ms on localhost (1/2)
2017-08-09 15:29:30,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 177.0 (TID 354) in 13 ms on localhost (2/2)
2017-08-09 15:29:30,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 177.0, whose tasks have all completed, from pool 
2017-08-09 15:29:30,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 177 (foreachPartition at streamingProcess.scala:50) finished in 0.014 s
2017-08-09 15:29:30,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 177 finished: foreachPartition at streamingProcess.scala:50, took 0.027696 s
2017-08-09 15:29:30,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263770000 ms.0 from job set of time 1502263770000 ms
2017-08-09 15:29:30,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1502263770000 ms (execution: 0.062 s)
2017-08-09 15:29:30,080 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 353 from persistence list
2017-08-09 15:29:30,081 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 353
2017-08-09 15:29:30,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 352 from persistence list
2017-08-09 15:29:30,081 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 352
2017-08-09 15:29:30,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:30,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263760000 ms
2017-08-09 15:29:35,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263775000 ms
2017-08-09 15:29:35,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263775000 ms.0 from job set of time 1502263775000 ms
2017-08-09 15:29:35,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 178 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 178 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 178 (MapPartitionsRDD[357] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:35,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_178 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-09 15:29:35,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_178_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-09 15:29:35,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_178_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:35,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 178 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:35,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 178 (MapPartitionsRDD[357] at map at streamingProcess.scala:48)
2017-08-09 15:29:35,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 178.0 with 2 tasks
2017-08-09 15:29:35,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 178.0 (TID 356, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:35,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 178.0 (TID 357, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:35,058 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 178.0 (TID 356)
2017-08-09 15:29:35,058 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 178.0 (TID 357)
2017-08-09 15:29:35,061 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:35,061 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:35,065 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 178.0 (TID 356). 714 bytes result sent to driver
2017-08-09 15:29:35,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 178.0 (TID 357). 714 bytes result sent to driver
2017-08-09 15:29:35,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 178.0 (TID 356) in 11 ms on localhost (1/2)
2017-08-09 15:29:35,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 178.0 (TID 357) in 10 ms on localhost (2/2)
2017-08-09 15:29:35,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 178.0, whose tasks have all completed, from pool 
2017-08-09 15:29:35,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 178 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-09 15:29:35,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 178 finished: foreachPartition at streamingProcess.scala:50, took 0.023330 s
2017-08-09 15:29:35,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263775000 ms.0 from job set of time 1502263775000 ms
2017-08-09 15:29:35,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1502263775000 ms (execution: 0.049 s)
2017-08-09 15:29:35,068 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 355 from persistence list
2017-08-09 15:29:35,068 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 355
2017-08-09 15:29:35,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 354 from persistence list
2017-08-09 15:29:35,069 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 354
2017-08-09 15:29:35,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:35,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263765000 ms
2017-08-09 15:29:40,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263780000 ms
2017-08-09 15:29:40,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263780000 ms.0 from job set of time 1502263780000 ms
2017-08-09 15:29:40,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 179 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 179 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 179 (MapPartitionsRDD[359] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_179 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-09 15:29:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_179_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-09 15:29:40,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_179_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 179 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 179 (MapPartitionsRDD[359] at map at streamingProcess.scala:48)
2017-08-09 15:29:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 179.0 with 2 tasks
2017-08-09 15:29:40,053 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 179.0 (TID 358, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:40,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 179.0 (TID 359, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:40,054 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 179.0 (TID 358)
2017-08-09 15:29:40,056 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 179.0 (TID 359)
2017-08-09 15:29:40,057 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:40,058 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:40,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 179.0 (TID 358). 714 bytes result sent to driver
2017-08-09 15:29:40,061 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 179.0 (TID 359). 714 bytes result sent to driver
2017-08-09 15:29:40,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 179.0 (TID 358) in 9 ms on localhost (1/2)
2017-08-09 15:29:40,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 179.0 (TID 359) in 10 ms on localhost (2/2)
2017-08-09 15:29:40,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 179.0, whose tasks have all completed, from pool 
2017-08-09 15:29:40,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 179 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-09 15:29:40,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 179 finished: foreachPartition at streamingProcess.scala:50, took 0.020867 s
2017-08-09 15:29:40,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263780000 ms.0 from job set of time 1502263780000 ms
2017-08-09 15:29:40,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1502263780000 ms (execution: 0.046 s)
2017-08-09 15:29:40,064 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 357 from persistence list
2017-08-09 15:29:40,064 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 357
2017-08-09 15:29:40,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 356 from persistence list
2017-08-09 15:29:40,064 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 356
2017-08-09 15:29:40,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:40,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263770000 ms
2017-08-09 15:29:45,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263785000 ms
2017-08-09 15:29:45,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263785000 ms.0 from job set of time 1502263785000 ms
2017-08-09 15:29:45,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 180 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 180 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 180 (MapPartitionsRDD[361] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:45,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_180 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-09 15:29:45,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_180_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-09 15:29:45,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_180_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:45,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 180 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:45,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 180 (MapPartitionsRDD[361] at map at streamingProcess.scala:48)
2017-08-09 15:29:45,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 180.0 with 2 tasks
2017-08-09 15:29:45,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 180.0 (TID 360, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:45,076 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 180.0 (TID 361, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:45,078 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 180.0 (TID 360)
2017-08-09 15:29:45,078 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 180.0 (TID 361)
2017-08-09 15:29:45,085 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:45,085 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:45,089 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 180.0 (TID 360). 714 bytes result sent to driver
2017-08-09 15:29:45,089 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 180.0 (TID 361). 714 bytes result sent to driver
2017-08-09 15:29:45,092 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 180.0 (TID 361) in 17 ms on localhost (1/2)
2017-08-09 15:29:45,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 180.0 (TID 360) in 18 ms on localhost (2/2)
2017-08-09 15:29:45,093 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 180.0, whose tasks have all completed, from pool 
2017-08-09 15:29:45,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 180 (foreachPartition at streamingProcess.scala:50) finished in 0.019 s
2017-08-09 15:29:45,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 180 finished: foreachPartition at streamingProcess.scala:50, took 0.039137 s
2017-08-09 15:29:45,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263785000 ms.0 from job set of time 1502263785000 ms
2017-08-09 15:29:45,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.094 s for time 1502263785000 ms (execution: 0.075 s)
2017-08-09 15:29:45,094 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 359 from persistence list
2017-08-09 15:29:45,094 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 359
2017-08-09 15:29:45,094 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 358 from persistence list
2017-08-09 15:29:45,095 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 358
2017-08-09 15:29:45,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:45,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263775000 ms
2017-08-09 15:29:50,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263790000 ms
2017-08-09 15:29:50,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263790000 ms.0 from job set of time 1502263790000 ms
2017-08-09 15:29:50,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 181 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 181 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 181 (MapPartitionsRDD[363] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:50,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_181 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-09 15:29:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_181_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-09 15:29:50,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_181_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:29:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 181 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:50,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 181 (MapPartitionsRDD[363] at map at streamingProcess.scala:48)
2017-08-09 15:29:50,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 181.0 with 2 tasks
2017-08-09 15:29:50,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 181.0 (TID 362, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:50,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 181.0 (TID 363, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:50,072 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 181.0 (TID 363)
2017-08-09 15:29:50,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 181.0 (TID 362)
2017-08-09 15:29:50,075 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:50,076 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:50,078 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 181.0 (TID 362). 714 bytes result sent to driver
2017-08-09 15:29:50,080 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 181.0 (TID 363). 714 bytes result sent to driver
2017-08-09 15:29:50,081 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 181.0 (TID 362) in 11 ms on localhost (1/2)
2017-08-09 15:29:50,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 181.0 (TID 363) in 11 ms on localhost (2/2)
2017-08-09 15:29:50,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 181.0, whose tasks have all completed, from pool 
2017-08-09 15:29:50,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 181 (foreachPartition at streamingProcess.scala:50) finished in 0.012 s
2017-08-09 15:29:50,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 181 finished: foreachPartition at streamingProcess.scala:50, took 0.029459 s
2017-08-09 15:29:50,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263790000 ms.0 from job set of time 1502263790000 ms
2017-08-09 15:29:50,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1502263790000 ms (execution: 0.064 s)
2017-08-09 15:29:50,083 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 361 from persistence list
2017-08-09 15:29:50,084 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 361
2017-08-09 15:29:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 360 from persistence list
2017-08-09 15:29:50,084 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 360
2017-08-09 15:29:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263780000 ms
2017-08-09 15:29:55,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263795000 ms
2017-08-09 15:29:55,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263795000 ms.0 from job set of time 1502263795000 ms
2017-08-09 15:29:55,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:29:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 182 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:29:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 182 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:29:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:29:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:29:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 182 (MapPartitionsRDD[365] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:29:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_182 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-09 15:29:55,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_182_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-09 15:29:55,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_182_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:29:55,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 182 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:29:55,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 182 (MapPartitionsRDD[365] at map at streamingProcess.scala:48)
2017-08-09 15:29:55,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 182.0 with 2 tasks
2017-08-09 15:29:55,075 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 182.0 (TID 364, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:29:55,076 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 182.0 (TID 365, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:29:55,076 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 182.0 (TID 365)
2017-08-09 15:29:55,076 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 182.0 (TID 364)
2017-08-09 15:29:55,080 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:29:55,080 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:29:55,084 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 182.0 (TID 365). 714 bytes result sent to driver
2017-08-09 15:29:55,084 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 182.0 (TID 364). 714 bytes result sent to driver
2017-08-09 15:29:55,086 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 182.0 (TID 365) in 11 ms on localhost (1/2)
2017-08-09 15:29:55,086 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 182.0 (TID 364) in 12 ms on localhost (2/2)
2017-08-09 15:29:55,086 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 182.0, whose tasks have all completed, from pool 
2017-08-09 15:29:55,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 182 (foreachPartition at streamingProcess.scala:50) finished in 0.012 s
2017-08-09 15:29:55,087 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 182 finished: foreachPartition at streamingProcess.scala:50, took 0.032846 s
2017-08-09 15:29:55,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263795000 ms.0 from job set of time 1502263795000 ms
2017-08-09 15:29:55,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1502263795000 ms (execution: 0.068 s)
2017-08-09 15:29:55,087 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 363 from persistence list
2017-08-09 15:29:55,088 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 363
2017-08-09 15:29:55,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 362 from persistence list
2017-08-09 15:29:55,088 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 362
2017-08-09 15:29:55,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:29:55,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263785000 ms
2017-08-09 15:30:00,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263800000 ms
2017-08-09 15:30:00,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263800000 ms.0 from job set of time 1502263800000 ms
2017-08-09 15:30:00,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:30:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 183 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:30:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 183 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:30:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:30:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:30:00,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 183 (MapPartitionsRDD[367] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:30:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_183 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-09 15:30:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_183_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-09 15:30:00,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_183_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:00,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 183 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:30:00,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 183 (MapPartitionsRDD[367] at map at streamingProcess.scala:48)
2017-08-09 15:30:00,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 183.0 with 2 tasks
2017-08-09 15:30:00,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 183.0 (TID 366, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:30:00,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 183.0 (TID 367, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:30:00,069 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 183.0 (TID 366)
2017-08-09 15:30:00,069 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 183.0 (TID 367)
2017-08-09 15:30:00,072 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:30:00,072 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:30:00,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 183.0 (TID 367). 714 bytes result sent to driver
2017-08-09 15:30:00,077 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 183.0 (TID 366). 714 bytes result sent to driver
2017-08-09 15:30:00,079 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 183.0 (TID 367) in 11 ms on localhost (1/2)
2017-08-09 15:30:00,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 183.0 (TID 366) in 11 ms on localhost (2/2)
2017-08-09 15:30:00,081 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 183.0, whose tasks have all completed, from pool 
2017-08-09 15:30:00,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 183 (foreachPartition at streamingProcess.scala:50) finished in 0.014 s
2017-08-09 15:30:00,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 183 finished: foreachPartition at streamingProcess.scala:50, took 0.027836 s
2017-08-09 15:30:00,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263800000 ms.0 from job set of time 1502263800000 ms
2017-08-09 15:30:00,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1502263800000 ms (execution: 0.062 s)
2017-08-09 15:30:00,082 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 365 from persistence list
2017-08-09 15:30:00,082 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 365
2017-08-09 15:30:00,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 364 from persistence list
2017-08-09 15:30:00,083 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 364
2017-08-09 15:30:00,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:30:00,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263790000 ms
2017-08-09 15:30:05,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263805000 ms
2017-08-09 15:30:05,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263805000 ms.0 from job set of time 1502263805000 ms
2017-08-09 15:30:05,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:30:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 184 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:30:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 184 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:30:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:30:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:30:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 184 (MapPartitionsRDD[369] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:30:05,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_184 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-09 15:30:05,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_184_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-09 15:30:05,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_184_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:05,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 184 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:30:05,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 184 (MapPartitionsRDD[369] at map at streamingProcess.scala:48)
2017-08-09 15:30:05,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 184.0 with 2 tasks
2017-08-09 15:30:05,053 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 184.0 (TID 368, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:30:05,053 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 184.0 (TID 369, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:30:05,054 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 184.0 (TID 369)
2017-08-09 15:30:05,054 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 184.0 (TID 368)
2017-08-09 15:30:05,056 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:30:05,056 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:30:05,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 184.0 (TID 369). 714 bytes result sent to driver
2017-08-09 15:30:05,060 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 184.0 (TID 368). 714 bytes result sent to driver
2017-08-09 15:30:05,061 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 184.0 (TID 369) in 8 ms on localhost (1/2)
2017-08-09 15:30:05,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 184.0 (TID 368) in 8 ms on localhost (2/2)
2017-08-09 15:30:05,062 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 184.0, whose tasks have all completed, from pool 
2017-08-09 15:30:05,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 184 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-09 15:30:05,062 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 184 finished: foreachPartition at streamingProcess.scala:50, took 0.018723 s
2017-08-09 15:30:05,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263805000 ms.0 from job set of time 1502263805000 ms
2017-08-09 15:30:05,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.062 s for time 1502263805000 ms (execution: 0.043 s)
2017-08-09 15:30:05,063 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 367 from persistence list
2017-08-09 15:30:05,063 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 367
2017-08-09 15:30:05,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 366 from persistence list
2017-08-09 15:30:05,063 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 366
2017-08-09 15:30:05,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:30:05,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263795000 ms
2017-08-09 15:30:10,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263810000 ms
2017-08-09 15:30:10,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263810000 ms.0 from job set of time 1502263810000 ms
2017-08-09 15:30:10,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:30:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 185 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:30:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 185 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:30:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:30:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:30:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 185 (MapPartitionsRDD[371] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:30:10,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_185 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-09 15:30:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_185_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-09 15:30:10,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_185_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 185 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:30:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 185 (MapPartitionsRDD[371] at map at streamingProcess.scala:48)
2017-08-09 15:30:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 185.0 with 2 tasks
2017-08-09 15:30:10,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 185.0 (TID 370, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:30:10,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 185.0 (TID 371, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:30:10,054 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 185.0 (TID 371)
2017-08-09 15:30:10,054 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 185.0 (TID 370)
2017-08-09 15:30:10,057 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:30:10,057 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:30:10,062 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 185.0 (TID 371). 714 bytes result sent to driver
2017-08-09 15:30:10,062 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 185.0 (TID 370). 714 bytes result sent to driver
2017-08-09 15:30:10,064 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 185.0 (TID 371) in 11 ms on localhost (1/2)
2017-08-09 15:30:10,064 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 185.0 (TID 370) in 12 ms on localhost (2/2)
2017-08-09 15:30:10,065 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 185.0, whose tasks have all completed, from pool 
2017-08-09 15:30:10,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 185 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-09 15:30:10,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 185 finished: foreachPartition at streamingProcess.scala:50, took 0.026123 s
2017-08-09 15:30:10,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263810000 ms.0 from job set of time 1502263810000 ms
2017-08-09 15:30:10,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1502263810000 ms (execution: 0.048 s)
2017-08-09 15:30:10,066 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 369 from persistence list
2017-08-09 15:30:10,066 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 369
2017-08-09 15:30:10,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 368 from persistence list
2017-08-09 15:30:10,067 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 368
2017-08-09 15:30:10,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:30:10,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263800000 ms
2017-08-09 15:30:15,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502263815000 ms
2017-08-09 15:30:15,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502263815000 ms.0 from job set of time 1502263815000 ms
2017-08-09 15:30:15,028 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-09 15:30:15,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 186 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-09 15:30:15,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 186 (foreachPartition at streamingProcess.scala:50)
2017-08-09 15:30:15,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-09 15:30:15,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-09 15:30:15,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 186 (MapPartitionsRDD[373] at map at streamingProcess.scala:48), which has no missing parents
2017-08-09 15:30:15,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_186 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-09 15:30:15,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_186_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-09 15:30:15,036 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_186_piece0 in memory on 192.168.31.111:50628 (size: 25.1 KB, free: 413.5 MB)
2017-08-09 15:30:15,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 186 from broadcast at DAGScheduler.scala:1012
2017-08-09 15:30:15,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 186 (MapPartitionsRDD[373] at map at streamingProcess.scala:48)
2017-08-09 15:30:15,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 186.0 with 2 tasks
2017-08-09 15:30:15,038 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 186.0 (TID 372, localhost, partition 0, ANY, 5657 bytes)
2017-08-09 15:30:15,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 186.0 (TID 373, localhost, partition 1, ANY, 5657 bytes)
2017-08-09 15:30:15,040 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 186.0 (TID 372)
2017-08-09 15:30:15,040 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 186.0 (TID 373)
2017-08-09 15:30:15,042 [Executor task launch worker-5] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-09 15:30:15,042 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-09 15:30:15,045 [Executor task launch worker-5] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 186.0 (TID 372). 714 bytes result sent to driver
2017-08-09 15:30:15,045 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 186.0 (TID 373). 714 bytes result sent to driver
2017-08-09 15:30:15,047 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 186.0 (TID 372) in 10 ms on localhost (1/2)
2017-08-09 15:30:15,047 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 186.0 (TID 373) in 9 ms on localhost (2/2)
2017-08-09 15:30:15,047 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 186.0, whose tasks have all completed, from pool 
2017-08-09 15:30:15,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 186 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-09 15:30:15,048 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 186 finished: foreachPartition at streamingProcess.scala:50, took 0.019406 s
2017-08-09 15:30:15,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502263815000 ms.0 from job set of time 1502263815000 ms
2017-08-09 15:30:15,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1502263815000 ms (execution: 0.033 s)
2017-08-09 15:30:15,048 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 371 from persistence list
2017-08-09 15:30:15,049 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 371
2017-08-09 15:30:15,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 370 from persistence list
2017-08-09 15:30:15,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-09 15:30:15,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502263805000 ms
2017-08-09 15:30:15,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_186_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:15,069 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 370
2017-08-09 15:30:15,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_172_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:15,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_173_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:15,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_174_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-09 15:30:15,079 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_175_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:30:15,080 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_176_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:30:15,081 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_177_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:30:15,082 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_178_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-09 15:30:15,083 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_179_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:30:15,083 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_180_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:30:15,085 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_181_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:30:15,087 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_182_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-09 15:30:15,089 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_183_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:30:15,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_184_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-09 15:30:15,093 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_185_piece0 on 192.168.31.111:50628 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:01:46,166 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-10 09:01:46,795 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-10 09:01:47,767 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-10 09:01:47,768 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-10 09:01:47,769 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-10 09:01:47,770 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-10 09:01:47,771 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-10 09:01:48,700 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 49883.
2017-08-10 09:01:48,755 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-10 09:01:48,811 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-10 09:01:48,863 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-7037e1a9-a04b-4253-84e7-89ca66d541e3
2017-08-10 09:01:48,898 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-10 09:01:48,977 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-10 09:01:49,161 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @4525ms
2017-08-10 09:01:49,317 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-10 09:01:49,335 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e70ea3{/jobs,null,AVAILABLE}
2017-08-10 09:01:49,335 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fe7086{/jobs/json,null,AVAILABLE}
2017-08-10 09:01:49,335 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15d8c96{/jobs/job,null,AVAILABLE}
2017-08-10 09:01:49,336 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/jobs/job/json,null,AVAILABLE}
2017-08-10 09:01:49,336 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/stages,null,AVAILABLE}
2017-08-10 09:01:49,336 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/stages/json,null,AVAILABLE}
2017-08-10 09:01:49,336 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/stages/stage,null,AVAILABLE}
2017-08-10 09:01:49,337 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/stages/stage/json,null,AVAILABLE}
2017-08-10 09:01:49,337 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/stages/pool,null,AVAILABLE}
2017-08-10 09:01:49,337 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/stages/pool/json,null,AVAILABLE}
2017-08-10 09:01:49,337 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/storage,null,AVAILABLE}
2017-08-10 09:01:49,337 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/storage/json,null,AVAILABLE}
2017-08-10 09:01:49,337 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/storage/rdd,null,AVAILABLE}
2017-08-10 09:01:49,338 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/storage/rdd/json,null,AVAILABLE}
2017-08-10 09:01:49,338 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/environment,null,AVAILABLE}
2017-08-10 09:01:49,338 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/environment/json,null,AVAILABLE}
2017-08-10 09:01:49,338 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/executors,null,AVAILABLE}
2017-08-10 09:01:49,338 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/executors/json,null,AVAILABLE}
2017-08-10 09:01:49,338 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0fba8{/executors/threadDump,null,AVAILABLE}
2017-08-10 09:01:49,339 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5de5a4{/executors/threadDump/json,null,AVAILABLE}
2017-08-10 09:01:49,345 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1ef6856{/static,null,AVAILABLE}
2017-08-10 09:01:49,346 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@b7a938{/,null,AVAILABLE}
2017-08-10 09:01:49,346 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1af2e7d{/api,null,AVAILABLE}
2017-08-10 09:01:49,347 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@648ce9{/stages/stage/kill,null,AVAILABLE}
2017-08-10 09:01:49,354 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@73c5a4{HTTP/1.1}{0.0.0.0:4040}
2017-08-10 09:01:49,355 [main] INFO  [org.spark_project.jetty.server.Server] - Started @4718ms
2017-08-10 09:01:49,355 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-10 09:01:49,358 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-10 09:01:49,506 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-10 09:01:49,547 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49904.
2017-08-10 09:01:49,547 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:49904
2017-08-10 09:01:49,569 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 49904)
2017-08-10 09:01:49,573 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:49904 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 49904)
2017-08-10 09:01:49,578 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 49904)
2017-08-10 09:01:49,813 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15f4131{/metrics/json,null,AVAILABLE}
2017-08-10 09:01:50,698 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-08-10 09:01:50,699 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-10 09:01:50,699 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-10 09:01:50,700 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-08-10 09:01:50,700 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@1f545f2
2017-08-10 09:01:50,701 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-08-10 09:01:50,701 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-10 09:01:50,701 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-10 09:01:50,701 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-08-10 09:01:50,701 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@117304d
2017-08-10 09:01:50,702 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-08-10 09:01:50,702 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-10 09:01:50,702 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-10 09:01:50,702 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-08-10 09:01:50,702 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@17e6f9b
2017-08-10 09:01:50,783 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1502326915000
2017-08-10 09:01:50,785 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1502326915000 ms
2017-08-10 09:01:50,801 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-10 09:01:50,804 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@504e66{/streaming,null,AVAILABLE}
2017-08-10 09:01:50,805 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1c4d5e1{/streaming/json,null,AVAILABLE}
2017-08-10 09:01:50,805 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1835e65{/streaming/batch,null,AVAILABLE}
2017-08-10 09:01:50,806 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@231dfd{/streaming/batch/json,null,AVAILABLE}
2017-08-10 09:01:50,807 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@161a019{/static/streaming,null,AVAILABLE}
2017-08-10 09:01:50,808 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-10 09:01:55,307 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326915000 ms
2017-08-10 09:01:55,321 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326915000 ms.0 from job set of time 1502326915000 ms
2017-08-10 09:01:55,392 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:01:55,416 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:01:55,416 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:01:55,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:01:55,419 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:01:55,435 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:01:55,578 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-10 09:01:55,588 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 34.5 KB, free 413.9 MB)
2017-08-10 09:01:55,893 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.9 MB)
2017-08-10 09:01:55,895 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:01:55,899 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:01:55,904 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcess.scala:48)
2017-08-10 09:01:55,907 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-10 09:01:55,956 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-08-10 09:01:55,959 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-08-10 09:01:55,984 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-10 09:01:55,984 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-10 09:01:56,024 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 0 -> 25
2017-08-10 09:01:56,024 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 0 -> 25
2017-08-10 09:01:56,237 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-08-10 09:01:56,581 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-08-10 09:02:00,511 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326920000 ms
2017-08-10 09:02:05,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326925000 ms
2017-08-10 09:02:06,755 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-08-10 09:02:06,778 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 10818 ms on localhost (1/2)
2017-08-10 09:02:07,990 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-08-10 09:02:07,996 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 12059 ms on localhost (2/2)
2017-08-10 09:02:08,000 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcess.scala:50) finished in 12.077 s
2017-08-10 09:02:08,002 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-08-10 09:02:08,018 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcess.scala:50, took 12.624829 s
2017-08-10 09:02:08,029 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326915000 ms.0 from job set of time 1502326915000 ms
2017-08-10 09:02:08,030 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 13.025 s for time 1502326915000 ms (execution: 12.712 s)
2017-08-10 09:02:08,031 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326920000 ms.0 from job set of time 1502326920000 ms
2017-08-10 09:02:08,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:02:08,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:02:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:02:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:02:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:02:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:02:08,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:02:08,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:02:08,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-10 09:02:08,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:02:08,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:02:08,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:02:08,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcess.scala:48)
2017-08-10 09:02:08,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-08-10 09:02:08,073 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-08-10 09:02:08,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-08-10 09:02:08,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-08-10 09:02:08,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-08-10 09:02:08,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 25 -> 50
2017-08-10 09:02:08,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 25 -> 50
2017-08-10 09:02:10,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326930000 ms
2017-08-10 09:02:15,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326935000 ms
2017-08-10 09:02:19,423 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 787 bytes result sent to driver
2017-08-10 09:02:19,427 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 11353 ms on localhost (1/2)
2017-08-10 09:02:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326940000 ms
2017-08-10 09:02:25,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326945000 ms
2017-08-10 09:02:30,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326950000 ms
2017-08-10 09:02:31,847 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 787 bytes result sent to driver
2017-08-10 09:02:31,851 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 23779 ms on localhost (2/2)
2017-08-10 09:02:31,851 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-08-10 09:02:31,851 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcess.scala:50) finished in 23.780 s
2017-08-10 09:02:31,852 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: foreachPartition at streamingProcess.scala:50, took 23.797256 s
2017-08-10 09:02:31,852 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326920000 ms.0 from job set of time 1502326920000 ms
2017-08-10 09:02:31,853 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 31.852 s for time 1502326920000 ms (execution: 23.821 s)
2017-08-10 09:02:31,853 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326925000 ms.0 from job set of time 1502326925000 ms
2017-08-10 09:02:31,861 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-08-10 09:02:31,884 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-08-10 09:02:31,884 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:02:31,884 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-10 09:02:31,886 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-08-10 09:02:31,886 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:02:31,887 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-08-10 09:02:31,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:02:31,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:02:31,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:02:31,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:02:31,888 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:02:31,892 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:02:31,898 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:02:31,899 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:02:31,899 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:02:31,900 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcess.scala:48)
2017-08-10 09:02:31,900 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2017-08-10 09:02:31,906 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5655 bytes)
2017-08-10 09:02:31,908 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5655 bytes)
2017-08-10 09:02:31,908 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2017-08-10 09:02:31,908 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2017-08-10 09:02:31,912 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 50 -> 75
2017-08-10 09:02:31,912 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 50 -> 75
2017-08-10 09:02:33,113 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:02:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326955000 ms
2017-08-10 09:02:35,408 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 787 bytes result sent to driver
2017-08-10 09:02:35,411 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 3509 ms on localhost (1/2)
2017-08-10 09:02:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326960000 ms
2017-08-10 09:02:45,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326965000 ms
2017-08-10 09:02:50,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326970000 ms
2017-08-10 09:02:55,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326975000 ms
2017-08-10 09:02:57,683 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2.0 (TID 5). 787 bytes result sent to driver
2017-08-10 09:02:57,687 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2.0 (TID 5) in 25780 ms on localhost (2/2)
2017-08-10 09:02:57,687 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-08-10 09:02:57,687 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (foreachPartition at streamingProcess.scala:50) finished in 25.787 s
2017-08-10 09:02:57,688 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: foreachPartition at streamingProcess.scala:50, took 25.800129 s
2017-08-10 09:02:57,688 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326925000 ms.0 from job set of time 1502326925000 ms
2017-08-10 09:02:57,689 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 3 from persistence list
2017-08-10 09:02:57,689 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 52.688 s for time 1502326925000 ms (execution: 25.835 s)
2017-08-10 09:02:57,689 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326930000 ms.0 from job set of time 1502326930000 ms
2017-08-10 09:02:57,689 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 3
2017-08-10 09:02:57,689 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2 from persistence list
2017-08-10 09:02:57,690 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:02:57,690 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326915000 ms
2017-08-10 09:02:57,690 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2
2017-08-10 09:02:57,699 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:02:57,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:02:57,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 3 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:02:57,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:02:57,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:02:57,701 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:02:57,704 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:02:57,709 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:02:57,709 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:02:57,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:02:57,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcess.scala:48)
2017-08-10 09:02:57,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 3.0 with 2 tasks
2017-08-10 09:02:57,712 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 3.0 (TID 6, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:02:57,712 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 3.0 (TID 7, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:02:57,712 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 3.0 (TID 7)
2017-08-10 09:02:57,712 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 3.0 (TID 6)
2017-08-10 09:02:57,715 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 75 -> 100
2017-08-10 09:02:57,716 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 75 -> 100
2017-08-10 09:02:58,307 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:02:59,809 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 3.0 (TID 7). 787 bytes result sent to driver
2017-08-10 09:02:59,811 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 3.0 (TID 7) in 2099 ms on localhost (1/2)
2017-08-10 09:03:00,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326980000 ms
2017-08-10 09:03:05,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326985000 ms
2017-08-10 09:03:10,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326990000 ms
2017-08-10 09:03:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502326995000 ms
2017-08-10 09:03:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327000000 ms
2017-08-10 09:03:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327005000 ms
2017-08-10 09:03:30,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327010000 ms
2017-08-10 09:03:34,637 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 3.0 (TID 6). 787 bytes result sent to driver
2017-08-10 09:03:34,641 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 3.0 (TID 6) in 36930 ms on localhost (2/2)
2017-08-10 09:03:34,642 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-08-10 09:03:34,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 3 (foreachPartition at streamingProcess.scala:50) finished in 36.932 s
2017-08-10 09:03:34,643 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: foreachPartition at streamingProcess.scala:50, took 36.941422 s
2017-08-10 09:03:34,643 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326930000 ms.0 from job set of time 1502326930000 ms
2017-08-10 09:03:34,645 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 84.643 s for time 1502326930000 ms (execution: 36.954 s)
2017-08-10 09:03:34,645 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326935000 ms.0 from job set of time 1502326935000 ms
2017-08-10 09:03:34,645 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 5 from persistence list
2017-08-10 09:03:34,646 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 5
2017-08-10 09:03:34,646 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 4 from persistence list
2017-08-10 09:03:34,646 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 4
2017-08-10 09:03:34,646 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:03:34,647 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326920000 ms
2017-08-10 09:03:34,665 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:03:34,666 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 4 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:03:34,667 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:03:34,667 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:03:34,667 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:03:34,667 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:03:34,673 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:03:34,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:03:34,681 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:03:34,682 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:03:34,682 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcess.scala:48)
2017-08-10 09:03:34,682 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 2 tasks
2017-08-10 09:03:34,684 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 8, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:03:34,686 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 4.0 (TID 9, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:03:34,687 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 4.0 (TID 9)
2017-08-10 09:03:34,687 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 8)
2017-08-10 09:03:34,693 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 100 -> 125
2017-08-10 09:03:34,693 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 100 -> 125
2017-08-10 09:03:35,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327015000 ms
2017-08-10 09:03:37,396 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_3_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:03:40,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327020000 ms
2017-08-10 09:03:40,279 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 4.0 (TID 9). 787 bytes result sent to driver
2017-08-10 09:03:40,282 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 4.0 (TID 9) in 5597 ms on localhost (1/2)
2017-08-10 09:03:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327025000 ms
2017-08-10 09:03:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327030000 ms
2017-08-10 09:03:55,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327035000 ms
2017-08-10 09:04:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327040000 ms
2017-08-10 09:04:05,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327045000 ms
2017-08-10 09:04:10,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327050000 ms
2017-08-10 09:04:15,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327055000 ms
2017-08-10 09:04:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327060000 ms
2017-08-10 09:04:25,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327065000 ms
2017-08-10 09:04:26,624 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 8). 787 bytes result sent to driver
2017-08-10 09:04:26,626 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 8) in 51943 ms on localhost (2/2)
2017-08-10 09:04:26,626 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-08-10 09:04:26,626 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (foreachPartition at streamingProcess.scala:50) finished in 51.943 s
2017-08-10 09:04:26,627 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 4 finished: foreachPartition at streamingProcess.scala:50, took 51.958454 s
2017-08-10 09:04:26,627 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326935000 ms.0 from job set of time 1502326935000 ms
2017-08-10 09:04:26,627 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 131.627 s for time 1502326935000 ms (execution: 51.982 s)
2017-08-10 09:04:26,628 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 7 from persistence list
2017-08-10 09:04:26,628 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326940000 ms.0 from job set of time 1502326940000 ms
2017-08-10 09:04:26,628 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 7
2017-08-10 09:04:26,628 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 6 from persistence list
2017-08-10 09:04:26,628 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 6
2017-08-10 09:04:26,628 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:04:26,629 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326925000 ms
2017-08-10 09:04:26,639 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:04:26,640 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 5 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:04:26,640 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 5 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:04:26,641 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:04:26,641 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:04:26,641 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:04:26,644 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:04:26,648 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:04:26,649 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:04:26,650 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:04:26,650 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcess.scala:48)
2017-08-10 09:04:26,650 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 5.0 with 2 tasks
2017-08-10 09:04:26,652 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 5.0 (TID 10, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:04:26,652 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 5.0 (TID 11, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:04:26,652 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 5.0 (TID 11)
2017-08-10 09:04:26,653 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 5.0 (TID 10)
2017-08-10 09:04:26,655 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 125 -> 150
2017-08-10 09:04:26,655 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 125 -> 150
2017-08-10 09:04:30,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327070000 ms
2017-08-10 09:04:35,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327075000 ms
2017-08-10 09:04:38,164 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 5.0 (TID 11). 714 bytes result sent to driver
2017-08-10 09:04:38,167 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 5.0 (TID 11) in 11515 ms on localhost (1/2)
2017-08-10 09:04:40,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327080000 ms
2017-08-10 09:04:45,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327085000 ms
2017-08-10 09:04:45,123 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_4_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:04:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327090000 ms
2017-08-10 09:04:51,714 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 5.0 (TID 10). 787 bytes result sent to driver
2017-08-10 09:04:51,716 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 5.0 (TID 10) in 25065 ms on localhost (2/2)
2017-08-10 09:04:51,716 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-08-10 09:04:51,716 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 5 (foreachPartition at streamingProcess.scala:50) finished in 25.065 s
2017-08-10 09:04:51,716 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 5 finished: foreachPartition at streamingProcess.scala:50, took 25.075325 s
2017-08-10 09:04:51,717 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326940000 ms.0 from job set of time 1502326940000 ms
2017-08-10 09:04:51,717 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 151.717 s for time 1502326940000 ms (execution: 25.089 s)
2017-08-10 09:04:51,717 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326945000 ms.0 from job set of time 1502326945000 ms
2017-08-10 09:04:51,717 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 9 from persistence list
2017-08-10 09:04:51,718 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 9
2017-08-10 09:04:51,718 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 8 from persistence list
2017-08-10 09:04:51,719 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 8
2017-08-10 09:04:51,719 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:04:51,719 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326930000 ms
2017-08-10 09:04:51,732 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:04:51,733 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 6 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:04:51,733 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 6 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:04:51,733 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:04:51,733 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:04:51,734 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:04:51,736 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:04:51,740 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:04:51,741 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_6_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:04:51,742 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:04:51,742 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcess.scala:48)
2017-08-10 09:04:51,743 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 6.0 with 2 tasks
2017-08-10 09:04:51,744 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 6.0 (TID 12, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:04:51,745 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 6.0 (TID 13, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:04:51,745 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 6.0 (TID 12)
2017-08-10 09:04:51,745 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 6.0 (TID 13)
2017-08-10 09:04:51,747 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 150 -> 175
2017-08-10 09:04:51,747 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 150 -> 175
2017-08-10 09:04:54,301 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_5_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:04:55,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327095000 ms
2017-08-10 09:05:00,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327100000 ms
2017-08-10 09:05:05,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327105000 ms
2017-08-10 09:05:05,684 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 6.0 (TID 13). 787 bytes result sent to driver
2017-08-10 09:05:05,687 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 6.0 (TID 13) in 13943 ms on localhost (1/2)
2017-08-10 09:05:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327110000 ms
2017-08-10 09:05:14,546 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 6.0 (TID 12). 787 bytes result sent to driver
2017-08-10 09:05:14,548 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 6.0 (TID 12) in 22805 ms on localhost (2/2)
2017-08-10 09:05:14,548 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2017-08-10 09:05:14,548 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 6 (foreachPartition at streamingProcess.scala:50) finished in 22.805 s
2017-08-10 09:05:14,549 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 6 finished: foreachPartition at streamingProcess.scala:50, took 22.815048 s
2017-08-10 09:05:14,549 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326945000 ms.0 from job set of time 1502326945000 ms
2017-08-10 09:05:14,549 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 169.549 s for time 1502326945000 ms (execution: 22.832 s)
2017-08-10 09:05:14,549 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 11 from persistence list
2017-08-10 09:05:14,549 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326950000 ms.0 from job set of time 1502326950000 ms
2017-08-10 09:05:14,549 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 11
2017-08-10 09:05:14,549 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 10 from persistence list
2017-08-10 09:05:14,550 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 10
2017-08-10 09:05:14,550 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:05:14,550 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326935000 ms
2017-08-10 09:05:14,561 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:05:14,561 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 7 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:05:14,561 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 7 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:05:14,562 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:05:14,562 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:05:14,562 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:05:14,565 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:05:14,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:05:14,575 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_7_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:05:14,575 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:05:14,575 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcess.scala:48)
2017-08-10 09:05:14,576 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 7.0 with 2 tasks
2017-08-10 09:05:14,577 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 7.0 (TID 14, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:05:14,578 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 7.0 (TID 15, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:05:14,579 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 7.0 (TID 14)
2017-08-10 09:05:14,579 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 7.0 (TID 15)
2017-08-10 09:05:14,585 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 175 -> 200
2017-08-10 09:05:14,585 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 175 -> 200
2017-08-10 09:05:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327115000 ms
2017-08-10 09:05:17,032 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_6_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:05:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327120000 ms
2017-08-10 09:05:25,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327125000 ms
2017-08-10 09:05:26,578 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 7.0 (TID 14). 787 bytes result sent to driver
2017-08-10 09:05:26,580 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 7.0 (TID 14) in 12004 ms on localhost (1/2)
2017-08-10 09:05:30,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327130000 ms
2017-08-10 09:05:32,102 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 7.0 (TID 15). 787 bytes result sent to driver
2017-08-10 09:05:32,104 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 7.0 (TID 15) in 17526 ms on localhost (2/2)
2017-08-10 09:05:32,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 7 (foreachPartition at streamingProcess.scala:50) finished in 17.529 s
2017-08-10 09:05:32,105 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2017-08-10 09:05:32,105 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 7 finished: foreachPartition at streamingProcess.scala:50, took 17.543210 s
2017-08-10 09:05:32,105 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326950000 ms.0 from job set of time 1502326950000 ms
2017-08-10 09:05:32,106 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 182.105 s for time 1502326950000 ms (execution: 17.556 s)
2017-08-10 09:05:32,106 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 13 from persistence list
2017-08-10 09:05:32,106 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326955000 ms.0 from job set of time 1502326955000 ms
2017-08-10 09:05:32,106 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 13
2017-08-10 09:05:32,106 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 12 from persistence list
2017-08-10 09:05:32,107 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 12
2017-08-10 09:05:32,107 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:05:32,107 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326940000 ms
2017-08-10 09:05:32,117 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:05:32,117 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 8 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:05:32,117 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 8 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:05:32,117 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:05:32,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:05:32,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:05:32,121 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:05:32,125 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:05:32,126 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_8_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:05:32,126 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:05:32,127 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcess.scala:48)
2017-08-10 09:05:32,127 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 8.0 with 2 tasks
2017-08-10 09:05:32,128 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 8.0 (TID 16, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:05:32,129 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 8.0 (TID 17, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:05:32,129 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 8.0 (TID 16)
2017-08-10 09:05:32,129 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 8.0 (TID 17)
2017-08-10 09:05:32,132 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 200 -> 225
2017-08-10 09:05:32,132 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 200 -> 225
2017-08-10 09:05:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327135000 ms
2017-08-10 09:05:40,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327140000 ms
2017-08-10 09:05:40,910 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_7_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:05:45,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327145000 ms
2017-08-10 09:05:49,875 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 8.0 (TID 16). 787 bytes result sent to driver
2017-08-10 09:05:49,877 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 8.0 (TID 16) in 17750 ms on localhost (1/2)
2017-08-10 09:05:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327150000 ms
2017-08-10 09:05:54,453 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 8.0 (TID 17). 787 bytes result sent to driver
2017-08-10 09:05:54,460 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 8.0 (TID 17) in 22331 ms on localhost (2/2)
2017-08-10 09:05:54,460 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 8 (foreachPartition at streamingProcess.scala:50) finished in 22.333 s
2017-08-10 09:05:54,460 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2017-08-10 09:05:54,461 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 8 finished: foreachPartition at streamingProcess.scala:50, took 22.343216 s
2017-08-10 09:05:54,463 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326955000 ms.0 from job set of time 1502326955000 ms
2017-08-10 09:05:54,463 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 199.463 s for time 1502326955000 ms (execution: 22.357 s)
2017-08-10 09:05:54,464 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 15 from persistence list
2017-08-10 09:05:54,464 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326960000 ms.0 from job set of time 1502326960000 ms
2017-08-10 09:05:54,465 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 15
2017-08-10 09:05:54,465 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 14 from persistence list
2017-08-10 09:05:54,466 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 14
2017-08-10 09:05:54,467 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:05:54,467 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326945000 ms
2017-08-10 09:05:54,505 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:05:54,507 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 9 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:05:54,508 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 9 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:05:54,508 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:05:54,508 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:05:54,509 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:05:54,519 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:05:54,531 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:05:54,533 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_9_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:05:54,534 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:05:54,535 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcess.scala:48)
2017-08-10 09:05:54,535 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 9.0 with 2 tasks
2017-08-10 09:05:54,538 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 9.0 (TID 18, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:05:54,539 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 9.0 (TID 19, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:05:54,540 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 9.0 (TID 18)
2017-08-10 09:05:54,540 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 9.0 (TID 19)
2017-08-10 09:05:54,545 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 225 -> 250
2017-08-10 09:05:54,546 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 225 -> 250
2017-08-10 09:05:55,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327155000 ms
2017-08-10 09:05:57,908 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_8_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:00,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327160000 ms
2017-08-10 09:06:05,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327165000 ms
2017-08-10 09:06:06,389 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 9.0 (TID 18). 787 bytes result sent to driver
2017-08-10 09:06:06,391 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 9.0 (TID 18) in 11855 ms on localhost (1/2)
2017-08-10 09:06:09,996 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 9.0 (TID 19). 787 bytes result sent to driver
2017-08-10 09:06:09,998 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 9.0 (TID 19) in 15460 ms on localhost (2/2)
2017-08-10 09:06:09,998 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2017-08-10 09:06:09,999 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 9 (foreachPartition at streamingProcess.scala:50) finished in 15.462 s
2017-08-10 09:06:09,999 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 9 finished: foreachPartition at streamingProcess.scala:50, took 15.492178 s
2017-08-10 09:06:09,999 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326960000 ms.0 from job set of time 1502326960000 ms
2017-08-10 09:06:09,999 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 209.999 s for time 1502326960000 ms (execution: 15.535 s)
2017-08-10 09:06:09,999 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 17 from persistence list
2017-08-10 09:06:09,999 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326965000 ms.0 from job set of time 1502326965000 ms
2017-08-10 09:06:10,000 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 17
2017-08-10 09:06:10,000 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 16 from persistence list
2017-08-10 09:06:10,000 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 16
2017-08-10 09:06:10,000 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:06:10,000 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326950000 ms
2017-08-10 09:06:10,011 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:06:10,012 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 10 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:06:10,012 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 10 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:06:10,012 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:06:10,012 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:06:10,012 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:06:10,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327170000 ms
2017-08-10 09:06:10,015 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:06:10,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:06:10,021 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_10_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:10,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:06:10,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcess.scala:48)
2017-08-10 09:06:10,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 10.0 with 2 tasks
2017-08-10 09:06:10,023 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 10.0 (TID 20, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:06:10,023 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 10.0 (TID 21, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:06:10,024 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 10.0 (TID 20)
2017-08-10 09:06:10,024 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 10.0 (TID 21)
2017-08-10 09:06:10,026 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 250 -> 275
2017-08-10 09:06:10,026 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 250 -> 275
2017-08-10 09:06:13,880 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_9_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327175000 ms
2017-08-10 09:06:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327180000 ms
2017-08-10 09:06:22,558 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 10.0 (TID 20). 787 bytes result sent to driver
2017-08-10 09:06:22,560 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 10.0 (TID 20) in 12538 ms on localhost (1/2)
2017-08-10 09:06:24,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 10.0 (TID 21). 787 bytes result sent to driver
2017-08-10 09:06:24,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 10.0 (TID 21) in 14043 ms on localhost (2/2)
2017-08-10 09:06:24,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2017-08-10 09:06:24,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 10 (foreachPartition at streamingProcess.scala:50) finished in 14.044 s
2017-08-10 09:06:24,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 10 finished: foreachPartition at streamingProcess.scala:50, took 14.054894 s
2017-08-10 09:06:24,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326965000 ms.0 from job set of time 1502326965000 ms
2017-08-10 09:06:24,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 219.068 s for time 1502326965000 ms (execution: 14.069 s)
2017-08-10 09:06:24,068 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 19 from persistence list
2017-08-10 09:06:24,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326970000 ms.0 from job set of time 1502326970000 ms
2017-08-10 09:06:24,068 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 19
2017-08-10 09:06:24,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 18 from persistence list
2017-08-10 09:06:24,069 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 18
2017-08-10 09:06:24,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:06:24,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326955000 ms
2017-08-10 09:06:24,080 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:06:24,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 11 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:06:24,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 11 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:06:24,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:06:24,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:06:24,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:06:24,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:06:24,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:06:24,089 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_11_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:24,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 11 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:06:24,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcess.scala:48)
2017-08-10 09:06:24,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 11.0 with 2 tasks
2017-08-10 09:06:24,091 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 11.0 (TID 22, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:06:24,091 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 11.0 (TID 23, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:06:24,091 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 11.0 (TID 22)
2017-08-10 09:06:24,091 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 11.0 (TID 23)
2017-08-10 09:06:24,094 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 275 -> 300
2017-08-10 09:06:24,094 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 275 -> 300
2017-08-10 09:06:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327185000 ms
2017-08-10 09:06:26,429 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_10_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:30,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327190000 ms
2017-08-10 09:06:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327195000 ms
2017-08-10 09:06:37,242 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 11.0 (TID 23). 787 bytes result sent to driver
2017-08-10 09:06:37,245 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 11.0 (TID 23) in 13154 ms on localhost (1/2)
2017-08-10 09:06:40,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327200000 ms
2017-08-10 09:06:44,550 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 11.0 (TID 22). 787 bytes result sent to driver
2017-08-10 09:06:44,553 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 11.0 (TID 22) in 20463 ms on localhost (2/2)
2017-08-10 09:06:44,553 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2017-08-10 09:06:44,553 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 11 (foreachPartition at streamingProcess.scala:50) finished in 20.463 s
2017-08-10 09:06:44,553 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 11 finished: foreachPartition at streamingProcess.scala:50, took 20.472312 s
2017-08-10 09:06:44,554 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326970000 ms.0 from job set of time 1502326970000 ms
2017-08-10 09:06:44,554 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 234.554 s for time 1502326970000 ms (execution: 20.486 s)
2017-08-10 09:06:44,554 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326975000 ms.0 from job set of time 1502326975000 ms
2017-08-10 09:06:44,554 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 21 from persistence list
2017-08-10 09:06:44,555 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 21
2017-08-10 09:06:44,555 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 20 from persistence list
2017-08-10 09:06:44,555 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 20
2017-08-10 09:06:44,555 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:06:44,555 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326960000 ms
2017-08-10 09:06:44,566 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:06:44,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 12 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:06:44,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 12 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:06:44,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:06:44,567 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:06:44,567 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:06:44,570 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:06:44,575 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:06:44,576 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_12_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:44,576 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:06:44,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcess.scala:48)
2017-08-10 09:06:44,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 12.0 with 2 tasks
2017-08-10 09:06:44,578 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 12.0 (TID 24, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:06:44,579 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 12.0 (TID 25, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:06:44,579 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 12.0 (TID 24)
2017-08-10 09:06:44,579 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 12.0 (TID 25)
2017-08-10 09:06:44,582 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 300 -> 325
2017-08-10 09:06:44,582 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 300 -> 325
2017-08-10 09:06:45,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327205000 ms
2017-08-10 09:06:45,333 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_11_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:06:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327210000 ms
2017-08-10 09:06:55,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327215000 ms
2017-08-10 09:06:56,920 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 12.0 (TID 24). 787 bytes result sent to driver
2017-08-10 09:06:56,923 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 12.0 (TID 24) in 12346 ms on localhost (1/2)
2017-08-10 09:07:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327220000 ms
2017-08-10 09:07:02,568 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 12.0 (TID 25). 787 bytes result sent to driver
2017-08-10 09:07:02,571 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 12.0 (TID 25) in 17992 ms on localhost (2/2)
2017-08-10 09:07:02,571 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2017-08-10 09:07:02,571 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 12 (foreachPartition at streamingProcess.scala:50) finished in 17.994 s
2017-08-10 09:07:02,571 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 12 finished: foreachPartition at streamingProcess.scala:50, took 18.004436 s
2017-08-10 09:07:02,572 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326975000 ms.0 from job set of time 1502326975000 ms
2017-08-10 09:07:02,572 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 247.572 s for time 1502326975000 ms (execution: 18.018 s)
2017-08-10 09:07:02,572 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326980000 ms.0 from job set of time 1502326980000 ms
2017-08-10 09:07:02,572 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 23 from persistence list
2017-08-10 09:07:02,572 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 23
2017-08-10 09:07:02,572 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 22 from persistence list
2017-08-10 09:07:02,573 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 22
2017-08-10 09:07:02,573 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:07:02,573 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326965000 ms
2017-08-10 09:07:02,583 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:07:02,584 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 13 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:07:02,584 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 13 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:07:02,585 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:07:02,585 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:07:02,585 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:07:02,588 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:07:02,592 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:07:02,593 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_13_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:07:02,594 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 13 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:07:02,594 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcess.scala:48)
2017-08-10 09:07:02,594 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 13.0 with 2 tasks
2017-08-10 09:07:02,595 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 13.0 (TID 26, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:07:02,596 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 13.0 (TID 27, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:07:02,596 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 13.0 (TID 27)
2017-08-10 09:07:02,596 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 13.0 (TID 26)
2017-08-10 09:07:02,599 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 325 -> 350
2017-08-10 09:07:02,599 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 325 -> 350
2017-08-10 09:07:02,843 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_12_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:07:05,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327225000 ms
2017-08-10 09:07:10,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327230000 ms
2017-08-10 09:07:11,628 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 13.0 (TID 26). 787 bytes result sent to driver
2017-08-10 09:07:11,631 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 13.0 (TID 26) in 9036 ms on localhost (1/2)
2017-08-10 09:07:15,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327235000 ms
2017-08-10 09:07:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327240000 ms
2017-08-10 09:07:22,444 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 13.0 (TID 27). 787 bytes result sent to driver
2017-08-10 09:07:22,453 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 13.0 (TID 27) in 19856 ms on localhost (2/2)
2017-08-10 09:07:22,453 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2017-08-10 09:07:22,454 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 13 (foreachPartition at streamingProcess.scala:50) finished in 19.859 s
2017-08-10 09:07:22,455 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 13 finished: foreachPartition at streamingProcess.scala:50, took 19.869890 s
2017-08-10 09:07:22,456 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326980000 ms.0 from job set of time 1502326980000 ms
2017-08-10 09:07:22,457 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 262.456 s for time 1502326980000 ms (execution: 19.884 s)
2017-08-10 09:07:22,457 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 25 from persistence list
2017-08-10 09:07:22,457 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326985000 ms.0 from job set of time 1502326985000 ms
2017-08-10 09:07:22,458 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 25
2017-08-10 09:07:22,458 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 24 from persistence list
2017-08-10 09:07:22,459 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 24
2017-08-10 09:07:22,459 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:07:22,460 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326970000 ms
2017-08-10 09:07:22,483 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:07:22,484 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 14 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:07:22,484 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 14 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:07:22,484 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:07:22,485 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:07:22,485 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:07:22,491 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:07:22,495 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:07:22,496 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_14_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:07:22,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:07:22,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcess.scala:48)
2017-08-10 09:07:22,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 14.0 with 2 tasks
2017-08-10 09:07:22,498 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 14.0 (TID 28, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:07:22,499 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 14.0 (TID 29, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:07:22,499 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 14.0 (TID 29)
2017-08-10 09:07:22,499 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 14.0 (TID 28)
2017-08-10 09:07:22,502 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 350 -> 375
2017-08-10 09:07:22,502 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 350 -> 375
2017-08-10 09:07:24,574 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_13_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:07:25,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327245000 ms
2017-08-10 09:07:30,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327250000 ms
2017-08-10 09:07:35,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327255000 ms
2017-08-10 09:07:39,113 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 14.0 (TID 29). 787 bytes result sent to driver
2017-08-10 09:07:39,116 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 14.0 (TID 29) in 16618 ms on localhost (1/2)
2017-08-10 09:07:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327260000 ms
2017-08-10 09:07:45,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327265000 ms
2017-08-10 09:07:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327270000 ms
2017-08-10 09:07:55,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327275000 ms
2017-08-10 09:08:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327280000 ms
2017-08-10 09:08:05,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327285000 ms
2017-08-10 09:08:10,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327290000 ms
2017-08-10 09:08:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327295000 ms
2017-08-10 09:08:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327300000 ms
2017-08-10 09:08:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327305000 ms
2017-08-10 09:08:30,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327310000 ms
2017-08-10 09:08:35,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327315000 ms
2017-08-10 09:08:40,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327320000 ms
2017-08-10 09:08:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327325000 ms
2017-08-10 09:08:47,798 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 14.0 (TID 28). 787 bytes result sent to driver
2017-08-10 09:08:47,801 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 14.0 (TID 28) in 85304 ms on localhost (2/2)
2017-08-10 09:08:47,801 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 14 (foreachPartition at streamingProcess.scala:50) finished in 85.304 s
2017-08-10 09:08:47,801 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2017-08-10 09:08:47,802 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 14 finished: foreachPartition at streamingProcess.scala:50, took 85.314085 s
2017-08-10 09:08:47,802 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326985000 ms.0 from job set of time 1502326985000 ms
2017-08-10 09:08:47,802 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 342.802 s for time 1502326985000 ms (execution: 85.345 s)
2017-08-10 09:08:47,802 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 27 from persistence list
2017-08-10 09:08:47,802 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326990000 ms.0 from job set of time 1502326990000 ms
2017-08-10 09:08:47,803 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 26 from persistence list
2017-08-10 09:08:47,803 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 27
2017-08-10 09:08:47,804 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:08:47,804 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326975000 ms
2017-08-10 09:08:47,804 [block-manager-slave-async-thread-pool-8] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 26
2017-08-10 09:08:47,818 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:08:47,818 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 15 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:08:47,819 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 15 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:08:47,819 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:08:47,819 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:08:47,819 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:08:47,822 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:08:47,830 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:08:47,830 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_15_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:08:47,831 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 15 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:08:47,831 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcess.scala:48)
2017-08-10 09:08:47,831 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 15.0 with 2 tasks
2017-08-10 09:08:47,833 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 15.0 (TID 30, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:08:47,833 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 15.0 (TID 31, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:08:47,833 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 15.0 (TID 30)
2017-08-10 09:08:47,834 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 15.0 (TID 31)
2017-08-10 09:08:47,840 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 375 -> 400
2017-08-10 09:08:47,840 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 375 -> 400
2017-08-10 09:08:48,907 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_14_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:08:50,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327330000 ms
2017-08-10 09:08:55,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327335000 ms
2017-08-10 09:08:58,202 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 15.0 (TID 30). 874 bytes result sent to driver
2017-08-10 09:08:58,206 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 15.0 (TID 30) in 10374 ms on localhost (1/2)
2017-08-10 09:09:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327340000 ms
2017-08-10 09:09:05,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327345000 ms
2017-08-10 09:09:10,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327350000 ms
2017-08-10 09:09:15,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327355000 ms
2017-08-10 09:09:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327360000 ms
2017-08-10 09:09:25,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327365000 ms
2017-08-10 09:09:30,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327370000 ms
2017-08-10 09:09:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327375000 ms
2017-08-10 09:09:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327380000 ms
2017-08-10 09:09:45,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327385000 ms
2017-08-10 09:09:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327390000 ms
2017-08-10 09:09:55,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327395000 ms
2017-08-10 09:10:00,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327400000 ms
2017-08-10 09:10:05,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327405000 ms
2017-08-10 09:10:10,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327410000 ms
2017-08-10 09:10:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327415000 ms
2017-08-10 09:10:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327420000 ms
2017-08-10 09:10:25,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327425000 ms
2017-08-10 09:10:30,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327430000 ms
2017-08-10 09:10:35,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327435000 ms
2017-08-10 09:10:40,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327440000 ms
2017-08-10 09:10:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327445000 ms
2017-08-10 09:10:50,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327450000 ms
2017-08-10 09:10:51,942 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 15.0 (TID 31). 787 bytes result sent to driver
2017-08-10 09:10:51,945 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 15.0 (TID 31) in 124112 ms on localhost (2/2)
2017-08-10 09:10:51,946 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 15 (foreachPartition at streamingProcess.scala:50) finished in 124.114 s
2017-08-10 09:10:51,946 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2017-08-10 09:10:51,946 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 15 finished: foreachPartition at streamingProcess.scala:50, took 124.121489 s
2017-08-10 09:10:51,946 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326990000 ms.0 from job set of time 1502326990000 ms
2017-08-10 09:10:51,947 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 461.946 s for time 1502326990000 ms (execution: 124.144 s)
2017-08-10 09:10:51,947 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 29 from persistence list
2017-08-10 09:10:51,947 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502326995000 ms.0 from job set of time 1502326995000 ms
2017-08-10 09:10:51,947 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 29
2017-08-10 09:10:51,948 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 28 from persistence list
2017-08-10 09:10:51,948 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:10:51,948 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326980000 ms
2017-08-10 09:10:51,949 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 28
2017-08-10 09:10:51,959 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:10:51,960 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 16 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:10:51,960 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 16 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:10:51,960 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:10:51,960 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:10:51,961 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:10:51,964 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:10:51,968 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:10:51,969 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_16_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:10:51,969 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 16 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:10:51,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcess.scala:48)
2017-08-10 09:10:51,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 16.0 with 2 tasks
2017-08-10 09:10:51,971 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 16.0 (TID 32, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:10:51,972 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 16.0 (TID 33, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:10:51,972 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 16.0 (TID 32)
2017-08-10 09:10:51,972 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 16.0 (TID 33)
2017-08-10 09:10:51,975 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 400 -> 425
2017-08-10 09:10:51,975 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 400 -> 425
2017-08-10 09:10:54,937 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_15_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:10:55,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327455000 ms
2017-08-10 09:11:00,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327460000 ms
2017-08-10 09:11:03,531 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 16.0 (TID 32). 874 bytes result sent to driver
2017-08-10 09:11:03,534 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 16.0 (TID 32) in 11564 ms on localhost (1/2)
2017-08-10 09:11:05,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327465000 ms
2017-08-10 09:11:10,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327470000 ms
2017-08-10 09:11:15,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327475000 ms
2017-08-10 09:11:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327480000 ms
2017-08-10 09:11:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327485000 ms
2017-08-10 09:11:30,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327490000 ms
2017-08-10 09:11:35,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327495000 ms
2017-08-10 09:11:40,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327500000 ms
2017-08-10 09:11:41,620 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 16.0 (TID 33). 787 bytes result sent to driver
2017-08-10 09:11:41,629 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 16.0 (TID 33) in 49657 ms on localhost (2/2)
2017-08-10 09:11:41,629 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2017-08-10 09:11:41,630 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 16 (foreachPartition at streamingProcess.scala:50) finished in 49.659 s
2017-08-10 09:11:41,631 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 16 finished: foreachPartition at streamingProcess.scala:50, took 49.668722 s
2017-08-10 09:11:41,632 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502326995000 ms.0 from job set of time 1502326995000 ms
2017-08-10 09:11:41,633 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 31 from persistence list
2017-08-10 09:11:41,633 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 506.632 s for time 1502326995000 ms (execution: 49.685 s)
2017-08-10 09:11:41,634 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327000000 ms.0 from job set of time 1502327000000 ms
2017-08-10 09:11:41,636 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 31
2017-08-10 09:11:41,636 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 30 from persistence list
2017-08-10 09:11:41,637 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 30
2017-08-10 09:11:41,637 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:11:41,638 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326985000 ms
2017-08-10 09:11:41,663 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_16_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:11:41,668 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:11:41,669 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 17 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:11:41,669 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 17 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:11:41,669 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:11:41,669 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:11:41,670 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:11:41,673 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:11:41,677 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:11:41,678 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_17_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:11:41,678 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 17 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:11:41,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcess.scala:48)
2017-08-10 09:11:41,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 17.0 with 2 tasks
2017-08-10 09:11:41,680 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 17.0 (TID 34, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:11:41,681 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 17.0 (TID 35, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:11:41,681 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 17.0 (TID 34)
2017-08-10 09:11:41,681 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 17.0 (TID 35)
2017-08-10 09:11:41,684 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 425 -> 450
2017-08-10 09:11:41,685 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 425 -> 450
2017-08-10 09:11:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327505000 ms
2017-08-10 09:11:45,866 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 17.0 (TID 35). 787 bytes result sent to driver
2017-08-10 09:11:45,869 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 17.0 (TID 35) in 4189 ms on localhost (1/2)
2017-08-10 09:11:47,554 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 17.0 (TID 34). 787 bytes result sent to driver
2017-08-10 09:11:47,556 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 17.0 (TID 34) in 5877 ms on localhost (2/2)
2017-08-10 09:11:47,556 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2017-08-10 09:11:47,556 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 17 (foreachPartition at streamingProcess.scala:50) finished in 5.877 s
2017-08-10 09:11:47,556 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 17 finished: foreachPartition at streamingProcess.scala:50, took 5.887586 s
2017-08-10 09:11:47,557 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327000000 ms.0 from job set of time 1502327000000 ms
2017-08-10 09:11:47,557 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 507.557 s for time 1502327000000 ms (execution: 5.923 s)
2017-08-10 09:11:47,557 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 33 from persistence list
2017-08-10 09:11:47,557 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327005000 ms.0 from job set of time 1502327005000 ms
2017-08-10 09:11:47,558 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 33
2017-08-10 09:11:47,558 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 32 from persistence list
2017-08-10 09:11:47,558 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 32
2017-08-10 09:11:47,558 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:11:47,558 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326990000 ms
2017-08-10 09:11:47,568 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:11:47,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 18 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:11:47,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 18 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:11:47,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:11:47,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:11:47,569 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:11:47,572 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:11:47,576 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:11:47,576 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_18_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:11:47,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 18 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:11:47,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcess.scala:48)
2017-08-10 09:11:47,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 18.0 with 2 tasks
2017-08-10 09:11:47,579 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 18.0 (TID 36, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:11:47,580 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 18.0 (TID 37, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:11:47,580 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 18.0 (TID 37)
2017-08-10 09:11:47,580 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 18.0 (TID 36)
2017-08-10 09:11:47,583 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 450 -> 475
2017-08-10 09:11:47,583 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 450 -> 475
2017-08-10 09:11:48,677 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_17_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:11:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327510000 ms
2017-08-10 09:11:54,124 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 18.0 (TID 37). 787 bytes result sent to driver
2017-08-10 09:11:54,127 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 18.0 (TID 37) in 6548 ms on localhost (1/2)
2017-08-10 09:11:55,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327515000 ms
2017-08-10 09:12:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327520000 ms
2017-08-10 09:12:05,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327525000 ms
2017-08-10 09:12:05,099 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 18.0 (TID 36). 787 bytes result sent to driver
2017-08-10 09:12:05,102 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 18.0 (TID 36) in 17524 ms on localhost (2/2)
2017-08-10 09:12:05,102 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 18 (foreachPartition at streamingProcess.scala:50) finished in 17.524 s
2017-08-10 09:12:05,102 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2017-08-10 09:12:05,102 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 18 finished: foreachPartition at streamingProcess.scala:50, took 17.532954 s
2017-08-10 09:12:05,103 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327005000 ms.0 from job set of time 1502327005000 ms
2017-08-10 09:12:05,103 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 520.103 s for time 1502327005000 ms (execution: 17.546 s)
2017-08-10 09:12:05,103 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 35 from persistence list
2017-08-10 09:12:05,103 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327010000 ms.0 from job set of time 1502327010000 ms
2017-08-10 09:12:05,104 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 35
2017-08-10 09:12:05,104 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 34 from persistence list
2017-08-10 09:12:05,104 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 34
2017-08-10 09:12:05,104 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:12:05,104 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502326995000 ms
2017-08-10 09:12:05,115 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:12:05,116 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 19 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:12:05,116 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 19 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:12:05,116 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:12:05,116 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:12:05,116 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 19 (MapPartitionsRDD[39] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:12:05,119 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:12:05,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:12:05,124 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_19_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:05,124 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 19 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:12:05,125 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at map at streamingProcess.scala:48)
2017-08-10 09:12:05,125 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 19.0 with 2 tasks
2017-08-10 09:12:05,126 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 19.0 (TID 38, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:12:05,127 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 19.0 (TID 39, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:12:05,127 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 19.0 (TID 39)
2017-08-10 09:12:05,127 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 19.0 (TID 38)
2017-08-10 09:12:05,130 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 475 -> 500
2017-08-10 09:12:05,131 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 475 -> 500
2017-08-10 09:12:06,111 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_18_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:09,358 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 19.0 (TID 38). 787 bytes result sent to driver
2017-08-10 09:12:09,361 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 19.0 (TID 38) in 4235 ms on localhost (1/2)
2017-08-10 09:12:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327530000 ms
2017-08-10 09:12:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327535000 ms
2017-08-10 09:12:16,029 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 19.0 (TID 39). 787 bytes result sent to driver
2017-08-10 09:12:16,032 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 19.0 (TID 39) in 10905 ms on localhost (2/2)
2017-08-10 09:12:16,032 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 19.0, whose tasks have all completed, from pool 
2017-08-10 09:12:16,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 19 (foreachPartition at streamingProcess.scala:50) finished in 10.907 s
2017-08-10 09:12:16,032 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 19 finished: foreachPartition at streamingProcess.scala:50, took 10.916616 s
2017-08-10 09:12:16,033 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327010000 ms.0 from job set of time 1502327010000 ms
2017-08-10 09:12:16,033 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 526.033 s for time 1502327010000 ms (execution: 10.930 s)
2017-08-10 09:12:16,033 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 37 from persistence list
2017-08-10 09:12:16,033 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327015000 ms.0 from job set of time 1502327015000 ms
2017-08-10 09:12:16,033 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 37
2017-08-10 09:12:16,034 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 36 from persistence list
2017-08-10 09:12:16,034 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 36
2017-08-10 09:12:16,034 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:12:16,034 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327000000 ms
2017-08-10 09:12:16,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:12:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 20 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:12:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 20 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:12:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:12:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:12:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 20 (MapPartitionsRDD[41] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:12:16,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:12:16,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:12:16,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_20_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 20 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:12:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at map at streamingProcess.scala:48)
2017-08-10 09:12:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 20.0 with 2 tasks
2017-08-10 09:12:16,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 20.0 (TID 40, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:12:16,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 20.0 (TID 41, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:12:16,060 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 20.0 (TID 40)
2017-08-10 09:12:16,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 20.0 (TID 41)
2017-08-10 09:12:16,063 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 500 -> 525
2017-08-10 09:12:16,063 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 500 -> 525
2017-08-10 09:12:16,684 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_19_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:17,364 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 20.0 (TID 40). 787 bytes result sent to driver
2017-08-10 09:12:17,366 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 20.0 (TID 40) in 1307 ms on localhost (1/2)
2017-08-10 09:12:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327540000 ms
2017-08-10 09:12:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327545000 ms
2017-08-10 09:12:30,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327550000 ms
2017-08-10 09:12:34,159 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 20.0 (TID 41). 787 bytes result sent to driver
2017-08-10 09:12:34,162 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 20.0 (TID 41) in 18103 ms on localhost (2/2)
2017-08-10 09:12:34,162 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 20.0, whose tasks have all completed, from pool 
2017-08-10 09:12:34,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 20 (foreachPartition at streamingProcess.scala:50) finished in 18.104 s
2017-08-10 09:12:34,162 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 20 finished: foreachPartition at streamingProcess.scala:50, took 18.114135 s
2017-08-10 09:12:34,163 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327015000 ms.0 from job set of time 1502327015000 ms
2017-08-10 09:12:34,163 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 539.163 s for time 1502327015000 ms (execution: 18.130 s)
2017-08-10 09:12:34,163 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 39 from persistence list
2017-08-10 09:12:34,163 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327020000 ms.0 from job set of time 1502327020000 ms
2017-08-10 09:12:34,163 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 39
2017-08-10 09:12:34,163 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 38 from persistence list
2017-08-10 09:12:34,164 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 38
2017-08-10 09:12:34,164 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:12:34,164 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327005000 ms
2017-08-10 09:12:34,176 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:12:34,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 21 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:12:34,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 21 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:12:34,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:12:34,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:12:34,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 21 (MapPartitionsRDD[43] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:12:34,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:12:34,185 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:12:34,188 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_21_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:34,189 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 21 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:12:34,190 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at map at streamingProcess.scala:48)
2017-08-10 09:12:34,190 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 21.0 with 2 tasks
2017-08-10 09:12:34,192 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 21.0 (TID 42, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:12:34,192 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 21.0 (TID 43, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:12:34,193 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 21.0 (TID 42)
2017-08-10 09:12:34,193 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 21.0 (TID 43)
2017-08-10 09:12:34,195 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 525 -> 550
2017-08-10 09:12:34,195 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 525 -> 550
2017-08-10 09:12:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327555000 ms
2017-08-10 09:12:35,435 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_20_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:35,948 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 21.0 (TID 43). 787 bytes result sent to driver
2017-08-10 09:12:35,950 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 21.0 (TID 43) in 1758 ms on localhost (1/2)
2017-08-10 09:12:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327560000 ms
2017-08-10 09:12:40,817 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 21.0 (TID 42). 787 bytes result sent to driver
2017-08-10 09:12:40,821 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 21.0 (TID 42) in 6630 ms on localhost (2/2)
2017-08-10 09:12:40,821 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 21 (foreachPartition at streamingProcess.scala:50) finished in 6.631 s
2017-08-10 09:12:40,821 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 21.0, whose tasks have all completed, from pool 
2017-08-10 09:12:40,822 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 21 finished: foreachPartition at streamingProcess.scala:50, took 6.644346 s
2017-08-10 09:12:40,822 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327020000 ms.0 from job set of time 1502327020000 ms
2017-08-10 09:12:40,822 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 540.822 s for time 1502327020000 ms (execution: 6.659 s)
2017-08-10 09:12:40,822 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 41 from persistence list
2017-08-10 09:12:40,823 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327025000 ms.0 from job set of time 1502327025000 ms
2017-08-10 09:12:40,823 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 41
2017-08-10 09:12:40,823 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 40 from persistence list
2017-08-10 09:12:40,823 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 40
2017-08-10 09:12:40,823 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:12:40,824 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327010000 ms
2017-08-10 09:12:40,837 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:12:40,838 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 22 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:12:40,838 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 22 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:12:40,838 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:12:40,839 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:12:40,839 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 22 (MapPartitionsRDD[45] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:12:40,843 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:12:40,848 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:12:40,848 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_22_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:40,849 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 22 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:12:40,849 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at map at streamingProcess.scala:48)
2017-08-10 09:12:40,849 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 22.0 with 2 tasks
2017-08-10 09:12:40,851 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 22.0 (TID 44, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:12:40,851 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 22.0 (TID 45, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:12:40,851 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 22.0 (TID 44)
2017-08-10 09:12:40,851 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 22.0 (TID 45)
2017-08-10 09:12:40,855 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 550 -> 575
2017-08-10 09:12:40,855 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 550 -> 575
2017-08-10 09:12:41,216 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_21_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:45,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327565000 ms
2017-08-10 09:12:46,527 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 22.0 (TID 45). 787 bytes result sent to driver
2017-08-10 09:12:46,530 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 22.0 (TID 45) in 5679 ms on localhost (1/2)
2017-08-10 09:12:50,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327570000 ms
2017-08-10 09:12:50,700 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 22.0 (TID 44). 787 bytes result sent to driver
2017-08-10 09:12:50,702 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 22.0 (TID 44) in 9852 ms on localhost (2/2)
2017-08-10 09:12:50,703 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 22.0, whose tasks have all completed, from pool 
2017-08-10 09:12:50,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 22 (foreachPartition at streamingProcess.scala:50) finished in 9.853 s
2017-08-10 09:12:50,703 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 22 finished: foreachPartition at streamingProcess.scala:50, took 9.864846 s
2017-08-10 09:12:50,704 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327025000 ms.0 from job set of time 1502327025000 ms
2017-08-10 09:12:50,704 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 545.704 s for time 1502327025000 ms (execution: 9.882 s)
2017-08-10 09:12:50,704 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 43 from persistence list
2017-08-10 09:12:50,704 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327030000 ms.0 from job set of time 1502327030000 ms
2017-08-10 09:12:50,704 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 43
2017-08-10 09:12:50,704 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 42 from persistence list
2017-08-10 09:12:50,705 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 42
2017-08-10 09:12:50,705 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:12:50,705 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327015000 ms
2017-08-10 09:12:50,716 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:12:50,717 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 23 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:12:50,717 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 23 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:12:50,717 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:12:50,717 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:12:50,718 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 23 (MapPartitionsRDD[47] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:12:50,720 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:12:50,724 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:12:50,725 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_23_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:12:50,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 23 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:12:50,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 23 (MapPartitionsRDD[47] at map at streamingProcess.scala:48)
2017-08-10 09:12:50,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 23.0 with 2 tasks
2017-08-10 09:12:50,727 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 23.0 (TID 46, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:12:50,728 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 23.0 (TID 47, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:12:50,728 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 23.0 (TID 47)
2017-08-10 09:12:50,728 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 23.0 (TID 46)
2017-08-10 09:12:50,730 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 575 -> 600
2017-08-10 09:12:50,730 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 575 -> 600
2017-08-10 09:12:55,033 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327575000 ms
2017-08-10 09:12:56,331 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_22_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:13:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327580000 ms
2017-08-10 09:13:04,907 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 23.0 (TID 47). 787 bytes result sent to driver
2017-08-10 09:13:04,911 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 23.0 (TID 47) in 14184 ms on localhost (1/2)
2017-08-10 09:13:05,265 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327585000 ms
2017-08-10 09:13:08,443 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 23.0 (TID 46). 787 bytes result sent to driver
2017-08-10 09:13:08,445 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 23.0 (TID 46) in 17719 ms on localhost (2/2)
2017-08-10 09:13:08,445 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 23.0, whose tasks have all completed, from pool 
2017-08-10 09:13:08,445 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 23 (foreachPartition at streamingProcess.scala:50) finished in 17.719 s
2017-08-10 09:13:08,446 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 23 finished: foreachPartition at streamingProcess.scala:50, took 17.727886 s
2017-08-10 09:13:08,446 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327030000 ms.0 from job set of time 1502327030000 ms
2017-08-10 09:13:08,446 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 558.446 s for time 1502327030000 ms (execution: 17.742 s)
2017-08-10 09:13:08,446 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327035000 ms.0 from job set of time 1502327035000 ms
2017-08-10 09:13:08,448 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 45 from persistence list
2017-08-10 09:13:08,449 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 44 from persistence list
2017-08-10 09:13:08,449 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 45
2017-08-10 09:13:08,450 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 44
2017-08-10 09:13:08,450 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:13:08,450 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327020000 ms
2017-08-10 09:13:08,460 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:13:08,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 24 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:13:08,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 24 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:13:08,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:13:08,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:13:08,462 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 24 (MapPartitionsRDD[49] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:13:08,465 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:13:08,474 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:13:08,475 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_24_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:13:08,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 24 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:13:08,476 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at map at streamingProcess.scala:48)
2017-08-10 09:13:08,476 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 24.0 with 2 tasks
2017-08-10 09:13:08,477 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 24.0 (TID 48, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:13:08,478 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 24.0 (TID 49, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:13:08,478 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 24.0 (TID 48)
2017-08-10 09:13:08,478 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 24.0 (TID 49)
2017-08-10 09:13:08,484 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 600 -> 625
2017-08-10 09:13:08,484 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 600 -> 625
2017-08-10 09:13:10,177 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327590000 ms
2017-08-10 09:13:15,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327595000 ms
2017-08-10 09:13:15,926 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 24.0 (TID 49). 714 bytes result sent to driver
2017-08-10 09:13:15,929 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 24.0 (TID 49) in 7451 ms on localhost (1/2)
2017-08-10 09:13:20,141 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327600000 ms
2017-08-10 09:13:22,269 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_23_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:13:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327605000 ms
2017-08-10 09:13:30,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327610000 ms
2017-08-10 09:13:32,143 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 24.0 (TID 48). 787 bytes result sent to driver
2017-08-10 09:13:32,145 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 24.0 (TID 48) in 23668 ms on localhost (2/2)
2017-08-10 09:13:32,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 24 (foreachPartition at streamingProcess.scala:50) finished in 23.669 s
2017-08-10 09:13:32,145 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 24.0, whose tasks have all completed, from pool 
2017-08-10 09:13:32,145 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 24 finished: foreachPartition at streamingProcess.scala:50, took 23.683740 s
2017-08-10 09:13:32,146 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327035000 ms.0 from job set of time 1502327035000 ms
2017-08-10 09:13:32,146 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 577.146 s for time 1502327035000 ms (execution: 23.700 s)
2017-08-10 09:13:32,146 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 47 from persistence list
2017-08-10 09:13:32,147 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327040000 ms.0 from job set of time 1502327040000 ms
2017-08-10 09:13:32,147 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 47
2017-08-10 09:13:32,147 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 46 from persistence list
2017-08-10 09:13:32,148 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 46
2017-08-10 09:13:32,148 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:13:32,148 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327025000 ms
2017-08-10 09:13:32,158 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:13:32,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 25 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:13:32,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 25 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:13:32,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:13:32,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:13:32,160 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 25 (MapPartitionsRDD[51] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:13:32,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:13:32,166 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:13:32,167 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_25_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:13:32,167 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 25 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:13:32,167 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at map at streamingProcess.scala:48)
2017-08-10 09:13:32,167 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 25.0 with 2 tasks
2017-08-10 09:13:32,169 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 25.0 (TID 50, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:13:32,169 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 25.0 (TID 51, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:13:32,170 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 25.0 (TID 51)
2017-08-10 09:13:32,170 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 25.0 (TID 50)
2017-08-10 09:13:32,173 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 625 -> 650
2017-08-10 09:13:32,173 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 625 -> 650
2017-08-10 09:13:32,316 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_24_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:13:35,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327615000 ms
2017-08-10 09:13:40,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327620000 ms
2017-08-10 09:13:42,986 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 25.0 (TID 51). 787 bytes result sent to driver
2017-08-10 09:13:42,988 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 25.0 (TID 51) in 10819 ms on localhost (1/2)
2017-08-10 09:13:45,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327625000 ms
2017-08-10 09:13:46,205 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 25.0 (TID 50). 787 bytes result sent to driver
2017-08-10 09:13:46,207 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 25.0 (TID 50) in 14038 ms on localhost (2/2)
2017-08-10 09:13:46,207 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 25.0, whose tasks have all completed, from pool 
2017-08-10 09:13:46,207 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 25 (foreachPartition at streamingProcess.scala:50) finished in 14.039 s
2017-08-10 09:13:46,207 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 25 finished: foreachPartition at streamingProcess.scala:50, took 14.047745 s
2017-08-10 09:13:46,208 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327040000 ms.0 from job set of time 1502327040000 ms
2017-08-10 09:13:46,208 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 586.208 s for time 1502327040000 ms (execution: 14.061 s)
2017-08-10 09:13:46,208 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 49 from persistence list
2017-08-10 09:13:46,208 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327045000 ms.0 from job set of time 1502327045000 ms
2017-08-10 09:13:46,208 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 49
2017-08-10 09:13:46,208 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 48 from persistence list
2017-08-10 09:13:46,209 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 48
2017-08-10 09:13:46,209 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:13:46,209 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327030000 ms
2017-08-10 09:13:46,219 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:13:46,219 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 26 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:13:46,219 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 26 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:13:46,220 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:13:46,220 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:13:46,220 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 26 (MapPartitionsRDD[53] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:13:46,223 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:13:46,226 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:13:46,226 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_26_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:13:46,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 26 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:13:46,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 26 (MapPartitionsRDD[53] at map at streamingProcess.scala:48)
2017-08-10 09:13:46,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 26.0 with 2 tasks
2017-08-10 09:13:46,229 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 26.0 (TID 52, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:13:46,230 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 26.0 (TID 53, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:13:46,230 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 26.0 (TID 52)
2017-08-10 09:13:46,230 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 26.0 (TID 53)
2017-08-10 09:13:46,232 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 650 -> 653
2017-08-10 09:13:46,232 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 650 -> 675
2017-08-10 09:13:46,995 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 26.0 (TID 52). 714 bytes result sent to driver
2017-08-10 09:13:46,997 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 26.0 (TID 52) in 769 ms on localhost (1/2)
2017-08-10 09:13:50,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327630000 ms
2017-08-10 09:13:55,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327635000 ms
2017-08-10 09:13:58,292 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_25_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327640000 ms
2017-08-10 09:14:05,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327645000 ms
2017-08-10 09:14:07,133 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 26.0 (TID 53). 787 bytes result sent to driver
2017-08-10 09:14:07,135 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 26.0 (TID 53) in 20906 ms on localhost (2/2)
2017-08-10 09:14:07,136 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 26.0, whose tasks have all completed, from pool 
2017-08-10 09:14:07,136 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 26 (foreachPartition at streamingProcess.scala:50) finished in 20.909 s
2017-08-10 09:14:07,136 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 26 finished: foreachPartition at streamingProcess.scala:50, took 20.916254 s
2017-08-10 09:14:07,137 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327045000 ms.0 from job set of time 1502327045000 ms
2017-08-10 09:14:07,137 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 602.137 s for time 1502327045000 ms (execution: 20.929 s)
2017-08-10 09:14:07,137 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 51 from persistence list
2017-08-10 09:14:07,137 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327050000 ms.0 from job set of time 1502327050000 ms
2017-08-10 09:14:07,137 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 51
2017-08-10 09:14:07,137 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 50 from persistence list
2017-08-10 09:14:07,138 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 50
2017-08-10 09:14:07,138 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:07,138 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327035000 ms
2017-08-10 09:14:07,152 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:07,152 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 27 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:07,153 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 27 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:07,153 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:07,153 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:07,153 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 27 (MapPartitionsRDD[55] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:07,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:07,161 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:07,162 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_27_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:07,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 27 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:07,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at map at streamingProcess.scala:48)
2017-08-10 09:14:07,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 27.0 with 2 tasks
2017-08-10 09:14:07,164 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 27.0 (TID 54, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:14:07,165 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 27.0 (TID 55, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:14:07,165 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 27.0 (TID 54)
2017-08-10 09:14:07,165 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 27.0 (TID 55)
2017-08-10 09:14:07,167 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:07,167 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 675 -> 698
2017-08-10 09:14:07,170 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 27.0 (TID 54). 714 bytes result sent to driver
2017-08-10 09:14:07,173 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 27.0 (TID 54) in 9 ms on localhost (1/2)
2017-08-10 09:14:10,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327650000 ms
2017-08-10 09:14:10,694 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_26_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:15,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327655000 ms
2017-08-10 09:14:16,196 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 27.0 (TID 55). 787 bytes result sent to driver
2017-08-10 09:14:16,198 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 27.0 (TID 55) in 9034 ms on localhost (2/2)
2017-08-10 09:14:16,198 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 27.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 27 (foreachPartition at streamingProcess.scala:50) finished in 9.035 s
2017-08-10 09:14:16,199 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 27 finished: foreachPartition at streamingProcess.scala:50, took 9.046204 s
2017-08-10 09:14:16,199 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327050000 ms.0 from job set of time 1502327050000 ms
2017-08-10 09:14:16,199 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 606.199 s for time 1502327050000 ms (execution: 9.062 s)
2017-08-10 09:14:16,199 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 53 from persistence list
2017-08-10 09:14:16,199 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327055000 ms.0 from job set of time 1502327055000 ms
2017-08-10 09:14:16,200 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 53
2017-08-10 09:14:16,200 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 52 from persistence list
2017-08-10 09:14:16,200 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 52
2017-08-10 09:14:16,200 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,200 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327040000 ms
2017-08-10 09:14:16,210 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,211 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 28 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,211 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 28 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,211 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,211 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,211 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 28 (MapPartitionsRDD[57] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,214 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:16,218 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:16,219 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_28_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,219 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 28 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,219 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,219 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 28.0 with 2 tasks
2017-08-10 09:14:16,221 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 28.0 (TID 56, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:14:16,221 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 28.0 (TID 57, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:14:16,221 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 28.0 (TID 57)
2017-08-10 09:14:16,221 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 28.0 (TID 56)
2017-08-10 09:14:16,224 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,224 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,227 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 28.0 (TID 56). 714 bytes result sent to driver
2017-08-10 09:14:16,227 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 28.0 (TID 57). 714 bytes result sent to driver
2017-08-10 09:14:16,229 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 28.0 (TID 56) in 9 ms on localhost (1/2)
2017-08-10 09:14:16,229 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 28.0 (TID 57) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,229 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 28.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,229 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 28 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:14:16,230 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 28 finished: foreachPartition at streamingProcess.scala:50, took 0.019560 s
2017-08-10 09:14:16,230 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327055000 ms.0 from job set of time 1502327055000 ms
2017-08-10 09:14:16,230 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 601.230 s for time 1502327055000 ms (execution: 0.031 s)
2017-08-10 09:14:16,230 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 55 from persistence list
2017-08-10 09:14:16,230 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327060000 ms.0 from job set of time 1502327060000 ms
2017-08-10 09:14:16,231 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 55
2017-08-10 09:14:16,231 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 54 from persistence list
2017-08-10 09:14:16,231 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 54
2017-08-10 09:14:16,231 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,231 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327045000 ms
2017-08-10 09:14:16,241 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 29 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 29 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 29 (MapPartitionsRDD[59] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,245 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:16,249 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:16,249 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_29_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,250 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 29 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,250 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 29 (MapPartitionsRDD[59] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,250 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 29.0 with 2 tasks
2017-08-10 09:14:16,251 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 29.0 (TID 58, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:14:16,252 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 29.0 (TID 59, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:14:16,252 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 29.0 (TID 59)
2017-08-10 09:14:16,252 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 29.0 (TID 58)
2017-08-10 09:14:16,255 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,255 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,258 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 29.0 (TID 59). 714 bytes result sent to driver
2017-08-10 09:14:16,258 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 29.0 (TID 58). 714 bytes result sent to driver
2017-08-10 09:14:16,260 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 29.0 (TID 59) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,260 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 29.0 (TID 58) in 10 ms on localhost (2/2)
2017-08-10 09:14:16,260 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 29.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 29 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:14:16,260 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 29 finished: foreachPartition at streamingProcess.scala:50, took 0.019168 s
2017-08-10 09:14:16,261 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327060000 ms.0 from job set of time 1502327060000 ms
2017-08-10 09:14:16,261 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 596.261 s for time 1502327060000 ms (execution: 0.031 s)
2017-08-10 09:14:16,261 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 57 from persistence list
2017-08-10 09:14:16,261 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327065000 ms.0 from job set of time 1502327065000 ms
2017-08-10 09:14:16,261 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 57
2017-08-10 09:14:16,262 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 56 from persistence list
2017-08-10 09:14:16,262 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 56
2017-08-10 09:14:16,262 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,262 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327050000 ms
2017-08-10 09:14:16,273 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,273 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 30 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,274 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 30 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,274 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,274 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,274 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 30 (MapPartitionsRDD[61] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:16,280 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:16,282 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_30_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,282 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 30 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 30.0 with 2 tasks
2017-08-10 09:14:16,284 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 30.0 (TID 60, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:14:16,285 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 30.0 (TID 61, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:14:16,285 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 30.0 (TID 60)
2017-08-10 09:14:16,285 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 30.0 (TID 61)
2017-08-10 09:14:16,287 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,287 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,290 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 30.0 (TID 60). 714 bytes result sent to driver
2017-08-10 09:14:16,290 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 30.0 (TID 61). 714 bytes result sent to driver
2017-08-10 09:14:16,292 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 30.0 (TID 61) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,292 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 30.0 (TID 60) in 9 ms on localhost (2/2)
2017-08-10 09:14:16,292 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 30.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,292 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 30 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,293 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 30 finished: foreachPartition at streamingProcess.scala:50, took 0.019895 s
2017-08-10 09:14:16,293 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327065000 ms.0 from job set of time 1502327065000 ms
2017-08-10 09:14:16,293 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 591.293 s for time 1502327065000 ms (execution: 0.032 s)
2017-08-10 09:14:16,293 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 59 from persistence list
2017-08-10 09:14:16,293 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327070000 ms.0 from job set of time 1502327070000 ms
2017-08-10 09:14:16,294 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 59
2017-08-10 09:14:16,294 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 58 from persistence list
2017-08-10 09:14:16,294 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 58
2017-08-10 09:14:16,294 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,294 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327055000 ms
2017-08-10 09:14:16,304 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,305 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 31 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,305 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 31 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,305 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,305 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,305 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 31 (MapPartitionsRDD[63] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,308 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:16,312 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:16,312 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_31_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 31 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 31.0 with 2 tasks
2017-08-10 09:14:16,314 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 31.0 (TID 62, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:14:16,315 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 31.0 (TID 63, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:14:16,315 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 31.0 (TID 62)
2017-08-10 09:14:16,315 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 31.0 (TID 63)
2017-08-10 09:14:16,317 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,317 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,320 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 31.0 (TID 62). 714 bytes result sent to driver
2017-08-10 09:14:16,320 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 31.0 (TID 63). 714 bytes result sent to driver
2017-08-10 09:14:16,322 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 31.0 (TID 62) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,322 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 31.0 (TID 63) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,322 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 31.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,322 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 31 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,322 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 31 finished: foreachPartition at streamingProcess.scala:50, took 0.017955 s
2017-08-10 09:14:16,323 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327070000 ms.0 from job set of time 1502327070000 ms
2017-08-10 09:14:16,323 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 586.323 s for time 1502327070000 ms (execution: 0.030 s)
2017-08-10 09:14:16,323 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327075000 ms.0 from job set of time 1502327075000 ms
2017-08-10 09:14:16,323 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 61 from persistence list
2017-08-10 09:14:16,324 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 60 from persistence list
2017-08-10 09:14:16,324 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 61
2017-08-10 09:14:16,324 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,324 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327060000 ms
2017-08-10 09:14:16,324 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 60
2017-08-10 09:14:16,334 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 32 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 32 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 32 (MapPartitionsRDD[65] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,338 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:16,341 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:16,342 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_32_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 32 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,343 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 32.0 with 2 tasks
2017-08-10 09:14:16,344 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 32.0 (TID 64, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:14:16,345 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 32.0 (TID 65, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:14:16,345 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 32.0 (TID 64)
2017-08-10 09:14:16,345 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 32.0 (TID 65)
2017-08-10 09:14:16,347 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,347 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,350 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 32.0 (TID 65). 714 bytes result sent to driver
2017-08-10 09:14:16,350 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 32.0 (TID 64). 714 bytes result sent to driver
2017-08-10 09:14:16,352 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 32.0 (TID 65) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,352 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 32.0 (TID 64) in 9 ms on localhost (2/2)
2017-08-10 09:14:16,352 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 32.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,352 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 32 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,353 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 32 finished: foreachPartition at streamingProcess.scala:50, took 0.018232 s
2017-08-10 09:14:16,353 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327075000 ms.0 from job set of time 1502327075000 ms
2017-08-10 09:14:16,353 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 581.353 s for time 1502327075000 ms (execution: 0.030 s)
2017-08-10 09:14:16,353 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 63 from persistence list
2017-08-10 09:14:16,353 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327080000 ms.0 from job set of time 1502327080000 ms
2017-08-10 09:14:16,354 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 62 from persistence list
2017-08-10 09:14:16,354 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 63
2017-08-10 09:14:16,354 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,354 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 62
2017-08-10 09:14:16,354 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327065000 ms
2017-08-10 09:14:16,364 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 33 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 33 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 33 (MapPartitionsRDD[67] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,368 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:16,371 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:16,372 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_33_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,373 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 33 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,373 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 33 (MapPartitionsRDD[67] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,373 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 33.0 with 2 tasks
2017-08-10 09:14:16,374 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 33.0 (TID 66, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,374 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 33.0 (TID 67, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,375 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 33.0 (TID 66)
2017-08-10 09:14:16,375 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 33.0 (TID 67)
2017-08-10 09:14:16,377 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,377 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,380 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 33.0 (TID 66). 714 bytes result sent to driver
2017-08-10 09:14:16,380 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 33.0 (TID 67). 714 bytes result sent to driver
2017-08-10 09:14:16,381 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 33.0 (TID 67) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,382 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 33.0 (TID 66) in 9 ms on localhost (2/2)
2017-08-10 09:14:16,382 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 33.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,382 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 33 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,382 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 33 finished: foreachPartition at streamingProcess.scala:50, took 0.018034 s
2017-08-10 09:14:16,382 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327080000 ms.0 from job set of time 1502327080000 ms
2017-08-10 09:14:16,383 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 65 from persistence list
2017-08-10 09:14:16,383 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 576.382 s for time 1502327080000 ms (execution: 0.029 s)
2017-08-10 09:14:16,383 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327085000 ms.0 from job set of time 1502327085000 ms
2017-08-10 09:14:16,383 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 65
2017-08-10 09:14:16,383 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 64 from persistence list
2017-08-10 09:14:16,383 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 64
2017-08-10 09:14:16,384 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,384 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327070000 ms
2017-08-10 09:14:16,394 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 34 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 34 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,395 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,395 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,395 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 34 (MapPartitionsRDD[69] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,398 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_34 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:16,400 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_34_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:16,401 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_34_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,402 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 34 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,402 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 34 (MapPartitionsRDD[69] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,402 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 34.0 with 2 tasks
2017-08-10 09:14:16,403 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 34.0 (TID 68, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,404 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 34.0 (TID 69, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,404 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 34.0 (TID 68)
2017-08-10 09:14:16,404 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 34.0 (TID 69)
2017-08-10 09:14:16,406 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,406 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,409 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 34.0 (TID 68). 714 bytes result sent to driver
2017-08-10 09:14:16,409 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 34.0 (TID 69). 714 bytes result sent to driver
2017-08-10 09:14:16,411 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 34.0 (TID 69) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,411 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 34.0 (TID 68) in 9 ms on localhost (2/2)
2017-08-10 09:14:16,411 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 34.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,411 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 34 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,412 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 34 finished: foreachPartition at streamingProcess.scala:50, took 0.017645 s
2017-08-10 09:14:16,412 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327085000 ms.0 from job set of time 1502327085000 ms
2017-08-10 09:14:16,412 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 571.412 s for time 1502327085000 ms (execution: 0.029 s)
2017-08-10 09:14:16,412 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 67 from persistence list
2017-08-10 09:14:16,412 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327090000 ms.0 from job set of time 1502327090000 ms
2017-08-10 09:14:16,412 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 67
2017-08-10 09:14:16,413 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 66 from persistence list
2017-08-10 09:14:16,413 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 66
2017-08-10 09:14:16,413 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,413 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327075000 ms
2017-08-10 09:14:16,423 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,424 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 35 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,424 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 35 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,424 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,424 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,424 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 35 (MapPartitionsRDD[71] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,427 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_35 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:16,430 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_35_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:16,430 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_35_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,430 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 35 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,430 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 35 (MapPartitionsRDD[71] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,431 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 35.0 with 2 tasks
2017-08-10 09:14:16,432 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 35.0 (TID 70, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,432 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 35.0 (TID 71, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,432 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 35.0 (TID 70)
2017-08-10 09:14:16,432 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 35.0 (TID 71)
2017-08-10 09:14:16,434 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,434 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,437 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 35.0 (TID 71). 714 bytes result sent to driver
2017-08-10 09:14:16,437 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 35.0 (TID 70). 714 bytes result sent to driver
2017-08-10 09:14:16,439 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 35.0 (TID 70) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,439 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 35.0 (TID 71) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,439 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 35.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,439 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 35 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,440 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 35 finished: foreachPartition at streamingProcess.scala:50, took 0.016866 s
2017-08-10 09:14:16,440 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327090000 ms.0 from job set of time 1502327090000 ms
2017-08-10 09:14:16,440 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 566.440 s for time 1502327090000 ms (execution: 0.028 s)
2017-08-10 09:14:16,440 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 69 from persistence list
2017-08-10 09:14:16,440 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327095000 ms.0 from job set of time 1502327095000 ms
2017-08-10 09:14:16,441 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 69
2017-08-10 09:14:16,441 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 68 from persistence list
2017-08-10 09:14:16,441 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,441 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327080000 ms
2017-08-10 09:14:16,442 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 68
2017-08-10 09:14:16,452 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,452 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 36 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,452 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 36 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,452 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,452 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,453 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 36 (MapPartitionsRDD[73] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_36 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:16,463 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_36_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:16,464 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_36_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,464 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_27_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,465 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 36 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,465 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 36 (MapPartitionsRDD[73] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,465 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 36.0 with 2 tasks
2017-08-10 09:14:16,466 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_28_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,466 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 36.0 (TID 72, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,467 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 36.0 (TID 73, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,468 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 36.0 (TID 72)
2017-08-10 09:14:16,468 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 36.0 (TID 73)
2017-08-10 09:14:16,469 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_29_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,470 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_30_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,471 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,471 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,472 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_31_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,474 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_32_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,474 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 36.0 (TID 73). 714 bytes result sent to driver
2017-08-10 09:14:16,474 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 36.0 (TID 72). 714 bytes result sent to driver
2017-08-10 09:14:16,475 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_33_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,476 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 36.0 (TID 73) in 9 ms on localhost (1/2)
2017-08-10 09:14:16,476 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 36.0 (TID 72) in 11 ms on localhost (2/2)
2017-08-10 09:14:16,476 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 36.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,477 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_34_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,477 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 36 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:14:16,477 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 36 finished: foreachPartition at streamingProcess.scala:50, took 0.025238 s
2017-08-10 09:14:16,478 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327095000 ms.0 from job set of time 1502327095000 ms
2017-08-10 09:14:16,478 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 561.478 s for time 1502327095000 ms (execution: 0.038 s)
2017-08-10 09:14:16,478 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327100000 ms.0 from job set of time 1502327100000 ms
2017-08-10 09:14:16,478 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_35_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,478 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 71 from persistence list
2017-08-10 09:14:16,479 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 71
2017-08-10 09:14:16,479 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 70 from persistence list
2017-08-10 09:14:16,479 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 70
2017-08-10 09:14:16,479 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,479 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327085000 ms
2017-08-10 09:14:16,490 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,490 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 37 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,490 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 37 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,491 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,491 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,491 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 37 (MapPartitionsRDD[75] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,494 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_37 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:16,496 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_37_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:16,497 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_37_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 37 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 37 (MapPartitionsRDD[75] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 37.0 with 2 tasks
2017-08-10 09:14:16,498 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 37.0 (TID 74, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,499 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 37.0 (TID 75, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,499 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 37.0 (TID 75)
2017-08-10 09:14:16,499 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 37.0 (TID 74)
2017-08-10 09:14:16,501 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,501 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,504 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 37.0 (TID 74). 714 bytes result sent to driver
2017-08-10 09:14:16,504 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 37.0 (TID 75). 714 bytes result sent to driver
2017-08-10 09:14:16,505 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 37.0 (TID 74) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,506 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 37.0 (TID 75) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,506 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 37.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,506 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 37 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,506 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 37 finished: foreachPartition at streamingProcess.scala:50, took 0.016092 s
2017-08-10 09:14:16,506 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327100000 ms.0 from job set of time 1502327100000 ms
2017-08-10 09:14:16,507 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 556.506 s for time 1502327100000 ms (execution: 0.028 s)
2017-08-10 09:14:16,507 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 73 from persistence list
2017-08-10 09:14:16,507 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327105000 ms.0 from job set of time 1502327105000 ms
2017-08-10 09:14:16,507 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 73
2017-08-10 09:14:16,507 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 72 from persistence list
2017-08-10 09:14:16,507 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 72
2017-08-10 09:14:16,507 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,507 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327090000 ms
2017-08-10 09:14:16,517 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 38 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 38 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 38 (MapPartitionsRDD[77] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,520 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_38 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:16,523 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_38_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:16,523 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_38_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,524 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 38 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,524 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 38 (MapPartitionsRDD[77] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,524 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 38.0 with 2 tasks
2017-08-10 09:14:16,525 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 38.0 (TID 76, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,525 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 38.0 (TID 77, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,525 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 38.0 (TID 76)
2017-08-10 09:14:16,525 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 38.0 (TID 77)
2017-08-10 09:14:16,527 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,527 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,530 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 38.0 (TID 76). 714 bytes result sent to driver
2017-08-10 09:14:16,530 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 38.0 (TID 77). 714 bytes result sent to driver
2017-08-10 09:14:16,532 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 38.0 (TID 77) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,532 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 38.0 (TID 76) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,532 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 38.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,532 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 38 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,532 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 38 finished: foreachPartition at streamingProcess.scala:50, took 0.015284 s
2017-08-10 09:14:16,533 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327105000 ms.0 from job set of time 1502327105000 ms
2017-08-10 09:14:16,533 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 551.533 s for time 1502327105000 ms (execution: 0.026 s)
2017-08-10 09:14:16,533 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 75 from persistence list
2017-08-10 09:14:16,533 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327110000 ms.0 from job set of time 1502327110000 ms
2017-08-10 09:14:16,533 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 75
2017-08-10 09:14:16,533 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 74 from persistence list
2017-08-10 09:14:16,534 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 74
2017-08-10 09:14:16,534 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,534 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327095000 ms
2017-08-10 09:14:16,544 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,544 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 39 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,544 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 39 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,544 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,545 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,545 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 39 (MapPartitionsRDD[79] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,548 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_39 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:16,550 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_39_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:16,551 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_39_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,551 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 39 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,552 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 39 (MapPartitionsRDD[79] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,552 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 39.0 with 2 tasks
2017-08-10 09:14:16,553 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 39.0 (TID 78, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,554 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 39.0 (TID 79, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,554 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 39.0 (TID 78)
2017-08-10 09:14:16,554 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 39.0 (TID 79)
2017-08-10 09:14:16,556 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,556 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,559 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 39.0 (TID 78). 714 bytes result sent to driver
2017-08-10 09:14:16,559 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 39.0 (TID 79). 714 bytes result sent to driver
2017-08-10 09:14:16,560 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 39.0 (TID 78) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,560 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 39.0 (TID 79) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,560 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 39.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,560 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 39 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,561 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 39 finished: foreachPartition at streamingProcess.scala:50, took 0.017084 s
2017-08-10 09:14:16,562 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327110000 ms.0 from job set of time 1502327110000 ms
2017-08-10 09:14:16,562 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 77 from persistence list
2017-08-10 09:14:16,562 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 546.562 s for time 1502327110000 ms (execution: 0.029 s)
2017-08-10 09:14:16,562 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327115000 ms.0 from job set of time 1502327115000 ms
2017-08-10 09:14:16,562 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 77
2017-08-10 09:14:16,562 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 76 from persistence list
2017-08-10 09:14:16,563 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 76
2017-08-10 09:14:16,563 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,563 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327100000 ms
2017-08-10 09:14:16,572 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 40 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 40 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,573 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 40 (MapPartitionsRDD[81] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,576 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_40 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:16,580 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_40_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:16,581 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_40_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,583 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 40 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,583 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 40 (MapPartitionsRDD[81] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,583 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 40.0 with 2 tasks
2017-08-10 09:14:16,588 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 40.0 (TID 80, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,589 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 40.0 (TID 81, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,590 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 40.0 (TID 80)
2017-08-10 09:14:16,591 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 40.0 (TID 81)
2017-08-10 09:14:16,592 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,593 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,596 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 40.0 (TID 81). 714 bytes result sent to driver
2017-08-10 09:14:16,597 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 40.0 (TID 80). 714 bytes result sent to driver
2017-08-10 09:14:16,599 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 40.0 (TID 81) in 10 ms on localhost (1/2)
2017-08-10 09:14:16,600 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 40.0 (TID 80) in 15 ms on localhost (2/2)
2017-08-10 09:14:16,601 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 40.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,601 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 40 (foreachPartition at streamingProcess.scala:50) finished in 0.017 s
2017-08-10 09:14:16,602 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 40 finished: foreachPartition at streamingProcess.scala:50, took 0.028899 s
2017-08-10 09:14:16,602 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327115000 ms.0 from job set of time 1502327115000 ms
2017-08-10 09:14:16,602 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 541.602 s for time 1502327115000 ms (execution: 0.040 s)
2017-08-10 09:14:16,602 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327120000 ms.0 from job set of time 1502327120000 ms
2017-08-10 09:14:16,603 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 79 from persistence list
2017-08-10 09:14:16,603 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 79
2017-08-10 09:14:16,603 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 78 from persistence list
2017-08-10 09:14:16,604 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 78
2017-08-10 09:14:16,604 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,604 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327105000 ms
2017-08-10 09:14:16,612 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,613 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 41 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,613 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 41 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,613 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,613 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,613 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 41 (MapPartitionsRDD[83] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,616 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_41 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:16,619 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_41_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:16,619 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_41_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,620 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 41 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,620 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 41 (MapPartitionsRDD[83] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,620 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 41.0 with 2 tasks
2017-08-10 09:14:16,621 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 41.0 (TID 82, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,622 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 41.0 (TID 83, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,622 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 41.0 (TID 82)
2017-08-10 09:14:16,622 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 41.0 (TID 83)
2017-08-10 09:14:16,623 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,623 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,626 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 41.0 (TID 82). 714 bytes result sent to driver
2017-08-10 09:14:16,626 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 41.0 (TID 83). 714 bytes result sent to driver
2017-08-10 09:14:16,628 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 41.0 (TID 83) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,628 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 41.0 (TID 82) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,628 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 41.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,628 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 41 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,629 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 41 finished: foreachPartition at streamingProcess.scala:50, took 0.015756 s
2017-08-10 09:14:16,629 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327120000 ms.0 from job set of time 1502327120000 ms
2017-08-10 09:14:16,629 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 81 from persistence list
2017-08-10 09:14:16,629 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 536.629 s for time 1502327120000 ms (execution: 0.027 s)
2017-08-10 09:14:16,629 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327125000 ms.0 from job set of time 1502327125000 ms
2017-08-10 09:14:16,629 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 81
2017-08-10 09:14:16,629 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 80 from persistence list
2017-08-10 09:14:16,630 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 80
2017-08-10 09:14:16,630 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,630 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327110000 ms
2017-08-10 09:14:16,640 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,640 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 42 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,640 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 42 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,640 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,640 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,641 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 42 (MapPartitionsRDD[85] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,644 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_42 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:16,647 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_42_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:16,647 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_42_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,647 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 42 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,647 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 42 (MapPartitionsRDD[85] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,648 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 42.0 with 2 tasks
2017-08-10 09:14:16,648 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 42.0 (TID 84, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,649 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 42.0 (TID 85, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,649 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 42.0 (TID 85)
2017-08-10 09:14:16,649 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 42.0 (TID 84)
2017-08-10 09:14:16,651 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,651 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,654 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 42.0 (TID 85). 714 bytes result sent to driver
2017-08-10 09:14:16,654 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 42.0 (TID 84). 714 bytes result sent to driver
2017-08-10 09:14:16,655 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 42.0 (TID 85) in 6 ms on localhost (1/2)
2017-08-10 09:14:16,656 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 42.0 (TID 84) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,656 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 42.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,656 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 42 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,656 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 42 finished: foreachPartition at streamingProcess.scala:50, took 0.016277 s
2017-08-10 09:14:16,656 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327125000 ms.0 from job set of time 1502327125000 ms
2017-08-10 09:14:16,657 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 531.656 s for time 1502327125000 ms (execution: 0.027 s)
2017-08-10 09:14:16,657 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 83 from persistence list
2017-08-10 09:14:16,657 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327130000 ms.0 from job set of time 1502327130000 ms
2017-08-10 09:14:16,657 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 83
2017-08-10 09:14:16,657 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 82 from persistence list
2017-08-10 09:14:16,658 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 82
2017-08-10 09:14:16,658 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,658 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327115000 ms
2017-08-10 09:14:16,667 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 43 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 43 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 43 (MapPartitionsRDD[87] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,671 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_43 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:16,673 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_43_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:16,674 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_43_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,674 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 43 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,674 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 43 (MapPartitionsRDD[87] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,674 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 43.0 with 2 tasks
2017-08-10 09:14:16,675 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 43.0 (TID 86, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,675 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 43.0 (TID 87, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,676 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 43.0 (TID 86)
2017-08-10 09:14:16,676 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 43.0 (TID 87)
2017-08-10 09:14:16,677 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,677 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,680 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 43.0 (TID 86). 714 bytes result sent to driver
2017-08-10 09:14:16,680 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 43.0 (TID 87). 714 bytes result sent to driver
2017-08-10 09:14:16,682 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 43.0 (TID 86) in 6 ms on localhost (1/2)
2017-08-10 09:14:16,682 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 43.0 (TID 87) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,682 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 43.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,682 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 43 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,682 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 43 finished: foreachPartition at streamingProcess.scala:50, took 0.015164 s
2017-08-10 09:14:16,683 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327130000 ms.0 from job set of time 1502327130000 ms
2017-08-10 09:14:16,683 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 85 from persistence list
2017-08-10 09:14:16,683 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 526.683 s for time 1502327130000 ms (execution: 0.026 s)
2017-08-10 09:14:16,683 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327135000 ms.0 from job set of time 1502327135000 ms
2017-08-10 09:14:16,683 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 85
2017-08-10 09:14:16,683 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 84 from persistence list
2017-08-10 09:14:16,684 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 84
2017-08-10 09:14:16,684 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,684 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327120000 ms
2017-08-10 09:14:16,693 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,693 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 44 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,694 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 44 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,694 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,694 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,694 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 44 (MapPartitionsRDD[89] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,696 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_44 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:16,699 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_44_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:16,700 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_44_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 44 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 44 (MapPartitionsRDD[89] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 44.0 with 2 tasks
2017-08-10 09:14:16,701 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 44.0 (TID 88, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,702 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 44.0 (TID 89, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,702 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 44.0 (TID 89)
2017-08-10 09:14:16,702 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 44.0 (TID 88)
2017-08-10 09:14:16,704 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,704 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,706 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 44.0 (TID 88). 714 bytes result sent to driver
2017-08-10 09:14:16,706 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 44.0 (TID 89). 714 bytes result sent to driver
2017-08-10 09:14:16,708 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 44.0 (TID 88) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,708 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 44.0 (TID 89) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,708 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 44.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,708 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 44 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,708 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 44 finished: foreachPartition at streamingProcess.scala:50, took 0.015280 s
2017-08-10 09:14:16,709 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327135000 ms.0 from job set of time 1502327135000 ms
2017-08-10 09:14:16,709 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 521.709 s for time 1502327135000 ms (execution: 0.026 s)
2017-08-10 09:14:16,709 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 87 from persistence list
2017-08-10 09:14:16,709 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327140000 ms.0 from job set of time 1502327140000 ms
2017-08-10 09:14:16,709 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 87
2017-08-10 09:14:16,709 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 86 from persistence list
2017-08-10 09:14:16,710 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 86
2017-08-10 09:14:16,710 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,710 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327125000 ms
2017-08-10 09:14:16,719 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,720 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 45 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,720 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 45 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,720 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,720 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,720 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 45 (MapPartitionsRDD[91] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,723 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_45 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:16,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_45_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:16,726 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_45_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 45 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 45 (MapPartitionsRDD[91] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 45.0 with 2 tasks
2017-08-10 09:14:16,727 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 45.0 (TID 90, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,728 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 45.0 (TID 91, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,728 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 45.0 (TID 90)
2017-08-10 09:14:16,728 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 45.0 (TID 91)
2017-08-10 09:14:16,730 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,730 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,733 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 45.0 (TID 91). 714 bytes result sent to driver
2017-08-10 09:14:16,733 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 45.0 (TID 90). 714 bytes result sent to driver
2017-08-10 09:14:16,734 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 45.0 (TID 91) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,734 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 45.0 (TID 90) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,734 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 45.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,734 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 45 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:16,735 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 45 finished: foreachPartition at streamingProcess.scala:50, took 0.015176 s
2017-08-10 09:14:16,735 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327140000 ms.0 from job set of time 1502327140000 ms
2017-08-10 09:14:16,735 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 516.735 s for time 1502327140000 ms (execution: 0.026 s)
2017-08-10 09:14:16,735 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 89 from persistence list
2017-08-10 09:14:16,735 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327145000 ms.0 from job set of time 1502327145000 ms
2017-08-10 09:14:16,736 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 89
2017-08-10 09:14:16,736 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 88 from persistence list
2017-08-10 09:14:16,736 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 88
2017-08-10 09:14:16,736 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,736 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327130000 ms
2017-08-10 09:14:16,746 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,747 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 46 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,747 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 46 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,747 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,747 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,747 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 46 (MapPartitionsRDD[93] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,750 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_46 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:16,752 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_46_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:16,753 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_46_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,754 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 46 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,754 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 46 (MapPartitionsRDD[93] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,754 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 46.0 with 2 tasks
2017-08-10 09:14:16,755 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 46.0 (TID 92, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,755 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 46.0 (TID 93, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,755 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 46.0 (TID 92)
2017-08-10 09:14:16,755 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 46.0 (TID 93)
2017-08-10 09:14:16,757 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,757 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,759 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 46.0 (TID 93). 714 bytes result sent to driver
2017-08-10 09:14:16,759 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 46.0 (TID 92). 714 bytes result sent to driver
2017-08-10 09:14:16,761 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 46.0 (TID 92) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,763 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 46.0 (TID 93) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,763 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 46.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,763 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 46 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,763 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 46 finished: foreachPartition at streamingProcess.scala:50, took 0.016675 s
2017-08-10 09:14:16,763 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327145000 ms.0 from job set of time 1502327145000 ms
2017-08-10 09:14:16,763 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 511.763 s for time 1502327145000 ms (execution: 0.028 s)
2017-08-10 09:14:16,763 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 91 from persistence list
2017-08-10 09:14:16,764 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327150000 ms.0 from job set of time 1502327150000 ms
2017-08-10 09:14:16,764 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 91
2017-08-10 09:14:16,764 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 90 from persistence list
2017-08-10 09:14:16,765 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 90
2017-08-10 09:14:16,765 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,765 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327135000 ms
2017-08-10 09:14:16,775 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,775 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 47 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,775 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 47 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,775 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,776 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,776 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 47 (MapPartitionsRDD[95] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_47 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:16,782 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_47_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:16,782 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_47_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,783 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 47 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,783 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 47 (MapPartitionsRDD[95] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,783 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 47.0 with 2 tasks
2017-08-10 09:14:16,784 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 47.0 (TID 94, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,784 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 47.0 (TID 95, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,784 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 47.0 (TID 94)
2017-08-10 09:14:16,784 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 47.0 (TID 95)
2017-08-10 09:14:16,786 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,786 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,789 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 47.0 (TID 95). 714 bytes result sent to driver
2017-08-10 09:14:16,789 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 47.0 (TID 94). 714 bytes result sent to driver
2017-08-10 09:14:16,790 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 47.0 (TID 95) in 6 ms on localhost (1/2)
2017-08-10 09:14:16,790 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 47.0 (TID 94) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,791 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 47.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,791 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 47 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,791 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 47 finished: foreachPartition at streamingProcess.scala:50, took 0.015932 s
2017-08-10 09:14:16,791 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327150000 ms.0 from job set of time 1502327150000 ms
2017-08-10 09:14:16,792 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 506.791 s for time 1502327150000 ms (execution: 0.027 s)
2017-08-10 09:14:16,792 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 93 from persistence list
2017-08-10 09:14:16,792 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327155000 ms.0 from job set of time 1502327155000 ms
2017-08-10 09:14:16,792 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 93
2017-08-10 09:14:16,792 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 92 from persistence list
2017-08-10 09:14:16,792 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 92
2017-08-10 09:14:16,792 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,793 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327140000 ms
2017-08-10 09:14:16,803 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 48 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 48 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,804 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,804 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 48 (MapPartitionsRDD[97] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,806 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_48 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:16,809 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_48_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:16,809 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_48_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,809 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 48 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 48 (MapPartitionsRDD[97] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 48.0 with 2 tasks
2017-08-10 09:14:16,811 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 48.0 (TID 96, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,811 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 48.0 (TID 97, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,811 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 48.0 (TID 96)
2017-08-10 09:14:16,811 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 48.0 (TID 97)
2017-08-10 09:14:16,813 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,813 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,816 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 48.0 (TID 97). 714 bytes result sent to driver
2017-08-10 09:14:16,816 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 48.0 (TID 96). 714 bytes result sent to driver
2017-08-10 09:14:16,817 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 48.0 (TID 97) in 6 ms on localhost (1/2)
2017-08-10 09:14:16,817 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 48.0 (TID 96) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,818 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 48.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,818 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 48 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,818 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 48 finished: foreachPartition at streamingProcess.scala:50, took 0.014986 s
2017-08-10 09:14:16,818 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327155000 ms.0 from job set of time 1502327155000 ms
2017-08-10 09:14:16,818 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 501.818 s for time 1502327155000 ms (execution: 0.026 s)
2017-08-10 09:14:16,818 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 95 from persistence list
2017-08-10 09:14:16,819 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327160000 ms.0 from job set of time 1502327160000 ms
2017-08-10 09:14:16,819 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 95
2017-08-10 09:14:16,819 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 94 from persistence list
2017-08-10 09:14:16,819 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 94
2017-08-10 09:14:16,819 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,819 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327145000 ms
2017-08-10 09:14:16,830 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,830 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 49 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,830 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 49 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,830 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,830 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,830 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 49 (MapPartitionsRDD[99] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,833 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_49 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:16,836 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_49_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:16,836 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_49_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,836 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 49 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,836 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 49 (MapPartitionsRDD[99] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,836 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 49.0 with 2 tasks
2017-08-10 09:14:16,837 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 49.0 (TID 98, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,838 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 49.0 (TID 99, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,838 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 49.0 (TID 98)
2017-08-10 09:14:16,838 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 49.0 (TID 99)
2017-08-10 09:14:16,840 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,840 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,842 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 49.0 (TID 99). 714 bytes result sent to driver
2017-08-10 09:14:16,842 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 49.0 (TID 98). 714 bytes result sent to driver
2017-08-10 09:14:16,844 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 49.0 (TID 99) in 6 ms on localhost (1/2)
2017-08-10 09:14:16,844 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 49.0 (TID 98) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,844 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 49.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,844 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 49 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:16,844 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 49 finished: foreachPartition at streamingProcess.scala:50, took 0.014362 s
2017-08-10 09:14:16,844 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327160000 ms.0 from job set of time 1502327160000 ms
2017-08-10 09:14:16,845 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 496.844 s for time 1502327160000 ms (execution: 0.025 s)
2017-08-10 09:14:16,845 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 97 from persistence list
2017-08-10 09:14:16,845 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327165000 ms.0 from job set of time 1502327165000 ms
2017-08-10 09:14:16,845 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 97
2017-08-10 09:14:16,845 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 96 from persistence list
2017-08-10 09:14:16,845 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 96
2017-08-10 09:14:16,845 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,845 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327150000 ms
2017-08-10 09:14:16,856 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,856 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 50 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,856 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 50 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,856 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,857 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,857 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 50 (MapPartitionsRDD[101] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,860 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_50 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:16,863 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_50_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:16,864 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_50_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:16,864 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 50 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,864 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 50 (MapPartitionsRDD[101] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,864 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 50.0 with 2 tasks
2017-08-10 09:14:16,865 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 50.0 (TID 100, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,866 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 50.0 (TID 101, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,866 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 50.0 (TID 100)
2017-08-10 09:14:16,866 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 50.0 (TID 101)
2017-08-10 09:14:16,868 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,868 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,870 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 50.0 (TID 100). 714 bytes result sent to driver
2017-08-10 09:14:16,870 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 50.0 (TID 101). 714 bytes result sent to driver
2017-08-10 09:14:16,872 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 50.0 (TID 101) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,872 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 50.0 (TID 100) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,872 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 50.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,872 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 50 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,872 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 50 finished: foreachPartition at streamingProcess.scala:50, took 0.016212 s
2017-08-10 09:14:16,873 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327165000 ms.0 from job set of time 1502327165000 ms
2017-08-10 09:14:16,873 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 491.873 s for time 1502327165000 ms (execution: 0.028 s)
2017-08-10 09:14:16,873 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 99 from persistence list
2017-08-10 09:14:16,873 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327170000 ms.0 from job set of time 1502327170000 ms
2017-08-10 09:14:16,874 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 99
2017-08-10 09:14:16,874 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 98 from persistence list
2017-08-10 09:14:16,874 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 98
2017-08-10 09:14:16,874 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,874 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327155000 ms
2017-08-10 09:14:16,884 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,884 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 51 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,884 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 51 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,884 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,885 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,885 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 51 (MapPartitionsRDD[103] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_51 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:16,890 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_51_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:16,891 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_51_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:16,891 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 51 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,891 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 51 (MapPartitionsRDD[103] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,891 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 51.0 with 2 tasks
2017-08-10 09:14:16,892 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 51.0 (TID 102, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,892 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 51.0 (TID 103, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,893 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 51.0 (TID 102)
2017-08-10 09:14:16,893 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 51.0 (TID 103)
2017-08-10 09:14:16,894 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,894 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,897 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 51.0 (TID 102). 714 bytes result sent to driver
2017-08-10 09:14:16,897 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 51.0 (TID 103). 714 bytes result sent to driver
2017-08-10 09:14:16,899 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 51.0 (TID 103) in 7 ms on localhost (1/2)
2017-08-10 09:14:16,899 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 51.0 (TID 102) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,899 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 51.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,899 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 51 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,900 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 51 finished: foreachPartition at streamingProcess.scala:50, took 0.015441 s
2017-08-10 09:14:16,900 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327170000 ms.0 from job set of time 1502327170000 ms
2017-08-10 09:14:16,900 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 486.900 s for time 1502327170000 ms (execution: 0.027 s)
2017-08-10 09:14:16,900 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 101 from persistence list
2017-08-10 09:14:16,900 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327175000 ms.0 from job set of time 1502327175000 ms
2017-08-10 09:14:16,900 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 101
2017-08-10 09:14:16,900 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 100 from persistence list
2017-08-10 09:14:16,901 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 100
2017-08-10 09:14:16,901 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,901 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327160000 ms
2017-08-10 09:14:16,912 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_37_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:16,914 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_51_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,915 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_36_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,916 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_38_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,918 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_39_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:16,919 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_40_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,921 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_41_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,923 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,923 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_42_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,923 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 52 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,923 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 52 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,923 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,923 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,924 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_43_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:16,924 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 52 (MapPartitionsRDD[105] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,925 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_44_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,927 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_45_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,928 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_46_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,928 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_47_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,929 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_48_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,929 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_52 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:16,930 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_49_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,931 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_50_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,933 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_52_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:16,933 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_52_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,933 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 52 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,933 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 52 (MapPartitionsRDD[105] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,933 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 52.0 with 2 tasks
2017-08-10 09:14:16,934 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 52.0 (TID 104, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,935 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 52.0 (TID 105, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,935 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 52.0 (TID 104)
2017-08-10 09:14:16,935 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 52.0 (TID 105)
2017-08-10 09:14:16,937 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,937 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,941 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 52.0 (TID 104). 714 bytes result sent to driver
2017-08-10 09:14:16,941 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 52.0 (TID 105). 714 bytes result sent to driver
2017-08-10 09:14:16,942 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 52.0 (TID 104) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,942 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 52.0 (TID 105) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,942 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 52.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,943 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 52 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:16,943 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 52 finished: foreachPartition at streamingProcess.scala:50, took 0.020058 s
2017-08-10 09:14:16,943 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327175000 ms.0 from job set of time 1502327175000 ms
2017-08-10 09:14:16,943 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 481.943 s for time 1502327175000 ms (execution: 0.043 s)
2017-08-10 09:14:16,943 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 103 from persistence list
2017-08-10 09:14:16,943 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327180000 ms.0 from job set of time 1502327180000 ms
2017-08-10 09:14:16,944 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 102 from persistence list
2017-08-10 09:14:16,944 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 103
2017-08-10 09:14:16,944 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,944 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 102
2017-08-10 09:14:16,944 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327165000 ms
2017-08-10 09:14:16,955 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,955 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 53 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,955 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 53 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,955 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,955 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,956 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 53 (MapPartitionsRDD[107] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,958 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_53 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:16,962 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_53_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:16,962 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_53_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:16,962 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 53 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 53 (MapPartitionsRDD[107] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 53.0 with 2 tasks
2017-08-10 09:14:16,964 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 53.0 (TID 106, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,964 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 53.0 (TID 107, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,964 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 53.0 (TID 106)
2017-08-10 09:14:16,964 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 53.0 (TID 107)
2017-08-10 09:14:16,966 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,966 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,969 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 53.0 (TID 107). 714 bytes result sent to driver
2017-08-10 09:14:16,969 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 53.0 (TID 106). 714 bytes result sent to driver
2017-08-10 09:14:16,971 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 53.0 (TID 107) in 6 ms on localhost (1/2)
2017-08-10 09:14:16,971 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 53.0 (TID 106) in 8 ms on localhost (2/2)
2017-08-10 09:14:16,971 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 53.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,971 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 53 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,971 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 53 finished: foreachPartition at streamingProcess.scala:50, took 0.016374 s
2017-08-10 09:14:16,972 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327180000 ms.0 from job set of time 1502327180000 ms
2017-08-10 09:14:16,972 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 476.972 s for time 1502327180000 ms (execution: 0.029 s)
2017-08-10 09:14:16,972 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 105 from persistence list
2017-08-10 09:14:16,972 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327185000 ms.0 from job set of time 1502327185000 ms
2017-08-10 09:14:16,972 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 105
2017-08-10 09:14:16,972 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 104 from persistence list
2017-08-10 09:14:16,972 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 104
2017-08-10 09:14:16,972 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:16,972 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327170000 ms
2017-08-10 09:14:16,983 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:16,983 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 54 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:16,983 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 54 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:16,983 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:16,983 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:16,983 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 54 (MapPartitionsRDD[109] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:16,986 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_54 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:16,989 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_54_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:16,990 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_54_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:16,990 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 54 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:16,990 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 54 (MapPartitionsRDD[109] at map at streamingProcess.scala:48)
2017-08-10 09:14:16,990 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 54.0 with 2 tasks
2017-08-10 09:14:16,991 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 54.0 (TID 108, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:16,992 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 54.0 (TID 109, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:16,992 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 54.0 (TID 108)
2017-08-10 09:14:16,992 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 54.0 (TID 109)
2017-08-10 09:14:16,994 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:16,994 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:16,996 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 54.0 (TID 109). 714 bytes result sent to driver
2017-08-10 09:14:16,996 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 54.0 (TID 108). 714 bytes result sent to driver
2017-08-10 09:14:16,998 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 54.0 (TID 108) in 8 ms on localhost (1/2)
2017-08-10 09:14:16,998 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 54.0 (TID 109) in 7 ms on localhost (2/2)
2017-08-10 09:14:16,998 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 54.0, whose tasks have all completed, from pool 
2017-08-10 09:14:16,998 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 54 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:16,999 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 54 finished: foreachPartition at streamingProcess.scala:50, took 0.015782 s
2017-08-10 09:14:16,999 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327185000 ms.0 from job set of time 1502327185000 ms
2017-08-10 09:14:16,999 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 471.999 s for time 1502327185000 ms (execution: 0.027 s)
2017-08-10 09:14:16,999 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 107 from persistence list
2017-08-10 09:14:16,999 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327190000 ms.0 from job set of time 1502327190000 ms
2017-08-10 09:14:16,999 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 107
2017-08-10 09:14:16,999 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 106 from persistence list
2017-08-10 09:14:17,000 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 106
2017-08-10 09:14:17,000 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,000 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327175000 ms
2017-08-10 09:14:17,010 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,011 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 55 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,011 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 55 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,011 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,011 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,011 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 55 (MapPartitionsRDD[111] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,014 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_55 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:17,016 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_55_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:17,017 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_55_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 55 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 55 (MapPartitionsRDD[111] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 55.0 with 2 tasks
2017-08-10 09:14:17,019 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 55.0 (TID 110, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,019 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 55.0 (TID 111, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,019 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 55.0 (TID 110)
2017-08-10 09:14:17,019 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 55.0 (TID 111)
2017-08-10 09:14:17,021 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,021 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,024 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 55.0 (TID 111). 714 bytes result sent to driver
2017-08-10 09:14:17,024 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 55.0 (TID 110). 714 bytes result sent to driver
2017-08-10 09:14:17,025 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 55.0 (TID 111) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,026 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 55.0 (TID 110) in 8 ms on localhost (2/2)
2017-08-10 09:14:17,026 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 55.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 55 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:17,026 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 55 finished: foreachPartition at streamingProcess.scala:50, took 0.015448 s
2017-08-10 09:14:17,027 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327190000 ms.0 from job set of time 1502327190000 ms
2017-08-10 09:14:17,027 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 467.026 s for time 1502327190000 ms (execution: 0.027 s)
2017-08-10 09:14:17,027 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 109 from persistence list
2017-08-10 09:14:17,027 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327195000 ms.0 from job set of time 1502327195000 ms
2017-08-10 09:14:17,027 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 109
2017-08-10 09:14:17,027 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 108 from persistence list
2017-08-10 09:14:17,027 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 108
2017-08-10 09:14:17,027 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,027 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327180000 ms
2017-08-10 09:14:17,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 56 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 56 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 56 (MapPartitionsRDD[113] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_56 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:17,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_56_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:17,044 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_56_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 56 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 56 (MapPartitionsRDD[113] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 56.0 with 2 tasks
2017-08-10 09:14:17,046 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 56.0 (TID 112, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,046 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 56.0 (TID 113, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,046 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 56.0 (TID 112)
2017-08-10 09:14:17,046 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 56.0 (TID 113)
2017-08-10 09:14:17,048 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,048 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,051 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 56.0 (TID 112). 714 bytes result sent to driver
2017-08-10 09:14:17,051 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 56.0 (TID 113). 714 bytes result sent to driver
2017-08-10 09:14:17,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 56.0 (TID 112) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,052 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 56.0 (TID 113) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,053 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 56.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 56 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:17,053 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 56 finished: foreachPartition at streamingProcess.scala:50, took 0.015215 s
2017-08-10 09:14:17,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327195000 ms.0 from job set of time 1502327195000 ms
2017-08-10 09:14:17,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 462.053 s for time 1502327195000 ms (execution: 0.026 s)
2017-08-10 09:14:17,053 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 111 from persistence list
2017-08-10 09:14:17,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327200000 ms.0 from job set of time 1502327200000 ms
2017-08-10 09:14:17,054 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 111
2017-08-10 09:14:17,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 110 from persistence list
2017-08-10 09:14:17,054 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 110
2017-08-10 09:14:17,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327185000 ms
2017-08-10 09:14:17,064 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 57 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 57 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 57 (MapPartitionsRDD[115] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_57 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:17,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_57_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:17,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_57_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 57 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 57 (MapPartitionsRDD[115] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 57.0 with 2 tasks
2017-08-10 09:14:17,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 57.0 (TID 114, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 57.0 (TID 115, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,072 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 57.0 (TID 114)
2017-08-10 09:14:17,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 57.0 (TID 115)
2017-08-10 09:14:17,073 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,074 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,076 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 57.0 (TID 115). 714 bytes result sent to driver
2017-08-10 09:14:17,076 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 57.0 (TID 114). 714 bytes result sent to driver
2017-08-10 09:14:17,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 57.0 (TID 115) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 57.0 (TID 114) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 57.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 57 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 57 finished: foreachPartition at streamingProcess.scala:50, took 0.014384 s
2017-08-10 09:14:17,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327200000 ms.0 from job set of time 1502327200000 ms
2017-08-10 09:14:17,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 457.079 s for time 1502327200000 ms (execution: 0.026 s)
2017-08-10 09:14:17,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 113 from persistence list
2017-08-10 09:14:17,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327205000 ms.0 from job set of time 1502327205000 ms
2017-08-10 09:14:17,079 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 113
2017-08-10 09:14:17,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 112 from persistence list
2017-08-10 09:14:17,079 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 112
2017-08-10 09:14:17,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327190000 ms
2017-08-10 09:14:17,089 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 58 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 58 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 58 (MapPartitionsRDD[117] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_58 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:17,095 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_58_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:17,096 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_58_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 58 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 58 (MapPartitionsRDD[117] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 58.0 with 2 tasks
2017-08-10 09:14:17,097 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 58.0 (TID 116, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,097 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 58.0 (TID 117, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,097 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 58.0 (TID 116)
2017-08-10 09:14:17,097 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 58.0 (TID 117)
2017-08-10 09:14:17,099 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,099 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,102 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 58.0 (TID 117). 714 bytes result sent to driver
2017-08-10 09:14:17,102 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 58.0 (TID 116). 714 bytes result sent to driver
2017-08-10 09:14:17,103 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 58.0 (TID 117) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,103 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 58.0 (TID 116) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,103 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 58.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,104 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 58 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,104 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 58 finished: foreachPartition at streamingProcess.scala:50, took 0.014781 s
2017-08-10 09:14:17,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327205000 ms.0 from job set of time 1502327205000 ms
2017-08-10 09:14:17,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 452.104 s for time 1502327205000 ms (execution: 0.025 s)
2017-08-10 09:14:17,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327210000 ms.0 from job set of time 1502327210000 ms
2017-08-10 09:14:17,105 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 115 from persistence list
2017-08-10 09:14:17,105 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 115
2017-08-10 09:14:17,106 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 114 from persistence list
2017-08-10 09:14:17,106 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 114
2017-08-10 09:14:17,106 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,106 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327195000 ms
2017-08-10 09:14:17,114 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 59 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 59 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 59 (MapPartitionsRDD[119] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_59 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:17,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_59_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:17,120 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_59_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,121 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 59 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,121 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 59 (MapPartitionsRDD[119] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,121 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 59.0 with 2 tasks
2017-08-10 09:14:17,122 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 59.0 (TID 118, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,123 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 59.0 (TID 119, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,123 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 59.0 (TID 118)
2017-08-10 09:14:17,123 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 59.0 (TID 119)
2017-08-10 09:14:17,125 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,125 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,127 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 59.0 (TID 119). 714 bytes result sent to driver
2017-08-10 09:14:17,127 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 59.0 (TID 118). 714 bytes result sent to driver
2017-08-10 09:14:17,129 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 59.0 (TID 119) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,129 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 59.0 (TID 118) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,129 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 59.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,130 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 59 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,130 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 59 finished: foreachPartition at streamingProcess.scala:50, took 0.015477 s
2017-08-10 09:14:17,130 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327210000 ms.0 from job set of time 1502327210000 ms
2017-08-10 09:14:17,131 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 117 from persistence list
2017-08-10 09:14:17,131 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 447.130 s for time 1502327210000 ms (execution: 0.026 s)
2017-08-10 09:14:17,131 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327215000 ms.0 from job set of time 1502327215000 ms
2017-08-10 09:14:17,131 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 117
2017-08-10 09:14:17,131 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 116 from persistence list
2017-08-10 09:14:17,131 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 116
2017-08-10 09:14:17,131 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,131 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327200000 ms
2017-08-10 09:14:17,141 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,141 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 60 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,141 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 60 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,142 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,142 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,142 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 60 (MapPartitionsRDD[121] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,144 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_60 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:17,147 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_60_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:17,148 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_60_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,148 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 60 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,148 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 60 (MapPartitionsRDD[121] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,148 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 60.0 with 2 tasks
2017-08-10 09:14:17,149 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 60.0 (TID 120, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,150 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 60.0 (TID 121, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,150 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 60.0 (TID 121)
2017-08-10 09:14:17,150 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 60.0 (TID 120)
2017-08-10 09:14:17,152 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,152 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,155 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 60.0 (TID 120). 714 bytes result sent to driver
2017-08-10 09:14:17,155 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 60.0 (TID 121). 714 bytes result sent to driver
2017-08-10 09:14:17,157 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 60.0 (TID 120) in 9 ms on localhost (1/2)
2017-08-10 09:14:17,157 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 60.0 (TID 121) in 8 ms on localhost (2/2)
2017-08-10 09:14:17,157 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 60.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 60 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:17,158 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 60 finished: foreachPartition at streamingProcess.scala:50, took 0.016413 s
2017-08-10 09:14:17,158 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327215000 ms.0 from job set of time 1502327215000 ms
2017-08-10 09:14:17,158 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 442.158 s for time 1502327215000 ms (execution: 0.027 s)
2017-08-10 09:14:17,158 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327220000 ms.0 from job set of time 1502327220000 ms
2017-08-10 09:14:17,158 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 119 from persistence list
2017-08-10 09:14:17,158 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 119
2017-08-10 09:14:17,158 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 118 from persistence list
2017-08-10 09:14:17,159 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 118
2017-08-10 09:14:17,159 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,159 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327205000 ms
2017-08-10 09:14:17,169 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 61 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 61 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 61 (MapPartitionsRDD[123] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,173 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_61 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:17,176 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_61_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:17,177 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_61_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,177 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 61 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,177 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 61 (MapPartitionsRDD[123] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,177 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 61.0 with 2 tasks
2017-08-10 09:14:17,178 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 61.0 (TID 122, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,178 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 61.0 (TID 123, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,179 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 61.0 (TID 122)
2017-08-10 09:14:17,179 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 61.0 (TID 123)
2017-08-10 09:14:17,180 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,180 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,183 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 61.0 (TID 123). 714 bytes result sent to driver
2017-08-10 09:14:17,183 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 61.0 (TID 122). 714 bytes result sent to driver
2017-08-10 09:14:17,185 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 61.0 (TID 123) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,185 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 61.0 (TID 122) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,185 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 61.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,186 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 61 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,186 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 61 finished: foreachPartition at streamingProcess.scala:50, took 0.016108 s
2017-08-10 09:14:17,186 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327220000 ms.0 from job set of time 1502327220000 ms
2017-08-10 09:14:17,186 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 437.186 s for time 1502327220000 ms (execution: 0.028 s)
2017-08-10 09:14:17,186 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 121 from persistence list
2017-08-10 09:14:17,186 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327225000 ms.0 from job set of time 1502327225000 ms
2017-08-10 09:14:17,187 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 120 from persistence list
2017-08-10 09:14:17,187 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,187 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 121
2017-08-10 09:14:17,187 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 120
2017-08-10 09:14:17,187 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327210000 ms
2017-08-10 09:14:17,197 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 62 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 62 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,198 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 62 (MapPartitionsRDD[125] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,201 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_62 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:17,204 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_62_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:17,206 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_62_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,206 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 62 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,206 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 62 (MapPartitionsRDD[125] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,206 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 62.0 with 2 tasks
2017-08-10 09:14:17,207 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 62.0 (TID 124, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,208 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 62.0 (TID 125, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,208 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 62.0 (TID 125)
2017-08-10 09:14:17,208 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 62.0 (TID 124)
2017-08-10 09:14:17,210 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,210 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,213 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 62.0 (TID 124). 714 bytes result sent to driver
2017-08-10 09:14:17,213 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 62.0 (TID 125). 714 bytes result sent to driver
2017-08-10 09:14:17,215 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 62.0 (TID 124) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,215 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 62.0 (TID 125) in 8 ms on localhost (2/2)
2017-08-10 09:14:17,215 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 62.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,215 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 62 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:17,215 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 62 finished: foreachPartition at streamingProcess.scala:50, took 0.017714 s
2017-08-10 09:14:17,215 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327225000 ms.0 from job set of time 1502327225000 ms
2017-08-10 09:14:17,216 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 432.215 s for time 1502327225000 ms (execution: 0.029 s)
2017-08-10 09:14:17,216 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327230000 ms.0 from job set of time 1502327230000 ms
2017-08-10 09:14:17,216 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 123 from persistence list
2017-08-10 09:14:17,216 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 123
2017-08-10 09:14:17,216 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 122 from persistence list
2017-08-10 09:14:17,216 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 122
2017-08-10 09:14:17,216 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,216 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327215000 ms
2017-08-10 09:14:17,226 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 63 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 63 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 63 (MapPartitionsRDD[127] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,230 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_63 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:17,232 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_63_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:17,232 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_63_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,233 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 63 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,233 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 63 (MapPartitionsRDD[127] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,233 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 63.0 with 2 tasks
2017-08-10 09:14:17,233 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 63.0 (TID 126, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,234 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 63.0 (TID 127, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,234 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 63.0 (TID 126)
2017-08-10 09:14:17,234 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 63.0 (TID 127)
2017-08-10 09:14:17,235 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,235 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,238 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 63.0 (TID 126). 714 bytes result sent to driver
2017-08-10 09:14:17,238 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 63.0 (TID 127). 714 bytes result sent to driver
2017-08-10 09:14:17,239 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 63.0 (TID 127) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,239 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 63.0 (TID 126) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,240 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 63.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,240 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 63 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,240 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 63 finished: foreachPartition at streamingProcess.scala:50, took 0.013917 s
2017-08-10 09:14:17,240 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327230000 ms.0 from job set of time 1502327230000 ms
2017-08-10 09:14:17,240 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 427.240 s for time 1502327230000 ms (execution: 0.024 s)
2017-08-10 09:14:17,240 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 125 from persistence list
2017-08-10 09:14:17,240 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327235000 ms.0 from job set of time 1502327235000 ms
2017-08-10 09:14:17,240 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 125
2017-08-10 09:14:17,241 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 124 from persistence list
2017-08-10 09:14:17,241 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,241 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327220000 ms
2017-08-10 09:14:17,241 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 124
2017-08-10 09:14:17,250 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,251 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 64 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,251 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 64 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,251 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,251 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,251 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 64 (MapPartitionsRDD[129] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,254 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_64 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:17,257 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_64_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:17,257 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_64_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,258 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 64 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,258 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 64 (MapPartitionsRDD[129] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,258 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 64.0 with 2 tasks
2017-08-10 09:14:17,258 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 64.0 (TID 128, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,259 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 64.0 (TID 129, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,259 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 64.0 (TID 128)
2017-08-10 09:14:17,259 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 64.0 (TID 129)
2017-08-10 09:14:17,261 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,261 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,263 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 64.0 (TID 129). 714 bytes result sent to driver
2017-08-10 09:14:17,263 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 64.0 (TID 128). 714 bytes result sent to driver
2017-08-10 09:14:17,264 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 64.0 (TID 129) in 5 ms on localhost (1/2)
2017-08-10 09:14:17,264 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 64.0 (TID 128) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,264 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 64.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 64 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,265 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 64 finished: foreachPartition at streamingProcess.scala:50, took 0.014364 s
2017-08-10 09:14:17,265 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327235000 ms.0 from job set of time 1502327235000 ms
2017-08-10 09:14:17,265 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 422.265 s for time 1502327235000 ms (execution: 0.025 s)
2017-08-10 09:14:17,265 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327240000 ms.0 from job set of time 1502327240000 ms
2017-08-10 09:14:17,266 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 127 from persistence list
2017-08-10 09:14:17,266 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 127
2017-08-10 09:14:17,266 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 126 from persistence list
2017-08-10 09:14:17,266 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 126
2017-08-10 09:14:17,266 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,266 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327225000 ms
2017-08-10 09:14:17,275 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 65 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 65 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 65 (MapPartitionsRDD[131] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,279 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_65 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:17,281 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_65_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:17,281 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_65_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,282 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 65 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,282 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 65 (MapPartitionsRDD[131] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,282 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 65.0 with 2 tasks
2017-08-10 09:14:17,282 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 65.0 (TID 130, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,283 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 65.0 (TID 131, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,283 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 65.0 (TID 130)
2017-08-10 09:14:17,283 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 65.0 (TID 131)
2017-08-10 09:14:17,284 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,284 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,287 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 65.0 (TID 130). 714 bytes result sent to driver
2017-08-10 09:14:17,288 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 65.0 (TID 131). 714 bytes result sent to driver
2017-08-10 09:14:17,289 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 65.0 (TID 131) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,289 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 65.0 (TID 130) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,289 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 65.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,289 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 65 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,290 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 65 finished: foreachPartition at streamingProcess.scala:50, took 0.014262 s
2017-08-10 09:14:17,290 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327240000 ms.0 from job set of time 1502327240000 ms
2017-08-10 09:14:17,290 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 417.290 s for time 1502327240000 ms (execution: 0.025 s)
2017-08-10 09:14:17,290 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327245000 ms.0 from job set of time 1502327245000 ms
2017-08-10 09:14:17,290 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 129 from persistence list
2017-08-10 09:14:17,290 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 129
2017-08-10 09:14:17,290 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 128 from persistence list
2017-08-10 09:14:17,291 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 128
2017-08-10 09:14:17,291 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,291 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327230000 ms
2017-08-10 09:14:17,300 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 66 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 66 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 66 (MapPartitionsRDD[133] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,303 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_66 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:17,305 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_66_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:17,305 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_66_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 66 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 66 (MapPartitionsRDD[133] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 66.0 with 2 tasks
2017-08-10 09:14:17,306 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 66.0 (TID 132, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,307 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 66.0 (TID 133, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,307 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 66.0 (TID 133)
2017-08-10 09:14:17,307 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 66.0 (TID 132)
2017-08-10 09:14:17,309 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,309 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,311 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 66.0 (TID 133). 714 bytes result sent to driver
2017-08-10 09:14:17,311 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 66.0 (TID 132). 714 bytes result sent to driver
2017-08-10 09:14:17,313 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 66.0 (TID 133) in 5 ms on localhost (1/2)
2017-08-10 09:14:17,313 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 66.0 (TID 132) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,313 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 66.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 66 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,313 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 66 finished: foreachPartition at streamingProcess.scala:50, took 0.013281 s
2017-08-10 09:14:17,313 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327245000 ms.0 from job set of time 1502327245000 ms
2017-08-10 09:14:17,314 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 412.313 s for time 1502327245000 ms (execution: 0.023 s)
2017-08-10 09:14:17,314 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 131 from persistence list
2017-08-10 09:14:17,314 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327250000 ms.0 from job set of time 1502327250000 ms
2017-08-10 09:14:17,314 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 131
2017-08-10 09:14:17,314 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 130 from persistence list
2017-08-10 09:14:17,314 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 130
2017-08-10 09:14:17,314 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,314 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327235000 ms
2017-08-10 09:14:17,323 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,324 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 67 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,324 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 67 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,324 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,324 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,324 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 67 (MapPartitionsRDD[135] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,327 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_67 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:17,329 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_67_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:17,330 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_67_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,330 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 67 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,330 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 67 (MapPartitionsRDD[135] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,331 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 67.0 with 2 tasks
2017-08-10 09:14:17,331 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 67.0 (TID 134, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,331 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 67.0 (TID 135, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,332 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 67.0 (TID 134)
2017-08-10 09:14:17,332 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 67.0 (TID 135)
2017-08-10 09:14:17,333 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,333 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,336 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 67.0 (TID 134). 714 bytes result sent to driver
2017-08-10 09:14:17,336 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 67.0 (TID 135). 714 bytes result sent to driver
2017-08-10 09:14:17,344 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_52_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,344 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 67.0 (TID 134) in 13 ms on localhost (1/2)
2017-08-10 09:14:17,344 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 67.0 (TID 135) in 13 ms on localhost (2/2)
2017-08-10 09:14:17,344 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 67.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,345 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 67 (foreachPartition at streamingProcess.scala:50) finished in 0.014 s
2017-08-10 09:14:17,345 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 67 finished: foreachPartition at streamingProcess.scala:50, took 0.021404 s
2017-08-10 09:14:17,345 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_53_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,345 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327250000 ms.0 from job set of time 1502327250000 ms
2017-08-10 09:14:17,345 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 407.345 s for time 1502327250000 ms (execution: 0.031 s)
2017-08-10 09:14:17,345 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327255000 ms.0 from job set of time 1502327255000 ms
2017-08-10 09:14:17,345 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 133 from persistence list
2017-08-10 09:14:17,346 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 133
2017-08-10 09:14:17,346 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 132 from persistence list
2017-08-10 09:14:17,346 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 132
2017-08-10 09:14:17,346 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_54_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,346 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,346 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327240000 ms
2017-08-10 09:14:17,347 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_55_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,348 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_56_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,349 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_57_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,350 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_58_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,351 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_59_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,352 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_60_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,353 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_61_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,354 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_62_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,355 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_63_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,356 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_64_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,357 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_65_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:17,357 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_66_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:17,358 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,358 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 68 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,358 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 68 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,358 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,358 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,358 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 68 (MapPartitionsRDD[137] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,361 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_68 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:17,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_68_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:17,364 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_68_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:17,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 68 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 68 (MapPartitionsRDD[137] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 68.0 with 2 tasks
2017-08-10 09:14:17,365 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 68.0 (TID 136, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,365 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 68.0 (TID 137, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,365 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 68.0 (TID 137)
2017-08-10 09:14:17,365 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 68.0 (TID 136)
2017-08-10 09:14:17,367 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,367 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,370 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 68.0 (TID 136). 714 bytes result sent to driver
2017-08-10 09:14:17,370 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 68.0 (TID 137). 714 bytes result sent to driver
2017-08-10 09:14:17,371 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 68.0 (TID 137) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,372 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 68.0 (TID 136) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,372 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 68 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:17,372 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 68.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,372 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 68 finished: foreachPartition at streamingProcess.scala:50, took 0.013912 s
2017-08-10 09:14:17,372 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327255000 ms.0 from job set of time 1502327255000 ms
2017-08-10 09:14:17,372 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 402.372 s for time 1502327255000 ms (execution: 0.027 s)
2017-08-10 09:14:17,372 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 135 from persistence list
2017-08-10 09:14:17,372 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327260000 ms.0 from job set of time 1502327260000 ms
2017-08-10 09:14:17,373 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 135
2017-08-10 09:14:17,373 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 134 from persistence list
2017-08-10 09:14:17,373 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 134
2017-08-10 09:14:17,373 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,373 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327245000 ms
2017-08-10 09:14:17,382 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,383 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 69 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,383 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 69 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,383 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,383 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,383 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 69 (MapPartitionsRDD[139] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_69 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:17,388 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_69_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:17,389 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_69_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,389 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 69 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,389 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 69 (MapPartitionsRDD[139] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,389 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 69.0 with 2 tasks
2017-08-10 09:14:17,390 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 69.0 (TID 138, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,390 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 69.0 (TID 139, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,390 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 69.0 (TID 138)
2017-08-10 09:14:17,390 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 69.0 (TID 139)
2017-08-10 09:14:17,392 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,392 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,394 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 69.0 (TID 139). 714 bytes result sent to driver
2017-08-10 09:14:17,394 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 69.0 (TID 138). 714 bytes result sent to driver
2017-08-10 09:14:17,396 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 69.0 (TID 139) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,396 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 69.0 (TID 138) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,396 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 69.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,396 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 69 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,396 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 69 finished: foreachPartition at streamingProcess.scala:50, took 0.013804 s
2017-08-10 09:14:17,397 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327260000 ms.0 from job set of time 1502327260000 ms
2017-08-10 09:14:17,397 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 397.397 s for time 1502327260000 ms (execution: 0.025 s)
2017-08-10 09:14:17,397 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 137 from persistence list
2017-08-10 09:14:17,397 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327265000 ms.0 from job set of time 1502327265000 ms
2017-08-10 09:14:17,397 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 137
2017-08-10 09:14:17,397 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 136 from persistence list
2017-08-10 09:14:17,397 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 136
2017-08-10 09:14:17,397 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,397 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327250000 ms
2017-08-10 09:14:17,407 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 70 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 70 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 70 (MapPartitionsRDD[141] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,410 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_70 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:17,413 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_70_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:17,415 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_70_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 70 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 70 (MapPartitionsRDD[141] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 70.0 with 2 tasks
2017-08-10 09:14:17,416 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 70.0 (TID 140, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,416 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 70.0 (TID 141, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,416 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 70.0 (TID 140)
2017-08-10 09:14:17,416 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 70.0 (TID 141)
2017-08-10 09:14:17,418 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,418 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,420 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 70.0 (TID 140). 714 bytes result sent to driver
2017-08-10 09:14:17,420 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 70.0 (TID 141). 714 bytes result sent to driver
2017-08-10 09:14:17,421 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 70.0 (TID 140) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,422 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 70.0 (TID 141) in 5 ms on localhost (2/2)
2017-08-10 09:14:17,422 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 70.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,422 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 70 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,422 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 70 finished: foreachPartition at streamingProcess.scala:50, took 0.015144 s
2017-08-10 09:14:17,422 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327265000 ms.0 from job set of time 1502327265000 ms
2017-08-10 09:14:17,422 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 392.422 s for time 1502327265000 ms (execution: 0.025 s)
2017-08-10 09:14:17,422 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 139 from persistence list
2017-08-10 09:14:17,422 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327270000 ms.0 from job set of time 1502327270000 ms
2017-08-10 09:14:17,423 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 139
2017-08-10 09:14:17,423 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 138 from persistence list
2017-08-10 09:14:17,423 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 138
2017-08-10 09:14:17,423 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,423 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327255000 ms
2017-08-10 09:14:17,432 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,432 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 71 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,432 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 71 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,432 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,432 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,432 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 71 (MapPartitionsRDD[143] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,435 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_71 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:17,437 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_71_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:17,438 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_71_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,438 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 71 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,438 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 71 (MapPartitionsRDD[143] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,438 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 71.0 with 2 tasks
2017-08-10 09:14:17,439 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 71.0 (TID 142, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,439 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 71.0 (TID 143, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,439 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 71.0 (TID 142)
2017-08-10 09:14:17,439 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 71.0 (TID 143)
2017-08-10 09:14:17,441 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,441 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,443 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 71.0 (TID 143). 714 bytes result sent to driver
2017-08-10 09:14:17,443 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 71.0 (TID 142). 714 bytes result sent to driver
2017-08-10 09:14:17,445 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 71.0 (TID 143) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,445 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 71.0 (TID 142) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,445 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 71.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,445 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 71 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,445 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 71 finished: foreachPartition at streamingProcess.scala:50, took 0.013298 s
2017-08-10 09:14:17,445 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327270000 ms.0 from job set of time 1502327270000 ms
2017-08-10 09:14:17,446 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 387.445 s for time 1502327270000 ms (execution: 0.023 s)
2017-08-10 09:14:17,446 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 141 from persistence list
2017-08-10 09:14:17,446 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327275000 ms.0 from job set of time 1502327275000 ms
2017-08-10 09:14:17,446 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 141
2017-08-10 09:14:17,446 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 140 from persistence list
2017-08-10 09:14:17,446 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 140
2017-08-10 09:14:17,446 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,446 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327260000 ms
2017-08-10 09:14:17,455 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 72 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 72 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 72 (MapPartitionsRDD[145] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,458 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_72 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:17,460 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_72_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:17,461 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_72_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 72 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 72 (MapPartitionsRDD[145] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 72.0 with 2 tasks
2017-08-10 09:14:17,462 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 72.0 (TID 144, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,462 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 72.0 (TID 145, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,462 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 72.0 (TID 144)
2017-08-10 09:14:17,462 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 72.0 (TID 145)
2017-08-10 09:14:17,464 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,464 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,467 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 72.0 (TID 144). 714 bytes result sent to driver
2017-08-10 09:14:17,467 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 72.0 (TID 145). 714 bytes result sent to driver
2017-08-10 09:14:17,468 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 72.0 (TID 144) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,468 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 72.0 (TID 145) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,468 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 72.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 72 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,468 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 72 finished: foreachPartition at streamingProcess.scala:50, took 0.012928 s
2017-08-10 09:14:17,469 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327275000 ms.0 from job set of time 1502327275000 ms
2017-08-10 09:14:17,469 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 382.469 s for time 1502327275000 ms (execution: 0.023 s)
2017-08-10 09:14:17,469 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 143 from persistence list
2017-08-10 09:14:17,469 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327280000 ms.0 from job set of time 1502327280000 ms
2017-08-10 09:14:17,469 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 143
2017-08-10 09:14:17,469 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 142 from persistence list
2017-08-10 09:14:17,469 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 142
2017-08-10 09:14:17,469 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,470 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327265000 ms
2017-08-10 09:14:17,479 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,480 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 73 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,480 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 73 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,480 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,480 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,480 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 73 (MapPartitionsRDD[147] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,482 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_73 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:17,485 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_73_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:17,485 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_73_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,485 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 73 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,485 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 73 (MapPartitionsRDD[147] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,485 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 73.0 with 2 tasks
2017-08-10 09:14:17,486 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 73.0 (TID 146, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,486 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 73.0 (TID 147, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,486 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 73.0 (TID 146)
2017-08-10 09:14:17,486 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 73.0 (TID 147)
2017-08-10 09:14:17,488 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,488 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,490 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 73.0 (TID 146). 714 bytes result sent to driver
2017-08-10 09:14:17,491 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 73.0 (TID 147). 714 bytes result sent to driver
2017-08-10 09:14:17,491 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 73.0 (TID 146) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,492 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 73.0 (TID 147) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,492 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 73.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,492 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 73 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,492 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 73 finished: foreachPartition at streamingProcess.scala:50, took 0.012635 s
2017-08-10 09:14:17,492 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327280000 ms.0 from job set of time 1502327280000 ms
2017-08-10 09:14:17,492 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 377.492 s for time 1502327280000 ms (execution: 0.023 s)
2017-08-10 09:14:17,493 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327285000 ms.0 from job set of time 1502327285000 ms
2017-08-10 09:14:17,493 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 145 from persistence list
2017-08-10 09:14:17,493 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 145
2017-08-10 09:14:17,493 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 144 from persistence list
2017-08-10 09:14:17,493 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 144
2017-08-10 09:14:17,493 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,493 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327270000 ms
2017-08-10 09:14:17,504 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,504 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 74 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,505 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 74 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,505 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,505 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,505 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 74 (MapPartitionsRDD[149] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,507 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_74 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:17,511 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_74_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:17,511 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_74_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,511 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 74 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,511 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 74 (MapPartitionsRDD[149] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,511 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 74.0 with 2 tasks
2017-08-10 09:14:17,512 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 74.0 (TID 148, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,512 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 74.0 (TID 149, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,512 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 74.0 (TID 148)
2017-08-10 09:14:17,512 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 74.0 (TID 149)
2017-08-10 09:14:17,514 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,514 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,516 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 74.0 (TID 148). 714 bytes result sent to driver
2017-08-10 09:14:17,516 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 74.0 (TID 149). 714 bytes result sent to driver
2017-08-10 09:14:17,518 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 74.0 (TID 149) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,518 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 74.0 (TID 148) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,518 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 74.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 74 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:17,518 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 74 finished: foreachPartition at streamingProcess.scala:50, took 0.014125 s
2017-08-10 09:14:17,519 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327285000 ms.0 from job set of time 1502327285000 ms
2017-08-10 09:14:17,519 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 372.519 s for time 1502327285000 ms (execution: 0.026 s)
2017-08-10 09:14:17,519 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327290000 ms.0 from job set of time 1502327290000 ms
2017-08-10 09:14:17,519 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 147 from persistence list
2017-08-10 09:14:17,519 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 147
2017-08-10 09:14:17,519 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 146 from persistence list
2017-08-10 09:14:17,519 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 146
2017-08-10 09:14:17,519 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,520 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327275000 ms
2017-08-10 09:14:17,531 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,532 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 75 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,532 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 75 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,532 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,532 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,532 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 75 (MapPartitionsRDD[151] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,535 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_75 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:17,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_75_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:17,538 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_75_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,538 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 75 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,538 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 75 (MapPartitionsRDD[151] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,538 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 75.0 with 2 tasks
2017-08-10 09:14:17,539 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 75.0 (TID 150, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,539 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 75.0 (TID 151, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,539 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 75.0 (TID 151)
2017-08-10 09:14:17,539 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 75.0 (TID 150)
2017-08-10 09:14:17,541 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,541 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,543 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 75.0 (TID 151). 714 bytes result sent to driver
2017-08-10 09:14:17,543 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 75.0 (TID 150). 714 bytes result sent to driver
2017-08-10 09:14:17,545 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 75.0 (TID 150) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,545 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 75.0 (TID 151) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,545 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 75.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,545 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 75 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,545 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 75 finished: foreachPartition at streamingProcess.scala:50, took 0.013676 s
2017-08-10 09:14:17,545 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327290000 ms.0 from job set of time 1502327290000 ms
2017-08-10 09:14:17,545 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 367.545 s for time 1502327290000 ms (execution: 0.026 s)
2017-08-10 09:14:17,545 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 149 from persistence list
2017-08-10 09:14:17,546 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327295000 ms.0 from job set of time 1502327295000 ms
2017-08-10 09:14:17,546 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 149
2017-08-10 09:14:17,546 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 148 from persistence list
2017-08-10 09:14:17,546 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 148
2017-08-10 09:14:17,546 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,546 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327280000 ms
2017-08-10 09:14:17,557 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,558 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 76 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,558 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 76 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,558 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,558 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,558 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 76 (MapPartitionsRDD[153] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,561 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_76 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:17,565 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_76_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:17,565 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_76_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,565 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 76 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 76 (MapPartitionsRDD[153] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 76.0 with 2 tasks
2017-08-10 09:14:17,566 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 76.0 (TID 152, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,567 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 76.0 (TID 153, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,567 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 76.0 (TID 152)
2017-08-10 09:14:17,567 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 76.0 (TID 153)
2017-08-10 09:14:17,569 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,569 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,572 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 76.0 (TID 153). 714 bytes result sent to driver
2017-08-10 09:14:17,572 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 76.0 (TID 152). 714 bytes result sent to driver
2017-08-10 09:14:17,573 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 76.0 (TID 153) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,574 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 76.0 (TID 152) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,574 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 76.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,574 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 76 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:17,574 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 76 finished: foreachPartition at streamingProcess.scala:50, took 0.016331 s
2017-08-10 09:14:17,574 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327295000 ms.0 from job set of time 1502327295000 ms
2017-08-10 09:14:17,574 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 362.574 s for time 1502327295000 ms (execution: 0.028 s)
2017-08-10 09:14:17,574 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 151 from persistence list
2017-08-10 09:14:17,574 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327300000 ms.0 from job set of time 1502327300000 ms
2017-08-10 09:14:17,575 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 151
2017-08-10 09:14:17,575 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 150 from persistence list
2017-08-10 09:14:17,575 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 150
2017-08-10 09:14:17,575 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,575 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327285000 ms
2017-08-10 09:14:17,585 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,586 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 77 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,586 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 77 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,586 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,586 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,586 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 77 (MapPartitionsRDD[155] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,588 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_77 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:17,591 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_77_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:17,591 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_77_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,591 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 77 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,591 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 77 (MapPartitionsRDD[155] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,591 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 77.0 with 2 tasks
2017-08-10 09:14:17,592 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 77.0 (TID 154, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,592 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 77.0 (TID 155, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,592 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 77.0 (TID 154)
2017-08-10 09:14:17,592 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 77.0 (TID 155)
2017-08-10 09:14:17,594 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,594 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,597 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 77.0 (TID 155). 714 bytes result sent to driver
2017-08-10 09:14:17,597 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 77.0 (TID 154). 714 bytes result sent to driver
2017-08-10 09:14:17,598 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 77.0 (TID 155) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,598 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 77.0 (TID 154) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,598 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 77.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,598 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 77 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:17,598 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 77 finished: foreachPartition at streamingProcess.scala:50, took 0.013027 s
2017-08-10 09:14:17,599 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327300000 ms.0 from job set of time 1502327300000 ms
2017-08-10 09:14:17,599 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 357.599 s for time 1502327300000 ms (execution: 0.025 s)
2017-08-10 09:14:17,599 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 153 from persistence list
2017-08-10 09:14:17,599 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327305000 ms.0 from job set of time 1502327305000 ms
2017-08-10 09:14:17,599 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 153
2017-08-10 09:14:17,599 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 152 from persistence list
2017-08-10 09:14:17,599 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 152
2017-08-10 09:14:17,599 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,599 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327290000 ms
2017-08-10 09:14:17,609 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,609 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 78 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,609 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 78 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,609 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,610 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,610 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 78 (MapPartitionsRDD[157] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,612 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_78 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:17,614 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_78_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:17,614 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_78_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,615 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 78 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,615 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 78 (MapPartitionsRDD[157] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,615 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 78.0 with 2 tasks
2017-08-10 09:14:17,616 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 78.0 (TID 156, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,616 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 78.0 (TID 157, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,616 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 78.0 (TID 156)
2017-08-10 09:14:17,616 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 78.0 (TID 157)
2017-08-10 09:14:17,618 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,618 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,620 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 78.0 (TID 156). 714 bytes result sent to driver
2017-08-10 09:14:17,620 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 78.0 (TID 157). 714 bytes result sent to driver
2017-08-10 09:14:17,621 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 78.0 (TID 156) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,622 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 78.0 (TID 157) in 5 ms on localhost (2/2)
2017-08-10 09:14:17,622 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 78.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,622 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 78 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,622 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 78 finished: foreachPartition at streamingProcess.scala:50, took 0.012678 s
2017-08-10 09:14:17,622 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327305000 ms.0 from job set of time 1502327305000 ms
2017-08-10 09:14:17,622 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 352.622 s for time 1502327305000 ms (execution: 0.023 s)
2017-08-10 09:14:17,622 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 155 from persistence list
2017-08-10 09:14:17,622 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327310000 ms.0 from job set of time 1502327310000 ms
2017-08-10 09:14:17,623 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 154 from persistence list
2017-08-10 09:14:17,623 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 155
2017-08-10 09:14:17,623 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 154
2017-08-10 09:14:17,623 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,623 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327295000 ms
2017-08-10 09:14:17,632 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,632 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 79 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,632 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 79 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,632 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,632 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,632 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 79 (MapPartitionsRDD[159] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,635 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_79 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:17,637 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_79_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:17,638 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_79_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,638 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 79 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,638 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 79 (MapPartitionsRDD[159] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,638 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 79.0 with 2 tasks
2017-08-10 09:14:17,639 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 79.0 (TID 158, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,639 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 79.0 (TID 159, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,639 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 79.0 (TID 158)
2017-08-10 09:14:17,639 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 79.0 (TID 159)
2017-08-10 09:14:17,640 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,640 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,643 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 79.0 (TID 158). 714 bytes result sent to driver
2017-08-10 09:14:17,643 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 79.0 (TID 159). 714 bytes result sent to driver
2017-08-10 09:14:17,644 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 79.0 (TID 158) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,644 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 79.0 (TID 159) in 5 ms on localhost (2/2)
2017-08-10 09:14:17,644 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 79.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,645 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 79 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:17,645 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 79 finished: foreachPartition at streamingProcess.scala:50, took 0.012947 s
2017-08-10 09:14:17,645 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327310000 ms.0 from job set of time 1502327310000 ms
2017-08-10 09:14:17,645 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 347.645 s for time 1502327310000 ms (execution: 0.023 s)
2017-08-10 09:14:17,645 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 157 from persistence list
2017-08-10 09:14:17,645 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327315000 ms.0 from job set of time 1502327315000 ms
2017-08-10 09:14:17,645 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 157
2017-08-10 09:14:17,645 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 156 from persistence list
2017-08-10 09:14:17,646 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 156
2017-08-10 09:14:17,646 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,646 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327300000 ms
2017-08-10 09:14:17,655 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 80 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 80 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 80 (MapPartitionsRDD[161] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,658 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_80 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:17,660 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_80_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:17,661 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_80_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 80 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 80 (MapPartitionsRDD[161] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 80.0 with 2 tasks
2017-08-10 09:14:17,662 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 80.0 (TID 160, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,662 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 80.0 (TID 161, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,662 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 80.0 (TID 160)
2017-08-10 09:14:17,662 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 80.0 (TID 161)
2017-08-10 09:14:17,664 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,664 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,667 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 80.0 (TID 160). 714 bytes result sent to driver
2017-08-10 09:14:17,667 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 80.0 (TID 161). 714 bytes result sent to driver
2017-08-10 09:14:17,668 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 80.0 (TID 160) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,668 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 80.0 (TID 161) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,668 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 80.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 80 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,668 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 80 finished: foreachPartition at streamingProcess.scala:50, took 0.013292 s
2017-08-10 09:14:17,669 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327315000 ms.0 from job set of time 1502327315000 ms
2017-08-10 09:14:17,669 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 342.669 s for time 1502327315000 ms (execution: 0.024 s)
2017-08-10 09:14:17,669 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 159 from persistence list
2017-08-10 09:14:17,669 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327320000 ms.0 from job set of time 1502327320000 ms
2017-08-10 09:14:17,669 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 159
2017-08-10 09:14:17,669 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 158 from persistence list
2017-08-10 09:14:17,669 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 158
2017-08-10 09:14:17,669 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,669 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327305000 ms
2017-08-10 09:14:17,679 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 81 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 81 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,679 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,680 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 81 (MapPartitionsRDD[163] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,682 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_81 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:17,685 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_81_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:17,685 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_81_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 81 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 81 (MapPartitionsRDD[163] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 81.0 with 2 tasks
2017-08-10 09:14:17,686 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 81.0 (TID 162, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,687 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 81.0 (TID 163, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,687 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 81.0 (TID 162)
2017-08-10 09:14:17,687 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 81.0 (TID 163)
2017-08-10 09:14:17,688 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,688 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,691 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 81.0 (TID 162). 714 bytes result sent to driver
2017-08-10 09:14:17,691 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 81.0 (TID 163). 714 bytes result sent to driver
2017-08-10 09:14:17,692 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 81.0 (TID 162) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,692 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 81.0 (TID 163) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,692 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 81.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,692 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 81 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:17,692 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 81 finished: foreachPartition at streamingProcess.scala:50, took 0.013431 s
2017-08-10 09:14:17,693 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327320000 ms.0 from job set of time 1502327320000 ms
2017-08-10 09:14:17,693 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 337.693 s for time 1502327320000 ms (execution: 0.024 s)
2017-08-10 09:14:17,693 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327325000 ms.0 from job set of time 1502327325000 ms
2017-08-10 09:14:17,693 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 161 from persistence list
2017-08-10 09:14:17,693 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 161
2017-08-10 09:14:17,693 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 160 from persistence list
2017-08-10 09:14:17,694 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 160
2017-08-10 09:14:17,694 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,694 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327310000 ms
2017-08-10 09:14:17,703 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 82 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 82 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 82 (MapPartitionsRDD[165] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,706 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_82 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:17,708 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_82_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:17,708 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_82_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,708 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 82 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,708 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 82 (MapPartitionsRDD[165] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,709 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 82.0 with 2 tasks
2017-08-10 09:14:17,709 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 82.0 (TID 164, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,709 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 82.0 (TID 165, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,709 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 82.0 (TID 164)
2017-08-10 09:14:17,709 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 82.0 (TID 165)
2017-08-10 09:14:17,711 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,711 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,714 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 82.0 (TID 164). 714 bytes result sent to driver
2017-08-10 09:14:17,714 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 82.0 (TID 165). 714 bytes result sent to driver
2017-08-10 09:14:17,715 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 82.0 (TID 164) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,715 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 82.0 (TID 165) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,715 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 82.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,715 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 82 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:17,715 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 82 finished: foreachPartition at streamingProcess.scala:50, took 0.012116 s
2017-08-10 09:14:17,715 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327325000 ms.0 from job set of time 1502327325000 ms
2017-08-10 09:14:17,716 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 332.715 s for time 1502327325000 ms (execution: 0.022 s)
2017-08-10 09:14:17,716 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 163 from persistence list
2017-08-10 09:14:17,716 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327330000 ms.0 from job set of time 1502327330000 ms
2017-08-10 09:14:17,716 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 163
2017-08-10 09:14:17,716 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 162 from persistence list
2017-08-10 09:14:17,716 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 162
2017-08-10 09:14:17,716 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,716 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327315000 ms
2017-08-10 09:14:17,725 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 83 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 83 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 83 (MapPartitionsRDD[167] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,728 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_83 stored as values in memory (estimated size 34.5 KB, free 412.9 MB)
2017-08-10 09:14:17,740 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_83_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:17,741 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_83_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,741 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 83 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,741 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_67_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,742 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 83 (MapPartitionsRDD[167] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,742 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 83.0 with 2 tasks
2017-08-10 09:14:17,742 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_68_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:17,742 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 83.0 (TID 166, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,743 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 83.0 (TID 167, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,743 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 83.0 (TID 166)
2017-08-10 09:14:17,743 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 83.0 (TID 167)
2017-08-10 09:14:17,743 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_69_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,744 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_70_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,744 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_71_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,745 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,745 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,745 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_72_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,746 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_73_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,747 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_74_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,747 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 83.0 (TID 166). 714 bytes result sent to driver
2017-08-10 09:14:17,747 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 83.0 (TID 167). 714 bytes result sent to driver
2017-08-10 09:14:17,747 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_75_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,748 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_76_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,749 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 83.0 (TID 166) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,749 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 83.0 (TID 167) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,749 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 83.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,749 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 83 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,749 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_77_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,749 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 83 finished: foreachPartition at streamingProcess.scala:50, took 0.023934 s
2017-08-10 09:14:17,750 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327330000 ms.0 from job set of time 1502327330000 ms
2017-08-10 09:14:17,750 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 327.750 s for time 1502327330000 ms (execution: 0.034 s)
2017-08-10 09:14:17,750 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327335000 ms.0 from job set of time 1502327335000 ms
2017-08-10 09:14:17,750 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 165 from persistence list
2017-08-10 09:14:17,750 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 164 from persistence list
2017-08-10 09:14:17,750 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_78_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,750 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,750 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327320000 ms
2017-08-10 09:14:17,750 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 165
2017-08-10 09:14:17,750 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 164
2017-08-10 09:14:17,751 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_79_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,752 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_80_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,753 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_81_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:17,753 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_82_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:17,760 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,760 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 84 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,760 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 84 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,760 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,760 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,760 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 84 (MapPartitionsRDD[169] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,763 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_84 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:17,765 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_84_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:17,766 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_84_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:17,766 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 84 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,766 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 84 (MapPartitionsRDD[169] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,766 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 84.0 with 2 tasks
2017-08-10 09:14:17,767 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 84.0 (TID 168, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,767 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 84.0 (TID 169, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,767 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 84.0 (TID 168)
2017-08-10 09:14:17,768 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 84.0 (TID 169)
2017-08-10 09:14:17,769 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,769 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,771 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 84.0 (TID 169). 714 bytes result sent to driver
2017-08-10 09:14:17,771 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 84.0 (TID 168). 714 bytes result sent to driver
2017-08-10 09:14:17,773 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 84.0 (TID 169) in 5 ms on localhost (1/2)
2017-08-10 09:14:17,773 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 84.0 (TID 168) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,773 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 84.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,773 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 84 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,773 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 84 finished: foreachPartition at streamingProcess.scala:50, took 0.012974 s
2017-08-10 09:14:17,773 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327335000 ms.0 from job set of time 1502327335000 ms
2017-08-10 09:14:17,773 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 322.773 s for time 1502327335000 ms (execution: 0.023 s)
2017-08-10 09:14:17,773 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327340000 ms.0 from job set of time 1502327340000 ms
2017-08-10 09:14:17,774 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 167 from persistence list
2017-08-10 09:14:17,774 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 166 from persistence list
2017-08-10 09:14:17,774 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 166
2017-08-10 09:14:17,774 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 167
2017-08-10 09:14:17,774 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,774 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327325000 ms
2017-08-10 09:14:17,784 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,784 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 85 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,784 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 85 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,784 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,784 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,785 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 85 (MapPartitionsRDD[171] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,787 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_85 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:17,790 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_85_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:17,791 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_85_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,791 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 85 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,791 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 85 (MapPartitionsRDD[171] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,791 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 85.0 with 2 tasks
2017-08-10 09:14:17,792 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 85.0 (TID 170, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,792 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 85.0 (TID 171, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,792 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 85.0 (TID 171)
2017-08-10 09:14:17,792 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 85.0 (TID 170)
2017-08-10 09:14:17,794 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,794 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,796 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 85.0 (TID 170). 714 bytes result sent to driver
2017-08-10 09:14:17,796 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 85.0 (TID 171). 714 bytes result sent to driver
2017-08-10 09:14:17,798 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 85.0 (TID 170) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,799 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 85.0 (TID 171) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,799 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 85.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,799 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 85 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:17,799 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 85 finished: foreachPartition at streamingProcess.scala:50, took 0.015520 s
2017-08-10 09:14:17,800 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327340000 ms.0 from job set of time 1502327340000 ms
2017-08-10 09:14:17,800 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 169 from persistence list
2017-08-10 09:14:17,800 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 317.800 s for time 1502327340000 ms (execution: 0.027 s)
2017-08-10 09:14:17,800 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327345000 ms.0 from job set of time 1502327345000 ms
2017-08-10 09:14:17,800 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 169
2017-08-10 09:14:17,801 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 168 from persistence list
2017-08-10 09:14:17,801 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 168
2017-08-10 09:14:17,801 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,801 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327330000 ms
2017-08-10 09:14:17,810 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 86 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 86 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,810 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 86 (MapPartitionsRDD[173] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,813 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_86 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:17,815 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_86_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:17,815 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_86_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,816 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 86 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,816 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 86 (MapPartitionsRDD[173] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,816 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 86.0 with 2 tasks
2017-08-10 09:14:17,816 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 86.0 (TID 172, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,817 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 86.0 (TID 173, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,817 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 86.0 (TID 172)
2017-08-10 09:14:17,817 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 86.0 (TID 173)
2017-08-10 09:14:17,818 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,819 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,822 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 86.0 (TID 172). 714 bytes result sent to driver
2017-08-10 09:14:17,822 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 86.0 (TID 173). 714 bytes result sent to driver
2017-08-10 09:14:17,823 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 86.0 (TID 172) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,823 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 86.0 (TID 173) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,823 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 86.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,823 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 86 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,823 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 86 finished: foreachPartition at streamingProcess.scala:50, took 0.013426 s
2017-08-10 09:14:17,824 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327345000 ms.0 from job set of time 1502327345000 ms
2017-08-10 09:14:17,824 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 312.824 s for time 1502327345000 ms (execution: 0.024 s)
2017-08-10 09:14:17,824 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 171 from persistence list
2017-08-10 09:14:17,824 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327350000 ms.0 from job set of time 1502327350000 ms
2017-08-10 09:14:17,824 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 171
2017-08-10 09:14:17,824 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 170 from persistence list
2017-08-10 09:14:17,824 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 170
2017-08-10 09:14:17,824 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,825 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327335000 ms
2017-08-10 09:14:17,834 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,834 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 87 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,834 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 87 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,834 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,834 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,834 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 87 (MapPartitionsRDD[175] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,837 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_87 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:17,839 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_87_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:17,840 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_87_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,840 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 87 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,840 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 87 (MapPartitionsRDD[175] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,840 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 87.0 with 2 tasks
2017-08-10 09:14:17,841 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 87.0 (TID 174, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,841 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 87.0 (TID 175, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,841 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 87.0 (TID 174)
2017-08-10 09:14:17,841 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 87.0 (TID 175)
2017-08-10 09:14:17,843 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,843 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,846 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 87.0 (TID 175). 714 bytes result sent to driver
2017-08-10 09:14:17,846 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 87.0 (TID 174). 714 bytes result sent to driver
2017-08-10 09:14:17,847 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 87.0 (TID 175) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,847 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 87.0 (TID 174) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,847 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 87.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,847 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 87 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:17,848 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 87 finished: foreachPartition at streamingProcess.scala:50, took 0.013608 s
2017-08-10 09:14:17,848 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327350000 ms.0 from job set of time 1502327350000 ms
2017-08-10 09:14:17,848 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 307.848 s for time 1502327350000 ms (execution: 0.024 s)
2017-08-10 09:14:17,848 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 173 from persistence list
2017-08-10 09:14:17,848 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327355000 ms.0 from job set of time 1502327355000 ms
2017-08-10 09:14:17,848 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 173
2017-08-10 09:14:17,848 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 172 from persistence list
2017-08-10 09:14:17,849 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,849 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 172
2017-08-10 09:14:17,849 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327340000 ms
2017-08-10 09:14:17,858 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 88 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 88 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 88 (MapPartitionsRDD[177] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,861 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_88 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:17,863 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_88_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:17,863 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_88_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:17,864 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 88 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,864 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 88 (MapPartitionsRDD[177] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,864 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 88.0 with 2 tasks
2017-08-10 09:14:17,864 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 88.0 (TID 176, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,865 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 88.0 (TID 177, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,865 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 88.0 (TID 177)
2017-08-10 09:14:17,865 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 88.0 (TID 176)
2017-08-10 09:14:17,867 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,867 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,869 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 88.0 (TID 177). 714 bytes result sent to driver
2017-08-10 09:14:17,869 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 88.0 (TID 176). 714 bytes result sent to driver
2017-08-10 09:14:17,870 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 88.0 (TID 176) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,870 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 88.0 (TID 177) in 6 ms on localhost (2/2)
2017-08-10 09:14:17,871 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 88.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,871 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 88 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,871 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 88 finished: foreachPartition at streamingProcess.scala:50, took 0.012867 s
2017-08-10 09:14:17,871 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327355000 ms.0 from job set of time 1502327355000 ms
2017-08-10 09:14:17,871 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 302.871 s for time 1502327355000 ms (execution: 0.023 s)
2017-08-10 09:14:17,871 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 175 from persistence list
2017-08-10 09:14:17,871 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327360000 ms.0 from job set of time 1502327360000 ms
2017-08-10 09:14:17,872 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 175
2017-08-10 09:14:17,872 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 174 from persistence list
2017-08-10 09:14:17,872 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 174
2017-08-10 09:14:17,872 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,872 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327345000 ms
2017-08-10 09:14:17,881 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 89 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 89 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,882 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 89 (MapPartitionsRDD[179] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,885 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_89 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:17,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_89_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:17,888 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_89_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,888 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 89 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,888 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 89 (MapPartitionsRDD[179] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,888 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 89.0 with 2 tasks
2017-08-10 09:14:17,889 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 89.0 (TID 178, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,890 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 89.0 (TID 179, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,890 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 89.0 (TID 178)
2017-08-10 09:14:17,890 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 89.0 (TID 179)
2017-08-10 09:14:17,892 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,892 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,895 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 89.0 (TID 179). 714 bytes result sent to driver
2017-08-10 09:14:17,895 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 89.0 (TID 178). 714 bytes result sent to driver
2017-08-10 09:14:17,896 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 89.0 (TID 179) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,896 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 89.0 (TID 178) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,896 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 89.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,896 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 89 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,896 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 89 finished: foreachPartition at streamingProcess.scala:50, took 0.015437 s
2017-08-10 09:14:17,897 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327360000 ms.0 from job set of time 1502327360000 ms
2017-08-10 09:14:17,897 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 297.897 s for time 1502327360000 ms (execution: 0.026 s)
2017-08-10 09:14:17,897 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327365000 ms.0 from job set of time 1502327365000 ms
2017-08-10 09:14:17,897 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 177 from persistence list
2017-08-10 09:14:17,898 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 177
2017-08-10 09:14:17,898 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 176 from persistence list
2017-08-10 09:14:17,898 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 176
2017-08-10 09:14:17,898 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,898 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327350000 ms
2017-08-10 09:14:17,908 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,908 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 90 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,908 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 90 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,908 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,908 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,908 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 90 (MapPartitionsRDD[181] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,911 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_90 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:17,914 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_90_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:17,915 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_90_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,915 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 90 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,915 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 90 (MapPartitionsRDD[181] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,915 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 90.0 with 2 tasks
2017-08-10 09:14:17,916 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 90.0 (TID 180, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,916 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 90.0 (TID 181, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,917 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 90.0 (TID 180)
2017-08-10 09:14:17,917 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 90.0 (TID 181)
2017-08-10 09:14:17,918 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,918 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,921 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 90.0 (TID 181). 714 bytes result sent to driver
2017-08-10 09:14:17,921 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 90.0 (TID 180). 714 bytes result sent to driver
2017-08-10 09:14:17,923 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 90.0 (TID 180) in 6 ms on localhost (1/2)
2017-08-10 09:14:17,923 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 90.0 (TID 181) in 7 ms on localhost (2/2)
2017-08-10 09:14:17,923 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 90.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,923 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 90 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:17,923 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 90 finished: foreachPartition at streamingProcess.scala:50, took 0.015514 s
2017-08-10 09:14:17,924 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327365000 ms.0 from job set of time 1502327365000 ms
2017-08-10 09:14:17,924 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 292.924 s for time 1502327365000 ms (execution: 0.027 s)
2017-08-10 09:14:17,924 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327370000 ms.0 from job set of time 1502327370000 ms
2017-08-10 09:14:17,924 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 179 from persistence list
2017-08-10 09:14:17,925 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 178 from persistence list
2017-08-10 09:14:17,925 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 178
2017-08-10 09:14:17,925 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 179
2017-08-10 09:14:17,925 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,925 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327355000 ms
2017-08-10 09:14:17,934 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,935 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 91 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,935 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 91 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,935 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,935 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,935 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 91 (MapPartitionsRDD[183] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,938 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_91 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:17,941 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_91_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:17,942 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_91_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,942 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 91 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,942 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 91 (MapPartitionsRDD[183] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,942 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 91.0 with 2 tasks
2017-08-10 09:14:17,943 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 91.0 (TID 182, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,943 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 91.0 (TID 183, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,943 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 91.0 (TID 182)
2017-08-10 09:14:17,945 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,945 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 91.0 (TID 183)
2017-08-10 09:14:17,947 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,949 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 91.0 (TID 183). 714 bytes result sent to driver
2017-08-10 09:14:17,949 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 91.0 (TID 182). 714 bytes result sent to driver
2017-08-10 09:14:17,951 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 91.0 (TID 183) in 7 ms on localhost (1/2)
2017-08-10 09:14:17,951 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 91.0 (TID 182) in 9 ms on localhost (2/2)
2017-08-10 09:14:17,951 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 91.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,951 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 91 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:17,951 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 91 finished: foreachPartition at streamingProcess.scala:50, took 0.016643 s
2017-08-10 09:14:17,951 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327370000 ms.0 from job set of time 1502327370000 ms
2017-08-10 09:14:17,951 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 287.951 s for time 1502327370000 ms (execution: 0.027 s)
2017-08-10 09:14:17,952 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327375000 ms.0 from job set of time 1502327375000 ms
2017-08-10 09:14:17,952 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 181 from persistence list
2017-08-10 09:14:17,952 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 181
2017-08-10 09:14:17,952 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 180 from persistence list
2017-08-10 09:14:17,952 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 180
2017-08-10 09:14:17,952 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,953 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327360000 ms
2017-08-10 09:14:17,963 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 92 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 92 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,963 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 92 (MapPartitionsRDD[185] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,967 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_92 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:17,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_92_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:17,971 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_92_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:17,971 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 92 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,971 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 92 (MapPartitionsRDD[185] at map at streamingProcess.scala:48)
2017-08-10 09:14:17,971 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 92.0 with 2 tasks
2017-08-10 09:14:17,972 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 92.0 (TID 184, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:17,972 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 92.0 (TID 185, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:17,972 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 92.0 (TID 184)
2017-08-10 09:14:17,974 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:17,974 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 92.0 (TID 185)
2017-08-10 09:14:17,976 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:17,978 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 92.0 (TID 185). 714 bytes result sent to driver
2017-08-10 09:14:17,979 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 92.0 (TID 184). 714 bytes result sent to driver
2017-08-10 09:14:17,981 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 92.0 (TID 185) in 9 ms on localhost (1/2)
2017-08-10 09:14:17,982 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 92.0 (TID 184) in 10 ms on localhost (2/2)
2017-08-10 09:14:17,982 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 92.0, whose tasks have all completed, from pool 
2017-08-10 09:14:17,982 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 92 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:14:17,983 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 92 finished: foreachPartition at streamingProcess.scala:50, took 0.019677 s
2017-08-10 09:14:17,983 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327375000 ms.0 from job set of time 1502327375000 ms
2017-08-10 09:14:17,983 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 282.983 s for time 1502327375000 ms (execution: 0.031 s)
2017-08-10 09:14:17,983 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327380000 ms.0 from job set of time 1502327380000 ms
2017-08-10 09:14:17,983 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 183 from persistence list
2017-08-10 09:14:17,983 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 182 from persistence list
2017-08-10 09:14:17,984 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 182
2017-08-10 09:14:17,984 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 183
2017-08-10 09:14:17,984 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:17,984 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327365000 ms
2017-08-10 09:14:17,993 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:17,994 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 93 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:17,994 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 93 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:17,994 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:17,994 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:17,994 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 93 (MapPartitionsRDD[187] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:17,996 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_93 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:17,999 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_93_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:17,999 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_93_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:17,999 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 93 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:17,999 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 93 (MapPartitionsRDD[187] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,000 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 93.0 with 2 tasks
2017-08-10 09:14:18,000 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 93.0 (TID 186, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,000 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 93.0 (TID 187, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,001 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 93.0 (TID 186)
2017-08-10 09:14:18,001 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 93.0 (TID 187)
2017-08-10 09:14:18,002 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,002 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,005 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 93.0 (TID 187). 714 bytes result sent to driver
2017-08-10 09:14:18,005 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 93.0 (TID 186). 714 bytes result sent to driver
2017-08-10 09:14:18,007 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 93.0 (TID 187) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,007 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 93.0 (TID 186) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,007 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 93.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,008 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 93 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:18,008 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 93 finished: foreachPartition at streamingProcess.scala:50, took 0.014688 s
2017-08-10 09:14:18,009 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327380000 ms.0 from job set of time 1502327380000 ms
2017-08-10 09:14:18,009 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 278.009 s for time 1502327380000 ms (execution: 0.026 s)
2017-08-10 09:14:18,009 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 185 from persistence list
2017-08-10 09:14:18,009 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327385000 ms.0 from job set of time 1502327385000 ms
2017-08-10 09:14:18,010 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 185
2017-08-10 09:14:18,010 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 184 from persistence list
2017-08-10 09:14:18,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327370000 ms
2017-08-10 09:14:18,011 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 184
2017-08-10 09:14:18,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 94 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 94 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 94 (MapPartitionsRDD[189] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_94 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:18,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_94_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:18,029 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_94_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 94 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 94 (MapPartitionsRDD[189] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 94.0 with 2 tasks
2017-08-10 09:14:18,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 94.0 (TID 188, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 94.0 (TID 189, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,031 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 94.0 (TID 188)
2017-08-10 09:14:18,031 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 94.0 (TID 189)
2017-08-10 09:14:18,032 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,032 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,035 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 94.0 (TID 189). 801 bytes result sent to driver
2017-08-10 09:14:18,035 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 94.0 (TID 188). 714 bytes result sent to driver
2017-08-10 09:14:18,037 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 94.0 (TID 188) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,037 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 94.0 (TID 189) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,037 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 94.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 94 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,038 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 94 finished: foreachPartition at streamingProcess.scala:50, took 0.013896 s
2017-08-10 09:14:18,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327385000 ms.0 from job set of time 1502327385000 ms
2017-08-10 09:14:18,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 273.040 s for time 1502327385000 ms (execution: 0.031 s)
2017-08-10 09:14:18,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327390000 ms.0 from job set of time 1502327390000 ms
2017-08-10 09:14:18,043 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 187 from persistence list
2017-08-10 09:14:18,048 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 186 from persistence list
2017-08-10 09:14:18,048 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 187
2017-08-10 09:14:18,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327375000 ms
2017-08-10 09:14:18,048 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 186
2017-08-10 09:14:18,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 95 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 95 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 95 (MapPartitionsRDD[191] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_95 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:18,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_95_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:18,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_95_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 95 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 95 (MapPartitionsRDD[191] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 95.0 with 2 tasks
2017-08-10 09:14:18,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 95.0 (TID 190, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 95.0 (TID 191, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,057 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 95.0 (TID 190)
2017-08-10 09:14:18,057 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 95.0 (TID 191)
2017-08-10 09:14:18,059 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,059 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,062 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 95.0 (TID 191). 801 bytes result sent to driver
2017-08-10 09:14:18,062 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 95.0 (TID 190). 714 bytes result sent to driver
2017-08-10 09:14:18,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 95.0 (TID 191) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 95.0 (TID 190) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 95.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 95 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:18,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 95 finished: foreachPartition at streamingProcess.scala:50, took 0.014580 s
2017-08-10 09:14:18,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327390000 ms.0 from job set of time 1502327390000 ms
2017-08-10 09:14:18,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 268.064 s for time 1502327390000 ms (execution: 0.024 s)
2017-08-10 09:14:18,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327395000 ms.0 from job set of time 1502327395000 ms
2017-08-10 09:14:18,064 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 189 from persistence list
2017-08-10 09:14:18,065 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 189
2017-08-10 09:14:18,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 188 from persistence list
2017-08-10 09:14:18,065 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 188
2017-08-10 09:14:18,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327380000 ms
2017-08-10 09:14:18,075 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 96 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 96 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 96 (MapPartitionsRDD[193] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_96 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:18,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_96_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:18,081 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_96_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 96 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 96 (MapPartitionsRDD[193] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 96.0 with 2 tasks
2017-08-10 09:14:18,082 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 96.0 (TID 192, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,082 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 96.0 (TID 193, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,082 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 96.0 (TID 192)
2017-08-10 09:14:18,083 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 96.0 (TID 193)
2017-08-10 09:14:18,084 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,084 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,086 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 96.0 (TID 193). 714 bytes result sent to driver
2017-08-10 09:14:18,086 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 96.0 (TID 192). 714 bytes result sent to driver
2017-08-10 09:14:18,088 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 96.0 (TID 193) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 96.0 (TID 192) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 96.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 96 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 96 finished: foreachPartition at streamingProcess.scala:50, took 0.013247 s
2017-08-10 09:14:18,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327395000 ms.0 from job set of time 1502327395000 ms
2017-08-10 09:14:18,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 263.089 s for time 1502327395000 ms (execution: 0.025 s)
2017-08-10 09:14:18,089 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 191 from persistence list
2017-08-10 09:14:18,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327400000 ms.0 from job set of time 1502327400000 ms
2017-08-10 09:14:18,089 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 191
2017-08-10 09:14:18,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 190 from persistence list
2017-08-10 09:14:18,089 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 190
2017-08-10 09:14:18,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327385000 ms
2017-08-10 09:14:18,098 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,099 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 97 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,099 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 97 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,099 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,099 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,099 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 97 (MapPartitionsRDD[195] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,102 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_97 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:18,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_97_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:18,105 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_97_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 97 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 97 (MapPartitionsRDD[195] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 97.0 with 2 tasks
2017-08-10 09:14:18,106 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 97.0 (TID 194, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,106 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 97.0 (TID 195, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,106 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 97.0 (TID 194)
2017-08-10 09:14:18,106 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 97.0 (TID 195)
2017-08-10 09:14:18,108 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,108 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,110 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 97.0 (TID 195). 714 bytes result sent to driver
2017-08-10 09:14:18,110 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 97.0 (TID 194). 714 bytes result sent to driver
2017-08-10 09:14:18,111 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 97.0 (TID 195) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,112 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 97.0 (TID 194) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,112 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 97 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,112 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 97.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,112 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 97 finished: foreachPartition at streamingProcess.scala:50, took 0.013295 s
2017-08-10 09:14:18,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327400000 ms.0 from job set of time 1502327400000 ms
2017-08-10 09:14:18,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 258.112 s for time 1502327400000 ms (execution: 0.023 s)
2017-08-10 09:14:18,112 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 193 from persistence list
2017-08-10 09:14:18,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327405000 ms.0 from job set of time 1502327405000 ms
2017-08-10 09:14:18,112 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 193
2017-08-10 09:14:18,113 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 192 from persistence list
2017-08-10 09:14:18,113 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 192
2017-08-10 09:14:18,113 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,113 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327390000 ms
2017-08-10 09:14:18,122 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 98 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 98 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 98 (MapPartitionsRDD[197] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,125 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_98 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:18,128 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_98_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:18,128 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_98_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,128 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 98 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,128 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 98 (MapPartitionsRDD[197] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,128 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 98.0 with 2 tasks
2017-08-10 09:14:18,129 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 98.0 (TID 196, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,129 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 98.0 (TID 197, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,129 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 98.0 (TID 197)
2017-08-10 09:14:18,129 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 98.0 (TID 196)
2017-08-10 09:14:18,131 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,131 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,133 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 98.0 (TID 197). 714 bytes result sent to driver
2017-08-10 09:14:18,133 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 98.0 (TID 196). 714 bytes result sent to driver
2017-08-10 09:14:18,134 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 98.0 (TID 197) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,134 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 98.0 (TID 196) in 5 ms on localhost (2/2)
2017-08-10 09:14:18,135 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 98.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,135 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 98 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,135 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 98 finished: foreachPartition at streamingProcess.scala:50, took 0.012459 s
2017-08-10 09:14:18,135 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327405000 ms.0 from job set of time 1502327405000 ms
2017-08-10 09:14:18,135 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 253.135 s for time 1502327405000 ms (execution: 0.023 s)
2017-08-10 09:14:18,135 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 195 from persistence list
2017-08-10 09:14:18,135 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327410000 ms.0 from job set of time 1502327410000 ms
2017-08-10 09:14:18,135 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 195
2017-08-10 09:14:18,136 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 194 from persistence list
2017-08-10 09:14:18,136 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 194
2017-08-10 09:14:18,136 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,136 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327395000 ms
2017-08-10 09:14:18,145 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 99 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 99 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 99 (MapPartitionsRDD[199] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,148 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_99 stored as values in memory (estimated size 34.5 KB, free 412.9 MB)
2017-08-10 09:14:18,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_99_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:18,157 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_99_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,157 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_83_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 99 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 99 (MapPartitionsRDD[199] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 99.0 with 2 tasks
2017-08-10 09:14:18,158 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_84_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,158 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 99.0 (TID 198, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,158 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 99.0 (TID 199, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,158 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 99.0 (TID 198)
2017-08-10 09:14:18,158 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 99.0 (TID 199)
2017-08-10 09:14:18,158 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_85_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,159 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_86_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,160 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,160 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,160 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_87_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,161 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_88_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,162 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 99.0 (TID 199). 714 bytes result sent to driver
2017-08-10 09:14:18,162 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 99.0 (TID 198). 714 bytes result sent to driver
2017-08-10 09:14:18,162 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_89_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,163 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 99.0 (TID 199) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,163 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 99.0 (TID 198) in 5 ms on localhost (2/2)
2017-08-10 09:14:18,164 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 99.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,164 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 99 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,164 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 99 finished: foreachPartition at streamingProcess.scala:50, took 0.018845 s
2017-08-10 09:14:18,164 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327410000 ms.0 from job set of time 1502327410000 ms
2017-08-10 09:14:18,164 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_90_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,164 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 248.164 s for time 1502327410000 ms (execution: 0.029 s)
2017-08-10 09:14:18,164 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327415000 ms.0 from job set of time 1502327415000 ms
2017-08-10 09:14:18,164 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 197 from persistence list
2017-08-10 09:14:18,165 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 197
2017-08-10 09:14:18,165 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 196 from persistence list
2017-08-10 09:14:18,165 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_91_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,165 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 196
2017-08-10 09:14:18,165 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,165 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327400000 ms
2017-08-10 09:14:18,166 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_92_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,166 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_93_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,167 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_94_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,167 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_95_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,168 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_96_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,169 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_97_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,169 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_98_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,175 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,175 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 100 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,175 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 100 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,175 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,175 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,175 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 100 (MapPartitionsRDD[201] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_100 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:18,180 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_100_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:18,181 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_100_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 100 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 100 (MapPartitionsRDD[201] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 100.0 with 2 tasks
2017-08-10 09:14:18,182 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 100.0 (TID 200, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,182 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 100.0 (TID 201, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,182 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 100.0 (TID 200)
2017-08-10 09:14:18,182 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 100.0 (TID 201)
2017-08-10 09:14:18,184 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,184 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,186 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 100.0 (TID 200). 714 bytes result sent to driver
2017-08-10 09:14:18,186 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 100.0 (TID 201). 714 bytes result sent to driver
2017-08-10 09:14:18,188 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 100.0 (TID 200) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,188 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 100.0 (TID 201) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,188 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 100.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,188 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 100 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,188 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 100 finished: foreachPartition at streamingProcess.scala:50, took 0.013301 s
2017-08-10 09:14:18,188 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327415000 ms.0 from job set of time 1502327415000 ms
2017-08-10 09:14:18,188 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 243.188 s for time 1502327415000 ms (execution: 0.024 s)
2017-08-10 09:14:18,188 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 199 from persistence list
2017-08-10 09:14:18,188 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327420000 ms.0 from job set of time 1502327420000 ms
2017-08-10 09:14:18,189 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 198 from persistence list
2017-08-10 09:14:18,189 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 199
2017-08-10 09:14:18,189 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 198
2017-08-10 09:14:18,189 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,189 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327405000 ms
2017-08-10 09:14:18,198 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,199 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 101 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,199 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 101 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,199 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,199 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,199 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 101 (MapPartitionsRDD[203] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,201 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_101 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:18,204 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_101_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:18,205 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_101_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 101 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 101 (MapPartitionsRDD[203] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 101.0 with 2 tasks
2017-08-10 09:14:18,205 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 101.0 (TID 202, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,206 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 101.0 (TID 203, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,206 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 101.0 (TID 202)
2017-08-10 09:14:18,206 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 101.0 (TID 203)
2017-08-10 09:14:18,207 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,207 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,210 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 101.0 (TID 203). 714 bytes result sent to driver
2017-08-10 09:14:18,210 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 101.0 (TID 202). 714 bytes result sent to driver
2017-08-10 09:14:18,212 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 101.0 (TID 202) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,212 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 101.0 (TID 203) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,212 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 101.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,212 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 101 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,212 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 101 finished: foreachPartition at streamingProcess.scala:50, took 0.013843 s
2017-08-10 09:14:18,212 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327420000 ms.0 from job set of time 1502327420000 ms
2017-08-10 09:14:18,212 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 238.212 s for time 1502327420000 ms (execution: 0.024 s)
2017-08-10 09:14:18,212 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 201 from persistence list
2017-08-10 09:14:18,213 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327425000 ms.0 from job set of time 1502327425000 ms
2017-08-10 09:14:18,213 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 201
2017-08-10 09:14:18,213 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 200 from persistence list
2017-08-10 09:14:18,213 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 200
2017-08-10 09:14:18,213 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,213 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327410000 ms
2017-08-10 09:14:18,222 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,222 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 102 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,222 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 102 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,222 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,222 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,223 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 102 (MapPartitionsRDD[205] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,225 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_102 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:18,228 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_102_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:18,228 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_102_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,229 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 102 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,229 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 102 (MapPartitionsRDD[205] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,229 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 102.0 with 2 tasks
2017-08-10 09:14:18,229 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 102.0 (TID 204, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,230 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 102.0 (TID 205, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,230 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 102.0 (TID 204)
2017-08-10 09:14:18,230 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 102.0 (TID 205)
2017-08-10 09:14:18,231 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,231 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,234 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 102.0 (TID 204). 714 bytes result sent to driver
2017-08-10 09:14:18,234 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 102.0 (TID 205). 714 bytes result sent to driver
2017-08-10 09:14:18,235 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 102.0 (TID 205) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,236 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 102.0 (TID 204) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,236 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 102.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,236 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 102 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,236 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 102 finished: foreachPartition at streamingProcess.scala:50, took 0.013728 s
2017-08-10 09:14:18,236 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327425000 ms.0 from job set of time 1502327425000 ms
2017-08-10 09:14:18,236 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 233.236 s for time 1502327425000 ms (execution: 0.023 s)
2017-08-10 09:14:18,236 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 203 from persistence list
2017-08-10 09:14:18,236 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327430000 ms.0 from job set of time 1502327430000 ms
2017-08-10 09:14:18,237 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 202 from persistence list
2017-08-10 09:14:18,237 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 203
2017-08-10 09:14:18,237 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 202
2017-08-10 09:14:18,237 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,237 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327415000 ms
2017-08-10 09:14:18,246 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,246 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 103 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,246 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 103 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,246 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,246 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,246 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 103 (MapPartitionsRDD[207] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,249 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_103 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:18,251 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_103_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:18,252 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_103_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,252 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 103 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,252 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 103 (MapPartitionsRDD[207] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,252 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 103.0 with 2 tasks
2017-08-10 09:14:18,253 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 103.0 (TID 206, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,253 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 103.0 (TID 207, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,253 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 103.0 (TID 206)
2017-08-10 09:14:18,253 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 103.0 (TID 207)
2017-08-10 09:14:18,255 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,255 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,257 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 103.0 (TID 206). 714 bytes result sent to driver
2017-08-10 09:14:18,257 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 103.0 (TID 207). 714 bytes result sent to driver
2017-08-10 09:14:18,258 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 103.0 (TID 207) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,258 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 103.0 (TID 206) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,259 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 103.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,259 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 103 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,259 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 103 finished: foreachPartition at streamingProcess.scala:50, took 0.012942 s
2017-08-10 09:14:18,259 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327430000 ms.0 from job set of time 1502327430000 ms
2017-08-10 09:14:18,259 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 228.259 s for time 1502327430000 ms (execution: 0.023 s)
2017-08-10 09:14:18,259 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 205 from persistence list
2017-08-10 09:14:18,259 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327435000 ms.0 from job set of time 1502327435000 ms
2017-08-10 09:14:18,259 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 205
2017-08-10 09:14:18,260 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 204 from persistence list
2017-08-10 09:14:18,260 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 204
2017-08-10 09:14:18,260 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,260 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327420000 ms
2017-08-10 09:14:18,271 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 104 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 104 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 104 (MapPartitionsRDD[209] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,274 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_104 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:18,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_104_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:18,276 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_104_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 104 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 104 (MapPartitionsRDD[209] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 104.0 with 2 tasks
2017-08-10 09:14:18,277 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 104.0 (TID 208, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,278 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 104.0 (TID 209, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,278 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 104.0 (TID 209)
2017-08-10 09:14:18,278 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 104.0 (TID 208)
2017-08-10 09:14:18,279 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,279 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,282 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 104.0 (TID 208). 714 bytes result sent to driver
2017-08-10 09:14:18,282 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 104.0 (TID 209). 714 bytes result sent to driver
2017-08-10 09:14:18,283 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 104.0 (TID 209) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,283 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 104.0 (TID 208) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,283 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 104.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 104 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,283 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 104 finished: foreachPartition at streamingProcess.scala:50, took 0.012557 s
2017-08-10 09:14:18,283 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327435000 ms.0 from job set of time 1502327435000 ms
2017-08-10 09:14:18,284 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 223.283 s for time 1502327435000 ms (execution: 0.024 s)
2017-08-10 09:14:18,284 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327440000 ms.0 from job set of time 1502327440000 ms
2017-08-10 09:14:18,284 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 207 from persistence list
2017-08-10 09:14:18,284 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 206 from persistence list
2017-08-10 09:14:18,284 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 207
2017-08-10 09:14:18,285 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,285 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327425000 ms
2017-08-10 09:14:18,285 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 206
2017-08-10 09:14:18,295 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,295 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 105 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,295 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 105 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,295 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,295 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,295 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 105 (MapPartitionsRDD[211] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,298 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_105 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:18,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_105_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:18,301 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_105_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 105 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 105 (MapPartitionsRDD[211] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 105.0 with 2 tasks
2017-08-10 09:14:18,302 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 105.0 (TID 210, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,302 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 105.0 (TID 211, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,303 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 105.0 (TID 210)
2017-08-10 09:14:18,303 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 105.0 (TID 211)
2017-08-10 09:14:18,304 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,304 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,307 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 105.0 (TID 211). 714 bytes result sent to driver
2017-08-10 09:14:18,307 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 105.0 (TID 210). 714 bytes result sent to driver
2017-08-10 09:14:18,308 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 105.0 (TID 210) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,308 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 105.0 (TID 211) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,308 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 105.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,308 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 105 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,309 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 105 finished: foreachPartition at streamingProcess.scala:50, took 0.013841 s
2017-08-10 09:14:18,309 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327440000 ms.0 from job set of time 1502327440000 ms
2017-08-10 09:14:18,309 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 218.309 s for time 1502327440000 ms (execution: 0.025 s)
2017-08-10 09:14:18,309 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 209 from persistence list
2017-08-10 09:14:18,309 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327445000 ms.0 from job set of time 1502327445000 ms
2017-08-10 09:14:18,309 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 209
2017-08-10 09:14:18,309 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 208 from persistence list
2017-08-10 09:14:18,310 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,310 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 208
2017-08-10 09:14:18,310 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327430000 ms
2017-08-10 09:14:18,319 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 106 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 106 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,320 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 106 (MapPartitionsRDD[213] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,322 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_106 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:18,325 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_106_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:18,325 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_106_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,325 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 106 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,325 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 106 (MapPartitionsRDD[213] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,325 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 106.0 with 2 tasks
2017-08-10 09:14:18,326 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 106.0 (TID 212, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,326 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 106.0 (TID 213, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,326 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 106.0 (TID 213)
2017-08-10 09:14:18,326 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 106.0 (TID 212)
2017-08-10 09:14:18,328 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,328 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,330 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 106.0 (TID 212). 714 bytes result sent to driver
2017-08-10 09:14:18,330 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 106.0 (TID 213). 714 bytes result sent to driver
2017-08-10 09:14:18,331 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 106.0 (TID 212) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,331 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 106.0 (TID 213) in 5 ms on localhost (2/2)
2017-08-10 09:14:18,331 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 106.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,332 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 106 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,332 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 106 finished: foreachPartition at streamingProcess.scala:50, took 0.012630 s
2017-08-10 09:14:18,332 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327445000 ms.0 from job set of time 1502327445000 ms
2017-08-10 09:14:18,332 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 213.332 s for time 1502327445000 ms (execution: 0.023 s)
2017-08-10 09:14:18,332 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 211 from persistence list
2017-08-10 09:14:18,332 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327450000 ms.0 from job set of time 1502327450000 ms
2017-08-10 09:14:18,332 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 211
2017-08-10 09:14:18,332 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 210 from persistence list
2017-08-10 09:14:18,333 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 210
2017-08-10 09:14:18,333 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,333 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327435000 ms
2017-08-10 09:14:18,342 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,343 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 107 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,343 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 107 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,343 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,343 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,343 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 107 (MapPartitionsRDD[215] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,345 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_107 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:18,349 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_107_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:18,349 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_107_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,349 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 107 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,349 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 107 (MapPartitionsRDD[215] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,349 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 107.0 with 2 tasks
2017-08-10 09:14:18,350 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 107.0 (TID 214, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,350 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 107.0 (TID 215, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,350 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 107.0 (TID 215)
2017-08-10 09:14:18,350 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 107.0 (TID 214)
2017-08-10 09:14:18,352 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,352 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,355 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 107.0 (TID 215). 714 bytes result sent to driver
2017-08-10 09:14:18,355 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 107.0 (TID 214). 714 bytes result sent to driver
2017-08-10 09:14:18,356 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 107.0 (TID 214) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,356 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 107.0 (TID 215) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,356 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 107.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,356 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 107 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,357 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 107 finished: foreachPartition at streamingProcess.scala:50, took 0.014193 s
2017-08-10 09:14:18,357 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327450000 ms.0 from job set of time 1502327450000 ms
2017-08-10 09:14:18,357 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 208.357 s for time 1502327450000 ms (execution: 0.025 s)
2017-08-10 09:14:18,357 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 213 from persistence list
2017-08-10 09:14:18,357 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327455000 ms.0 from job set of time 1502327455000 ms
2017-08-10 09:14:18,357 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 212 from persistence list
2017-08-10 09:14:18,357 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 213
2017-08-10 09:14:18,357 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,358 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327440000 ms
2017-08-10 09:14:18,358 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 212
2017-08-10 09:14:18,367 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,367 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 108 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,367 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 108 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,367 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,367 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,367 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 108 (MapPartitionsRDD[217] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,370 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_108 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:18,373 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_108_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:18,373 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_108_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,374 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 108 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,374 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 108 (MapPartitionsRDD[217] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,374 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 108.0 with 2 tasks
2017-08-10 09:14:18,374 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 108.0 (TID 216, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,375 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 108.0 (TID 217, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,375 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 108.0 (TID 217)
2017-08-10 09:14:18,375 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 108.0 (TID 216)
2017-08-10 09:14:18,377 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,377 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,379 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 108.0 (TID 217). 714 bytes result sent to driver
2017-08-10 09:14:18,379 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 108.0 (TID 216). 714 bytes result sent to driver
2017-08-10 09:14:18,380 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 108.0 (TID 217) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,381 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 108.0 (TID 216) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,381 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 108.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,381 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 108 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,381 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 108 finished: foreachPartition at streamingProcess.scala:50, took 0.014034 s
2017-08-10 09:14:18,381 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327455000 ms.0 from job set of time 1502327455000 ms
2017-08-10 09:14:18,381 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 203.381 s for time 1502327455000 ms (execution: 0.024 s)
2017-08-10 09:14:18,381 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 215 from persistence list
2017-08-10 09:14:18,381 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327460000 ms.0 from job set of time 1502327460000 ms
2017-08-10 09:14:18,382 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 215
2017-08-10 09:14:18,382 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 214 from persistence list
2017-08-10 09:14:18,382 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 214
2017-08-10 09:14:18,382 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,382 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327445000 ms
2017-08-10 09:14:18,392 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,393 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 109 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,393 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 109 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,393 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,393 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,393 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 109 (MapPartitionsRDD[219] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,396 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_109 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:18,398 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_109_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:18,399 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_109_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 109 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 109 (MapPartitionsRDD[219] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 109.0 with 2 tasks
2017-08-10 09:14:18,400 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 109.0 (TID 218, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,400 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 109.0 (TID 219, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,400 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 109.0 (TID 219)
2017-08-10 09:14:18,400 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 109.0 (TID 218)
2017-08-10 09:14:18,402 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,402 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,405 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 109.0 (TID 218). 714 bytes result sent to driver
2017-08-10 09:14:18,405 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 109.0 (TID 219). 714 bytes result sent to driver
2017-08-10 09:14:18,406 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 109.0 (TID 219) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,407 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 109.0 (TID 218) in 8 ms on localhost (2/2)
2017-08-10 09:14:18,407 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 109.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,407 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 109 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:18,407 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 109 finished: foreachPartition at streamingProcess.scala:50, took 0.014495 s
2017-08-10 09:14:18,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327460000 ms.0 from job set of time 1502327460000 ms
2017-08-10 09:14:18,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 198.407 s for time 1502327460000 ms (execution: 0.026 s)
2017-08-10 09:14:18,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327465000 ms.0 from job set of time 1502327465000 ms
2017-08-10 09:14:18,407 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 217 from persistence list
2017-08-10 09:14:18,409 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 217
2017-08-10 09:14:18,409 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 216 from persistence list
2017-08-10 09:14:18,410 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 216
2017-08-10 09:14:18,410 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,410 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327450000 ms
2017-08-10 09:14:18,420 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 110 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 110 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 110 (MapPartitionsRDD[221] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,423 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_110 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:18,426 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_110_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:18,427 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_110_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,427 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 110 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,427 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 110 (MapPartitionsRDD[221] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,427 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 110.0 with 2 tasks
2017-08-10 09:14:18,428 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 110.0 (TID 220, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,429 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 110.0 (TID 221, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,429 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 110.0 (TID 220)
2017-08-10 09:14:18,429 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 110.0 (TID 221)
2017-08-10 09:14:18,431 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,431 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,433 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 110.0 (TID 221). 714 bytes result sent to driver
2017-08-10 09:14:18,433 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 110.0 (TID 220). 714 bytes result sent to driver
2017-08-10 09:14:18,435 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 110.0 (TID 221) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,435 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 110.0 (TID 220) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,435 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 110.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,436 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 110 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:18,436 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 110 finished: foreachPartition at streamingProcess.scala:50, took 0.016447 s
2017-08-10 09:14:18,437 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327465000 ms.0 from job set of time 1502327465000 ms
2017-08-10 09:14:18,437 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 193.437 s for time 1502327465000 ms (execution: 0.030 s)
2017-08-10 09:14:18,437 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327470000 ms.0 from job set of time 1502327470000 ms
2017-08-10 09:14:18,438 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 219 from persistence list
2017-08-10 09:14:18,438 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 218 from persistence list
2017-08-10 09:14:18,438 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 218
2017-08-10 09:14:18,438 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 219
2017-08-10 09:14:18,438 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,438 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327455000 ms
2017-08-10 09:14:18,447 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 111 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 111 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 111 (MapPartitionsRDD[223] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,451 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_111 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:18,454 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_111_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:18,455 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_111_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,455 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 111 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 111 (MapPartitionsRDD[223] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 111.0 with 2 tasks
2017-08-10 09:14:18,456 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 111.0 (TID 222, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,456 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 111.0 (TID 223, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,457 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 111.0 (TID 222)
2017-08-10 09:14:18,457 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 111.0 (TID 223)
2017-08-10 09:14:18,458 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,458 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,461 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 111.0 (TID 222). 714 bytes result sent to driver
2017-08-10 09:14:18,461 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 111.0 (TID 223). 714 bytes result sent to driver
2017-08-10 09:14:18,462 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 111.0 (TID 223) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,463 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 111.0 (TID 222) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,463 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 111.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,463 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 111 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,463 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 111 finished: foreachPartition at streamingProcess.scala:50, took 0.016160 s
2017-08-10 09:14:18,464 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327470000 ms.0 from job set of time 1502327470000 ms
2017-08-10 09:14:18,464 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 188.464 s for time 1502327470000 ms (execution: 0.027 s)
2017-08-10 09:14:18,464 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327475000 ms.0 from job set of time 1502327475000 ms
2017-08-10 09:14:18,464 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 221 from persistence list
2017-08-10 09:14:18,464 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 221
2017-08-10 09:14:18,464 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 220 from persistence list
2017-08-10 09:14:18,464 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 220
2017-08-10 09:14:18,465 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,465 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327460000 ms
2017-08-10 09:14:18,475 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 112 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 112 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,475 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 112 (MapPartitionsRDD[225] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,478 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_112 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:18,481 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_112_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:18,481 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_112_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,482 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 112 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,482 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 112 (MapPartitionsRDD[225] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,482 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 112.0 with 2 tasks
2017-08-10 09:14:18,482 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 112.0 (TID 224, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,483 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 112.0 (TID 225, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,483 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 112.0 (TID 224)
2017-08-10 09:14:18,483 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 112.0 (TID 225)
2017-08-10 09:14:18,485 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,485 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,487 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 112.0 (TID 225). 714 bytes result sent to driver
2017-08-10 09:14:18,487 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 112.0 (TID 224). 714 bytes result sent to driver
2017-08-10 09:14:18,488 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 112.0 (TID 224) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,489 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 112.0 (TID 225) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,489 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 112.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,489 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 112 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,489 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 112 finished: foreachPartition at streamingProcess.scala:50, took 0.014449 s
2017-08-10 09:14:18,489 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327475000 ms.0 from job set of time 1502327475000 ms
2017-08-10 09:14:18,490 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 183.489 s for time 1502327475000 ms (execution: 0.025 s)
2017-08-10 09:14:18,490 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327480000 ms.0 from job set of time 1502327480000 ms
2017-08-10 09:14:18,490 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 223 from persistence list
2017-08-10 09:14:18,490 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 222 from persistence list
2017-08-10 09:14:18,490 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 223
2017-08-10 09:14:18,490 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 222
2017-08-10 09:14:18,491 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,491 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327465000 ms
2017-08-10 09:14:18,499 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,500 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 113 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,500 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 113 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,500 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,500 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,500 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 113 (MapPartitionsRDD[227] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,503 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_113 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:18,506 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_113_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:18,506 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_113_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,507 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 113 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,507 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 113 (MapPartitionsRDD[227] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,507 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 113.0 with 2 tasks
2017-08-10 09:14:18,507 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 113.0 (TID 226, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,508 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 113.0 (TID 227, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,508 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 113.0 (TID 226)
2017-08-10 09:14:18,508 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 113.0 (TID 227)
2017-08-10 09:14:18,509 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,509 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,513 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 113.0 (TID 227). 714 bytes result sent to driver
2017-08-10 09:14:18,514 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 113.0 (TID 226). 714 bytes result sent to driver
2017-08-10 09:14:18,515 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 113.0 (TID 227) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,515 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 113.0 (TID 226) in 8 ms on localhost (2/2)
2017-08-10 09:14:18,516 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 113.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,516 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 113 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:14:18,516 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 113 finished: foreachPartition at streamingProcess.scala:50, took 0.016475 s
2017-08-10 09:14:18,516 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327480000 ms.0 from job set of time 1502327480000 ms
2017-08-10 09:14:18,516 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 178.516 s for time 1502327480000 ms (execution: 0.026 s)
2017-08-10 09:14:18,516 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327485000 ms.0 from job set of time 1502327485000 ms
2017-08-10 09:14:18,516 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 225 from persistence list
2017-08-10 09:14:18,517 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 224 from persistence list
2017-08-10 09:14:18,517 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 224
2017-08-10 09:14:18,517 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 225
2017-08-10 09:14:18,517 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,517 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327470000 ms
2017-08-10 09:14:18,527 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,527 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 114 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,527 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 114 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,527 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,527 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,527 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 114 (MapPartitionsRDD[229] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,530 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_114 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:18,533 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_114_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:18,533 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_114_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,533 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 114 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,533 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 114 (MapPartitionsRDD[229] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,533 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 114.0 with 2 tasks
2017-08-10 09:14:18,534 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 114.0 (TID 228, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,534 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 114.0 (TID 229, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,534 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 114.0 (TID 229)
2017-08-10 09:14:18,534 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 114.0 (TID 228)
2017-08-10 09:14:18,536 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,536 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,538 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 114.0 (TID 228). 714 bytes result sent to driver
2017-08-10 09:14:18,538 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 114.0 (TID 229). 714 bytes result sent to driver
2017-08-10 09:14:18,539 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 114.0 (TID 228) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,540 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 114.0 (TID 229) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,540 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 114.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,540 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 114 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,540 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 114 finished: foreachPartition at streamingProcess.scala:50, took 0.013302 s
2017-08-10 09:14:18,540 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327485000 ms.0 from job set of time 1502327485000 ms
2017-08-10 09:14:18,540 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 173.540 s for time 1502327485000 ms (execution: 0.024 s)
2017-08-10 09:14:18,540 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 227 from persistence list
2017-08-10 09:14:18,540 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327490000 ms.0 from job set of time 1502327490000 ms
2017-08-10 09:14:18,541 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 227
2017-08-10 09:14:18,541 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 226 from persistence list
2017-08-10 09:14:18,541 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,542 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327475000 ms
2017-08-10 09:14:18,542 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 226
2017-08-10 09:14:18,557 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_99_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,558 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_114_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,558 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_100_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,559 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_101_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,559 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,559 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 115 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,559 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 115 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,559 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,559 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,559 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 115 (MapPartitionsRDD[231] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,560 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_102_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,560 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_103_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,561 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_104_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,562 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_105_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,562 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_106_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,562 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_115 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:18,562 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_107_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,563 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_108_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,564 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_109_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,564 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_110_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,565 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_111_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,565 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_115_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:18,565 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_112_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,565 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_115_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 115 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 115 (MapPartitionsRDD[231] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 115.0 with 2 tasks
2017-08-10 09:14:18,566 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_113_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,566 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 115.0 (TID 230, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,567 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 115.0 (TID 231, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,567 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 115.0 (TID 231)
2017-08-10 09:14:18,567 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 115.0 (TID 230)
2017-08-10 09:14:18,568 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,568 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,571 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 115.0 (TID 230). 714 bytes result sent to driver
2017-08-10 09:14:18,571 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 115.0 (TID 231). 714 bytes result sent to driver
2017-08-10 09:14:18,572 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 115.0 (TID 230) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,572 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 115.0 (TID 231) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,572 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 115.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,572 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 115 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,572 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 115 finished: foreachPartition at streamingProcess.scala:50, took 0.013420 s
2017-08-10 09:14:18,573 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327490000 ms.0 from job set of time 1502327490000 ms
2017-08-10 09:14:18,573 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 168.573 s for time 1502327490000 ms (execution: 0.033 s)
2017-08-10 09:14:18,573 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327495000 ms.0 from job set of time 1502327495000 ms
2017-08-10 09:14:18,573 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 229 from persistence list
2017-08-10 09:14:18,573 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 229
2017-08-10 09:14:18,573 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 228 from persistence list
2017-08-10 09:14:18,573 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 228
2017-08-10 09:14:18,573 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,573 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327480000 ms
2017-08-10 09:14:18,582 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,582 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 116 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,582 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 116 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,583 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,583 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,583 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 116 (MapPartitionsRDD[233] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,586 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_116 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:18,589 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_116_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:18,589 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_116_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,590 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 116 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,590 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 116 (MapPartitionsRDD[233] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,590 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 116.0 with 2 tasks
2017-08-10 09:14:18,590 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 116.0 (TID 232, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,591 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 116.0 (TID 233, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,591 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 116.0 (TID 232)
2017-08-10 09:14:18,591 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 116.0 (TID 233)
2017-08-10 09:14:18,592 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,592 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,595 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 116.0 (TID 233). 714 bytes result sent to driver
2017-08-10 09:14:18,595 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 116.0 (TID 232). 714 bytes result sent to driver
2017-08-10 09:14:18,596 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 116.0 (TID 233) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,597 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 116.0 (TID 232) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,597 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 116.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,597 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 116 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,597 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 116 finished: foreachPartition at streamingProcess.scala:50, took 0.014811 s
2017-08-10 09:14:18,597 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327495000 ms.0 from job set of time 1502327495000 ms
2017-08-10 09:14:18,597 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 163.597 s for time 1502327495000 ms (execution: 0.024 s)
2017-08-10 09:14:18,597 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327500000 ms.0 from job set of time 1502327500000 ms
2017-08-10 09:14:18,598 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 231 from persistence list
2017-08-10 09:14:18,598 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 231
2017-08-10 09:14:18,598 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 230 from persistence list
2017-08-10 09:14:18,598 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 230
2017-08-10 09:14:18,598 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,598 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327485000 ms
2017-08-10 09:14:18,608 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,608 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 117 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,608 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 117 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,608 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,608 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,608 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 117 (MapPartitionsRDD[235] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,611 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_117 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:18,614 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_117_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:18,614 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_117_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,614 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 117 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,614 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 117 (MapPartitionsRDD[235] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,614 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 117.0 with 2 tasks
2017-08-10 09:14:18,615 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 117.0 (TID 234, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,615 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 117.0 (TID 235, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,615 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 117.0 (TID 234)
2017-08-10 09:14:18,615 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 117.0 (TID 235)
2017-08-10 09:14:18,617 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,617 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,620 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 117.0 (TID 235). 714 bytes result sent to driver
2017-08-10 09:14:18,620 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 117.0 (TID 234). 714 bytes result sent to driver
2017-08-10 09:14:18,622 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 117.0 (TID 235) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,622 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 117.0 (TID 234) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,622 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 117.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,622 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 117 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:18,622 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 117 finished: foreachPartition at streamingProcess.scala:50, took 0.014409 s
2017-08-10 09:14:18,623 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327500000 ms.0 from job set of time 1502327500000 ms
2017-08-10 09:14:18,623 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 158.622 s for time 1502327500000 ms (execution: 0.025 s)
2017-08-10 09:14:18,623 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 233 from persistence list
2017-08-10 09:14:18,623 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327505000 ms.0 from job set of time 1502327505000 ms
2017-08-10 09:14:18,623 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 233
2017-08-10 09:14:18,623 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 232 from persistence list
2017-08-10 09:14:18,623 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,623 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327490000 ms
2017-08-10 09:14:18,623 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 232
2017-08-10 09:14:18,635 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,636 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 118 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,636 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 118 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,636 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,636 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,636 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 118 (MapPartitionsRDD[237] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,639 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_118 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:18,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_118_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:18,642 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_118_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 118 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,643 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 118 (MapPartitionsRDD[237] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,643 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 118.0 with 2 tasks
2017-08-10 09:14:18,643 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 118.0 (TID 236, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,644 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 118.0 (TID 237, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,644 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 118.0 (TID 236)
2017-08-10 09:14:18,644 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 118.0 (TID 237)
2017-08-10 09:14:18,645 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,645 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,648 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 118.0 (TID 236). 714 bytes result sent to driver
2017-08-10 09:14:18,648 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 118.0 (TID 237). 714 bytes result sent to driver
2017-08-10 09:14:18,649 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 118.0 (TID 236) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,650 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 118.0 (TID 237) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,650 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 118.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,650 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 118 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,650 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 118 finished: foreachPartition at streamingProcess.scala:50, took 0.014763 s
2017-08-10 09:14:18,650 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327505000 ms.0 from job set of time 1502327505000 ms
2017-08-10 09:14:18,650 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 153.650 s for time 1502327505000 ms (execution: 0.027 s)
2017-08-10 09:14:18,650 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327510000 ms.0 from job set of time 1502327510000 ms
2017-08-10 09:14:18,650 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 235 from persistence list
2017-08-10 09:14:18,651 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 234 from persistence list
2017-08-10 09:14:18,651 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,651 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327495000 ms
2017-08-10 09:14:18,651 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 234
2017-08-10 09:14:18,652 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 235
2017-08-10 09:14:18,661 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,662 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 119 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,662 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 119 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,662 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,662 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,662 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 119 (MapPartitionsRDD[239] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,664 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_119 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:18,667 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_119_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:18,667 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_119_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,667 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 119 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 119 (MapPartitionsRDD[239] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,668 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 119.0 with 2 tasks
2017-08-10 09:14:18,668 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 119.0 (TID 238, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,669 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 119.0 (TID 239, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,669 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 119.0 (TID 239)
2017-08-10 09:14:18,669 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 119.0 (TID 238)
2017-08-10 09:14:18,670 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,670 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,673 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 119.0 (TID 238). 714 bytes result sent to driver
2017-08-10 09:14:18,673 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 119.0 (TID 239). 714 bytes result sent to driver
2017-08-10 09:14:18,675 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 119.0 (TID 239) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,675 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 119.0 (TID 238) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,675 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 119.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,675 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 119 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,675 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 119 finished: foreachPartition at streamingProcess.scala:50, took 0.014284 s
2017-08-10 09:14:18,676 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327510000 ms.0 from job set of time 1502327510000 ms
2017-08-10 09:14:18,676 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 148.676 s for time 1502327510000 ms (execution: 0.026 s)
2017-08-10 09:14:18,676 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 237 from persistence list
2017-08-10 09:14:18,676 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327515000 ms.0 from job set of time 1502327515000 ms
2017-08-10 09:14:18,676 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 237
2017-08-10 09:14:18,676 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 236 from persistence list
2017-08-10 09:14:18,676 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,676 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327500000 ms
2017-08-10 09:14:18,677 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 236
2017-08-10 09:14:18,686 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 120 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 120 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,686 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 120 (MapPartitionsRDD[241] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,689 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_120 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:18,692 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_120_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:18,693 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_120_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,693 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 120 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,693 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 120 (MapPartitionsRDD[241] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,693 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 120.0 with 2 tasks
2017-08-10 09:14:18,694 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 120.0 (TID 240, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,694 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 120.0 (TID 241, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,694 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 120.0 (TID 240)
2017-08-10 09:14:18,694 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 120.0 (TID 241)
2017-08-10 09:14:18,696 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,696 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,698 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 120.0 (TID 240). 714 bytes result sent to driver
2017-08-10 09:14:18,698 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 120.0 (TID 241). 714 bytes result sent to driver
2017-08-10 09:14:18,699 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 120.0 (TID 240) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,699 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 120.0 (TID 241) in 5 ms on localhost (2/2)
2017-08-10 09:14:18,699 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 120.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,700 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 120 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,700 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 120 finished: foreachPartition at streamingProcess.scala:50, took 0.013941 s
2017-08-10 09:14:18,700 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327515000 ms.0 from job set of time 1502327515000 ms
2017-08-10 09:14:18,700 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 143.700 s for time 1502327515000 ms (execution: 0.024 s)
2017-08-10 09:14:18,700 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 239 from persistence list
2017-08-10 09:14:18,700 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327520000 ms.0 from job set of time 1502327520000 ms
2017-08-10 09:14:18,700 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 239
2017-08-10 09:14:18,700 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 238 from persistence list
2017-08-10 09:14:18,700 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 238
2017-08-10 09:14:18,701 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,701 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327505000 ms
2017-08-10 09:14:18,709 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 121 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 121 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,710 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 121 (MapPartitionsRDD[243] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,712 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_121 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:18,715 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_121_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:18,716 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_121_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,716 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 121 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,716 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 121 (MapPartitionsRDD[243] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,716 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 121.0 with 2 tasks
2017-08-10 09:14:18,717 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 121.0 (TID 242, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,717 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 121.0 (TID 243, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,717 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 121.0 (TID 242)
2017-08-10 09:14:18,717 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 121.0 (TID 243)
2017-08-10 09:14:18,719 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,719 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,722 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 121.0 (TID 243). 714 bytes result sent to driver
2017-08-10 09:14:18,722 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 121.0 (TID 242). 714 bytes result sent to driver
2017-08-10 09:14:18,723 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 121.0 (TID 243) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,723 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 121.0 (TID 242) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,723 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 121.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,723 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 121 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,723 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 121 finished: foreachPartition at streamingProcess.scala:50, took 0.013915 s
2017-08-10 09:14:18,724 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327520000 ms.0 from job set of time 1502327520000 ms
2017-08-10 09:14:18,724 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 138.724 s for time 1502327520000 ms (execution: 0.024 s)
2017-08-10 09:14:18,724 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 241 from persistence list
2017-08-10 09:14:18,724 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327525000 ms.0 from job set of time 1502327525000 ms
2017-08-10 09:14:18,724 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 241
2017-08-10 09:14:18,724 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 240 from persistence list
2017-08-10 09:14:18,724 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 240
2017-08-10 09:14:18,724 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,724 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327510000 ms
2017-08-10 09:14:18,733 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,733 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 122 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,734 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 122 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,734 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,734 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,734 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 122 (MapPartitionsRDD[245] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,736 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_122 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:18,738 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_122_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:18,738 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_122_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,739 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 122 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,739 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 122 (MapPartitionsRDD[245] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,739 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 122.0 with 2 tasks
2017-08-10 09:14:18,739 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 122.0 (TID 244, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,740 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 122.0 (TID 245, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,740 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 122.0 (TID 245)
2017-08-10 09:14:18,740 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 122.0 (TID 244)
2017-08-10 09:14:18,741 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,741 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,744 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 122.0 (TID 245). 714 bytes result sent to driver
2017-08-10 09:14:18,744 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 122.0 (TID 244). 714 bytes result sent to driver
2017-08-10 09:14:18,745 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 122.0 (TID 245) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,745 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 122.0 (TID 244) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,745 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 122.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,745 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 122 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,745 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 122 finished: foreachPartition at streamingProcess.scala:50, took 0.012006 s
2017-08-10 09:14:18,746 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327525000 ms.0 from job set of time 1502327525000 ms
2017-08-10 09:14:18,746 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 133.745 s for time 1502327525000 ms (execution: 0.021 s)
2017-08-10 09:14:18,746 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 243 from persistence list
2017-08-10 09:14:18,746 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327530000 ms.0 from job set of time 1502327530000 ms
2017-08-10 09:14:18,746 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 243
2017-08-10 09:14:18,746 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 242 from persistence list
2017-08-10 09:14:18,746 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 242
2017-08-10 09:14:18,746 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,746 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327515000 ms
2017-08-10 09:14:18,755 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,755 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 123 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,755 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 123 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,755 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,755 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,755 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 123 (MapPartitionsRDD[247] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,758 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_123 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:18,761 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_123_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:18,761 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_123_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,761 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 123 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,761 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 123 (MapPartitionsRDD[247] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,762 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 123.0 with 2 tasks
2017-08-10 09:14:18,762 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 123.0 (TID 246, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,762 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 123.0 (TID 247, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,762 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 123.0 (TID 246)
2017-08-10 09:14:18,762 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 123.0 (TID 247)
2017-08-10 09:14:18,764 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,764 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,766 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 123.0 (TID 246). 714 bytes result sent to driver
2017-08-10 09:14:18,766 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 123.0 (TID 247). 714 bytes result sent to driver
2017-08-10 09:14:18,768 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 123.0 (TID 246) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,768 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 123.0 (TID 247) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,768 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 123.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,768 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 123 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,768 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 123 finished: foreachPartition at streamingProcess.scala:50, took 0.013228 s
2017-08-10 09:14:18,768 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327530000 ms.0 from job set of time 1502327530000 ms
2017-08-10 09:14:18,768 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 128.768 s for time 1502327530000 ms (execution: 0.022 s)
2017-08-10 09:14:18,768 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 245 from persistence list
2017-08-10 09:14:18,768 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327535000 ms.0 from job set of time 1502327535000 ms
2017-08-10 09:14:18,769 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 245
2017-08-10 09:14:18,769 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 244 from persistence list
2017-08-10 09:14:18,769 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 244
2017-08-10 09:14:18,769 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,769 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327520000 ms
2017-08-10 09:14:18,778 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 124 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 124 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 124 (MapPartitionsRDD[249] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,781 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_124 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:18,784 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_124_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:18,784 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_124_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,784 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 124 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,785 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 124 (MapPartitionsRDD[249] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,785 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 124.0 with 2 tasks
2017-08-10 09:14:18,786 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 124.0 (TID 248, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,786 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 124.0 (TID 249, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,786 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 124.0 (TID 249)
2017-08-10 09:14:18,786 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 124.0 (TID 248)
2017-08-10 09:14:18,788 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,788 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,791 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 124.0 (TID 248). 714 bytes result sent to driver
2017-08-10 09:14:18,791 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 124.0 (TID 249). 714 bytes result sent to driver
2017-08-10 09:14:18,792 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 124.0 (TID 249) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,792 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 124.0 (TID 248) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,792 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 124.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,792 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 124 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,792 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 124 finished: foreachPartition at streamingProcess.scala:50, took 0.013748 s
2017-08-10 09:14:18,792 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327535000 ms.0 from job set of time 1502327535000 ms
2017-08-10 09:14:18,793 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 123.792 s for time 1502327535000 ms (execution: 0.024 s)
2017-08-10 09:14:18,793 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 247 from persistence list
2017-08-10 09:14:18,793 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327540000 ms.0 from job set of time 1502327540000 ms
2017-08-10 09:14:18,793 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 247
2017-08-10 09:14:18,793 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 246 from persistence list
2017-08-10 09:14:18,793 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 246
2017-08-10 09:14:18,793 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,793 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327525000 ms
2017-08-10 09:14:18,803 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 125 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 125 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,803 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,804 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 125 (MapPartitionsRDD[251] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,806 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_125 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:18,808 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_125_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:18,809 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_125_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,809 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 125 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,809 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 125 (MapPartitionsRDD[251] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,809 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 125.0 with 2 tasks
2017-08-10 09:14:18,810 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 125.0 (TID 250, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,810 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 125.0 (TID 251, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,810 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 125.0 (TID 250)
2017-08-10 09:14:18,810 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 125.0 (TID 251)
2017-08-10 09:14:18,812 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,812 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,814 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 125.0 (TID 251). 714 bytes result sent to driver
2017-08-10 09:14:18,814 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 125.0 (TID 250). 714 bytes result sent to driver
2017-08-10 09:14:18,815 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 125.0 (TID 251) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,816 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 125.0 (TID 250) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,816 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 125.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,816 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 125 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:18,816 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 125 finished: foreachPartition at streamingProcess.scala:50, took 0.012914 s
2017-08-10 09:14:18,816 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327540000 ms.0 from job set of time 1502327540000 ms
2017-08-10 09:14:18,816 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 118.816 s for time 1502327540000 ms (execution: 0.023 s)
2017-08-10 09:14:18,816 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 249 from persistence list
2017-08-10 09:14:18,816 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327545000 ms.0 from job set of time 1502327545000 ms
2017-08-10 09:14:18,817 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 249
2017-08-10 09:14:18,817 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 248 from persistence list
2017-08-10 09:14:18,817 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,817 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 248
2017-08-10 09:14:18,817 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327530000 ms
2017-08-10 09:14:18,826 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,826 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 126 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,826 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 126 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,826 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,826 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,827 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 126 (MapPartitionsRDD[253] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,829 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_126 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:18,831 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_126_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:18,831 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_126_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,832 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 126 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,832 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 126 (MapPartitionsRDD[253] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,832 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 126.0 with 2 tasks
2017-08-10 09:14:18,832 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 126.0 (TID 252, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,833 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 126.0 (TID 253, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,833 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 126.0 (TID 252)
2017-08-10 09:14:18,833 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 126.0 (TID 253)
2017-08-10 09:14:18,835 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,835 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,838 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 126.0 (TID 253). 714 bytes result sent to driver
2017-08-10 09:14:18,838 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 126.0 (TID 252). 714 bytes result sent to driver
2017-08-10 09:14:18,839 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 126.0 (TID 253) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,839 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 126.0 (TID 252) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,839 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 126.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,839 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 126 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,840 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 126 finished: foreachPartition at streamingProcess.scala:50, took 0.014066 s
2017-08-10 09:14:18,841 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327545000 ms.0 from job set of time 1502327545000 ms
2017-08-10 09:14:18,841 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 113.841 s for time 1502327545000 ms (execution: 0.025 s)
2017-08-10 09:14:18,841 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327550000 ms.0 from job set of time 1502327550000 ms
2017-08-10 09:14:18,842 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 251 from persistence list
2017-08-10 09:14:18,842 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 251
2017-08-10 09:14:18,842 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 250 from persistence list
2017-08-10 09:14:18,843 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 250
2017-08-10 09:14:18,843 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,843 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327535000 ms
2017-08-10 09:14:18,852 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,853 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 127 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,853 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 127 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,853 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,853 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,853 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 127 (MapPartitionsRDD[255] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,855 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_127 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:18,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_127_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:18,858 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_127_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 127 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 127 (MapPartitionsRDD[255] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 127.0 with 2 tasks
2017-08-10 09:14:18,859 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 127.0 (TID 254, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,859 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 127.0 (TID 255, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,859 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 127.0 (TID 254)
2017-08-10 09:14:18,859 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 127.0 (TID 255)
2017-08-10 09:14:18,861 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,861 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,863 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 127.0 (TID 254). 714 bytes result sent to driver
2017-08-10 09:14:18,863 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 127.0 (TID 255). 714 bytes result sent to driver
2017-08-10 09:14:18,864 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 127.0 (TID 254) in 5 ms on localhost (1/2)
2017-08-10 09:14:18,865 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 127.0 (TID 255) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,865 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 127.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,865 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 127 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,865 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 127 finished: foreachPartition at streamingProcess.scala:50, took 0.012651 s
2017-08-10 09:14:18,865 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327550000 ms.0 from job set of time 1502327550000 ms
2017-08-10 09:14:18,865 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 108.865 s for time 1502327550000 ms (execution: 0.024 s)
2017-08-10 09:14:18,865 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 253 from persistence list
2017-08-10 09:14:18,865 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327555000 ms.0 from job set of time 1502327555000 ms
2017-08-10 09:14:18,866 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 253
2017-08-10 09:14:18,866 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 252 from persistence list
2017-08-10 09:14:18,866 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 252
2017-08-10 09:14:18,866 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,866 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327540000 ms
2017-08-10 09:14:18,875 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 128 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 128 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 128 (MapPartitionsRDD[257] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,878 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_128 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:18,880 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_128_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:18,881 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_128_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 128 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 128 (MapPartitionsRDD[257] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 128.0 with 2 tasks
2017-08-10 09:14:18,882 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 128.0 (TID 256, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,882 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 128.0 (TID 257, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,882 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 128.0 (TID 256)
2017-08-10 09:14:18,882 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 128.0 (TID 257)
2017-08-10 09:14:18,884 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,884 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,887 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 128.0 (TID 256). 714 bytes result sent to driver
2017-08-10 09:14:18,887 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 128.0 (TID 257). 714 bytes result sent to driver
2017-08-10 09:14:18,888 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 128.0 (TID 256) in 7 ms on localhost (1/2)
2017-08-10 09:14:18,888 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 128.0 (TID 257) in 6 ms on localhost (2/2)
2017-08-10 09:14:18,888 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 128.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,888 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 128 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,889 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 128 finished: foreachPartition at streamingProcess.scala:50, took 0.013632 s
2017-08-10 09:14:18,889 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327555000 ms.0 from job set of time 1502327555000 ms
2017-08-10 09:14:18,889 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 103.889 s for time 1502327555000 ms (execution: 0.024 s)
2017-08-10 09:14:18,889 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 255 from persistence list
2017-08-10 09:14:18,889 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327560000 ms.0 from job set of time 1502327560000 ms
2017-08-10 09:14:18,889 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 255
2017-08-10 09:14:18,889 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 254 from persistence list
2017-08-10 09:14:18,889 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 254
2017-08-10 09:14:18,889 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,890 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327545000 ms
2017-08-10 09:14:18,898 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,898 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 129 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,898 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 129 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,898 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,898 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,898 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 129 (MapPartitionsRDD[259] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,901 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_129 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:18,905 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_129_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:18,905 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_129_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,906 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 129 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,906 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 129 (MapPartitionsRDD[259] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,906 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 129.0 with 2 tasks
2017-08-10 09:14:18,907 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 129.0 (TID 258, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,908 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 129.0 (TID 259, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,908 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 129.0 (TID 258)
2017-08-10 09:14:18,908 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 129.0 (TID 259)
2017-08-10 09:14:18,910 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,910 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,912 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 129.0 (TID 259). 714 bytes result sent to driver
2017-08-10 09:14:18,912 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 129.0 (TID 258). 714 bytes result sent to driver
2017-08-10 09:14:18,919 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 129.0 (TID 259) in 11 ms on localhost (1/2)
2017-08-10 09:14:18,919 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 129.0 (TID 258) in 12 ms on localhost (2/2)
2017-08-10 09:14:18,920 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 129.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,920 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 129 (foreachPartition at streamingProcess.scala:50) finished in 0.014 s
2017-08-10 09:14:18,920 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 129 finished: foreachPartition at streamingProcess.scala:50, took 0.022277 s
2017-08-10 09:14:18,921 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327560000 ms.0 from job set of time 1502327560000 ms
2017-08-10 09:14:18,921 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 98.921 s for time 1502327560000 ms (execution: 0.032 s)
2017-08-10 09:14:18,921 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327565000 ms.0 from job set of time 1502327565000 ms
2017-08-10 09:14:18,921 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 257 from persistence list
2017-08-10 09:14:18,922 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 257
2017-08-10 09:14:18,922 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 256 from persistence list
2017-08-10 09:14:18,922 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 256
2017-08-10 09:14:18,922 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,922 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327550000 ms
2017-08-10 09:14:18,933 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,933 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 130 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,934 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 130 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,934 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,934 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,934 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 130 (MapPartitionsRDD[261] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,937 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_130 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:18,940 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_130_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:18,940 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_130_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,941 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 130 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,941 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 130 (MapPartitionsRDD[261] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,941 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 130.0 with 2 tasks
2017-08-10 09:14:18,942 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 130.0 (TID 260, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,942 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 130.0 (TID 261, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,942 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 130.0 (TID 260)
2017-08-10 09:14:18,942 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 130.0 (TID 261)
2017-08-10 09:14:18,944 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,944 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,947 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 130.0 (TID 261). 714 bytes result sent to driver
2017-08-10 09:14:18,947 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 130.0 (TID 260). 714 bytes result sent to driver
2017-08-10 09:14:18,948 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 130.0 (TID 261) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,948 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 130.0 (TID 260) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,948 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 130.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,948 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 130 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,949 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 130 finished: foreachPartition at streamingProcess.scala:50, took 0.015467 s
2017-08-10 09:14:18,949 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327565000 ms.0 from job set of time 1502327565000 ms
2017-08-10 09:14:18,949 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 93.949 s for time 1502327565000 ms (execution: 0.028 s)
2017-08-10 09:14:18,958 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 259 from persistence list
2017-08-10 09:14:18,958 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327570000 ms.0 from job set of time 1502327570000 ms
2017-08-10 09:14:18,958 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 259
2017-08-10 09:14:18,958 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 258 from persistence list
2017-08-10 09:14:18,959 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 258
2017-08-10 09:14:18,959 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,959 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327555000 ms
2017-08-10 09:14:18,959 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_130_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:18,959 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_115_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,962 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_116_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,964 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_117_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,965 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_118_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:18,966 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_119_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,967 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_120_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,968 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_121_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,969 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_122_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:18,970 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,970 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_123_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 131 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 131 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,970 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 131 (MapPartitionsRDD[263] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,970 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_124_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,971 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_125_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,972 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_126_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:18,972 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_127_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,972 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_131 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:18,975 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_128_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,976 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_131_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:18,976 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_129_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,976 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_131_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:18,977 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 131 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:18,977 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 131 (MapPartitionsRDD[263] at map at streamingProcess.scala:48)
2017-08-10 09:14:18,977 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 131.0 with 2 tasks
2017-08-10 09:14:18,978 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 131.0 (TID 262, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:18,978 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 131.0 (TID 263, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:18,978 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 131.0 (TID 262)
2017-08-10 09:14:18,978 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 131.0 (TID 263)
2017-08-10 09:14:18,980 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:18,980 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:18,983 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 131.0 (TID 262). 714 bytes result sent to driver
2017-08-10 09:14:18,983 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 131.0 (TID 263). 714 bytes result sent to driver
2017-08-10 09:14:18,984 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 131.0 (TID 263) in 6 ms on localhost (1/2)
2017-08-10 09:14:18,984 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 131.0 (TID 262) in 7 ms on localhost (2/2)
2017-08-10 09:14:18,984 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 131.0, whose tasks have all completed, from pool 
2017-08-10 09:14:18,984 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 131 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:18,985 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 131 finished: foreachPartition at streamingProcess.scala:50, took 0.014939 s
2017-08-10 09:14:18,985 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327570000 ms.0 from job set of time 1502327570000 ms
2017-08-10 09:14:18,985 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 88.985 s for time 1502327570000 ms (execution: 0.027 s)
2017-08-10 09:14:18,985 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 261 from persistence list
2017-08-10 09:14:18,985 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327575000 ms.0 from job set of time 1502327575000 ms
2017-08-10 09:14:18,986 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 261
2017-08-10 09:14:18,986 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 260 from persistence list
2017-08-10 09:14:18,986 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:18,986 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327560000 ms
2017-08-10 09:14:18,986 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 260
2017-08-10 09:14:18,995 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:18,995 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 132 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:18,995 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 132 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:18,995 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:18,995 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:18,996 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 132 (MapPartitionsRDD[265] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:18,998 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_132 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:19,000 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_132_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:19,000 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_132_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:19,000 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 132 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,000 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 132 (MapPartitionsRDD[265] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,000 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 132.0 with 2 tasks
2017-08-10 09:14:19,001 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 132.0 (TID 264, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,001 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 132.0 (TID 265, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,002 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 132.0 (TID 264)
2017-08-10 09:14:19,002 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 132.0 (TID 265)
2017-08-10 09:14:19,003 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,003 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,006 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 132.0 (TID 265). 714 bytes result sent to driver
2017-08-10 09:14:19,006 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 132.0 (TID 264). 714 bytes result sent to driver
2017-08-10 09:14:19,007 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 132.0 (TID 265) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,007 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 132.0 (TID 264) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,007 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 132.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,007 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 132 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:19,007 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 132 finished: foreachPartition at streamingProcess.scala:50, took 0.012014 s
2017-08-10 09:14:19,007 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327575000 ms.0 from job set of time 1502327575000 ms
2017-08-10 09:14:19,007 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 84.007 s for time 1502327575000 ms (execution: 0.022 s)
2017-08-10 09:14:19,007 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 263 from persistence list
2017-08-10 09:14:19,008 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327580000 ms.0 from job set of time 1502327580000 ms
2017-08-10 09:14:19,008 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 263
2017-08-10 09:14:19,008 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 262 from persistence list
2017-08-10 09:14:19,008 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 262
2017-08-10 09:14:19,008 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,008 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327565000 ms
2017-08-10 09:14:19,017 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 133 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 133 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 133 (MapPartitionsRDD[267] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_133 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:19,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_133_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:19,022 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_133_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 133 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 133 (MapPartitionsRDD[267] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 133.0 with 2 tasks
2017-08-10 09:14:19,023 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 133.0 (TID 266, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,024 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 133.0 (TID 267, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,024 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 133.0 (TID 267)
2017-08-10 09:14:19,024 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 133.0 (TID 266)
2017-08-10 09:14:19,025 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,025 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,028 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 133.0 (TID 266). 714 bytes result sent to driver
2017-08-10 09:14:19,028 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 133.0 (TID 267). 714 bytes result sent to driver
2017-08-10 09:14:19,029 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 133.0 (TID 266) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,029 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 133.0 (TID 267) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,029 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 133.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 133 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:19,029 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 133 finished: foreachPartition at streamingProcess.scala:50, took 0.012429 s
2017-08-10 09:14:19,029 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327580000 ms.0 from job set of time 1502327580000 ms
2017-08-10 09:14:19,030 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 79.029 s for time 1502327580000 ms (execution: 0.021 s)
2017-08-10 09:14:19,030 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 265 from persistence list
2017-08-10 09:14:19,030 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327585000 ms.0 from job set of time 1502327585000 ms
2017-08-10 09:14:19,030 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 265
2017-08-10 09:14:19,030 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 264 from persistence list
2017-08-10 09:14:19,030 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 264
2017-08-10 09:14:19,030 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,030 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327570000 ms
2017-08-10 09:14:19,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 134 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 134 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 134 (MapPartitionsRDD[269] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_134 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:19,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_134_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:19,045 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_134_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 134 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 134 (MapPartitionsRDD[269] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 134.0 with 2 tasks
2017-08-10 09:14:19,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 134.0 (TID 268, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 134.0 (TID 269, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,046 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 134.0 (TID 268)
2017-08-10 09:14:19,046 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 134.0 (TID 269)
2017-08-10 09:14:19,047 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,047 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,050 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 134.0 (TID 269). 714 bytes result sent to driver
2017-08-10 09:14:19,050 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 134.0 (TID 268). 714 bytes result sent to driver
2017-08-10 09:14:19,051 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 134.0 (TID 268) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 134.0 (TID 269) in 5 ms on localhost (2/2)
2017-08-10 09:14:19,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 134.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 134 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 134 finished: foreachPartition at streamingProcess.scala:50, took 0.012598 s
2017-08-10 09:14:19,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327585000 ms.0 from job set of time 1502327585000 ms
2017-08-10 09:14:19,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 74.052 s for time 1502327585000 ms (execution: 0.022 s)
2017-08-10 09:14:19,052 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 267 from persistence list
2017-08-10 09:14:19,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327590000 ms.0 from job set of time 1502327590000 ms
2017-08-10 09:14:19,052 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 267
2017-08-10 09:14:19,053 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 266 from persistence list
2017-08-10 09:14:19,053 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 266
2017-08-10 09:14:19,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327575000 ms
2017-08-10 09:14:19,062 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 135 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 135 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 135 (MapPartitionsRDD[271] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_135 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:19,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_135_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:19,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_135_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 135 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 135 (MapPartitionsRDD[271] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 135.0 with 2 tasks
2017-08-10 09:14:19,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 135.0 (TID 270, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 135.0 (TID 271, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,068 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 135.0 (TID 270)
2017-08-10 09:14:19,068 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 135.0 (TID 271)
2017-08-10 09:14:19,070 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,070 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 135.0 (TID 270). 714 bytes result sent to driver
2017-08-10 09:14:19,072 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 135.0 (TID 271). 714 bytes result sent to driver
2017-08-10 09:14:19,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 135.0 (TID 271) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 135.0 (TID 270) in 7 ms on localhost (2/2)
2017-08-10 09:14:19,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 135.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 135 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 135 finished: foreachPartition at streamingProcess.scala:50, took 0.012383 s
2017-08-10 09:14:19,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327590000 ms.0 from job set of time 1502327590000 ms
2017-08-10 09:14:19,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 69.074 s for time 1502327590000 ms (execution: 0.022 s)
2017-08-10 09:14:19,074 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 269 from persistence list
2017-08-10 09:14:19,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327595000 ms.0 from job set of time 1502327595000 ms
2017-08-10 09:14:19,075 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 269
2017-08-10 09:14:19,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 268 from persistence list
2017-08-10 09:14:19,075 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 268
2017-08-10 09:14:19,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327580000 ms
2017-08-10 09:14:19,085 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 136 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 136 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 136 (MapPartitionsRDD[273] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_136 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:19,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_136_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:19,091 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_136_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 136 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 136 (MapPartitionsRDD[273] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 136.0 with 2 tasks
2017-08-10 09:14:19,092 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 136.0 (TID 272, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,093 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 136.0 (TID 273, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,093 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 136.0 (TID 272)
2017-08-10 09:14:19,093 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 136.0 (TID 273)
2017-08-10 09:14:19,094 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,094 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,097 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 136.0 (TID 272). 714 bytes result sent to driver
2017-08-10 09:14:19,097 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 136.0 (TID 273). 714 bytes result sent to driver
2017-08-10 09:14:19,099 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 136.0 (TID 272) in 7 ms on localhost (1/2)
2017-08-10 09:14:19,099 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 136.0 (TID 273) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,099 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 136.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,099 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 136 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,099 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 136 finished: foreachPartition at streamingProcess.scala:50, took 0.013744 s
2017-08-10 09:14:19,099 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327595000 ms.0 from job set of time 1502327595000 ms
2017-08-10 09:14:19,100 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 64.099 s for time 1502327595000 ms (execution: 0.025 s)
2017-08-10 09:14:19,100 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 271 from persistence list
2017-08-10 09:14:19,100 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327600000 ms.0 from job set of time 1502327600000 ms
2017-08-10 09:14:19,100 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 271
2017-08-10 09:14:19,100 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 270 from persistence list
2017-08-10 09:14:19,100 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 270
2017-08-10 09:14:19,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327585000 ms
2017-08-10 09:14:19,109 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 137 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 137 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 137 (MapPartitionsRDD[275] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,112 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_137 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:19,114 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_137_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:19,115 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_137_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 137 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 137 (MapPartitionsRDD[275] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 137.0 with 2 tasks
2017-08-10 09:14:19,115 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 137.0 (TID 274, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,116 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 137.0 (TID 275, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,116 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 137.0 (TID 275)
2017-08-10 09:14:19,116 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 137.0 (TID 274)
2017-08-10 09:14:19,117 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,117 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,120 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 137.0 (TID 274). 714 bytes result sent to driver
2017-08-10 09:14:19,120 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 137.0 (TID 275). 714 bytes result sent to driver
2017-08-10 09:14:19,122 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 137.0 (TID 274) in 7 ms on localhost (1/2)
2017-08-10 09:14:19,122 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 137.0 (TID 275) in 7 ms on localhost (2/2)
2017-08-10 09:14:19,122 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 137.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,122 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 137 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,122 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 137 finished: foreachPartition at streamingProcess.scala:50, took 0.013005 s
2017-08-10 09:14:19,123 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327600000 ms.0 from job set of time 1502327600000 ms
2017-08-10 09:14:19,123 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 59.123 s for time 1502327600000 ms (execution: 0.023 s)
2017-08-10 09:14:19,123 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 273 from persistence list
2017-08-10 09:14:19,123 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327605000 ms.0 from job set of time 1502327605000 ms
2017-08-10 09:14:19,123 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 273
2017-08-10 09:14:19,123 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 272 from persistence list
2017-08-10 09:14:19,124 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,124 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327590000 ms
2017-08-10 09:14:19,124 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 272
2017-08-10 09:14:19,132 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,133 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 138 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,133 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 138 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,133 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,133 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,133 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 138 (MapPartitionsRDD[277] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,135 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_138 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:19,138 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_138_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:19,139 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_138_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,139 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 138 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,139 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 138 (MapPartitionsRDD[277] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,139 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 138.0 with 2 tasks
2017-08-10 09:14:19,139 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 138.0 (TID 276, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,140 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 138.0 (TID 277, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,140 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 138.0 (TID 276)
2017-08-10 09:14:19,140 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 138.0 (TID 277)
2017-08-10 09:14:19,141 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,141 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,144 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 138.0 (TID 276). 714 bytes result sent to driver
2017-08-10 09:14:19,144 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 138.0 (TID 277). 714 bytes result sent to driver
2017-08-10 09:14:19,145 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 138.0 (TID 277) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,146 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 138.0 (TID 276) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,146 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 138.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,146 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 138 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,146 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 138 finished: foreachPartition at streamingProcess.scala:50, took 0.013479 s
2017-08-10 09:14:19,146 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327605000 ms.0 from job set of time 1502327605000 ms
2017-08-10 09:14:19,146 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 54.146 s for time 1502327605000 ms (execution: 0.023 s)
2017-08-10 09:14:19,146 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 275 from persistence list
2017-08-10 09:14:19,146 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327610000 ms.0 from job set of time 1502327610000 ms
2017-08-10 09:14:19,147 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 275
2017-08-10 09:14:19,147 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 274 from persistence list
2017-08-10 09:14:19,147 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 274
2017-08-10 09:14:19,147 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,147 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327595000 ms
2017-08-10 09:14:19,156 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,156 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 139 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,156 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 139 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,156 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,156 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,156 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 139 (MapPartitionsRDD[279] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_139 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:19,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_139_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:19,162 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_139_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 139 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 139 (MapPartitionsRDD[279] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 139.0 with 2 tasks
2017-08-10 09:14:19,163 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 139.0 (TID 278, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,163 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 139.0 (TID 279, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,164 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 139.0 (TID 278)
2017-08-10 09:14:19,164 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 139.0 (TID 279)
2017-08-10 09:14:19,165 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,165 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,169 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 139.0 (TID 279). 714 bytes result sent to driver
2017-08-10 09:14:19,169 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 139.0 (TID 278). 714 bytes result sent to driver
2017-08-10 09:14:19,170 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 139.0 (TID 279) in 7 ms on localhost (1/2)
2017-08-10 09:14:19,170 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 139.0 (TID 278) in 7 ms on localhost (2/2)
2017-08-10 09:14:19,170 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 139.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 139 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,170 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 139 finished: foreachPartition at streamingProcess.scala:50, took 0.014583 s
2017-08-10 09:14:19,170 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327610000 ms.0 from job set of time 1502327610000 ms
2017-08-10 09:14:19,171 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 49.170 s for time 1502327610000 ms (execution: 0.024 s)
2017-08-10 09:14:19,171 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327615000 ms.0 from job set of time 1502327615000 ms
2017-08-10 09:14:19,171 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 277 from persistence list
2017-08-10 09:14:19,171 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 277
2017-08-10 09:14:19,171 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 276 from persistence list
2017-08-10 09:14:19,171 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 276
2017-08-10 09:14:19,171 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,171 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327600000 ms
2017-08-10 09:14:19,180 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 140 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 140 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 140 (MapPartitionsRDD[281] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,183 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_140 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:19,186 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_140_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:19,187 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_140_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 140 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 140 (MapPartitionsRDD[281] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 140.0 with 2 tasks
2017-08-10 09:14:19,187 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 140.0 (TID 280, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,188 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 140.0 (TID 281, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,188 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 140.0 (TID 281)
2017-08-10 09:14:19,188 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 140.0 (TID 280)
2017-08-10 09:14:19,189 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,189 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,192 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 140.0 (TID 281). 714 bytes result sent to driver
2017-08-10 09:14:19,192 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 140.0 (TID 280). 714 bytes result sent to driver
2017-08-10 09:14:19,193 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 140.0 (TID 281) in 5 ms on localhost (1/2)
2017-08-10 09:14:19,193 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 140.0 (TID 280) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,194 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 140.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,194 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 140 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,194 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 140 finished: foreachPartition at streamingProcess.scala:50, took 0.013599 s
2017-08-10 09:14:19,194 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327615000 ms.0 from job set of time 1502327615000 ms
2017-08-10 09:14:19,194 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 44.194 s for time 1502327615000 ms (execution: 0.023 s)
2017-08-10 09:14:19,194 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 279 from persistence list
2017-08-10 09:14:19,195 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327620000 ms.0 from job set of time 1502327620000 ms
2017-08-10 09:14:19,195 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 278 from persistence list
2017-08-10 09:14:19,195 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 279
2017-08-10 09:14:19,195 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,195 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 278
2017-08-10 09:14:19,195 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327605000 ms
2017-08-10 09:14:19,207 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,208 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 141 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,208 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 141 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,208 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,208 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,208 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 141 (MapPartitionsRDD[283] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,211 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_141 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:19,214 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_141_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:19,215 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_141_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,215 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 141 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,215 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 141 (MapPartitionsRDD[283] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,215 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 141.0 with 2 tasks
2017-08-10 09:14:19,216 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 141.0 (TID 282, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,216 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 141.0 (TID 283, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,216 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 141.0 (TID 282)
2017-08-10 09:14:19,217 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 141.0 (TID 283)
2017-08-10 09:14:19,218 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,219 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,222 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 141.0 (TID 282). 714 bytes result sent to driver
2017-08-10 09:14:19,222 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 141.0 (TID 283). 714 bytes result sent to driver
2017-08-10 09:14:19,223 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 141.0 (TID 283) in 7 ms on localhost (1/2)
2017-08-10 09:14:19,223 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 141.0 (TID 282) in 7 ms on localhost (2/2)
2017-08-10 09:14:19,223 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 141.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,223 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 141 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:19,223 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 141 finished: foreachPartition at streamingProcess.scala:50, took 0.016274 s
2017-08-10 09:14:19,224 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327620000 ms.0 from job set of time 1502327620000 ms
2017-08-10 09:14:19,224 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 39.224 s for time 1502327620000 ms (execution: 0.029 s)
2017-08-10 09:14:19,224 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327625000 ms.0 from job set of time 1502327625000 ms
2017-08-10 09:14:19,224 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 281 from persistence list
2017-08-10 09:14:19,225 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 281
2017-08-10 09:14:19,225 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 280 from persistence list
2017-08-10 09:14:19,225 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,225 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327610000 ms
2017-08-10 09:14:19,225 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 280
2017-08-10 09:14:19,235 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,235 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 142 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,236 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 142 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,236 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,236 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,236 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 142 (MapPartitionsRDD[285] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,239 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_142 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:14:19,241 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_142_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:19,241 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_142_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 142 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 142 (MapPartitionsRDD[285] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 142.0 with 2 tasks
2017-08-10 09:14:19,242 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 142.0 (TID 284, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,243 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 142.0 (TID 285, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,243 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 142.0 (TID 284)
2017-08-10 09:14:19,243 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 142.0 (TID 285)
2017-08-10 09:14:19,244 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,244 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,247 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 142.0 (TID 285). 714 bytes result sent to driver
2017-08-10 09:14:19,247 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 142.0 (TID 284). 714 bytes result sent to driver
2017-08-10 09:14:19,249 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 142.0 (TID 285) in 7 ms on localhost (1/2)
2017-08-10 09:14:19,249 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 142.0 (TID 284) in 7 ms on localhost (2/2)
2017-08-10 09:14:19,249 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 142.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,249 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 142 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,249 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 142 finished: foreachPartition at streamingProcess.scala:50, took 0.014097 s
2017-08-10 09:14:19,249 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327625000 ms.0 from job set of time 1502327625000 ms
2017-08-10 09:14:19,250 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 34.249 s for time 1502327625000 ms (execution: 0.025 s)
2017-08-10 09:14:19,250 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 283 from persistence list
2017-08-10 09:14:19,250 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327630000 ms.0 from job set of time 1502327630000 ms
2017-08-10 09:14:19,250 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 283
2017-08-10 09:14:19,250 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 282 from persistence list
2017-08-10 09:14:19,250 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 282
2017-08-10 09:14:19,250 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,250 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327615000 ms
2017-08-10 09:14:19,260 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 143 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 143 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 143 (MapPartitionsRDD[287] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,262 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_143 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:19,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_143_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:19,265 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_143_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 143 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 143 (MapPartitionsRDD[287] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 143.0 with 2 tasks
2017-08-10 09:14:19,266 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 143.0 (TID 286, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,266 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 143.0 (TID 287, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,266 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 143.0 (TID 286)
2017-08-10 09:14:19,266 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 143.0 (TID 287)
2017-08-10 09:14:19,268 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,268 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,271 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 143.0 (TID 286). 714 bytes result sent to driver
2017-08-10 09:14:19,271 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 143.0 (TID 287). 714 bytes result sent to driver
2017-08-10 09:14:19,272 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 143.0 (TID 286) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,272 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 143.0 (TID 287) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,272 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 143.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,272 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 143 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:19,272 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 143 finished: foreachPartition at streamingProcess.scala:50, took 0.012471 s
2017-08-10 09:14:19,273 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327630000 ms.0 from job set of time 1502327630000 ms
2017-08-10 09:14:19,273 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 29.273 s for time 1502327630000 ms (execution: 0.023 s)
2017-08-10 09:14:19,273 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327635000 ms.0 from job set of time 1502327635000 ms
2017-08-10 09:14:19,273 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 285 from persistence list
2017-08-10 09:14:19,273 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 285
2017-08-10 09:14:19,273 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 284 from persistence list
2017-08-10 09:14:19,273 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 284
2017-08-10 09:14:19,273 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,273 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327620000 ms
2017-08-10 09:14:19,282 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 144 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 144 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,283 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 144 (MapPartitionsRDD[289] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_144 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:14:19,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_144_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:14:19,289 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_144_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,289 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 144 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,289 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 144 (MapPartitionsRDD[289] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,289 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 144.0 with 2 tasks
2017-08-10 09:14:19,290 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 144.0 (TID 288, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,290 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 144.0 (TID 289, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,290 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 144.0 (TID 289)
2017-08-10 09:14:19,290 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 144.0 (TID 288)
2017-08-10 09:14:19,292 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,292 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,294 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 144.0 (TID 289). 714 bytes result sent to driver
2017-08-10 09:14:19,294 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 144.0 (TID 288). 714 bytes result sent to driver
2017-08-10 09:14:19,295 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 144.0 (TID 288) in 5 ms on localhost (1/2)
2017-08-10 09:14:19,295 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 144.0 (TID 289) in 5 ms on localhost (2/2)
2017-08-10 09:14:19,295 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 144.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,295 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 144 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:19,296 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 144 finished: foreachPartition at streamingProcess.scala:50, took 0.013192 s
2017-08-10 09:14:19,296 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327635000 ms.0 from job set of time 1502327635000 ms
2017-08-10 09:14:19,296 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 24.296 s for time 1502327635000 ms (execution: 0.023 s)
2017-08-10 09:14:19,296 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 287 from persistence list
2017-08-10 09:14:19,296 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327640000 ms.0 from job set of time 1502327640000 ms
2017-08-10 09:14:19,296 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 287
2017-08-10 09:14:19,296 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 286 from persistence list
2017-08-10 09:14:19,296 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 286
2017-08-10 09:14:19,297 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,297 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327625000 ms
2017-08-10 09:14:19,306 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 145 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 145 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,307 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,307 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 145 (MapPartitionsRDD[291] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,309 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_145 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:19,311 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_145_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:14:19,311 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_145_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:19,311 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 145 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,312 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 145 (MapPartitionsRDD[291] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,312 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 145.0 with 2 tasks
2017-08-10 09:14:19,312 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 145.0 (TID 290, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,312 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 145.0 (TID 291, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,312 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 145.0 (TID 290)
2017-08-10 09:14:19,312 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 145.0 (TID 291)
2017-08-10 09:14:19,314 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,314 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,316 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 145.0 (TID 290). 714 bytes result sent to driver
2017-08-10 09:14:19,316 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 145.0 (TID 291). 714 bytes result sent to driver
2017-08-10 09:14:19,318 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 145.0 (TID 290) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,318 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 145.0 (TID 291) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,318 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 145.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,318 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 145 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:19,318 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 145 finished: foreachPartition at streamingProcess.scala:50, took 0.011996 s
2017-08-10 09:14:19,318 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327640000 ms.0 from job set of time 1502327640000 ms
2017-08-10 09:14:19,319 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 19.318 s for time 1502327640000 ms (execution: 0.022 s)
2017-08-10 09:14:19,319 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 289 from persistence list
2017-08-10 09:14:19,319 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327645000 ms.0 from job set of time 1502327645000 ms
2017-08-10 09:14:19,319 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 288 from persistence list
2017-08-10 09:14:19,319 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,319 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327630000 ms
2017-08-10 09:14:19,319 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 289
2017-08-10 09:14:19,319 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 288
2017-08-10 09:14:19,328 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,329 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 146 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,329 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 146 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,329 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,329 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,329 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 146 (MapPartitionsRDD[293] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,332 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_146 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:14:19,334 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_146_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:14:19,334 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_146_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:19,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 146 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 146 (MapPartitionsRDD[293] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 146.0 with 2 tasks
2017-08-10 09:14:19,335 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 146.0 (TID 292, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,336 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 146.0 (TID 293, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,336 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 146.0 (TID 292)
2017-08-10 09:14:19,336 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 146.0 (TID 293)
2017-08-10 09:14:19,346 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,346 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,346 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_131_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:14:19,347 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_132_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,348 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_133_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,349 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_134_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,349 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 146.0 (TID 292). 787 bytes result sent to driver
2017-08-10 09:14:19,349 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 146.0 (TID 293). 787 bytes result sent to driver
2017-08-10 09:14:19,349 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_135_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:19,350 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_136_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,350 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 146.0 (TID 292) in 15 ms on localhost (1/2)
2017-08-10 09:14:19,350 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 146.0 (TID 293) in 15 ms on localhost (2/2)
2017-08-10 09:14:19,350 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 146.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,350 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 146 (foreachPartition at streamingProcess.scala:50) finished in 0.015 s
2017-08-10 09:14:19,351 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 146 finished: foreachPartition at streamingProcess.scala:50, took 0.022032 s
2017-08-10 09:14:19,351 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_137_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,351 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327645000 ms.0 from job set of time 1502327645000 ms
2017-08-10 09:14:19,351 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 14.351 s for time 1502327645000 ms (execution: 0.032 s)
2017-08-10 09:14:19,351 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327650000 ms.0 from job set of time 1502327650000 ms
2017-08-10 09:14:19,352 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 291 from persistence list
2017-08-10 09:14:19,352 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_138_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,352 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 290 from persistence list
2017-08-10 09:14:19,352 [block-manager-slave-async-thread-pool-12] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 290
2017-08-10 09:14:19,352 [block-manager-slave-async-thread-pool-11] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 291
2017-08-10 09:14:19,352 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_139_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:19,353 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,353 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327635000 ms
2017-08-10 09:14:19,353 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_140_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,354 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_141_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,355 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_142_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,355 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_143_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,356 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_144_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:19,356 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_145_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:19,362 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 147 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 147 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 147 (MapPartitionsRDD[295] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_147 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:14:19,368 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_147_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:14:19,368 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_147_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:14:19,368 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 147 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,368 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 147 (MapPartitionsRDD[295] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,368 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 147.0 with 2 tasks
2017-08-10 09:14:19,369 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 147.0 (TID 294, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,370 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 147.0 (TID 295, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,370 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 147.0 (TID 294)
2017-08-10 09:14:19,370 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 147.0 (TID 295)
2017-08-10 09:14:19,371 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,371 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,374 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 147.0 (TID 295). 714 bytes result sent to driver
2017-08-10 09:14:19,374 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 147.0 (TID 294). 714 bytes result sent to driver
2017-08-10 09:14:19,375 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 147.0 (TID 295) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,375 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 147.0 (TID 294) in 6 ms on localhost (2/2)
2017-08-10 09:14:19,375 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 147.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,375 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 147 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:14:19,376 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 147 finished: foreachPartition at streamingProcess.scala:50, took 0.013337 s
2017-08-10 09:14:19,376 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327650000 ms.0 from job set of time 1502327650000 ms
2017-08-10 09:14:19,376 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 9.376 s for time 1502327650000 ms (execution: 0.025 s)
2017-08-10 09:14:19,376 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 293 from persistence list
2017-08-10 09:14:19,376 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327655000 ms.0 from job set of time 1502327655000 ms
2017-08-10 09:14:19,376 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 293
2017-08-10 09:14:19,376 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 292 from persistence list
2017-08-10 09:14:19,376 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 292
2017-08-10 09:14:19,376 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,376 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327640000 ms
2017-08-10 09:14:19,385 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:19,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 148 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:19,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 148 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:19,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:19,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:19,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 148 (MapPartitionsRDD[297] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:19,388 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_148 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:19,390 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_148_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:14:19,391 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_148_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:19,391 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 148 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:19,391 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 148 (MapPartitionsRDD[297] at map at streamingProcess.scala:48)
2017-08-10 09:14:19,391 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 148.0 with 2 tasks
2017-08-10 09:14:19,392 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 148.0 (TID 296, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:19,392 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 148.0 (TID 297, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:19,392 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 148.0 (TID 297)
2017-08-10 09:14:19,392 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 148.0 (TID 296)
2017-08-10 09:14:19,394 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:19,394 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:19,397 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 148.0 (TID 296). 714 bytes result sent to driver
2017-08-10 09:14:19,397 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 148.0 (TID 297). 714 bytes result sent to driver
2017-08-10 09:14:19,398 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 148.0 (TID 297) in 6 ms on localhost (1/2)
2017-08-10 09:14:19,398 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 148.0 (TID 296) in 7 ms on localhost (2/2)
2017-08-10 09:14:19,398 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 148.0, whose tasks have all completed, from pool 
2017-08-10 09:14:19,398 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 148 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:19,398 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 148 finished: foreachPartition at streamingProcess.scala:50, took 0.012714 s
2017-08-10 09:14:19,398 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327655000 ms.0 from job set of time 1502327655000 ms
2017-08-10 09:14:19,399 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 4.398 s for time 1502327655000 ms (execution: 0.022 s)
2017-08-10 09:14:19,399 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 295 from persistence list
2017-08-10 09:14:19,399 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 295
2017-08-10 09:14:19,399 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 294 from persistence list
2017-08-10 09:14:19,399 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 294
2017-08-10 09:14:19,399 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:19,399 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327645000 ms
2017-08-10 09:14:20,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327660000 ms
2017-08-10 09:14:20,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327660000 ms.0 from job set of time 1502327660000 ms
2017-08-10 09:14:20,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 149 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 149 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:20,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 149 (MapPartitionsRDD[299] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:20,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_149 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:14:20,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_149_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:20,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_149_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:20,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 149 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:20,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 149 (MapPartitionsRDD[299] at map at streamingProcess.scala:48)
2017-08-10 09:14:20,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 149.0 with 2 tasks
2017-08-10 09:14:20,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 149.0 (TID 298, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:20,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 149.0 (TID 299, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:20,069 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 149.0 (TID 299)
2017-08-10 09:14:20,069 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 149.0 (TID 298)
2017-08-10 09:14:20,072 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:20,072 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:20,075 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 149.0 (TID 298). 714 bytes result sent to driver
2017-08-10 09:14:20,075 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 149.0 (TID 299). 714 bytes result sent to driver
2017-08-10 09:14:20,077 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 149.0 (TID 298) in 9 ms on localhost (1/2)
2017-08-10 09:14:20,077 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 149.0 (TID 299) in 8 ms on localhost (2/2)
2017-08-10 09:14:20,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 149.0, whose tasks have all completed, from pool 
2017-08-10 09:14:20,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 149 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:14:20,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 149 finished: foreachPartition at streamingProcess.scala:50, took 0.024603 s
2017-08-10 09:14:20,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327660000 ms.0 from job set of time 1502327660000 ms
2017-08-10 09:14:20,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1502327660000 ms (execution: 0.057 s)
2017-08-10 09:14:20,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 297 from persistence list
2017-08-10 09:14:20,079 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 297
2017-08-10 09:14:20,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 296 from persistence list
2017-08-10 09:14:20,079 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 296
2017-08-10 09:14:20,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:20,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327650000 ms
2017-08-10 09:14:25,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327665000 ms
2017-08-10 09:14:25,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327665000 ms.0 from job set of time 1502327665000 ms
2017-08-10 09:14:25,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 150 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 150 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 150 (MapPartitionsRDD[301] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:25,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_150 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:14:25,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_150_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:14:25,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_150_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:25,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 150 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:25,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 150 (MapPartitionsRDD[301] at map at streamingProcess.scala:48)
2017-08-10 09:14:25,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 150.0 with 2 tasks
2017-08-10 09:14:25,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 150.0 (TID 300, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:25,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 150.0 (TID 301, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:25,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 150.0 (TID 300)
2017-08-10 09:14:25,075 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 150.0 (TID 301)
2017-08-10 09:14:25,077 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:25,078 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:25,082 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 150.0 (TID 300). 714 bytes result sent to driver
2017-08-10 09:14:25,082 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 150.0 (TID 301). 714 bytes result sent to driver
2017-08-10 09:14:25,084 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 150.0 (TID 300) in 11 ms on localhost (1/2)
2017-08-10 09:14:25,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 150.0 (TID 301) in 10 ms on localhost (2/2)
2017-08-10 09:14:25,085 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 150.0, whose tasks have all completed, from pool 
2017-08-10 09:14:25,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 150 (foreachPartition at streamingProcess.scala:50) finished in 0.012 s
2017-08-10 09:14:25,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 150 finished: foreachPartition at streamingProcess.scala:50, took 0.030985 s
2017-08-10 09:14:25,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327665000 ms.0 from job set of time 1502327665000 ms
2017-08-10 09:14:25,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1502327665000 ms (execution: 0.064 s)
2017-08-10 09:14:25,085 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 299 from persistence list
2017-08-10 09:14:25,086 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 299
2017-08-10 09:14:25,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 298 from persistence list
2017-08-10 09:14:25,086 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 298
2017-08-10 09:14:25,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:25,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327655000 ms
2017-08-10 09:14:30,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327670000 ms
2017-08-10 09:14:30,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327670000 ms.0 from job set of time 1502327670000 ms
2017-08-10 09:14:30,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 151 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 151 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:30,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 151 (MapPartitionsRDD[303] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_151 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_151_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:30,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_151_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:14:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 151 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 151 (MapPartitionsRDD[303] at map at streamingProcess.scala:48)
2017-08-10 09:14:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 151.0 with 2 tasks
2017-08-10 09:14:30,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 151.0 (TID 302, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:30,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 151.0 (TID 303, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:30,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 151.0 (TID 303)
2017-08-10 09:14:30,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 151.0 (TID 302)
2017-08-10 09:14:30,068 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:30,068 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:30,073 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 151.0 (TID 303). 714 bytes result sent to driver
2017-08-10 09:14:30,073 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 151.0 (TID 302). 714 bytes result sent to driver
2017-08-10 09:14:30,076 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 151.0 (TID 302) in 15 ms on localhost (1/2)
2017-08-10 09:14:30,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 151.0 (TID 303) in 14 ms on localhost (2/2)
2017-08-10 09:14:30,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 151.0, whose tasks have all completed, from pool 
2017-08-10 09:14:30,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 151 (foreachPartition at streamingProcess.scala:50) finished in 0.015 s
2017-08-10 09:14:30,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 151 finished: foreachPartition at streamingProcess.scala:50, took 0.031242 s
2017-08-10 09:14:30,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327670000 ms.0 from job set of time 1502327670000 ms
2017-08-10 09:14:30,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1502327670000 ms (execution: 0.058 s)
2017-08-10 09:14:30,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 301 from persistence list
2017-08-10 09:14:30,078 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 301
2017-08-10 09:14:30,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 300 from persistence list
2017-08-10 09:14:30,079 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 300
2017-08-10 09:14:30,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:30,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327660000 ms
2017-08-10 09:14:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327675000 ms
2017-08-10 09:14:35,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327675000 ms.0 from job set of time 1502327675000 ms
2017-08-10 09:14:35,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:35,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 152 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:35,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 152 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:35,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:35,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:35,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 152 (MapPartitionsRDD[305] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:35,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_152 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:14:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_152_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:14:35,029 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_152_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:35,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 152 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:35,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 152 (MapPartitionsRDD[305] at map at streamingProcess.scala:48)
2017-08-10 09:14:35,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 152.0 with 2 tasks
2017-08-10 09:14:35,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 152.0 (TID 304, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:35,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 152.0 (TID 305, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:35,030 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 152.0 (TID 305)
2017-08-10 09:14:35,030 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 152.0 (TID 304)
2017-08-10 09:14:35,031 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:35,032 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:35,035 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 152.0 (TID 305). 714 bytes result sent to driver
2017-08-10 09:14:35,035 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 152.0 (TID 304). 714 bytes result sent to driver
2017-08-10 09:14:35,036 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 152.0 (TID 304) in 7 ms on localhost (1/2)
2017-08-10 09:14:35,036 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 152.0 (TID 305) in 6 ms on localhost (2/2)
2017-08-10 09:14:35,036 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 152.0, whose tasks have all completed, from pool 
2017-08-10 09:14:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 152 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:14:35,036 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 152 finished: foreachPartition at streamingProcess.scala:50, took 0.013454 s
2017-08-10 09:14:35,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327675000 ms.0 from job set of time 1502327675000 ms
2017-08-10 09:14:35,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.037 s for time 1502327675000 ms (execution: 0.024 s)
2017-08-10 09:14:35,037 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 303 from persistence list
2017-08-10 09:14:35,037 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 303
2017-08-10 09:14:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 302 from persistence list
2017-08-10 09:14:35,037 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 302
2017-08-10 09:14:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327665000 ms
2017-08-10 09:14:40,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327680000 ms
2017-08-10 09:14:40,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327680000 ms.0 from job set of time 1502327680000 ms
2017-08-10 09:14:40,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 153 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 153 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 153 (MapPartitionsRDD[307] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:40,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_153 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_153_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:14:40,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_153_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 153 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 153 (MapPartitionsRDD[307] at map at streamingProcess.scala:48)
2017-08-10 09:14:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 153.0 with 2 tasks
2017-08-10 09:14:40,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 153.0 (TID 306, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:40,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 153.0 (TID 307, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:40,054 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 153.0 (TID 306)
2017-08-10 09:14:40,054 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 153.0 (TID 307)
2017-08-10 09:14:40,056 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:40,056 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:40,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 153.0 (TID 307). 714 bytes result sent to driver
2017-08-10 09:14:40,060 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 153.0 (TID 306). 714 bytes result sent to driver
2017-08-10 09:14:40,061 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 153.0 (TID 307) in 7 ms on localhost (1/2)
2017-08-10 09:14:40,061 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 153.0 (TID 306) in 8 ms on localhost (2/2)
2017-08-10 09:14:40,061 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 153.0, whose tasks have all completed, from pool 
2017-08-10 09:14:40,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 153 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:14:40,062 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 153 finished: foreachPartition at streamingProcess.scala:50, took 0.015669 s
2017-08-10 09:14:40,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327680000 ms.0 from job set of time 1502327680000 ms
2017-08-10 09:14:40,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.062 s for time 1502327680000 ms (execution: 0.043 s)
2017-08-10 09:14:40,062 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 305 from persistence list
2017-08-10 09:14:40,062 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 305
2017-08-10 09:14:40,062 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 304 from persistence list
2017-08-10 09:14:40,062 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 304
2017-08-10 09:14:40,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:40,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327670000 ms
2017-08-10 09:14:45,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327685000 ms
2017-08-10 09:14:45,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327685000 ms.0 from job set of time 1502327685000 ms
2017-08-10 09:14:45,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 154 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 154 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:45,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 154 (MapPartitionsRDD[309] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:45,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_154 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:14:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_154_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:45,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_154_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:45,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 154 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 154 (MapPartitionsRDD[309] at map at streamingProcess.scala:48)
2017-08-10 09:14:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 154.0 with 2 tasks
2017-08-10 09:14:45,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 154.0 (TID 308, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:45,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 154.0 (TID 309, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:45,062 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 154.0 (TID 309)
2017-08-10 09:14:45,062 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 154.0 (TID 308)
2017-08-10 09:14:45,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:45,066 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:45,070 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 154.0 (TID 308). 714 bytes result sent to driver
2017-08-10 09:14:45,070 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 154.0 (TID 309). 714 bytes result sent to driver
2017-08-10 09:14:45,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 154.0 (TID 309) in 12 ms on localhost (1/2)
2017-08-10 09:14:45,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 154.0 (TID 308) in 13 ms on localhost (2/2)
2017-08-10 09:14:45,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 154.0, whose tasks have all completed, from pool 
2017-08-10 09:14:45,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 154 (foreachPartition at streamingProcess.scala:50) finished in 0.014 s
2017-08-10 09:14:45,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 154 finished: foreachPartition at streamingProcess.scala:50, took 0.028631 s
2017-08-10 09:14:45,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327685000 ms.0 from job set of time 1502327685000 ms
2017-08-10 09:14:45,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1502327685000 ms (execution: 0.054 s)
2017-08-10 09:14:45,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 307 from persistence list
2017-08-10 09:14:45,076 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 307
2017-08-10 09:14:45,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 306 from persistence list
2017-08-10 09:14:45,076 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 306
2017-08-10 09:14:45,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:45,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327675000 ms
2017-08-10 09:14:50,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327690000 ms
2017-08-10 09:14:50,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327690000 ms.0 from job set of time 1502327690000 ms
2017-08-10 09:14:50,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 155 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:50,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 155 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:50,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:50,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:50,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 155 (MapPartitionsRDD[311] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_155 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_155_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:14:50,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_155_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:14:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 155 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 155 (MapPartitionsRDD[311] at map at streamingProcess.scala:48)
2017-08-10 09:14:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 155.0 with 2 tasks
2017-08-10 09:14:50,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 155.0 (TID 310, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:50,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 155.0 (TID 311, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:50,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 155.0 (TID 310)
2017-08-10 09:14:50,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 155.0 (TID 311)
2017-08-10 09:14:50,065 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:50,065 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:50,069 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 155.0 (TID 311). 714 bytes result sent to driver
2017-08-10 09:14:50,069 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 155.0 (TID 310). 714 bytes result sent to driver
2017-08-10 09:14:50,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 155.0 (TID 310) in 8 ms on localhost (1/2)
2017-08-10 09:14:50,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 155.0 (TID 311) in 9 ms on localhost (2/2)
2017-08-10 09:14:50,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 155.0, whose tasks have all completed, from pool 
2017-08-10 09:14:50,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 155 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:14:50,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 155 finished: foreachPartition at streamingProcess.scala:50, took 0.023260 s
2017-08-10 09:14:50,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327690000 ms.0 from job set of time 1502327690000 ms
2017-08-10 09:14:50,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1502327690000 ms (execution: 0.052 s)
2017-08-10 09:14:50,073 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 309 from persistence list
2017-08-10 09:14:50,073 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 309
2017-08-10 09:14:50,073 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 308 from persistence list
2017-08-10 09:14:50,073 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 308
2017-08-10 09:14:50,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:50,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327680000 ms
2017-08-10 09:14:55,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327695000 ms
2017-08-10 09:14:55,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327695000 ms.0 from job set of time 1502327695000 ms
2017-08-10 09:14:55,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:14:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 156 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:14:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 156 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:14:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:14:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:14:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 156 (MapPartitionsRDD[313] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:14:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_156 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:14:55,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_156_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:14:55,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_156_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:14:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 156 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:14:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 156 (MapPartitionsRDD[313] at map at streamingProcess.scala:48)
2017-08-10 09:14:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 156.0 with 2 tasks
2017-08-10 09:14:55,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 156.0 (TID 312, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:14:55,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 156.0 (TID 313, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:14:55,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 156.0 (TID 313)
2017-08-10 09:14:55,077 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 156.0 (TID 312)
2017-08-10 09:14:55,083 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:14:55,083 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:14:55,090 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 156.0 (TID 313). 714 bytes result sent to driver
2017-08-10 09:14:55,090 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 156.0 (TID 312). 714 bytes result sent to driver
2017-08-10 09:14:55,094 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 156.0 (TID 312) in 18 ms on localhost (1/2)
2017-08-10 09:14:55,094 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 156.0 (TID 313) in 18 ms on localhost (2/2)
2017-08-10 09:14:55,094 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 156.0, whose tasks have all completed, from pool 
2017-08-10 09:14:55,094 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 156 (foreachPartition at streamingProcess.scala:50) finished in 0.019 s
2017-08-10 09:14:55,095 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 156 finished: foreachPartition at streamingProcess.scala:50, took 0.040884 s
2017-08-10 09:14:55,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327695000 ms.0 from job set of time 1502327695000 ms
2017-08-10 09:14:55,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.096 s for time 1502327695000 ms (execution: 0.077 s)
2017-08-10 09:14:55,096 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 311 from persistence list
2017-08-10 09:14:55,097 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 311
2017-08-10 09:14:55,097 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 310 from persistence list
2017-08-10 09:14:55,097 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 310
2017-08-10 09:14:55,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:14:55,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327685000 ms
2017-08-10 09:15:00,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327700000 ms
2017-08-10 09:15:00,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327700000 ms.0 from job set of time 1502327700000 ms
2017-08-10 09:15:00,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 157 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 157 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 157 (MapPartitionsRDD[315] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_157 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:15:00,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_157_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:15:00,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_157_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:00,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 157 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:00,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 157 (MapPartitionsRDD[315] at map at streamingProcess.scala:48)
2017-08-10 09:15:00,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 157.0 with 2 tasks
2017-08-10 09:15:00,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 157.0 (TID 314, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:00,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 157.0 (TID 315, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:00,061 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 157.0 (TID 315)
2017-08-10 09:15:00,061 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 157.0 (TID 314)
2017-08-10 09:15:00,064 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:00,064 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:00,067 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 157.0 (TID 315). 714 bytes result sent to driver
2017-08-10 09:15:00,067 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 157.0 (TID 314). 714 bytes result sent to driver
2017-08-10 09:15:00,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 157.0 (TID 314) in 9 ms on localhost (1/2)
2017-08-10 09:15:00,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 157.0 (TID 315) in 9 ms on localhost (2/2)
2017-08-10 09:15:00,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 157.0, whose tasks have all completed, from pool 
2017-08-10 09:15:00,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 157 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:15:00,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 157 finished: foreachPartition at streamingProcess.scala:50, took 0.018521 s
2017-08-10 09:15:00,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327700000 ms.0 from job set of time 1502327700000 ms
2017-08-10 09:15:00,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1502327700000 ms (execution: 0.051 s)
2017-08-10 09:15:00,071 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 313 from persistence list
2017-08-10 09:15:00,071 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 313
2017-08-10 09:15:00,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 312 from persistence list
2017-08-10 09:15:00,071 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 312
2017-08-10 09:15:00,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:00,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327690000 ms
2017-08-10 09:15:05,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327705000 ms
2017-08-10 09:15:05,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327705000 ms.0 from job set of time 1502327705000 ms
2017-08-10 09:15:05,022 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:05,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 158 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:05,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 158 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:05,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:05,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:05,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 158 (MapPartitionsRDD[317] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_158 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:15:05,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_158_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:15:05,029 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_158_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:05,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 158 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:05,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 158 (MapPartitionsRDD[317] at map at streamingProcess.scala:48)
2017-08-10 09:15:05,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 158.0 with 2 tasks
2017-08-10 09:15:05,030 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 158.0 (TID 316, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:05,030 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 158.0 (TID 317, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:05,030 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 158.0 (TID 317)
2017-08-10 09:15:05,030 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 158.0 (TID 316)
2017-08-10 09:15:05,032 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:05,032 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:05,035 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 158.0 (TID 317). 714 bytes result sent to driver
2017-08-10 09:15:05,035 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 158.0 (TID 316). 714 bytes result sent to driver
2017-08-10 09:15:05,036 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 158.0 (TID 316) in 6 ms on localhost (1/2)
2017-08-10 09:15:05,036 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 158.0 (TID 317) in 6 ms on localhost (2/2)
2017-08-10 09:15:05,036 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 158.0, whose tasks have all completed, from pool 
2017-08-10 09:15:05,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 158 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:15:05,036 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 158 finished: foreachPartition at streamingProcess.scala:50, took 0.014063 s
2017-08-10 09:15:05,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327705000 ms.0 from job set of time 1502327705000 ms
2017-08-10 09:15:05,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.036 s for time 1502327705000 ms (execution: 0.024 s)
2017-08-10 09:15:05,037 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 315 from persistence list
2017-08-10 09:15:05,037 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 315
2017-08-10 09:15:05,037 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 314 from persistence list
2017-08-10 09:15:05,037 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 314
2017-08-10 09:15:05,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:05,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327695000 ms
2017-08-10 09:15:10,032 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327710000 ms
2017-08-10 09:15:10,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327710000 ms.0 from job set of time 1502327710000 ms
2017-08-10 09:15:10,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 159 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 159 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 159 (MapPartitionsRDD[319] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:10,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_159 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:15:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_159_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:15:10,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_159_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 159 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 159 (MapPartitionsRDD[319] at map at streamingProcess.scala:48)
2017-08-10 09:15:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 159.0 with 2 tasks
2017-08-10 09:15:10,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 159.0 (TID 318, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:10,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 159.0 (TID 319, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:10,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 159.0 (TID 318)
2017-08-10 09:15:10,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 159.0 (TID 319)
2017-08-10 09:15:10,067 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:10,067 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:10,071 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 159.0 (TID 319). 714 bytes result sent to driver
2017-08-10 09:15:10,071 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 159.0 (TID 318). 714 bytes result sent to driver
2017-08-10 09:15:10,073 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 159.0 (TID 318) in 9 ms on localhost (1/2)
2017-08-10 09:15:10,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 159.0 (TID 319) in 9 ms on localhost (2/2)
2017-08-10 09:15:10,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 159.0, whose tasks have all completed, from pool 
2017-08-10 09:15:10,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 159 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:15:10,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 159 finished: foreachPartition at streamingProcess.scala:50, took 0.026911 s
2017-08-10 09:15:10,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327710000 ms.0 from job set of time 1502327710000 ms
2017-08-10 09:15:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1502327710000 ms (execution: 0.037 s)
2017-08-10 09:15:10,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 317 from persistence list
2017-08-10 09:15:10,075 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 317
2017-08-10 09:15:10,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 316 from persistence list
2017-08-10 09:15:10,076 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 316
2017-08-10 09:15:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327700000 ms
2017-08-10 09:15:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327715000 ms
2017-08-10 09:15:15,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327715000 ms.0 from job set of time 1502327715000 ms
2017-08-10 09:15:15,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:15,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 160 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:15,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 160 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:15,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:15,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:15,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 160 (MapPartitionsRDD[321] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:15,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_160 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:15:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_160_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:15:15,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_160_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:15:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 160 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 160 (MapPartitionsRDD[321] at map at streamingProcess.scala:48)
2017-08-10 09:15:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 160.0 with 2 tasks
2017-08-10 09:15:15,031 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 160.0 (TID 320, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:15,031 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 160.0 (TID 321, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:15,032 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 160.0 (TID 320)
2017-08-10 09:15:15,032 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 160.0 (TID 321)
2017-08-10 09:15:15,033 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:15,034 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:15,036 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 160.0 (TID 320). 714 bytes result sent to driver
2017-08-10 09:15:15,036 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 160.0 (TID 321). 714 bytes result sent to driver
2017-08-10 09:15:15,038 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 160.0 (TID 320) in 6 ms on localhost (1/2)
2017-08-10 09:15:15,038 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 160.0 (TID 321) in 7 ms on localhost (2/2)
2017-08-10 09:15:15,038 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 160.0, whose tasks have all completed, from pool 
2017-08-10 09:15:15,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 160 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:15:15,038 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 160 finished: foreachPartition at streamingProcess.scala:50, took 0.015105 s
2017-08-10 09:15:15,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327715000 ms.0 from job set of time 1502327715000 ms
2017-08-10 09:15:15,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.038 s for time 1502327715000 ms (execution: 0.025 s)
2017-08-10 09:15:15,038 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 319 from persistence list
2017-08-10 09:15:15,039 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 319
2017-08-10 09:15:15,039 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 318 from persistence list
2017-08-10 09:15:15,039 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 318
2017-08-10 09:15:15,039 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:15,039 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327705000 ms
2017-08-10 09:15:20,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327720000 ms
2017-08-10 09:15:20,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327720000 ms.0 from job set of time 1502327720000 ms
2017-08-10 09:15:20,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 161 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 161 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 161 (MapPartitionsRDD[323] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:20,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_161 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:15:20,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_161_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:15:20,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_161_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:15:20,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_146_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:15:20,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 161 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:20,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 161 (MapPartitionsRDD[323] at map at streamingProcess.scala:48)
2017-08-10 09:15:20,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 161.0 with 2 tasks
2017-08-10 09:15:20,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 161.0 (TID 322, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:20,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_147_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:20,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 161.0 (TID 323, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:20,078 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 161.0 (TID 322)
2017-08-10 09:15:20,078 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 161.0 (TID 323)
2017-08-10 09:15:20,081 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_148_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:20,082 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:20,082 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:20,083 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_149_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:20,084 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_150_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:15:20,086 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_151_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:15:20,086 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 161.0 (TID 322). 714 bytes result sent to driver
2017-08-10 09:15:20,086 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 161.0 (TID 323). 714 bytes result sent to driver
2017-08-10 09:15:20,087 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_152_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:15:20,088 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_153_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:15:20,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 161.0 (TID 322) in 12 ms on localhost (1/2)
2017-08-10 09:15:20,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 161.0 (TID 323) in 11 ms on localhost (2/2)
2017-08-10 09:15:20,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 161.0, whose tasks have all completed, from pool 
2017-08-10 09:15:20,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 161 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-10 09:15:20,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 161 finished: foreachPartition at streamingProcess.scala:50, took 0.043833 s
2017-08-10 09:15:20,089 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_154_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:15:20,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327720000 ms.0 from job set of time 1502327720000 ms
2017-08-10 09:15:20,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1502327720000 ms (execution: 0.070 s)
2017-08-10 09:15:20,090 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 321 from persistence list
2017-08-10 09:15:20,090 [block-manager-slave-async-thread-pool-14] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 321
2017-08-10 09:15:20,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 320 from persistence list
2017-08-10 09:15:20,091 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 320
2017-08-10 09:15:20,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:20,091 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_155_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:20,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327710000 ms
2017-08-10 09:15:20,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_156_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:20,093 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_157_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:20,094 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_158_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:20,095 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_159_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:15:20,096 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_160_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:15:25,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327725000 ms
2017-08-10 09:15:25,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327725000 ms.0 from job set of time 1502327725000 ms
2017-08-10 09:15:25,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 162 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 162 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 162 (MapPartitionsRDD[325] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:25,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_162 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:15:25,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_162_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:15:25,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_162_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:15:25,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 162 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:25,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 162 (MapPartitionsRDD[325] at map at streamingProcess.scala:48)
2017-08-10 09:15:25,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 162.0 with 2 tasks
2017-08-10 09:15:25,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 162.0 (TID 324, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:25,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 162.0 (TID 325, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:25,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 162.0 (TID 325)
2017-08-10 09:15:25,061 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 162.0 (TID 324)
2017-08-10 09:15:25,064 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:25,064 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:25,067 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 162.0 (TID 325). 714 bytes result sent to driver
2017-08-10 09:15:25,067 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 162.0 (TID 324). 714 bytes result sent to driver
2017-08-10 09:15:25,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 162.0 (TID 325) in 9 ms on localhost (1/2)
2017-08-10 09:15:25,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 162.0 (TID 324) in 10 ms on localhost (2/2)
2017-08-10 09:15:25,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 162.0, whose tasks have all completed, from pool 
2017-08-10 09:15:25,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 162 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:15:25,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 162 finished: foreachPartition at streamingProcess.scala:50, took 0.021794 s
2017-08-10 09:15:25,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327725000 ms.0 from job set of time 1502327725000 ms
2017-08-10 09:15:25,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1502327725000 ms (execution: 0.051 s)
2017-08-10 09:15:25,071 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 323 from persistence list
2017-08-10 09:15:25,071 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 323
2017-08-10 09:15:25,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 322 from persistence list
2017-08-10 09:15:25,071 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 322
2017-08-10 09:15:25,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:25,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327715000 ms
2017-08-10 09:15:30,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327730000 ms
2017-08-10 09:15:30,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327730000 ms.0 from job set of time 1502327730000 ms
2017-08-10 09:15:30,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 163 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 163 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 163 (MapPartitionsRDD[327] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_163 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:15:30,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_163_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:15:30,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_163_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:30,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 163 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:30,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 163 (MapPartitionsRDD[327] at map at streamingProcess.scala:48)
2017-08-10 09:15:30,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 163.0 with 2 tasks
2017-08-10 09:15:30,076 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 163.0 (TID 326, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:30,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 163.0 (TID 327, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:30,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 163.0 (TID 327)
2017-08-10 09:15:30,077 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 163.0 (TID 326)
2017-08-10 09:15:30,083 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:30,083 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:30,089 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 163.0 (TID 326). 714 bytes result sent to driver
2017-08-10 09:15:30,089 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 163.0 (TID 327). 714 bytes result sent to driver
2017-08-10 09:15:30,092 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 163.0 (TID 326) in 17 ms on localhost (1/2)
2017-08-10 09:15:30,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 163.0 (TID 327) in 17 ms on localhost (2/2)
2017-08-10 09:15:30,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 163.0, whose tasks have all completed, from pool 
2017-08-10 09:15:30,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 163 (foreachPartition at streamingProcess.scala:50) finished in 0.018 s
2017-08-10 09:15:30,094 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 163 finished: foreachPartition at streamingProcess.scala:50, took 0.039657 s
2017-08-10 09:15:30,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327730000 ms.0 from job set of time 1502327730000 ms
2017-08-10 09:15:30,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.094 s for time 1502327730000 ms (execution: 0.074 s)
2017-08-10 09:15:30,095 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 325 from persistence list
2017-08-10 09:15:30,096 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 324 from persistence list
2017-08-10 09:15:30,096 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 325
2017-08-10 09:15:30,097 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 324
2017-08-10 09:15:30,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:30,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327720000 ms
2017-08-10 09:15:35,027 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327735000 ms
2017-08-10 09:15:35,031 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327735000 ms.0 from job set of time 1502327735000 ms
2017-08-10 09:15:35,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:35,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 164 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:35,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 164 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:35,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:35,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:35,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 164 (MapPartitionsRDD[329] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_164 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:15:35,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_164_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:15:35,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_164_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 164 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 164 (MapPartitionsRDD[329] at map at streamingProcess.scala:48)
2017-08-10 09:15:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 164.0 with 2 tasks
2017-08-10 09:15:35,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 164.0 (TID 328, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:35,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 164.0 (TID 329, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:35,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 164.0 (TID 328)
2017-08-10 09:15:35,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:35,069 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 164.0 (TID 329)
2017-08-10 09:15:35,071 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:35,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 164.0 (TID 328). 714 bytes result sent to driver
2017-08-10 09:15:35,075 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 164.0 (TID 329). 714 bytes result sent to driver
2017-08-10 09:15:35,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 164.0 (TID 329) in 15 ms on localhost (1/2)
2017-08-10 09:15:35,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 164.0 (TID 328) in 15 ms on localhost (2/2)
2017-08-10 09:15:35,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 164.0, whose tasks have all completed, from pool 
2017-08-10 09:15:35,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 164 (foreachPartition at streamingProcess.scala:50) finished in 0.017 s
2017-08-10 09:15:35,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 164 finished: foreachPartition at streamingProcess.scala:50, took 0.040923 s
2017-08-10 09:15:35,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327735000 ms.0 from job set of time 1502327735000 ms
2017-08-10 09:15:35,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1502327735000 ms (execution: 0.049 s)
2017-08-10 09:15:35,080 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 327 from persistence list
2017-08-10 09:15:35,080 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 327
2017-08-10 09:15:35,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 326 from persistence list
2017-08-10 09:15:35,080 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 326
2017-08-10 09:15:35,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:35,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327725000 ms
2017-08-10 09:15:40,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327740000 ms
2017-08-10 09:15:40,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327740000 ms.0 from job set of time 1502327740000 ms
2017-08-10 09:15:40,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 165 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 165 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 165 (MapPartitionsRDD[331] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_165 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:15:40,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_165_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:15:40,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_165_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:40,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 165 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:40,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 165 (MapPartitionsRDD[331] at map at streamingProcess.scala:48)
2017-08-10 09:15:40,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 165.0 with 2 tasks
2017-08-10 09:15:40,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 165.0 (TID 330, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:40,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 165.0 (TID 331, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:40,066 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 165.0 (TID 331)
2017-08-10 09:15:40,066 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 165.0 (TID 330)
2017-08-10 09:15:40,069 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:40,069 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:40,074 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 165.0 (TID 330). 714 bytes result sent to driver
2017-08-10 09:15:40,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 165.0 (TID 331). 714 bytes result sent to driver
2017-08-10 09:15:40,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 165.0 (TID 330) in 12 ms on localhost (1/2)
2017-08-10 09:15:40,076 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 165.0 (TID 331) in 11 ms on localhost (2/2)
2017-08-10 09:15:40,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 165 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-10 09:15:40,077 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 165.0, whose tasks have all completed, from pool 
2017-08-10 09:15:40,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 165 finished: foreachPartition at streamingProcess.scala:50, took 0.026703 s
2017-08-10 09:15:40,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327740000 ms.0 from job set of time 1502327740000 ms
2017-08-10 09:15:40,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1502327740000 ms (execution: 0.058 s)
2017-08-10 09:15:40,078 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 329 from persistence list
2017-08-10 09:15:40,078 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 329
2017-08-10 09:15:40,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 328 from persistence list
2017-08-10 09:15:40,079 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 328
2017-08-10 09:15:40,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:40,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327730000 ms
2017-08-10 09:15:45,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327745000 ms
2017-08-10 09:15:45,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327745000 ms.0 from job set of time 1502327745000 ms
2017-08-10 09:15:45,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:45,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 166 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:45,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 166 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:45,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:45,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:45,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 166 (MapPartitionsRDD[333] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_166 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:15:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_166_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:15:45,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_166_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:15:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 166 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 166 (MapPartitionsRDD[333] at map at streamingProcess.scala:48)
2017-08-10 09:15:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 166.0 with 2 tasks
2017-08-10 09:15:45,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 166.0 (TID 332, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:45,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 166.0 (TID 333, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:45,056 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 166.0 (TID 333)
2017-08-10 09:15:45,056 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 166.0 (TID 332)
2017-08-10 09:15:45,058 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:45,058 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:45,061 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 166.0 (TID 332). 714 bytes result sent to driver
2017-08-10 09:15:45,061 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 166.0 (TID 333). 714 bytes result sent to driver
2017-08-10 09:15:45,062 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 166.0 (TID 332) in 6 ms on localhost (1/2)
2017-08-10 09:15:45,062 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 166.0 (TID 333) in 6 ms on localhost (2/2)
2017-08-10 09:15:45,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 166.0, whose tasks have all completed, from pool 
2017-08-10 09:15:45,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 166 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:15:45,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 166 finished: foreachPartition at streamingProcess.scala:50, took 0.015785 s
2017-08-10 09:15:45,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327745000 ms.0 from job set of time 1502327745000 ms
2017-08-10 09:15:45,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1502327745000 ms (execution: 0.043 s)
2017-08-10 09:15:45,063 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 331 from persistence list
2017-08-10 09:15:45,063 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 331
2017-08-10 09:15:45,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 330 from persistence list
2017-08-10 09:15:45,064 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 330
2017-08-10 09:15:45,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:45,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327735000 ms
2017-08-10 09:15:50,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327750000 ms
2017-08-10 09:15:50,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327750000 ms.0 from job set of time 1502327750000 ms
2017-08-10 09:15:50,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 167 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 167 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 167 (MapPartitionsRDD[335] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_167 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:15:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_167_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:15:50,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_167_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:15:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 167 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 167 (MapPartitionsRDD[335] at map at streamingProcess.scala:48)
2017-08-10 09:15:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 167.0 with 2 tasks
2017-08-10 09:15:50,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 167.0 (TID 334, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:50,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 167.0 (TID 335, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:50,056 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 167.0 (TID 334)
2017-08-10 09:15:50,056 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 167.0 (TID 335)
2017-08-10 09:15:50,059 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:50,059 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:50,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 167.0 (TID 335). 714 bytes result sent to driver
2017-08-10 09:15:50,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 167.0 (TID 334). 714 bytes result sent to driver
2017-08-10 09:15:50,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 167.0 (TID 335) in 11 ms on localhost (1/2)
2017-08-10 09:15:50,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 167.0 (TID 334) in 11 ms on localhost (2/2)
2017-08-10 09:15:50,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 167.0, whose tasks have all completed, from pool 
2017-08-10 09:15:50,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 167 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:15:50,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 167 finished: foreachPartition at streamingProcess.scala:50, took 0.023962 s
2017-08-10 09:15:50,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327750000 ms.0 from job set of time 1502327750000 ms
2017-08-10 09:15:50,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1502327750000 ms (execution: 0.048 s)
2017-08-10 09:15:50,067 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 333 from persistence list
2017-08-10 09:15:50,068 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 333
2017-08-10 09:15:50,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 332 from persistence list
2017-08-10 09:15:50,068 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 332
2017-08-10 09:15:50,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:50,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327740000 ms
2017-08-10 09:15:55,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327755000 ms
2017-08-10 09:15:55,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327755000 ms.0 from job set of time 1502327755000 ms
2017-08-10 09:15:55,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:15:55,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 168 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:15:55,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 168 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:15:55,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:15:55,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:15:55,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 168 (MapPartitionsRDD[337] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:15:55,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_168 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:15:55,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_168_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:15:55,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_168_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:15:55,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 168 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:15:55,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 168 (MapPartitionsRDD[337] at map at streamingProcess.scala:48)
2017-08-10 09:15:55,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 168.0 with 2 tasks
2017-08-10 09:15:55,078 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 168.0 (TID 336, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:15:55,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 168.0 (TID 337, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:15:55,079 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 168.0 (TID 336)
2017-08-10 09:15:55,079 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 168.0 (TID 337)
2017-08-10 09:15:55,085 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:15:55,085 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:15:55,092 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 168.0 (TID 337). 714 bytes result sent to driver
2017-08-10 09:15:55,092 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 168.0 (TID 336). 714 bytes result sent to driver
2017-08-10 09:15:55,096 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 168.0 (TID 337) in 18 ms on localhost (1/2)
2017-08-10 09:15:55,096 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 168.0 (TID 336) in 19 ms on localhost (2/2)
2017-08-10 09:15:55,096 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 168.0, whose tasks have all completed, from pool 
2017-08-10 09:15:55,097 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 168 (foreachPartition at streamingProcess.scala:50) finished in 0.020 s
2017-08-10 09:15:55,097 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 168 finished: foreachPartition at streamingProcess.scala:50, took 0.041986 s
2017-08-10 09:15:55,098 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327755000 ms.0 from job set of time 1502327755000 ms
2017-08-10 09:15:55,098 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.098 s for time 1502327755000 ms (execution: 0.078 s)
2017-08-10 09:15:55,098 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 335 from persistence list
2017-08-10 09:15:55,099 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 334 from persistence list
2017-08-10 09:15:55,099 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 335
2017-08-10 09:15:55,100 [block-manager-slave-async-thread-pool-13] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 334
2017-08-10 09:15:55,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:15:55,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327745000 ms
2017-08-10 09:16:00,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327760000 ms
2017-08-10 09:16:00,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327760000 ms.0 from job set of time 1502327760000 ms
2017-08-10 09:16:00,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 169 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 169 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 169 (MapPartitionsRDD[339] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_169 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:16:00,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_169_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:16:00,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_169_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:16:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 169 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 169 (MapPartitionsRDD[339] at map at streamingProcess.scala:48)
2017-08-10 09:16:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 169.0 with 2 tasks
2017-08-10 09:16:00,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 169.0 (TID 338, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:00,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 169.0 (TID 339, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:00,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 169.0 (TID 339)
2017-08-10 09:16:00,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 169.0 (TID 338)
2017-08-10 09:16:00,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:00,066 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:00,069 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 169.0 (TID 339). 714 bytes result sent to driver
2017-08-10 09:16:00,069 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 169.0 (TID 338). 714 bytes result sent to driver
2017-08-10 09:16:00,071 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 169.0 (TID 339) in 9 ms on localhost (1/2)
2017-08-10 09:16:00,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 169.0 (TID 338) in 9 ms on localhost (2/2)
2017-08-10 09:16:00,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 169.0, whose tasks have all completed, from pool 
2017-08-10 09:16:00,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 169 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:16:00,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 169 finished: foreachPartition at streamingProcess.scala:50, took 0.019650 s
2017-08-10 09:16:00,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327760000 ms.0 from job set of time 1502327760000 ms
2017-08-10 09:16:00,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1502327760000 ms (execution: 0.053 s)
2017-08-10 09:16:00,072 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 337 from persistence list
2017-08-10 09:16:00,073 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 337
2017-08-10 09:16:00,073 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 336 from persistence list
2017-08-10 09:16:00,073 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 336
2017-08-10 09:16:00,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:00,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327750000 ms
2017-08-10 09:16:05,112 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327765000 ms
2017-08-10 09:16:05,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327765000 ms.0 from job set of time 1502327765000 ms
2017-08-10 09:16:05,124 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:05,124 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 170 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:05,124 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 170 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:05,124 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:05,125 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:05,125 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 170 (MapPartitionsRDD[341] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:05,127 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_170 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:16:05,131 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_170_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:16:05,132 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_170_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:16:05,132 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 170 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:05,132 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 170 (MapPartitionsRDD[341] at map at streamingProcess.scala:48)
2017-08-10 09:16:05,132 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 170.0 with 2 tasks
2017-08-10 09:16:05,133 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 170.0 (TID 340, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:05,133 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 170.0 (TID 341, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:05,134 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 170.0 (TID 340)
2017-08-10 09:16:05,135 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 170.0 (TID 341)
2017-08-10 09:16:05,136 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:05,137 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:05,139 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 170.0 (TID 340). 714 bytes result sent to driver
2017-08-10 09:16:05,139 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 170.0 (TID 341). 714 bytes result sent to driver
2017-08-10 09:16:05,140 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 170.0 (TID 341) in 7 ms on localhost (1/2)
2017-08-10 09:16:05,140 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 170.0 (TID 340) in 8 ms on localhost (2/2)
2017-08-10 09:16:05,141 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 170.0, whose tasks have all completed, from pool 
2017-08-10 09:16:05,141 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 170 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:16:05,141 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 170 finished: foreachPartition at streamingProcess.scala:50, took 0.016921 s
2017-08-10 09:16:05,141 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327765000 ms.0 from job set of time 1502327765000 ms
2017-08-10 09:16:05,141 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.141 s for time 1502327765000 ms (execution: 0.029 s)
2017-08-10 09:16:05,142 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 339 from persistence list
2017-08-10 09:16:05,142 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 338 from persistence list
2017-08-10 09:16:05,142 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 339
2017-08-10 09:16:05,143 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:05,142 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 338
2017-08-10 09:16:05,143 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327755000 ms
2017-08-10 09:16:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327770000 ms
2017-08-10 09:16:10,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327770000 ms.0 from job set of time 1502327770000 ms
2017-08-10 09:16:10,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 171 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 171 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 171 (MapPartitionsRDD[343] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_171 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:16:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_171_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:16:10,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_171_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 171 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 171 (MapPartitionsRDD[343] at map at streamingProcess.scala:48)
2017-08-10 09:16:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 171.0 with 2 tasks
2017-08-10 09:16:10,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 171.0 (TID 342, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:10,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 171.0 (TID 343, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:10,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 171.0 (TID 342)
2017-08-10 09:16:10,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 171.0 (TID 343)
2017-08-10 09:16:10,069 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:10,069 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:10,073 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 171.0 (TID 343). 714 bytes result sent to driver
2017-08-10 09:16:10,073 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 171.0 (TID 342). 714 bytes result sent to driver
2017-08-10 09:16:10,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 171.0 (TID 342) in 10 ms on localhost (1/2)
2017-08-10 09:16:10,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 171.0 (TID 343) in 9 ms on localhost (2/2)
2017-08-10 09:16:10,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 171.0, whose tasks have all completed, from pool 
2017-08-10 09:16:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 171 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:16:10,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 171 finished: foreachPartition at streamingProcess.scala:50, took 0.024406 s
2017-08-10 09:16:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327770000 ms.0 from job set of time 1502327770000 ms
2017-08-10 09:16:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1502327770000 ms (execution: 0.055 s)
2017-08-10 09:16:10,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 341 from persistence list
2017-08-10 09:16:10,075 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 341
2017-08-10 09:16:10,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 340 from persistence list
2017-08-10 09:16:10,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:10,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327760000 ms
2017-08-10 09:16:10,075 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 340
2017-08-10 09:16:15,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327775000 ms
2017-08-10 09:16:15,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327775000 ms.0 from job set of time 1502327775000 ms
2017-08-10 09:16:15,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 172 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 172 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 172 (MapPartitionsRDD[345] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_172 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:16:15,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_172_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:16:15,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_172_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 172 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 172 (MapPartitionsRDD[345] at map at streamingProcess.scala:48)
2017-08-10 09:16:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 172.0 with 2 tasks
2017-08-10 09:16:15,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 172.0 (TID 344, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:15,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 172.0 (TID 345, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:15,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 172.0 (TID 344)
2017-08-10 09:16:15,072 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 172.0 (TID 345)
2017-08-10 09:16:15,076 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:15,076 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:15,082 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 172.0 (TID 344). 714 bytes result sent to driver
2017-08-10 09:16:15,082 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 172.0 (TID 345). 714 bytes result sent to driver
2017-08-10 09:16:15,086 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 172.0 (TID 345) in 15 ms on localhost (1/2)
2017-08-10 09:16:15,086 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 172.0 (TID 344) in 16 ms on localhost (2/2)
2017-08-10 09:16:15,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 172.0, whose tasks have all completed, from pool 
2017-08-10 09:16:15,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 172 (foreachPartition at streamingProcess.scala:50) finished in 0.018 s
2017-08-10 09:16:15,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 172 finished: foreachPartition at streamingProcess.scala:50, took 0.036287 s
2017-08-10 09:16:15,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327775000 ms.0 from job set of time 1502327775000 ms
2017-08-10 09:16:15,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1502327775000 ms (execution: 0.070 s)
2017-08-10 09:16:15,090 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 343 from persistence list
2017-08-10 09:16:15,090 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 343
2017-08-10 09:16:15,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 342 from persistence list
2017-08-10 09:16:15,091 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 342
2017-08-10 09:16:15,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:15,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327765000 ms
2017-08-10 09:16:20,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327780000 ms
2017-08-10 09:16:20,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327780000 ms.0 from job set of time 1502327780000 ms
2017-08-10 09:16:20,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:20,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 173 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:20,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 173 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:20,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:20,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:20,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 173 (MapPartitionsRDD[347] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_173 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:16:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_173_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:16:20,045 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_173_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 173 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 173 (MapPartitionsRDD[347] at map at streamingProcess.scala:48)
2017-08-10 09:16:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 173.0 with 2 tasks
2017-08-10 09:16:20,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 173.0 (TID 346, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:20,047 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 173.0 (TID 347, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:20,047 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 173.0 (TID 346)
2017-08-10 09:16:20,047 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 173.0 (TID 347)
2017-08-10 09:16:20,049 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:20,049 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:20,052 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 173.0 (TID 346). 714 bytes result sent to driver
2017-08-10 09:16:20,052 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 173.0 (TID 347). 714 bytes result sent to driver
2017-08-10 09:16:20,054 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 173.0 (TID 346) in 8 ms on localhost (1/2)
2017-08-10 09:16:20,054 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 173.0 (TID 347) in 8 ms on localhost (2/2)
2017-08-10 09:16:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 173 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:16:20,054 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 173.0, whose tasks have all completed, from pool 
2017-08-10 09:16:20,054 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 173 finished: foreachPartition at streamingProcess.scala:50, took 0.014668 s
2017-08-10 09:16:20,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327780000 ms.0 from job set of time 1502327780000 ms
2017-08-10 09:16:20,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.055 s for time 1502327780000 ms (execution: 0.032 s)
2017-08-10 09:16:20,055 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 345 from persistence list
2017-08-10 09:16:20,055 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 345
2017-08-10 09:16:20,055 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 344 from persistence list
2017-08-10 09:16:20,055 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 344
2017-08-10 09:16:20,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:20,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327770000 ms
2017-08-10 09:16:25,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327785000 ms
2017-08-10 09:16:25,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327785000 ms.0 from job set of time 1502327785000 ms
2017-08-10 09:16:25,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 174 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 174 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 174 (MapPartitionsRDD[349] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:25,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_174 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:16:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_174_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:16:25,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_174_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:25,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 174 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:25,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 174 (MapPartitionsRDD[349] at map at streamingProcess.scala:48)
2017-08-10 09:16:25,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 174.0 with 2 tasks
2017-08-10 09:16:25,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 174.0 (TID 348, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:25,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 174.0 (TID 349, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:25,066 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 174.0 (TID 349)
2017-08-10 09:16:25,066 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 174.0 (TID 348)
2017-08-10 09:16:25,070 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:25,070 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:25,074 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 174.0 (TID 349). 714 bytes result sent to driver
2017-08-10 09:16:25,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 174.0 (TID 348). 714 bytes result sent to driver
2017-08-10 09:16:25,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 174.0 (TID 349) in 10 ms on localhost (1/2)
2017-08-10 09:16:25,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 174.0 (TID 348) in 11 ms on localhost (2/2)
2017-08-10 09:16:25,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 174.0, whose tasks have all completed, from pool 
2017-08-10 09:16:25,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 174 (foreachPartition at streamingProcess.scala:50) finished in 0.012 s
2017-08-10 09:16:25,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 174 finished: foreachPartition at streamingProcess.scala:50, took 0.026199 s
2017-08-10 09:16:25,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327785000 ms.0 from job set of time 1502327785000 ms
2017-08-10 09:16:25,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1502327785000 ms (execution: 0.057 s)
2017-08-10 09:16:25,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 347 from persistence list
2017-08-10 09:16:25,077 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 347
2017-08-10 09:16:25,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 346 from persistence list
2017-08-10 09:16:25,077 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 346
2017-08-10 09:16:25,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:25,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327775000 ms
2017-08-10 09:16:30,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327790000 ms
2017-08-10 09:16:30,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327790000 ms.0 from job set of time 1502327790000 ms
2017-08-10 09:16:30,034 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:30,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 175 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:30,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 175 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:30,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:30,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:30,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 175 (MapPartitionsRDD[351] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:30,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_175 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:16:30,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_175_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:16:30,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_175_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:16:30,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 175 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:30,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 175 (MapPartitionsRDD[351] at map at streamingProcess.scala:48)
2017-08-10 09:16:30,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 175.0 with 2 tasks
2017-08-10 09:16:30,046 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 175.0 (TID 350, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:30,046 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 175.0 (TID 351, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:30,046 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 175.0 (TID 350)
2017-08-10 09:16:30,046 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 175.0 (TID 351)
2017-08-10 09:16:30,049 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:30,049 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:30,053 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 175.0 (TID 351). 714 bytes result sent to driver
2017-08-10 09:16:30,053 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 175.0 (TID 350). 714 bytes result sent to driver
2017-08-10 09:16:30,055 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 175.0 (TID 350) in 10 ms on localhost (1/2)
2017-08-10 09:16:30,055 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 175.0 (TID 351) in 9 ms on localhost (2/2)
2017-08-10 09:16:30,055 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 175.0, whose tasks have all completed, from pool 
2017-08-10 09:16:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 175 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:16:30,056 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 175 finished: foreachPartition at streamingProcess.scala:50, took 0.021816 s
2017-08-10 09:16:30,056 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327790000 ms.0 from job set of time 1502327790000 ms
2017-08-10 09:16:30,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.056 s for time 1502327790000 ms (execution: 0.040 s)
2017-08-10 09:16:30,057 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 349 from persistence list
2017-08-10 09:16:30,057 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 349
2017-08-10 09:16:30,057 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 348 from persistence list
2017-08-10 09:16:30,057 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 348
2017-08-10 09:16:30,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:30,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327780000 ms
2017-08-10 09:16:35,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327795000 ms
2017-08-10 09:16:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327795000 ms.0 from job set of time 1502327795000 ms
2017-08-10 09:16:35,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 176 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 176 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 176 (MapPartitionsRDD[353] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:35,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_176 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:16:35,116 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_176_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:16:35,116 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_176_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:16:35,117 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 176 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:35,117 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_161_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:16:35,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 176 (MapPartitionsRDD[353] at map at streamingProcess.scala:48)
2017-08-10 09:16:35,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 176.0 with 2 tasks
2017-08-10 09:16:35,120 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_162_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:35,120 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 176.0 (TID 352, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:35,121 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 176.0 (TID 353, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:35,122 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 176.0 (TID 352)
2017-08-10 09:16:35,122 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_163_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:35,122 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 176.0 (TID 353)
2017-08-10 09:16:35,123 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_164_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:35,124 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_165_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:16:35,126 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_166_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:16:35,127 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_167_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:16:35,128 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_168_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:16:35,129 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:35,129 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:35,130 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_169_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:16:35,131 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_170_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:35,132 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_171_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:35,133 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 176.0 (TID 352). 714 bytes result sent to driver
2017-08-10 09:16:35,133 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 176.0 (TID 353). 714 bytes result sent to driver
2017-08-10 09:16:35,133 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_172_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:35,134 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_173_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:35,135 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 176.0 (TID 352) in 16 ms on localhost (1/2)
2017-08-10 09:16:35,135 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 176.0 (TID 353) in 15 ms on localhost (2/2)
2017-08-10 09:16:35,135 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 176.0, whose tasks have all completed, from pool 
2017-08-10 09:16:35,135 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_174_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:16:35,136 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 176 (foreachPartition at streamingProcess.scala:50) finished in 0.016 s
2017-08-10 09:16:35,136 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 176 finished: foreachPartition at streamingProcess.scala:50, took 0.076833 s
2017-08-10 09:16:35,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327795000 ms.0 from job set of time 1502327795000 ms
2017-08-10 09:16:35,137 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.136 s for time 1502327795000 ms (execution: 0.115 s)
2017-08-10 09:16:35,137 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 351 from persistence list
2017-08-10 09:16:35,137 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_175_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:16:35,137 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 351
2017-08-10 09:16:35,137 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 350 from persistence list
2017-08-10 09:16:35,137 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 350
2017-08-10 09:16:35,138 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:35,138 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327785000 ms
2017-08-10 09:16:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327800000 ms
2017-08-10 09:16:40,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327800000 ms.0 from job set of time 1502327800000 ms
2017-08-10 09:16:40,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 177 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 177 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 177 (MapPartitionsRDD[355] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:40,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_177 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:16:40,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_177_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:16:40,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_177_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:16:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 177 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 177 (MapPartitionsRDD[355] at map at streamingProcess.scala:48)
2017-08-10 09:16:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 177.0 with 2 tasks
2017-08-10 09:16:40,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 177.0 (TID 354, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:40,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 177.0 (TID 355, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:40,031 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 177.0 (TID 354)
2017-08-10 09:16:40,032 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 177.0 (TID 355)
2017-08-10 09:16:40,033 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:40,035 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:40,036 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 177.0 (TID 354). 714 bytes result sent to driver
2017-08-10 09:16:40,037 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 177.0 (TID 355). 714 bytes result sent to driver
2017-08-10 09:16:40,037 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 177.0 (TID 354) in 7 ms on localhost (1/2)
2017-08-10 09:16:40,039 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 177.0 (TID 355) in 7 ms on localhost (2/2)
2017-08-10 09:16:40,039 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 177.0, whose tasks have all completed, from pool 
2017-08-10 09:16:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 177 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:16:40,039 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 177 finished: foreachPartition at streamingProcess.scala:50, took 0.015541 s
2017-08-10 09:16:40,039 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327800000 ms.0 from job set of time 1502327800000 ms
2017-08-10 09:16:40,039 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.039 s for time 1502327800000 ms (execution: 0.026 s)
2017-08-10 09:16:40,039 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 353 from persistence list
2017-08-10 09:16:40,039 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 352 from persistence list
2017-08-10 09:16:40,039 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 353
2017-08-10 09:16:40,040 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 352
2017-08-10 09:16:40,040 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:40,040 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327790000 ms
2017-08-10 09:16:45,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327805000 ms
2017-08-10 09:16:45,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327805000 ms.0 from job set of time 1502327805000 ms
2017-08-10 09:16:45,056 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:45,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 178 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:45,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 178 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 178 (MapPartitionsRDD[357] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:45,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_178 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:16:45,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_178_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:16:45,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_178_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:45,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 178 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:45,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 178 (MapPartitionsRDD[357] at map at streamingProcess.scala:48)
2017-08-10 09:16:45,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 178.0 with 2 tasks
2017-08-10 09:16:45,078 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 178.0 (TID 356, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:45,079 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 178.0 (TID 357, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:45,080 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 178.0 (TID 357)
2017-08-10 09:16:45,080 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 178.0 (TID 356)
2017-08-10 09:16:45,085 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:45,085 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:45,090 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 178.0 (TID 357). 714 bytes result sent to driver
2017-08-10 09:16:45,090 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 178.0 (TID 356). 714 bytes result sent to driver
2017-08-10 09:16:45,094 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 178.0 (TID 356) in 15 ms on localhost (1/2)
2017-08-10 09:16:45,094 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 178.0 (TID 357) in 15 ms on localhost (2/2)
2017-08-10 09:16:45,094 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 178.0, whose tasks have all completed, from pool 
2017-08-10 09:16:45,095 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 178 (foreachPartition at streamingProcess.scala:50) finished in 0.018 s
2017-08-10 09:16:45,095 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 178 finished: foreachPartition at streamingProcess.scala:50, took 0.038475 s
2017-08-10 09:16:45,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327805000 ms.0 from job set of time 1502327805000 ms
2017-08-10 09:16:45,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.096 s for time 1502327805000 ms (execution: 0.076 s)
2017-08-10 09:16:45,096 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 355 from persistence list
2017-08-10 09:16:45,097 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 355
2017-08-10 09:16:45,097 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 354 from persistence list
2017-08-10 09:16:45,097 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 354
2017-08-10 09:16:45,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:45,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327795000 ms
2017-08-10 09:16:50,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327810000 ms
2017-08-10 09:16:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327810000 ms.0 from job set of time 1502327810000 ms
2017-08-10 09:16:50,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 179 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 179 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:50,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 179 (MapPartitionsRDD[359] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_179 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:16:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_179_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:16:50,048 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_179_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 179 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 179 (MapPartitionsRDD[359] at map at streamingProcess.scala:48)
2017-08-10 09:16:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 179.0 with 2 tasks
2017-08-10 09:16:50,049 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 179.0 (TID 358, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:50,049 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 179.0 (TID 359, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:50,050 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 179.0 (TID 358)
2017-08-10 09:16:50,050 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 179.0 (TID 359)
2017-08-10 09:16:50,052 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:50,052 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:50,055 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 179.0 (TID 358). 714 bytes result sent to driver
2017-08-10 09:16:50,055 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 179.0 (TID 359). 714 bytes result sent to driver
2017-08-10 09:16:50,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 179.0 (TID 359) in 8 ms on localhost (1/2)
2017-08-10 09:16:50,057 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 179.0 (TID 358) in 8 ms on localhost (2/2)
2017-08-10 09:16:50,057 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 179.0, whose tasks have all completed, from pool 
2017-08-10 09:16:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 179 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:16:50,058 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 179 finished: foreachPartition at streamingProcess.scala:50, took 0.017592 s
2017-08-10 09:16:50,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327810000 ms.0 from job set of time 1502327810000 ms
2017-08-10 09:16:50,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.058 s for time 1502327810000 ms (execution: 0.037 s)
2017-08-10 09:16:50,058 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 357 from persistence list
2017-08-10 09:16:50,058 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 357
2017-08-10 09:16:50,058 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 356 from persistence list
2017-08-10 09:16:50,059 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 356
2017-08-10 09:16:50,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:50,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327800000 ms
2017-08-10 09:16:55,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327815000 ms
2017-08-10 09:16:55,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327815000 ms.0 from job set of time 1502327815000 ms
2017-08-10 09:16:55,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:16:55,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 180 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:16:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 180 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:16:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:16:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:16:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 180 (MapPartitionsRDD[361] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:16:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_180 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:16:55,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_180_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:16:55,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_180_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:16:55,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 180 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:16:55,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 180 (MapPartitionsRDD[361] at map at streamingProcess.scala:48)
2017-08-10 09:16:55,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 180.0 with 2 tasks
2017-08-10 09:16:55,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 180.0 (TID 360, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:16:55,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 180.0 (TID 361, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:16:55,074 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 180.0 (TID 360)
2017-08-10 09:16:55,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 180.0 (TID 361)
2017-08-10 09:16:55,077 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:16:55,077 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:16:55,081 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 180.0 (TID 361). 714 bytes result sent to driver
2017-08-10 09:16:55,081 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 180.0 (TID 360). 714 bytes result sent to driver
2017-08-10 09:16:55,084 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 180.0 (TID 361) in 10 ms on localhost (1/2)
2017-08-10 09:16:55,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 180.0 (TID 360) in 11 ms on localhost (2/2)
2017-08-10 09:16:55,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 180.0, whose tasks have all completed, from pool 
2017-08-10 09:16:55,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 180 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:16:55,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 180 finished: foreachPartition at streamingProcess.scala:50, took 0.031105 s
2017-08-10 09:16:55,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327815000 ms.0 from job set of time 1502327815000 ms
2017-08-10 09:16:55,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1502327815000 ms (execution: 0.065 s)
2017-08-10 09:16:55,086 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 359 from persistence list
2017-08-10 09:16:55,086 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 359
2017-08-10 09:16:55,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 358 from persistence list
2017-08-10 09:16:55,087 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 358
2017-08-10 09:16:55,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:16:55,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327805000 ms
2017-08-10 09:17:00,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327820000 ms
2017-08-10 09:17:00,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327820000 ms.0 from job set of time 1502327820000 ms
2017-08-10 09:17:00,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 181 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 181 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 181 (MapPartitionsRDD[363] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_181 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:17:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_181_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:17:00,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_181_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:17:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 181 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 181 (MapPartitionsRDD[363] at map at streamingProcess.scala:48)
2017-08-10 09:17:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 181.0 with 2 tasks
2017-08-10 09:17:00,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 181.0 (TID 362, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:00,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 181.0 (TID 363, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:00,064 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 181.0 (TID 363)
2017-08-10 09:17:00,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 181.0 (TID 362)
2017-08-10 09:17:00,068 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:00,068 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:00,071 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 181.0 (TID 363). 714 bytes result sent to driver
2017-08-10 09:17:00,071 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 181.0 (TID 362). 714 bytes result sent to driver
2017-08-10 09:17:00,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 181.0 (TID 363) in 9 ms on localhost (1/2)
2017-08-10 09:17:00,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 181.0 (TID 362) in 11 ms on localhost (2/2)
2017-08-10 09:17:00,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 181.0, whose tasks have all completed, from pool 
2017-08-10 09:17:00,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 181 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:17:00,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 181 finished: foreachPartition at streamingProcess.scala:50, took 0.022377 s
2017-08-10 09:17:00,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327820000 ms.0 from job set of time 1502327820000 ms
2017-08-10 09:17:00,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1502327820000 ms (execution: 0.056 s)
2017-08-10 09:17:00,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 361 from persistence list
2017-08-10 09:17:00,075 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 361
2017-08-10 09:17:00,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 360 from persistence list
2017-08-10 09:17:00,076 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 360
2017-08-10 09:17:00,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:00,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327810000 ms
2017-08-10 09:17:05,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327825000 ms
2017-08-10 09:17:05,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327825000 ms.0 from job set of time 1502327825000 ms
2017-08-10 09:17:05,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 182 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 182 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 182 (MapPartitionsRDD[365] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:05,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_182 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:17:05,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_182_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:17:05,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_182_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:05,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 182 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 182 (MapPartitionsRDD[365] at map at streamingProcess.scala:48)
2017-08-10 09:17:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 182.0 with 2 tasks
2017-08-10 09:17:05,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 182.0 (TID 364, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:05,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 182.0 (TID 365, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:05,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 182.0 (TID 365)
2017-08-10 09:17:05,077 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 182.0 (TID 364)
2017-08-10 09:17:05,082 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:05,082 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:05,088 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 182.0 (TID 365). 714 bytes result sent to driver
2017-08-10 09:17:05,088 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 182.0 (TID 364). 714 bytes result sent to driver
2017-08-10 09:17:05,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 182.0 (TID 365) in 15 ms on localhost (1/2)
2017-08-10 09:17:05,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 182.0 (TID 364) in 17 ms on localhost (2/2)
2017-08-10 09:17:05,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 182.0, whose tasks have all completed, from pool 
2017-08-10 09:17:05,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 182 (foreachPartition at streamingProcess.scala:50) finished in 0.018 s
2017-08-10 09:17:05,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 182 finished: foreachPartition at streamingProcess.scala:50, took 0.038201 s
2017-08-10 09:17:05,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327825000 ms.0 from job set of time 1502327825000 ms
2017-08-10 09:17:05,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.093 s for time 1502327825000 ms (execution: 0.073 s)
2017-08-10 09:17:05,094 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 363 from persistence list
2017-08-10 09:17:05,094 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 363
2017-08-10 09:17:05,094 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 362 from persistence list
2017-08-10 09:17:05,095 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 362
2017-08-10 09:17:05,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:05,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327815000 ms
2017-08-10 09:17:10,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327830000 ms
2017-08-10 09:17:10,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327830000 ms.0 from job set of time 1502327830000 ms
2017-08-10 09:17:10,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 183 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 183 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 183 (MapPartitionsRDD[367] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_183 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:17:10,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_183_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:17:10,053 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_183_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:10,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 183 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 183 (MapPartitionsRDD[367] at map at streamingProcess.scala:48)
2017-08-10 09:17:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 183.0 with 2 tasks
2017-08-10 09:17:10,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 183.0 (TID 366, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:10,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 183.0 (TID 367, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:10,055 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 183.0 (TID 367)
2017-08-10 09:17:10,055 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 183.0 (TID 366)
2017-08-10 09:17:10,057 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:10,057 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:10,060 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 183.0 (TID 366). 714 bytes result sent to driver
2017-08-10 09:17:10,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 183.0 (TID 367). 714 bytes result sent to driver
2017-08-10 09:17:10,062 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 183.0 (TID 366) in 8 ms on localhost (1/2)
2017-08-10 09:17:10,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 183.0 (TID 367) in 8 ms on localhost (2/2)
2017-08-10 09:17:10,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 183.0, whose tasks have all completed, from pool 
2017-08-10 09:17:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 183 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:17:10,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 183 finished: foreachPartition at streamingProcess.scala:50, took 0.017295 s
2017-08-10 09:17:10,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327830000 ms.0 from job set of time 1502327830000 ms
2017-08-10 09:17:10,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1502327830000 ms (execution: 0.045 s)
2017-08-10 09:17:10,063 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 365 from persistence list
2017-08-10 09:17:10,063 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 365
2017-08-10 09:17:10,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 364 from persistence list
2017-08-10 09:17:10,064 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 364
2017-08-10 09:17:10,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:10,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327820000 ms
2017-08-10 09:17:15,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327835000 ms
2017-08-10 09:17:15,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327835000 ms.0 from job set of time 1502327835000 ms
2017-08-10 09:17:15,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 184 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 184 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 184 (MapPartitionsRDD[369] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:15,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_184 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:17:15,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_184_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:17:15,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_184_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:15,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 184 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:15,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 184 (MapPartitionsRDD[369] at map at streamingProcess.scala:48)
2017-08-10 09:17:15,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 184.0 with 2 tasks
2017-08-10 09:17:15,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 184.0 (TID 368, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:15,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 184.0 (TID 369, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:15,067 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 184.0 (TID 368)
2017-08-10 09:17:15,067 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 184.0 (TID 369)
2017-08-10 09:17:15,070 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:15,070 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:15,074 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 184.0 (TID 369). 714 bytes result sent to driver
2017-08-10 09:17:15,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 184.0 (TID 368). 714 bytes result sent to driver
2017-08-10 09:17:15,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 184.0 (TID 368) in 11 ms on localhost (1/2)
2017-08-10 09:17:15,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 184.0 (TID 369) in 10 ms on localhost (2/2)
2017-08-10 09:17:15,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 184.0, whose tasks have all completed, from pool 
2017-08-10 09:17:15,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 184 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:17:15,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 184 finished: foreachPartition at streamingProcess.scala:50, took 0.022662 s
2017-08-10 09:17:15,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327835000 ms.0 from job set of time 1502327835000 ms
2017-08-10 09:17:15,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1502327835000 ms (execution: 0.058 s)
2017-08-10 09:17:15,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 367 from persistence list
2017-08-10 09:17:15,078 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 367
2017-08-10 09:17:15,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 366 from persistence list
2017-08-10 09:17:15,078 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 366
2017-08-10 09:17:15,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:15,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327825000 ms
2017-08-10 09:17:20,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327840000 ms
2017-08-10 09:17:20,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327840000 ms.0 from job set of time 1502327840000 ms
2017-08-10 09:17:20,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 185 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 185 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:20,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:20,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 185 (MapPartitionsRDD[371] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_185 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:17:20,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_185_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:17:20,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_185_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:20,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 185 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:20,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 185 (MapPartitionsRDD[371] at map at streamingProcess.scala:48)
2017-08-10 09:17:20,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 185.0 with 2 tasks
2017-08-10 09:17:20,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 185.0 (TID 370, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:20,076 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 185.0 (TID 371, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:20,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 185.0 (TID 370)
2017-08-10 09:17:20,077 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 185.0 (TID 371)
2017-08-10 09:17:20,083 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:20,083 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:20,089 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 185.0 (TID 371). 714 bytes result sent to driver
2017-08-10 09:17:20,089 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 185.0 (TID 370). 714 bytes result sent to driver
2017-08-10 09:17:20,092 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 185.0 (TID 371) in 16 ms on localhost (1/2)
2017-08-10 09:17:20,093 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 185.0 (TID 370) in 17 ms on localhost (2/2)
2017-08-10 09:17:20,093 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 185.0, whose tasks have all completed, from pool 
2017-08-10 09:17:20,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 185 (foreachPartition at streamingProcess.scala:50) finished in 0.019 s
2017-08-10 09:17:20,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 185 finished: foreachPartition at streamingProcess.scala:50, took 0.039601 s
2017-08-10 09:17:20,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327840000 ms.0 from job set of time 1502327840000 ms
2017-08-10 09:17:20,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.093 s for time 1502327840000 ms (execution: 0.074 s)
2017-08-10 09:17:20,094 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 369 from persistence list
2017-08-10 09:17:20,094 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 369
2017-08-10 09:17:20,094 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 368 from persistence list
2017-08-10 09:17:20,095 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 368
2017-08-10 09:17:20,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:20,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327830000 ms
2017-08-10 09:17:25,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327845000 ms
2017-08-10 09:17:25,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327845000 ms.0 from job set of time 1502327845000 ms
2017-08-10 09:17:25,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 186 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 186 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 186 (MapPartitionsRDD[373] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_186 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:17:25,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_186_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:17:25,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_186_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:25,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 186 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:25,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 186 (MapPartitionsRDD[373] at map at streamingProcess.scala:48)
2017-08-10 09:17:25,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 186.0 with 2 tasks
2017-08-10 09:17:25,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 186.0 (TID 372, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:25,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 186.0 (TID 373, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:25,055 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 186.0 (TID 372)
2017-08-10 09:17:25,055 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 186.0 (TID 373)
2017-08-10 09:17:25,058 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:25,058 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:25,061 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 186.0 (TID 373). 714 bytes result sent to driver
2017-08-10 09:17:25,061 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 186.0 (TID 372). 714 bytes result sent to driver
2017-08-10 09:17:25,063 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 186.0 (TID 373) in 8 ms on localhost (1/2)
2017-08-10 09:17:25,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 186.0 (TID 372) in 9 ms on localhost (2/2)
2017-08-10 09:17:25,064 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 186.0, whose tasks have all completed, from pool 
2017-08-10 09:17:25,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 186 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:17:25,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 186 finished: foreachPartition at streamingProcess.scala:50, took 0.019028 s
2017-08-10 09:17:25,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327845000 ms.0 from job set of time 1502327845000 ms
2017-08-10 09:17:25,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1502327845000 ms (execution: 0.048 s)
2017-08-10 09:17:25,065 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 371 from persistence list
2017-08-10 09:17:25,065 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 371
2017-08-10 09:17:25,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 370 from persistence list
2017-08-10 09:17:25,065 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 370
2017-08-10 09:17:25,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:25,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327835000 ms
2017-08-10 09:17:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327850000 ms
2017-08-10 09:17:30,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327850000 ms.0 from job set of time 1502327850000 ms
2017-08-10 09:17:30,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 187 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 187 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 187 (MapPartitionsRDD[375] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_187 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:17:30,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_187_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:17:30,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_187_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:30,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 187 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:30,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 187 (MapPartitionsRDD[375] at map at streamingProcess.scala:48)
2017-08-10 09:17:30,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 187.0 with 2 tasks
2017-08-10 09:17:30,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 187.0 (TID 374, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:30,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 187.0 (TID 375, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:30,075 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 187.0 (TID 375)
2017-08-10 09:17:30,075 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 187.0 (TID 374)
2017-08-10 09:17:30,080 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:30,080 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:30,084 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 187.0 (TID 375). 714 bytes result sent to driver
2017-08-10 09:17:30,085 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 187.0 (TID 374). 714 bytes result sent to driver
2017-08-10 09:17:30,087 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 187.0 (TID 375) in 13 ms on localhost (1/2)
2017-08-10 09:17:30,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 187.0 (TID 374) in 16 ms on localhost (2/2)
2017-08-10 09:17:30,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 187.0, whose tasks have all completed, from pool 
2017-08-10 09:17:30,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 187 (foreachPartition at streamingProcess.scala:50) finished in 0.016 s
2017-08-10 09:17:30,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 187 finished: foreachPartition at streamingProcess.scala:50, took 0.034897 s
2017-08-10 09:17:30,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327850000 ms.0 from job set of time 1502327850000 ms
2017-08-10 09:17:30,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1502327850000 ms (execution: 0.069 s)
2017-08-10 09:17:30,089 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 373 from persistence list
2017-08-10 09:17:30,090 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 373
2017-08-10 09:17:30,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 372 from persistence list
2017-08-10 09:17:30,090 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 372
2017-08-10 09:17:30,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:30,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327840000 ms
2017-08-10 09:17:35,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327855000 ms
2017-08-10 09:17:35,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327855000 ms.0 from job set of time 1502327855000 ms
2017-08-10 09:17:35,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:35,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 188 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:35,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 188 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:35,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:35,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:35,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 188 (MapPartitionsRDD[377] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_188 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:17:35,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_188_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:17:35,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_188_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:35,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 188 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:35,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 188 (MapPartitionsRDD[377] at map at streamingProcess.scala:48)
2017-08-10 09:17:35,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 188.0 with 2 tasks
2017-08-10 09:17:35,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 188.0 (TID 376, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:35,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 188.0 (TID 377, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:35,055 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 188.0 (TID 376)
2017-08-10 09:17:35,055 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 188.0 (TID 377)
2017-08-10 09:17:35,059 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:35,059 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:35,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 188.0 (TID 377). 714 bytes result sent to driver
2017-08-10 09:17:35,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 188.0 (TID 376). 714 bytes result sent to driver
2017-08-10 09:17:35,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 188.0 (TID 377) in 11 ms on localhost (1/2)
2017-08-10 09:17:35,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 188.0 (TID 376) in 13 ms on localhost (2/2)
2017-08-10 09:17:35,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 188.0, whose tasks have all completed, from pool 
2017-08-10 09:17:35,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 188 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-10 09:17:35,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 188 finished: foreachPartition at streamingProcess.scala:50, took 0.027509 s
2017-08-10 09:17:35,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327855000 ms.0 from job set of time 1502327855000 ms
2017-08-10 09:17:35,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1502327855000 ms (execution: 0.051 s)
2017-08-10 09:17:35,067 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 375 from persistence list
2017-08-10 09:17:35,068 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 375
2017-08-10 09:17:35,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 374 from persistence list
2017-08-10 09:17:35,069 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 374
2017-08-10 09:17:35,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:35,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327845000 ms
2017-08-10 09:17:40,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327860000 ms
2017-08-10 09:17:40,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327860000 ms.0 from job set of time 1502327860000 ms
2017-08-10 09:17:40,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 189 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 189 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 189 (MapPartitionsRDD[379] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:40,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_189 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:17:40,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_189_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:17:40,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_189_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 189 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 189 (MapPartitionsRDD[379] at map at streamingProcess.scala:48)
2017-08-10 09:17:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 189.0 with 2 tasks
2017-08-10 09:17:40,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 189.0 (TID 378, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:40,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 189.0 (TID 379, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:40,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 189.0 (TID 378)
2017-08-10 09:17:40,074 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 189.0 (TID 379)
2017-08-10 09:17:40,077 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:40,077 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:40,080 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 189.0 (TID 378). 714 bytes result sent to driver
2017-08-10 09:17:40,080 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 189.0 (TID 379). 714 bytes result sent to driver
2017-08-10 09:17:40,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 189.0 (TID 379) in 8 ms on localhost (1/2)
2017-08-10 09:17:40,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 189.0 (TID 378) in 10 ms on localhost (2/2)
2017-08-10 09:17:40,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 189.0, whose tasks have all completed, from pool 
2017-08-10 09:17:40,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 189 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:17:40,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 189 finished: foreachPartition at streamingProcess.scala:50, took 0.027758 s
2017-08-10 09:17:40,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327860000 ms.0 from job set of time 1502327860000 ms
2017-08-10 09:17:40,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1502327860000 ms (execution: 0.063 s)
2017-08-10 09:17:40,084 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 377 from persistence list
2017-08-10 09:17:40,084 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 377
2017-08-10 09:17:40,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 376 from persistence list
2017-08-10 09:17:40,084 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 376
2017-08-10 09:17:40,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:40,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327850000 ms
2017-08-10 09:17:45,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327865000 ms
2017-08-10 09:17:45,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327865000 ms.0 from job set of time 1502327865000 ms
2017-08-10 09:17:45,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 190 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 190 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 190 (MapPartitionsRDD[381] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_190 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:17:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_190_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:17:45,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_190_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:17:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 190 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 190 (MapPartitionsRDD[381] at map at streamingProcess.scala:48)
2017-08-10 09:17:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 190.0 with 2 tasks
2017-08-10 09:17:45,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 190.0 (TID 380, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:45,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 190.0 (TID 381, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:45,057 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 190.0 (TID 381)
2017-08-10 09:17:45,057 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 190.0 (TID 380)
2017-08-10 09:17:45,059 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:45,059 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:45,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 190.0 (TID 381). 714 bytes result sent to driver
2017-08-10 09:17:45,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 190.0 (TID 380). 714 bytes result sent to driver
2017-08-10 09:17:45,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 190.0 (TID 381) in 7 ms on localhost (1/2)
2017-08-10 09:17:45,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 190.0 (TID 380) in 9 ms on localhost (2/2)
2017-08-10 09:17:45,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 190.0, whose tasks have all completed, from pool 
2017-08-10 09:17:45,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 190 (foreachPartition at streamingProcess.scala:50) finished in 0.009 s
2017-08-10 09:17:45,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 190 finished: foreachPartition at streamingProcess.scala:50, took 0.020680 s
2017-08-10 09:17:45,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327865000 ms.0 from job set of time 1502327865000 ms
2017-08-10 09:17:45,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1502327865000 ms (execution: 0.047 s)
2017-08-10 09:17:45,066 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 379 from persistence list
2017-08-10 09:17:45,066 [block-manager-slave-async-thread-pool-17] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 379
2017-08-10 09:17:45,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 378 from persistence list
2017-08-10 09:17:45,066 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 378
2017-08-10 09:17:45,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:45,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327855000 ms
2017-08-10 09:17:50,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327870000 ms
2017-08-10 09:17:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327870000 ms.0 from job set of time 1502327870000 ms
2017-08-10 09:17:50,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 191 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 191 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 191 (MapPartitionsRDD[383] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:50,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_191 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:17:50,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_191_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:17:50,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_176_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:50,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_191_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:17:50,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 191 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:50,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 191 (MapPartitionsRDD[383] at map at streamingProcess.scala:48)
2017-08-10 09:17:50,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 191.0 with 2 tasks
2017-08-10 09:17:50,093 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_177_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:50,093 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 191.0 (TID 382, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:50,093 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 191.0 (TID 383, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:50,093 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_178_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:50,093 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 191.0 (TID 382)
2017-08-10 09:17:50,093 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 191.0 (TID 383)
2017-08-10 09:17:50,094 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_179_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:50,095 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_180_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:17:50,095 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:50,095 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:50,096 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_181_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:50,097 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_182_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:50,098 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_183_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:50,098 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 191.0 (TID 383). 714 bytes result sent to driver
2017-08-10 09:17:50,098 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 191.0 (TID 382). 714 bytes result sent to driver
2017-08-10 09:17:50,099 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_184_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:17:50,100 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 191.0 (TID 383) in 7 ms on localhost (1/2)
2017-08-10 09:17:50,100 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_185_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:17:50,100 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 191.0 (TID 382) in 7 ms on localhost (2/2)
2017-08-10 09:17:50,100 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 191.0, whose tasks have all completed, from pool 
2017-08-10 09:17:50,100 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 191 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:17:50,100 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 191 finished: foreachPartition at streamingProcess.scala:50, took 0.045406 s
2017-08-10 09:17:50,100 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_186_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:17:50,100 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327870000 ms.0 from job set of time 1502327870000 ms
2017-08-10 09:17:50,100 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.100 s for time 1502327870000 ms (execution: 0.079 s)
2017-08-10 09:17:50,100 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 381 from persistence list
2017-08-10 09:17:50,101 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 381
2017-08-10 09:17:50,101 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 380 from persistence list
2017-08-10 09:17:50,101 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 380
2017-08-10 09:17:50,101 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:50,101 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_187_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:17:50,101 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327860000 ms
2017-08-10 09:17:50,102 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_188_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:17:50,104 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_189_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:17:50,105 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_190_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:17:55,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327875000 ms
2017-08-10 09:17:55,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327875000 ms.0 from job set of time 1502327875000 ms
2017-08-10 09:17:55,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:17:55,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 192 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:17:55,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 192 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:17:55,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:17:55,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:17:55,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 192 (MapPartitionsRDD[385] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:17:55,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_192 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:17:55,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_192_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:17:55,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_192_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:17:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 192 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:17:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 192 (MapPartitionsRDD[385] at map at streamingProcess.scala:48)
2017-08-10 09:17:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 192.0 with 2 tasks
2017-08-10 09:17:55,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 192.0 (TID 384, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:17:55,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 192.0 (TID 385, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:17:55,060 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 192.0 (TID 385)
2017-08-10 09:17:55,060 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 192.0 (TID 384)
2017-08-10 09:17:55,062 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:17:55,063 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:17:55,066 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 192.0 (TID 384). 714 bytes result sent to driver
2017-08-10 09:17:55,066 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 192.0 (TID 385). 714 bytes result sent to driver
2017-08-10 09:17:55,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 192.0 (TID 385) in 9 ms on localhost (1/2)
2017-08-10 09:17:55,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 192.0 (TID 384) in 10 ms on localhost (2/2)
2017-08-10 09:17:55,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 192.0, whose tasks have all completed, from pool 
2017-08-10 09:17:55,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 192 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:17:55,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 192 finished: foreachPartition at streamingProcess.scala:50, took 0.021325 s
2017-08-10 09:17:55,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327875000 ms.0 from job set of time 1502327875000 ms
2017-08-10 09:17:55,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1502327875000 ms (execution: 0.050 s)
2017-08-10 09:17:55,070 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 383 from persistence list
2017-08-10 09:17:55,070 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 383
2017-08-10 09:17:55,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 382 from persistence list
2017-08-10 09:17:55,070 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 382
2017-08-10 09:17:55,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:17:55,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327865000 ms
2017-08-10 09:18:00,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327880000 ms
2017-08-10 09:18:00,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327880000 ms.0 from job set of time 1502327880000 ms
2017-08-10 09:18:00,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 193 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 193 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 193 (MapPartitionsRDD[387] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_193 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:18:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_193_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:18:00,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_193_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:18:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 193 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 193 (MapPartitionsRDD[387] at map at streamingProcess.scala:48)
2017-08-10 09:18:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 193.0 with 2 tasks
2017-08-10 09:18:00,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 193.0 (TID 386, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:00,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 193.0 (TID 387, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:00,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 193.0 (TID 387)
2017-08-10 09:18:00,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 193.0 (TID 386)
2017-08-10 09:18:00,066 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:00,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:00,070 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 193.0 (TID 387). 714 bytes result sent to driver
2017-08-10 09:18:00,070 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 193.0 (TID 386). 714 bytes result sent to driver
2017-08-10 09:18:00,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 193.0 (TID 387) in 9 ms on localhost (1/2)
2017-08-10 09:18:00,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 193.0 (TID 386) in 10 ms on localhost (2/2)
2017-08-10 09:18:00,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 193.0, whose tasks have all completed, from pool 
2017-08-10 09:18:00,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 193 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:18:00,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 193 finished: foreachPartition at streamingProcess.scala:50, took 0.021657 s
2017-08-10 09:18:00,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327880000 ms.0 from job set of time 1502327880000 ms
2017-08-10 09:18:00,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1502327880000 ms (execution: 0.055 s)
2017-08-10 09:18:00,073 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 385 from persistence list
2017-08-10 09:18:00,073 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 385
2017-08-10 09:18:00,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 384 from persistence list
2017-08-10 09:18:00,074 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 384
2017-08-10 09:18:00,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:00,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327870000 ms
2017-08-10 09:18:05,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327885000 ms
2017-08-10 09:18:05,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327885000 ms.0 from job set of time 1502327885000 ms
2017-08-10 09:18:05,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 194 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 194 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 194 (MapPartitionsRDD[389] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:05,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_194 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:18:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_194_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:18:05,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_194_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:18:05,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 194 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:05,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 194 (MapPartitionsRDD[389] at map at streamingProcess.scala:48)
2017-08-10 09:18:05,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 194.0 with 2 tasks
2017-08-10 09:18:05,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 194.0 (TID 388, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:05,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 194.0 (TID 389, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:05,059 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 194.0 (TID 389)
2017-08-10 09:18:05,059 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 194.0 (TID 388)
2017-08-10 09:18:05,063 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:05,063 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:05,067 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 194.0 (TID 388). 714 bytes result sent to driver
2017-08-10 09:18:05,067 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 194.0 (TID 389). 714 bytes result sent to driver
2017-08-10 09:18:05,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 194.0 (TID 388) in 11 ms on localhost (1/2)
2017-08-10 09:18:05,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 194.0 (TID 389) in 10 ms on localhost (2/2)
2017-08-10 09:18:05,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 194.0, whose tasks have all completed, from pool 
2017-08-10 09:18:05,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 194 (foreachPartition at streamingProcess.scala:50) finished in 0.012 s
2017-08-10 09:18:05,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 194 finished: foreachPartition at streamingProcess.scala:50, took 0.026009 s
2017-08-10 09:18:05,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327885000 ms.0 from job set of time 1502327885000 ms
2017-08-10 09:18:05,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1502327885000 ms (execution: 0.053 s)
2017-08-10 09:18:05,071 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 387 from persistence list
2017-08-10 09:18:05,071 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 387
2017-08-10 09:18:05,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 386 from persistence list
2017-08-10 09:18:05,072 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 386
2017-08-10 09:18:05,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:05,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327875000 ms
2017-08-10 09:18:10,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327890000 ms
2017-08-10 09:18:10,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327890000 ms.0 from job set of time 1502327890000 ms
2017-08-10 09:18:10,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 195 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 195 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 195 (MapPartitionsRDD[391] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_195 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:18:10,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_195_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:18:10,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_195_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:18:10,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 195 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:10,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 195 (MapPartitionsRDD[391] at map at streamingProcess.scala:48)
2017-08-10 09:18:10,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 195.0 with 2 tasks
2017-08-10 09:18:10,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 195.0 (TID 390, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:10,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 195.0 (TID 391, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:10,059 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 195.0 (TID 390)
2017-08-10 09:18:10,059 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 195.0 (TID 391)
2017-08-10 09:18:10,062 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:10,062 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:10,066 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 195.0 (TID 391). 714 bytes result sent to driver
2017-08-10 09:18:10,066 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 195.0 (TID 390). 714 bytes result sent to driver
2017-08-10 09:18:10,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 195.0 (TID 391) in 9 ms on localhost (1/2)
2017-08-10 09:18:10,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 195.0 (TID 390) in 10 ms on localhost (2/2)
2017-08-10 09:18:10,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 195 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:18:10,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 195.0, whose tasks have all completed, from pool 
2017-08-10 09:18:10,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 195 finished: foreachPartition at streamingProcess.scala:50, took 0.021892 s
2017-08-10 09:18:10,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327890000 ms.0 from job set of time 1502327890000 ms
2017-08-10 09:18:10,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1502327890000 ms (execution: 0.051 s)
2017-08-10 09:18:10,070 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 389 from persistence list
2017-08-10 09:18:10,070 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 389
2017-08-10 09:18:10,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 388 from persistence list
2017-08-10 09:18:10,071 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 388
2017-08-10 09:18:10,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:10,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327880000 ms
2017-08-10 09:18:15,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327895000 ms
2017-08-10 09:18:15,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327895000 ms.0 from job set of time 1502327895000 ms
2017-08-10 09:18:15,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 196 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 196 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 196 (MapPartitionsRDD[393] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:15,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_196 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:18:15,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_196_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:18:15,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_196_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:18:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 196 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 196 (MapPartitionsRDD[393] at map at streamingProcess.scala:48)
2017-08-10 09:18:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 196.0 with 2 tasks
2017-08-10 09:18:15,064 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 196.0 (TID 392, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:15,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 196.0 (TID 393, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:15,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 196.0 (TID 393)
2017-08-10 09:18:15,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 196.0 (TID 392)
2017-08-10 09:18:15,068 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:15,068 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:15,071 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 196.0 (TID 393). 714 bytes result sent to driver
2017-08-10 09:18:15,071 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 196.0 (TID 392). 714 bytes result sent to driver
2017-08-10 09:18:15,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 196.0 (TID 393) in 9 ms on localhost (1/2)
2017-08-10 09:18:15,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 196.0 (TID 392) in 10 ms on localhost (2/2)
2017-08-10 09:18:15,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 196.0, whose tasks have all completed, from pool 
2017-08-10 09:18:15,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 196 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:18:15,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 196 finished: foreachPartition at streamingProcess.scala:50, took 0.021758 s
2017-08-10 09:18:15,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327895000 ms.0 from job set of time 1502327895000 ms
2017-08-10 09:18:15,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1502327895000 ms (execution: 0.056 s)
2017-08-10 09:18:15,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 391 from persistence list
2017-08-10 09:18:15,075 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 391
2017-08-10 09:18:15,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 390 from persistence list
2017-08-10 09:18:15,076 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 390
2017-08-10 09:18:15,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:15,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327885000 ms
2017-08-10 09:18:20,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327900000 ms
2017-08-10 09:18:20,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327900000 ms.0 from job set of time 1502327900000 ms
2017-08-10 09:18:20,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 197 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 197 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 197 (MapPartitionsRDD[395] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:20,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_197 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:18:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_197_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:18:20,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_197_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:18:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 197 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 197 (MapPartitionsRDD[395] at map at streamingProcess.scala:48)
2017-08-10 09:18:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 197.0 with 2 tasks
2017-08-10 09:18:20,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 197.0 (TID 394, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:20,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 197.0 (TID 395, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:20,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 197.0 (TID 395)
2017-08-10 09:18:20,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 197.0 (TID 394)
2017-08-10 09:18:20,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:20,066 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:20,071 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 197.0 (TID 395). 714 bytes result sent to driver
2017-08-10 09:18:20,071 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 197.0 (TID 394). 714 bytes result sent to driver
2017-08-10 09:18:20,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 197.0 (TID 395) in 12 ms on localhost (1/2)
2017-08-10 09:18:20,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 197.0 (TID 394) in 13 ms on localhost (2/2)
2017-08-10 09:18:20,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 197.0, whose tasks have all completed, from pool 
2017-08-10 09:18:20,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 197 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-10 09:18:20,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 197 finished: foreachPartition at streamingProcess.scala:50, took 0.029346 s
2017-08-10 09:18:20,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327900000 ms.0 from job set of time 1502327900000 ms
2017-08-10 09:18:20,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1502327900000 ms (execution: 0.058 s)
2017-08-10 09:18:20,076 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 393 from persistence list
2017-08-10 09:18:20,076 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 393
2017-08-10 09:18:20,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 392 from persistence list
2017-08-10 09:18:20,077 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 392
2017-08-10 09:18:20,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:20,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327890000 ms
2017-08-10 09:18:25,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327905000 ms
2017-08-10 09:18:25,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327905000 ms.0 from job set of time 1502327905000 ms
2017-08-10 09:18:25,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:25,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 198 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:25,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 198 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:25,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:25,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 198 (MapPartitionsRDD[397] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:25,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_198 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:18:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_198_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:18:25,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_198_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:18:25,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 198 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:25,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 198 (MapPartitionsRDD[397] at map at streamingProcess.scala:48)
2017-08-10 09:18:25,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 198.0 with 2 tasks
2017-08-10 09:18:25,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 198.0 (TID 396, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:25,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 198.0 (TID 397, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:25,059 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 198.0 (TID 396)
2017-08-10 09:18:25,059 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 198.0 (TID 397)
2017-08-10 09:18:25,061 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:25,062 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:25,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 198.0 (TID 397). 714 bytes result sent to driver
2017-08-10 09:18:25,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 198.0 (TID 396). 714 bytes result sent to driver
2017-08-10 09:18:25,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 198.0 (TID 397) in 9 ms on localhost (1/2)
2017-08-10 09:18:25,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 198.0 (TID 396) in 10 ms on localhost (2/2)
2017-08-10 09:18:25,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 198.0, whose tasks have all completed, from pool 
2017-08-10 09:18:25,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 198 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:18:25,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 198 finished: foreachPartition at streamingProcess.scala:50, took 0.021178 s
2017-08-10 09:18:25,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327905000 ms.0 from job set of time 1502327905000 ms
2017-08-10 09:18:25,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1502327905000 ms (execution: 0.050 s)
2017-08-10 09:18:25,069 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 395 from persistence list
2017-08-10 09:18:25,069 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 395
2017-08-10 09:18:25,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 394 from persistence list
2017-08-10 09:18:25,069 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 394
2017-08-10 09:18:25,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:25,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327895000 ms
2017-08-10 09:18:30,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327910000 ms
2017-08-10 09:18:30,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327910000 ms.0 from job set of time 1502327910000 ms
2017-08-10 09:18:30,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 199 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 199 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 199 (MapPartitionsRDD[399] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:30,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_199 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:18:30,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_199_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:18:30,033 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_199_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:18:30,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 199 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 199 (MapPartitionsRDD[399] at map at streamingProcess.scala:48)
2017-08-10 09:18:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 199.0 with 2 tasks
2017-08-10 09:18:30,034 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 199.0 (TID 398, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:30,034 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 199.0 (TID 399, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:30,035 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 199.0 (TID 398)
2017-08-10 09:18:30,035 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 199.0 (TID 399)
2017-08-10 09:18:30,036 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:30,036 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:30,039 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 199.0 (TID 399). 714 bytes result sent to driver
2017-08-10 09:18:30,039 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 199.0 (TID 398). 714 bytes result sent to driver
2017-08-10 09:18:30,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 199.0 (TID 399) in 6 ms on localhost (1/2)
2017-08-10 09:18:30,040 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 199.0 (TID 398) in 6 ms on localhost (2/2)
2017-08-10 09:18:30,041 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 199.0, whose tasks have all completed, from pool 
2017-08-10 09:18:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 199 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:18:30,041 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 199 finished: foreachPartition at streamingProcess.scala:50, took 0.013302 s
2017-08-10 09:18:30,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327910000 ms.0 from job set of time 1502327910000 ms
2017-08-10 09:18:30,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1502327910000 ms (execution: 0.027 s)
2017-08-10 09:18:30,041 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 397 from persistence list
2017-08-10 09:18:30,042 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 397
2017-08-10 09:18:30,042 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 396 from persistence list
2017-08-10 09:18:30,042 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 396
2017-08-10 09:18:30,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:30,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327900000 ms
2017-08-10 09:18:35,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327915000 ms
2017-08-10 09:18:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327915000 ms.0 from job set of time 1502327915000 ms
2017-08-10 09:18:35,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:35,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 200 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:35,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 200 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:35,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:35,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:35,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 200 (MapPartitionsRDD[401] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:35,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_200 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:18:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_200_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:18:35,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_200_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:18:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 200 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 200 (MapPartitionsRDD[401] at map at streamingProcess.scala:48)
2017-08-10 09:18:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 200.0 with 2 tasks
2017-08-10 09:18:35,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 200.0 (TID 400, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:35,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 200.0 (TID 401, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:35,062 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 200.0 (TID 401)
2017-08-10 09:18:35,062 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 200.0 (TID 400)
2017-08-10 09:18:35,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:35,066 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:35,070 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 200.0 (TID 400). 714 bytes result sent to driver
2017-08-10 09:18:35,070 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 200.0 (TID 401). 714 bytes result sent to driver
2017-08-10 09:18:35,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 200.0 (TID 400) in 13 ms on localhost (1/2)
2017-08-10 09:18:35,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 200.0 (TID 401) in 12 ms on localhost (2/2)
2017-08-10 09:18:35,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 200.0, whose tasks have all completed, from pool 
2017-08-10 09:18:35,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 200 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-10 09:18:35,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 200 finished: foreachPartition at streamingProcess.scala:50, took 0.026860 s
2017-08-10 09:18:35,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327915000 ms.0 from job set of time 1502327915000 ms
2017-08-10 09:18:35,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1502327915000 ms (execution: 0.053 s)
2017-08-10 09:18:35,074 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 399 from persistence list
2017-08-10 09:18:35,075 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 399
2017-08-10 09:18:35,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 398 from persistence list
2017-08-10 09:18:35,075 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 398
2017-08-10 09:18:35,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:35,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327905000 ms
2017-08-10 09:18:40,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327920000 ms
2017-08-10 09:18:40,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327920000 ms.0 from job set of time 1502327920000 ms
2017-08-10 09:18:40,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 201 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 201 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 201 (MapPartitionsRDD[403] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_201 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:18:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_201_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:18:40,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_201_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:18:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 201 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 201 (MapPartitionsRDD[403] at map at streamingProcess.scala:48)
2017-08-10 09:18:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 201.0 with 2 tasks
2017-08-10 09:18:40,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 201.0 (TID 402, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:40,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 201.0 (TID 403, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:40,058 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 201.0 (TID 403)
2017-08-10 09:18:40,058 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 201.0 (TID 402)
2017-08-10 09:18:40,061 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:40,061 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:40,065 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 201.0 (TID 402). 714 bytes result sent to driver
2017-08-10 09:18:40,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 201.0 (TID 403). 714 bytes result sent to driver
2017-08-10 09:18:40,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 201.0 (TID 402) in 10 ms on localhost (1/2)
2017-08-10 09:18:40,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 201.0 (TID 403) in 9 ms on localhost (2/2)
2017-08-10 09:18:40,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 201.0, whose tasks have all completed, from pool 
2017-08-10 09:18:40,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 201 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:18:40,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 201 finished: foreachPartition at streamingProcess.scala:50, took 0.020780 s
2017-08-10 09:18:40,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327920000 ms.0 from job set of time 1502327920000 ms
2017-08-10 09:18:40,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1502327920000 ms (execution: 0.049 s)
2017-08-10 09:18:40,068 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 401 from persistence list
2017-08-10 09:18:40,068 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 401
2017-08-10 09:18:40,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 400 from persistence list
2017-08-10 09:18:40,069 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 400
2017-08-10 09:18:40,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:40,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327910000 ms
2017-08-10 09:18:45,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327925000 ms
2017-08-10 09:18:45,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327925000 ms.0 from job set of time 1502327925000 ms
2017-08-10 09:18:45,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 202 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 202 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 202 (MapPartitionsRDD[405] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:45,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_202 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:18:45,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_202_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:18:45,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_202_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:18:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 202 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 202 (MapPartitionsRDD[405] at map at streamingProcess.scala:48)
2017-08-10 09:18:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 202.0 with 2 tasks
2017-08-10 09:18:45,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 202.0 (TID 404, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:45,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 202.0 (TID 405, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:45,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 202.0 (TID 404)
2017-08-10 09:18:45,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 202.0 (TID 405)
2017-08-10 09:18:45,067 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:45,067 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:45,071 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 202.0 (TID 404). 714 bytes result sent to driver
2017-08-10 09:18:45,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 202.0 (TID 405). 714 bytes result sent to driver
2017-08-10 09:18:45,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 202.0 (TID 405) in 12 ms on localhost (1/2)
2017-08-10 09:18:45,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 202.0 (TID 404) in 14 ms on localhost (2/2)
2017-08-10 09:18:45,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 202.0, whose tasks have all completed, from pool 
2017-08-10 09:18:45,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 202 (foreachPartition at streamingProcess.scala:50) finished in 0.015 s
2017-08-10 09:18:45,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 202 finished: foreachPartition at streamingProcess.scala:50, took 0.031642 s
2017-08-10 09:18:45,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327925000 ms.0 from job set of time 1502327925000 ms
2017-08-10 09:18:45,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1502327925000 ms (execution: 0.058 s)
2017-08-10 09:18:45,078 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 403 from persistence list
2017-08-10 09:18:45,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 402 from persistence list
2017-08-10 09:18:45,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:45,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327915000 ms
2017-08-10 09:18:45,081 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 402
2017-08-10 09:18:45,081 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 403
2017-08-10 09:18:50,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327930000 ms
2017-08-10 09:18:50,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327930000 ms.0 from job set of time 1502327930000 ms
2017-08-10 09:18:50,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 203 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 203 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 203 (MapPartitionsRDD[407] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:50,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_203 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:18:50,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_203_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:18:50,073 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_203_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:18:50,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 203 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:50,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 203 (MapPartitionsRDD[407] at map at streamingProcess.scala:48)
2017-08-10 09:18:50,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 203.0 with 2 tasks
2017-08-10 09:18:50,076 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 203.0 (TID 406, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:50,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 203.0 (TID 407, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:50,078 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 203.0 (TID 407)
2017-08-10 09:18:50,078 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 203.0 (TID 406)
2017-08-10 09:18:50,083 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:50,083 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:50,089 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 203.0 (TID 406). 714 bytes result sent to driver
2017-08-10 09:18:50,089 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 203.0 (TID 407). 714 bytes result sent to driver
2017-08-10 09:18:50,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 203.0 (TID 407) in 17 ms on localhost (1/2)
2017-08-10 09:18:50,093 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 203.0 (TID 406) in 18 ms on localhost (2/2)
2017-08-10 09:18:50,094 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 203 (foreachPartition at streamingProcess.scala:50) finished in 0.019 s
2017-08-10 09:18:50,094 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 203.0, whose tasks have all completed, from pool 
2017-08-10 09:18:50,094 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 203 finished: foreachPartition at streamingProcess.scala:50, took 0.040459 s
2017-08-10 09:18:50,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327930000 ms.0 from job set of time 1502327930000 ms
2017-08-10 09:18:50,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.095 s for time 1502327930000 ms (execution: 0.076 s)
2017-08-10 09:18:50,095 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 405 from persistence list
2017-08-10 09:18:50,095 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 405
2017-08-10 09:18:50,095 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 404 from persistence list
2017-08-10 09:18:50,096 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 404
2017-08-10 09:18:50,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:50,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327920000 ms
2017-08-10 09:18:55,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327935000 ms
2017-08-10 09:18:55,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327935000 ms.0 from job set of time 1502327935000 ms
2017-08-10 09:18:55,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:18:55,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 204 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:18:55,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 204 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:18:55,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:18:55,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:18:55,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 204 (MapPartitionsRDD[409] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:18:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_204 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:18:55,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_204_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:18:55,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_204_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:18:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 204 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:18:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 204 (MapPartitionsRDD[409] at map at streamingProcess.scala:48)
2017-08-10 09:18:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 204.0 with 2 tasks
2017-08-10 09:18:55,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 204.0 (TID 408, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:18:55,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 204.0 (TID 409, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:18:55,059 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 204.0 (TID 409)
2017-08-10 09:18:55,060 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 204.0 (TID 408)
2017-08-10 09:18:55,062 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:18:55,062 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:18:55,066 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 204.0 (TID 408). 714 bytes result sent to driver
2017-08-10 09:18:55,066 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 204.0 (TID 409). 714 bytes result sent to driver
2017-08-10 09:18:55,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 204.0 (TID 408) in 10 ms on localhost (1/2)
2017-08-10 09:18:55,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 204.0 (TID 409) in 10 ms on localhost (2/2)
2017-08-10 09:18:55,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 204.0, whose tasks have all completed, from pool 
2017-08-10 09:18:55,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 204 (foreachPartition at streamingProcess.scala:50) finished in 0.011 s
2017-08-10 09:18:55,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 204 finished: foreachPartition at streamingProcess.scala:50, took 0.022442 s
2017-08-10 09:18:55,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327935000 ms.0 from job set of time 1502327935000 ms
2017-08-10 09:18:55,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1502327935000 ms (execution: 0.051 s)
2017-08-10 09:18:55,070 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 407 from persistence list
2017-08-10 09:18:55,071 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 407
2017-08-10 09:18:55,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 406 from persistence list
2017-08-10 09:18:55,071 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 406
2017-08-10 09:18:55,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:18:55,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327925000 ms
2017-08-10 09:19:00,109 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327940000 ms
2017-08-10 09:19:00,109 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327940000 ms.0 from job set of time 1502327940000 ms
2017-08-10 09:19:00,120 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:00,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 205 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:00,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 205 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:00,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:00,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:00,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 205 (MapPartitionsRDD[411] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:00,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_205 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:19:00,126 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_205_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-10 09:19:00,126 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_205_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:19:00,127 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 205 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:00,127 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 205 (MapPartitionsRDD[411] at map at streamingProcess.scala:48)
2017-08-10 09:19:00,127 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 205.0 with 2 tasks
2017-08-10 09:19:00,128 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 205.0 (TID 410, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:00,128 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 205.0 (TID 411, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:00,128 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 205.0 (TID 410)
2017-08-10 09:19:00,128 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 205.0 (TID 411)
2017-08-10 09:19:00,130 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:00,130 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:00,133 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 205.0 (TID 410). 714 bytes result sent to driver
2017-08-10 09:19:00,133 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 205.0 (TID 411). 714 bytes result sent to driver
2017-08-10 09:19:00,135 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 205.0 (TID 410) in 8 ms on localhost (1/2)
2017-08-10 09:19:00,135 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 205.0 (TID 411) in 7 ms on localhost (2/2)
2017-08-10 09:19:00,135 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 205.0, whose tasks have all completed, from pool 
2017-08-10 09:19:00,135 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 205 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:19:00,135 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 205 finished: foreachPartition at streamingProcess.scala:50, took 0.015729 s
2017-08-10 09:19:00,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327940000 ms.0 from job set of time 1502327940000 ms
2017-08-10 09:19:00,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.136 s for time 1502327940000 ms (execution: 0.027 s)
2017-08-10 09:19:00,136 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 409 from persistence list
2017-08-10 09:19:00,136 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 409
2017-08-10 09:19:00,136 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 408 from persistence list
2017-08-10 09:19:00,136 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 408
2017-08-10 09:19:00,136 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:00,136 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327930000 ms
2017-08-10 09:19:05,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327945000 ms
2017-08-10 09:19:05,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327945000 ms.0 from job set of time 1502327945000 ms
2017-08-10 09:19:05,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:05,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 206 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:05,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 206 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:05,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:05,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:05,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 206 (MapPartitionsRDD[413] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:05,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_206 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-10 09:19:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_206_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-10 09:19:05,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_206_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:19:05,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_191_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-10 09:19:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 206 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 206 (MapPartitionsRDD[413] at map at streamingProcess.scala:48)
2017-08-10 09:19:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 206.0 with 2 tasks
2017-08-10 09:19:05,045 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_192_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:19:05,045 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 206.0 (TID 412, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:05,046 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 206.0 (TID 413, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:05,046 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 206.0 (TID 413)
2017-08-10 09:19:05,046 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 206.0 (TID 412)
2017-08-10 09:19:05,046 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_193_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:19:05,047 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_194_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:19:05,048 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:05,048 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_195_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:19:05,048 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:05,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_196_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:05,051 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 206.0 (TID 412). 714 bytes result sent to driver
2017-08-10 09:19:05,051 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 206.0 (TID 413). 714 bytes result sent to driver
2017-08-10 09:19:05,051 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_197_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:05,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 206.0 (TID 412) in 7 ms on localhost (1/2)
2017-08-10 09:19:05,053 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 206.0 (TID 413) in 7 ms on localhost (2/2)
2017-08-10 09:19:05,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 206 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:19:05,053 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 206.0, whose tasks have all completed, from pool 
2017-08-10 09:19:05,053 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 206 finished: foreachPartition at streamingProcess.scala:50, took 0.026181 s
2017-08-10 09:19:05,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_198_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:05,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327945000 ms.0 from job set of time 1502327945000 ms
2017-08-10 09:19:05,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.053 s for time 1502327945000 ms (execution: 0.037 s)
2017-08-10 09:19:05,053 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 411 from persistence list
2017-08-10 09:19:05,054 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 411
2017-08-10 09:19:05,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 410 from persistence list
2017-08-10 09:19:05,054 [block-manager-slave-async-thread-pool-19] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 410
2017-08-10 09:19:05,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:05,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_199_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:05,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327935000 ms
2017-08-10 09:19:05,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_200_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:05,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_201_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:05,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_202_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:05,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_203_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:05,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_204_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:19:05,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_205_piece0 on 192.168.31.111:49904 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:19:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327950000 ms
2017-08-10 09:19:10,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327950000 ms.0 from job set of time 1502327950000 ms
2017-08-10 09:19:10,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 207 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 207 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 207 (MapPartitionsRDD[415] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_207 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-10 09:19:10,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_207_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-10 09:19:10,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_207_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.9 MB)
2017-08-10 09:19:10,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 207 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:10,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 207 (MapPartitionsRDD[415] at map at streamingProcess.scala:48)
2017-08-10 09:19:10,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 207.0 with 2 tasks
2017-08-10 09:19:10,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 207.0 (TID 414, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:10,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 207.0 (TID 415, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:10,074 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 207.0 (TID 415)
2017-08-10 09:19:10,074 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 207.0 (TID 414)
2017-08-10 09:19:10,080 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:10,080 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:10,086 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 207.0 (TID 415). 714 bytes result sent to driver
2017-08-10 09:19:10,086 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 207.0 (TID 414). 714 bytes result sent to driver
2017-08-10 09:19:10,089 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 207.0 (TID 414) in 16 ms on localhost (1/2)
2017-08-10 09:19:10,089 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 207.0 (TID 415) in 15 ms on localhost (2/2)
2017-08-10 09:19:10,089 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 207.0, whose tasks have all completed, from pool 
2017-08-10 09:19:10,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 207 (foreachPartition at streamingProcess.scala:50) finished in 0.017 s
2017-08-10 09:19:10,090 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 207 finished: foreachPartition at streamingProcess.scala:50, took 0.037067 s
2017-08-10 09:19:10,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327950000 ms.0 from job set of time 1502327950000 ms
2017-08-10 09:19:10,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.091 s for time 1502327950000 ms (execution: 0.071 s)
2017-08-10 09:19:10,091 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 413 from persistence list
2017-08-10 09:19:10,092 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 413
2017-08-10 09:19:10,092 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 412 from persistence list
2017-08-10 09:19:10,092 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 412
2017-08-10 09:19:10,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:10,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327940000 ms
2017-08-10 09:19:15,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327955000 ms
2017-08-10 09:19:15,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327955000 ms.0 from job set of time 1502327955000 ms
2017-08-10 09:19:15,021 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:15,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 208 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:15,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 208 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:15,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:15,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:15,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 208 (MapPartitionsRDD[417] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:15,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_208 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:19:15,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_208_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-10 09:19:15,027 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_208_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:15,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 208 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:15,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 208 (MapPartitionsRDD[417] at map at streamingProcess.scala:48)
2017-08-10 09:19:15,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 208.0 with 2 tasks
2017-08-10 09:19:15,028 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 208.0 (TID 416, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:15,028 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 208.0 (TID 417, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:15,028 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 208.0 (TID 416)
2017-08-10 09:19:15,028 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 208.0 (TID 417)
2017-08-10 09:19:15,030 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:15,030 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:15,033 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 208.0 (TID 416). 714 bytes result sent to driver
2017-08-10 09:19:15,033 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 208.0 (TID 417). 714 bytes result sent to driver
2017-08-10 09:19:15,034 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 208.0 (TID 416) in 6 ms on localhost (1/2)
2017-08-10 09:19:15,034 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 208.0 (TID 417) in 6 ms on localhost (2/2)
2017-08-10 09:19:15,034 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 208.0, whose tasks have all completed, from pool 
2017-08-10 09:19:15,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 208 (foreachPartition at streamingProcess.scala:50) finished in 0.006 s
2017-08-10 09:19:15,034 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 208 finished: foreachPartition at streamingProcess.scala:50, took 0.012840 s
2017-08-10 09:19:15,035 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327955000 ms.0 from job set of time 1502327955000 ms
2017-08-10 09:19:15,035 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.035 s for time 1502327955000 ms (execution: 0.023 s)
2017-08-10 09:19:15,035 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 415 from persistence list
2017-08-10 09:19:15,035 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 415
2017-08-10 09:19:15,035 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 414 from persistence list
2017-08-10 09:19:15,035 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 414
2017-08-10 09:19:15,035 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:15,035 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327945000 ms
2017-08-10 09:19:20,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327960000 ms
2017-08-10 09:19:20,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327960000 ms.0 from job set of time 1502327960000 ms
2017-08-10 09:19:20,021 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:20,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 209 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:20,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 209 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:20,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:20,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:20,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 209 (MapPartitionsRDD[419] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:20,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_209 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-10 09:19:20,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_209_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:19:20,026 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_209_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:20,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 209 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:20,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 209 (MapPartitionsRDD[419] at map at streamingProcess.scala:48)
2017-08-10 09:19:20,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 209.0 with 2 tasks
2017-08-10 09:19:20,027 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 209.0 (TID 418, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:20,027 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 209.0 (TID 419, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:20,027 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 209.0 (TID 418)
2017-08-10 09:19:20,027 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 209.0 (TID 419)
2017-08-10 09:19:20,029 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:20,029 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:20,032 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 209.0 (TID 418). 714 bytes result sent to driver
2017-08-10 09:19:20,032 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 209.0 (TID 419). 714 bytes result sent to driver
2017-08-10 09:19:20,033 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 209.0 (TID 418) in 6 ms on localhost (1/2)
2017-08-10 09:19:20,034 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 209.0 (TID 419) in 7 ms on localhost (2/2)
2017-08-10 09:19:20,034 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 209.0, whose tasks have all completed, from pool 
2017-08-10 09:19:20,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 209 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:19:20,034 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 209 finished: foreachPartition at streamingProcess.scala:50, took 0.013297 s
2017-08-10 09:19:20,034 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327960000 ms.0 from job set of time 1502327960000 ms
2017-08-10 09:19:20,034 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.034 s for time 1502327960000 ms (execution: 0.022 s)
2017-08-10 09:19:20,034 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 417 from persistence list
2017-08-10 09:19:20,035 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 417
2017-08-10 09:19:20,035 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 416 from persistence list
2017-08-10 09:19:20,035 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 416
2017-08-10 09:19:20,035 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:20,036 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327950000 ms
2017-08-10 09:19:25,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327965000 ms
2017-08-10 09:19:25,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327965000 ms.0 from job set of time 1502327965000 ms
2017-08-10 09:19:25,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 210 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 210 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 210 (MapPartitionsRDD[421] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:25,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_210 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-10 09:19:25,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_210_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-10 09:19:25,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_210_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 210 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 210 (MapPartitionsRDD[421] at map at streamingProcess.scala:48)
2017-08-10 09:19:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 210.0 with 2 tasks
2017-08-10 09:19:25,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 210.0 (TID 420, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:25,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 210.0 (TID 421, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:25,063 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 210.0 (TID 421)
2017-08-10 09:19:25,063 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 210.0 (TID 420)
2017-08-10 09:19:25,066 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:25,066 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:25,071 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 210.0 (TID 421). 714 bytes result sent to driver
2017-08-10 09:19:25,071 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 210.0 (TID 420). 714 bytes result sent to driver
2017-08-10 09:19:25,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 210.0 (TID 421) in 12 ms on localhost (1/2)
2017-08-10 09:19:25,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 210.0 (TID 420) in 13 ms on localhost (2/2)
2017-08-10 09:19:25,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 210.0, whose tasks have all completed, from pool 
2017-08-10 09:19:25,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 210 (foreachPartition at streamingProcess.scala:50) finished in 0.013 s
2017-08-10 09:19:25,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 210 finished: foreachPartition at streamingProcess.scala:50, took 0.029252 s
2017-08-10 09:19:25,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327965000 ms.0 from job set of time 1502327965000 ms
2017-08-10 09:19:25,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1502327965000 ms (execution: 0.055 s)
2017-08-10 09:19:25,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 419 from persistence list
2017-08-10 09:19:25,076 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 419
2017-08-10 09:19:25,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 418 from persistence list
2017-08-10 09:19:25,076 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 418
2017-08-10 09:19:25,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:25,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327955000 ms
2017-08-10 09:19:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327970000 ms
2017-08-10 09:19:30,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327970000 ms.0 from job set of time 1502327970000 ms
2017-08-10 09:19:30,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 211 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 211 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 211 (MapPartitionsRDD[423] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_211 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:19:30,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_211_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:19:30,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_211_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.8 MB)
2017-08-10 09:19:30,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 211 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:30,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 211 (MapPartitionsRDD[423] at map at streamingProcess.scala:48)
2017-08-10 09:19:30,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 211.0 with 2 tasks
2017-08-10 09:19:30,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 211.0 (TID 422, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:30,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 211.0 (TID 423, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:30,076 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 211.0 (TID 422)
2017-08-10 09:19:30,076 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 211.0 (TID 423)
2017-08-10 09:19:30,084 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:30,084 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:30,090 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 211.0 (TID 423). 714 bytes result sent to driver
2017-08-10 09:19:30,090 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 211.0 (TID 422). 714 bytes result sent to driver
2017-08-10 09:19:30,093 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 211.0 (TID 423) in 18 ms on localhost (1/2)
2017-08-10 09:19:30,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 211.0 (TID 422) in 19 ms on localhost (2/2)
2017-08-10 09:19:30,094 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 211.0, whose tasks have all completed, from pool 
2017-08-10 09:19:30,094 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 211 (foreachPartition at streamingProcess.scala:50) finished in 0.020 s
2017-08-10 09:19:30,095 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 211 finished: foreachPartition at streamingProcess.scala:50, took 0.041326 s
2017-08-10 09:19:30,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327970000 ms.0 from job set of time 1502327970000 ms
2017-08-10 09:19:30,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.095 s for time 1502327970000 ms (execution: 0.075 s)
2017-08-10 09:19:30,096 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 421 from persistence list
2017-08-10 09:19:30,096 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 421
2017-08-10 09:19:30,096 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 420 from persistence list
2017-08-10 09:19:30,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:30,097 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 420
2017-08-10 09:19:30,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327960000 ms
2017-08-10 09:19:35,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327975000 ms
2017-08-10 09:19:35,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327975000 ms.0 from job set of time 1502327975000 ms
2017-08-10 09:19:35,022 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:35,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 212 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:35,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 212 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:35,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:35,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:35,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 212 (MapPartitionsRDD[425] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:35,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_212 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-10 09:19:35,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_212_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-10 09:19:35,027 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_212_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 212 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 212 (MapPartitionsRDD[425] at map at streamingProcess.scala:48)
2017-08-10 09:19:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 212.0 with 2 tasks
2017-08-10 09:19:35,028 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 212.0 (TID 424, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:35,029 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 212.0 (TID 425, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:35,029 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 212.0 (TID 424)
2017-08-10 09:19:35,029 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 212.0 (TID 425)
2017-08-10 09:19:35,031 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:35,031 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:35,034 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 212.0 (TID 424). 714 bytes result sent to driver
2017-08-10 09:19:35,034 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 212.0 (TID 425). 714 bytes result sent to driver
2017-08-10 09:19:35,035 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 212.0 (TID 425) in 6 ms on localhost (1/2)
2017-08-10 09:19:35,035 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 212.0 (TID 424) in 7 ms on localhost (2/2)
2017-08-10 09:19:35,035 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 212.0, whose tasks have all completed, from pool 
2017-08-10 09:19:35,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 212 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:19:35,035 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 212 finished: foreachPartition at streamingProcess.scala:50, took 0.013180 s
2017-08-10 09:19:35,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327975000 ms.0 from job set of time 1502327975000 ms
2017-08-10 09:19:35,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.036 s for time 1502327975000 ms (execution: 0.024 s)
2017-08-10 09:19:35,036 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 423 from persistence list
2017-08-10 09:19:35,036 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 423
2017-08-10 09:19:35,036 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 422 from persistence list
2017-08-10 09:19:35,037 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 422
2017-08-10 09:19:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327965000 ms
2017-08-10 09:19:40,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327980000 ms
2017-08-10 09:19:40,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327980000 ms.0 from job set of time 1502327980000 ms
2017-08-10 09:19:40,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 213 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 213 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 213 (MapPartitionsRDD[427] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:40,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_213 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:19:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_213_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-10 09:19:40,048 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_213_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 213 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 213 (MapPartitionsRDD[427] at map at streamingProcess.scala:48)
2017-08-10 09:19:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 213.0 with 2 tasks
2017-08-10 09:19:40,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 213.0 (TID 426, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:40,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 213.0 (TID 427, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:40,049 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 213.0 (TID 426)
2017-08-10 09:19:40,050 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 213.0 (TID 427)
2017-08-10 09:19:40,051 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:40,051 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:40,054 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 213.0 (TID 427). 714 bytes result sent to driver
2017-08-10 09:19:40,054 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 213.0 (TID 426). 714 bytes result sent to driver
2017-08-10 09:19:40,055 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 213.0 (TID 427) in 6 ms on localhost (1/2)
2017-08-10 09:19:40,056 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 213.0 (TID 426) in 7 ms on localhost (2/2)
2017-08-10 09:19:40,056 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 213.0, whose tasks have all completed, from pool 
2017-08-10 09:19:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 213 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:19:40,056 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 213 finished: foreachPartition at streamingProcess.scala:50, took 0.014322 s
2017-08-10 09:19:40,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327980000 ms.0 from job set of time 1502327980000 ms
2017-08-10 09:19:40,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.056 s for time 1502327980000 ms (execution: 0.036 s)
2017-08-10 09:19:40,057 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 425 from persistence list
2017-08-10 09:19:40,057 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 425
2017-08-10 09:19:40,057 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 424 from persistence list
2017-08-10 09:19:40,057 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 424
2017-08-10 09:19:40,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:40,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327970000 ms
2017-08-10 09:19:45,029 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327985000 ms
2017-08-10 09:19:45,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327985000 ms.0 from job set of time 1502327985000 ms
2017-08-10 09:19:45,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 214 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 214 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 214 (MapPartitionsRDD[429] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_214 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-10 09:19:45,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_214_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:19:45,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_214_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 214 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 214 (MapPartitionsRDD[429] at map at streamingProcess.scala:48)
2017-08-10 09:19:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 214.0 with 2 tasks
2017-08-10 09:19:45,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 214.0 (TID 428, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:45,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 214.0 (TID 429, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:45,059 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 214.0 (TID 428)
2017-08-10 09:19:45,061 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:45,061 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 214.0 (TID 429)
2017-08-10 09:19:45,065 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 214.0 (TID 428). 714 bytes result sent to driver
2017-08-10 09:19:45,067 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:45,072 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 214.0 (TID 429). 714 bytes result sent to driver
2017-08-10 09:19:45,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 214.0 (TID 429) in 15 ms on localhost (1/2)
2017-08-10 09:19:45,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 214.0 (TID 428) in 19 ms on localhost (2/2)
2017-08-10 09:19:45,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 214.0, whose tasks have all completed, from pool 
2017-08-10 09:19:45,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 214 (foreachPartition at streamingProcess.scala:50) finished in 0.019 s
2017-08-10 09:19:45,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 214 finished: foreachPartition at streamingProcess.scala:50, took 0.028681 s
2017-08-10 09:19:45,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327985000 ms.0 from job set of time 1502327985000 ms
2017-08-10 09:19:45,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1502327985000 ms (execution: 0.036 s)
2017-08-10 09:19:45,074 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 427 from persistence list
2017-08-10 09:19:45,074 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 427
2017-08-10 09:19:45,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 426 from persistence list
2017-08-10 09:19:45,075 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 426
2017-08-10 09:19:45,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:45,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327975000 ms
2017-08-10 09:19:50,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327990000 ms
2017-08-10 09:19:50,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327990000 ms.0 from job set of time 1502327990000 ms
2017-08-10 09:19:50,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:50,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 215 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:50,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 215 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:50,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:50,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 215 (MapPartitionsRDD[431] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_215 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:19:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_215_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-10 09:19:50,047 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_215_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.7 MB)
2017-08-10 09:19:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 215 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 215 (MapPartitionsRDD[431] at map at streamingProcess.scala:48)
2017-08-10 09:19:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 215.0 with 2 tasks
2017-08-10 09:19:50,049 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 215.0 (TID 430, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:50,049 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 215.0 (TID 431, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:50,049 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 215.0 (TID 430)
2017-08-10 09:19:50,049 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 215.0 (TID 431)
2017-08-10 09:19:50,052 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:50,052 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:50,055 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 215.0 (TID 431). 714 bytes result sent to driver
2017-08-10 09:19:50,055 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 215.0 (TID 430). 714 bytes result sent to driver
2017-08-10 09:19:50,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 215.0 (TID 430) in 9 ms on localhost (1/2)
2017-08-10 09:19:50,058 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 215.0 (TID 431) in 9 ms on localhost (2/2)
2017-08-10 09:19:50,058 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 215.0, whose tasks have all completed, from pool 
2017-08-10 09:19:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 215 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:19:50,058 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 215 finished: foreachPartition at streamingProcess.scala:50, took 0.020586 s
2017-08-10 09:19:50,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327990000 ms.0 from job set of time 1502327990000 ms
2017-08-10 09:19:50,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.058 s for time 1502327990000 ms (execution: 0.042 s)
2017-08-10 09:19:50,059 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 429 from persistence list
2017-08-10 09:19:50,059 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 429
2017-08-10 09:19:50,059 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 428 from persistence list
2017-08-10 09:19:50,059 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 428
2017-08-10 09:19:50,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:50,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327980000 ms
2017-08-10 09:19:55,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502327995000 ms
2017-08-10 09:19:55,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502327995000 ms.0 from job set of time 1502327995000 ms
2017-08-10 09:19:55,028 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:19:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 216 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:19:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 216 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:19:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:19:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:19:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 216 (MapPartitionsRDD[433] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:19:55,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_216 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-10 09:19:55,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_216_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:19:55,037 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_216_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:19:55,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 216 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:19:55,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 216 (MapPartitionsRDD[433] at map at streamingProcess.scala:48)
2017-08-10 09:19:55,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 216.0 with 2 tasks
2017-08-10 09:19:55,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 216.0 (TID 432, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:19:55,039 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 216.0 (TID 433, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:19:55,039 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 216.0 (TID 432)
2017-08-10 09:19:55,039 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 216.0 (TID 433)
2017-08-10 09:19:55,041 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:19:55,041 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:19:55,044 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 216.0 (TID 433). 714 bytes result sent to driver
2017-08-10 09:19:55,044 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 216.0 (TID 432). 714 bytes result sent to driver
2017-08-10 09:19:55,045 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 216.0 (TID 433) in 7 ms on localhost (1/2)
2017-08-10 09:19:55,045 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 216.0 (TID 432) in 7 ms on localhost (2/2)
2017-08-10 09:19:55,045 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 216.0, whose tasks have all completed, from pool 
2017-08-10 09:19:55,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 216 (foreachPartition at streamingProcess.scala:50) finished in 0.007 s
2017-08-10 09:19:55,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 216 finished: foreachPartition at streamingProcess.scala:50, took 0.017026 s
2017-08-10 09:19:55,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502327995000 ms.0 from job set of time 1502327995000 ms
2017-08-10 09:19:55,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.046 s for time 1502327995000 ms (execution: 0.032 s)
2017-08-10 09:19:55,046 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 431 from persistence list
2017-08-10 09:19:55,046 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 431
2017-08-10 09:19:55,046 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 430 from persistence list
2017-08-10 09:19:55,046 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 430
2017-08-10 09:19:55,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:19:55,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327985000 ms
2017-08-10 09:20:00,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502328000000 ms
2017-08-10 09:20:00,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502328000000 ms.0 from job set of time 1502328000000 ms
2017-08-10 09:20:00,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:20:00,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 217 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:20:00,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 217 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:20:00,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:20:00,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:20:00,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 217 (MapPartitionsRDD[435] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:20:00,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_217 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-10 09:20:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_217_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-10 09:20:00,032 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_217_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:20:00,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 217 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:20:00,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 217 (MapPartitionsRDD[435] at map at streamingProcess.scala:48)
2017-08-10 09:20:00,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 217.0 with 2 tasks
2017-08-10 09:20:00,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 217.0 (TID 434, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:20:00,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 217.0 (TID 435, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:20:00,033 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 217.0 (TID 434)
2017-08-10 09:20:00,033 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 217.0 (TID 435)
2017-08-10 09:20:00,035 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:20:00,035 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:20:00,038 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 217.0 (TID 434). 714 bytes result sent to driver
2017-08-10 09:20:00,038 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 217.0 (TID 435). 714 bytes result sent to driver
2017-08-10 09:20:00,040 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 217.0 (TID 434) in 8 ms on localhost (1/2)
2017-08-10 09:20:00,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 217.0 (TID 435) in 7 ms on localhost (2/2)
2017-08-10 09:20:00,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 217.0, whose tasks have all completed, from pool 
2017-08-10 09:20:00,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 217 (foreachPartition at streamingProcess.scala:50) finished in 0.008 s
2017-08-10 09:20:00,040 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 217 finished: foreachPartition at streamingProcess.scala:50, took 0.015414 s
2017-08-10 09:20:00,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502328000000 ms.0 from job set of time 1502328000000 ms
2017-08-10 09:20:00,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1502328000000 ms (execution: 0.029 s)
2017-08-10 09:20:00,041 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 433 from persistence list
2017-08-10 09:20:00,041 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 433
2017-08-10 09:20:00,041 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 432 from persistence list
2017-08-10 09:20:00,041 [block-manager-slave-async-thread-pool-18] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 432
2017-08-10 09:20:00,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:20:00,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327990000 ms
2017-08-10 09:20:05,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502328005000 ms
2017-08-10 09:20:05,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502328005000 ms.0 from job set of time 1502328005000 ms
2017-08-10 09:20:05,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:20:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 218 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:20:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 218 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:20:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:20:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:20:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 218 (MapPartitionsRDD[437] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:20:05,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_218 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:20:05,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_218_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:20:05,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_218_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:20:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 218 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:20:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 218 (MapPartitionsRDD[437] at map at streamingProcess.scala:48)
2017-08-10 09:20:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 218.0 with 2 tasks
2017-08-10 09:20:05,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 218.0 (TID 436, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:20:05,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 218.0 (TID 437, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:20:05,057 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 218.0 (TID 436)
2017-08-10 09:20:05,057 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 218.0 (TID 437)
2017-08-10 09:20:05,062 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:20:05,062 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:20:05,067 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 218.0 (TID 437). 714 bytes result sent to driver
2017-08-10 09:20:05,067 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 218.0 (TID 436). 714 bytes result sent to driver
2017-08-10 09:20:05,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 218.0 (TID 437) in 13 ms on localhost (1/2)
2017-08-10 09:20:05,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 218.0 (TID 436) in 13 ms on localhost (2/2)
2017-08-10 09:20:05,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 218.0, whose tasks have all completed, from pool 
2017-08-10 09:20:05,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 218 (foreachPartition at streamingProcess.scala:50) finished in 0.014 s
2017-08-10 09:20:05,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 218 finished: foreachPartition at streamingProcess.scala:50, took 0.026328 s
2017-08-10 09:20:05,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502328005000 ms.0 from job set of time 1502328005000 ms
2017-08-10 09:20:05,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1502328005000 ms (execution: 0.047 s)
2017-08-10 09:20:05,070 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 435 from persistence list
2017-08-10 09:20:05,070 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 435
2017-08-10 09:20:05,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 434 from persistence list
2017-08-10 09:20:05,071 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 434
2017-08-10 09:20:05,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:20:05,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502327995000 ms
2017-08-10 09:20:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502328010000 ms
2017-08-10 09:20:10,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502328010000 ms.0 from job set of time 1502328010000 ms
2017-08-10 09:20:10,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcess.scala:50
2017-08-10 09:20:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 219 (foreachPartition at streamingProcess.scala:50) with 2 output partitions
2017-08-10 09:20:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 219 (foreachPartition at streamingProcess.scala:50)
2017-08-10 09:20:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:20:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:20:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 219 (MapPartitionsRDD[439] at map at streamingProcess.scala:48), which has no missing parents
2017-08-10 09:20:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_219 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-10 09:20:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_219_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-10 09:20:10,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_219_piece0 in memory on 192.168.31.111:49904 (size: 25.1 KB, free: 413.6 MB)
2017-08-10 09:20:10,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 219 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:20:10,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 219 (MapPartitionsRDD[439] at map at streamingProcess.scala:48)
2017-08-10 09:20:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 219.0 with 2 tasks
2017-08-10 09:20:10,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 219.0 (TID 438, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:20:10,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 219.0 (TID 439, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:20:10,077 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 219.0 (TID 438)
2017-08-10 09:20:10,077 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 219.0 (TID 439)
2017-08-10 09:20:10,080 [Executor task launch worker-3] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:20:10,080 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:20:10,083 [Executor task launch worker-3] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 219.0 (TID 439). 714 bytes result sent to driver
2017-08-10 09:20:10,083 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 219.0 (TID 438). 714 bytes result sent to driver
2017-08-10 09:20:10,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 219.0 (TID 439) in 9 ms on localhost (1/2)
2017-08-10 09:20:10,086 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 219.0 (TID 438) in 10 ms on localhost (2/2)
2017-08-10 09:20:10,086 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 219.0, whose tasks have all completed, from pool 
2017-08-10 09:20:10,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 219 (foreachPartition at streamingProcess.scala:50) finished in 0.010 s
2017-08-10 09:20:10,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 219 finished: foreachPartition at streamingProcess.scala:50, took 0.031736 s
2017-08-10 09:20:10,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502328010000 ms.0 from job set of time 1502328010000 ms
2017-08-10 09:20:10,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1502328010000 ms (execution: 0.065 s)
2017-08-10 09:20:10,087 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 437 from persistence list
2017-08-10 09:20:10,087 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 437
2017-08-10 09:20:10,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 436 from persistence list
2017-08-10 09:20:10,088 [block-manager-slave-async-thread-pool-9] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 436
2017-08-10 09:20:10,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:20:10,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502328000000 ms
2017-08-10 09:37:13,573 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-10 09:37:13,801 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-10 09:37:13,888 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-10 09:37:13,888 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-10 09:37:13,889 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-10 09:37:13,889 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-10 09:37:13,890 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-10 09:37:14,590 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 51062.
2017-08-10 09:37:14,609 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-10 09:37:14,627 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-10 09:37:14,641 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-57aaa44c-8a1a-49dc-b979-fd7581d11224
2017-08-10 09:37:14,655 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-10 09:37:14,694 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-10 09:37:14,775 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2009ms
2017-08-10 09:37:14,861 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-10 09:37:14,876 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fe7086{/jobs,null,AVAILABLE}
2017-08-10 09:37:14,877 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15d8c96{/jobs/json,null,AVAILABLE}
2017-08-10 09:37:14,877 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/jobs/job,null,AVAILABLE}
2017-08-10 09:37:14,877 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/jobs/job/json,null,AVAILABLE}
2017-08-10 09:37:14,878 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/stages,null,AVAILABLE}
2017-08-10 09:37:14,878 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/stages/json,null,AVAILABLE}
2017-08-10 09:37:14,878 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/stages/stage,null,AVAILABLE}
2017-08-10 09:37:14,878 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/stages/stage/json,null,AVAILABLE}
2017-08-10 09:37:14,879 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/stages/pool,null,AVAILABLE}
2017-08-10 09:37:14,879 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/stages/pool/json,null,AVAILABLE}
2017-08-10 09:37:14,879 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/storage,null,AVAILABLE}
2017-08-10 09:37:14,879 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/storage/json,null,AVAILABLE}
2017-08-10 09:37:14,879 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/storage/rdd,null,AVAILABLE}
2017-08-10 09:37:14,879 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/storage/rdd/json,null,AVAILABLE}
2017-08-10 09:37:14,880 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/environment,null,AVAILABLE}
2017-08-10 09:37:14,880 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/environment/json,null,AVAILABLE}
2017-08-10 09:37:14,880 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/executors,null,AVAILABLE}
2017-08-10 09:37:14,880 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0fba8{/executors/json,null,AVAILABLE}
2017-08-10 09:37:14,880 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5de5a4{/executors/threadDump,null,AVAILABLE}
2017-08-10 09:37:14,881 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1ef6856{/executors/threadDump/json,null,AVAILABLE}
2017-08-10 09:37:14,885 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@b7a938{/static,null,AVAILABLE}
2017-08-10 09:37:14,886 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1af2e7d{/,null,AVAILABLE}
2017-08-10 09:37:14,887 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@648ce9{/api,null,AVAILABLE}
2017-08-10 09:37:14,887 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9bf9eb{/stages/stage/kill,null,AVAILABLE}
2017-08-10 09:37:14,894 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@1465e4b{HTTP/1.1}{0.0.0.0:4040}
2017-08-10 09:37:14,894 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2128ms
2017-08-10 09:37:14,894 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-10 09:37:14,896 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-10 09:37:14,965 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-10 09:37:15,003 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51083.
2017-08-10 09:37:15,003 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:51083
2017-08-10 09:37:15,005 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 51083)
2017-08-10 09:37:15,007 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:51083 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 51083)
2017-08-10 09:37:15,009 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 51083)
2017-08-10 09:37:15,166 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@518890{/metrics/json,null,AVAILABLE}
2017-08-10 09:37:15,827 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-08-10 09:37:15,828 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-10 09:37:15,828 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-10 09:37:15,829 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@1759585
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@8d8cf0
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-10 09:37:15,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-08-10 09:37:15,831 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@82aa6a
2017-08-10 09:37:15,880 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1502329040000
2017-08-10 09:37:15,881 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1502329040000 ms
2017-08-10 09:37:15,882 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-10 09:37:15,884 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1740dae{/streaming,null,AVAILABLE}
2017-08-10 09:37:15,884 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cf395{/streaming/json,null,AVAILABLE}
2017-08-10 09:37:15,885 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@82cd4f{/streaming/batch,null,AVAILABLE}
2017-08-10 09:37:15,887 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch/json,null,AVAILABLE}
2017-08-10 09:37:15,890 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cf9028{/static/streaming,null,AVAILABLE}
2017-08-10 09:37:15,890 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-10 09:37:20,121 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329040000 ms
2017-08-10 09:37:20,125 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329040000 ms.0 from job set of time 1502329040000 ms
2017-08-10 09:37:20,153 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:20,167 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:20,168 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:20,168 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:20,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:20,179 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:20,238 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-10 09:37:20,245 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:20,451 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:20,454 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:20,456 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:20,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:20,462 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-10 09:37:20,506 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-08-10 09:37:20,509 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-08-10 09:37:20,515 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-10 09:37:20,515 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-10 09:37:20,541 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 0 -> 25
2017-08-10 09:37:20,541 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 0 -> 25
2017-08-10 09:37:20,609 [Executor task launch worker-1] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-08-10 09:37:20,849 [Executor task launch worker-1] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-08-10 09:37:21,895 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-08-10 09:37:21,910 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 1425 ms on localhost (1/2)
2017-08-10 09:37:22,214 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-08-10 09:37:22,223 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 1714 ms on localhost (2/2)
2017-08-10 09:37:22,224 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.749 s
2017-08-10 09:37:22,225 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-08-10 09:37:22,231 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcessNoReg.scala:50, took 2.076342 s
2017-08-10 09:37:22,239 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329040000 ms.0 from job set of time 1502329040000 ms
2017-08-10 09:37:22,243 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.235 s for time 1502329040000 ms (execution: 2.112 s)
2017-08-10 09:37:22,252 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:22,259 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-10 09:37:25,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329045000 ms
2017-08-10 09:37:25,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329045000 ms.0 from job set of time 1502329045000 ms
2017-08-10 09:37:25,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:25,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:25,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:25,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:25,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:25,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:25,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-08-10 09:37:25,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-08-10 09:37:25,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-08-10 09:37:25,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-08-10 09:37:25,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-08-10 09:37:25,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 25 -> 50
2017-08-10 09:37:25,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 25 -> 50
2017-08-10 09:37:25,804 [Executor task launch worker-1] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-10 09:37:25,986 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 787 bytes result sent to driver
2017-08-10 09:37:25,995 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 920 ms on localhost (1/2)
2017-08-10 09:37:26,386 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 787 bytes result sent to driver
2017-08-10 09:37:26,390 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 1324 ms on localhost (2/2)
2017-08-10 09:37:26,391 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-08-10 09:37:26,391 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.325 s
2017-08-10 09:37:26,391 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.344613 s
2017-08-10 09:37:26,392 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329045000 ms.0 from job set of time 1502329045000 ms
2017-08-10 09:37:26,392 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.392 s for time 1502329045000 ms (execution: 1.368 s)
2017-08-10 09:37:26,392 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-08-10 09:37:26,400 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-08-10 09:37:26,402 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:26,403 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-08-10 09:37:26,403 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-10 09:37:26,403 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-08-10 09:37:30,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329050000 ms
2017-08-10 09:37:30,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329050000 ms.0 from job set of time 1502329050000 ms
2017-08-10 09:37:30,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:30,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2017-08-10 09:37:30,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5655 bytes)
2017-08-10 09:37:30,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5655 bytes)
2017-08-10 09:37:30,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2017-08-10 09:37:30,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2017-08-10 09:37:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 50 -> 75
2017-08-10 09:37:30,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 50 -> 75
2017-08-10 09:37:30,190 [Executor task launch worker-0] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-10 09:37:30,382 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:30,953 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2.0 (TID 5). 787 bytes result sent to driver
2017-08-10 09:37:30,957 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2.0 (TID 5) in 890 ms on localhost (1/2)
2017-08-10 09:37:30,966 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 787 bytes result sent to driver
2017-08-10 09:37:30,969 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 904 ms on localhost (2/2)
2017-08-10 09:37:30,969 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-08-10 09:37:30,969 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.905 s
2017-08-10 09:37:30,970 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.922008 s
2017-08-10 09:37:30,971 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329050000 ms.0 from job set of time 1502329050000 ms
2017-08-10 09:37:30,971 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.971 s for time 1502329050000 ms (execution: 0.948 s)
2017-08-10 09:37:30,971 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 3 from persistence list
2017-08-10 09:37:30,971 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 3
2017-08-10 09:37:30,972 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2 from persistence list
2017-08-10 09:37:30,972 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2
2017-08-10 09:37:30,972 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:30,973 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329040000 ms
2017-08-10 09:37:35,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329055000 ms
2017-08-10 09:37:35,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329055000 ms.0 from job set of time 1502329055000 ms
2017-08-10 09:37:35,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:35,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:35,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 3 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:35,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:35,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:35,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:35,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:35,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:35,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 3.0 with 2 tasks
2017-08-10 09:37:35,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 3.0 (TID 6, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:37:35,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 3.0 (TID 7, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:37:35,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 3.0 (TID 6)
2017-08-10 09:37:35,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 3.0 (TID 7)
2017-08-10 09:37:35,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 75 -> 100
2017-08-10 09:37:35,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 75 -> 100
2017-08-10 09:37:35,212 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:35,391 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 3.0 (TID 7). 787 bytes result sent to driver
2017-08-10 09:37:35,393 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 3.0 (TID 7) in 320 ms on localhost (1/2)
2017-08-10 09:37:35,456 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 3.0 (TID 6). 787 bytes result sent to driver
2017-08-10 09:37:35,458 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 3.0 (TID 6) in 388 ms on localhost (2/2)
2017-08-10 09:37:35,459 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-08-10 09:37:35,459 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 3 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.388 s
2017-08-10 09:37:35,459 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.411242 s
2017-08-10 09:37:35,459 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329055000 ms.0 from job set of time 1502329055000 ms
2017-08-10 09:37:35,460 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.459 s for time 1502329055000 ms (execution: 0.436 s)
2017-08-10 09:37:35,460 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 5 from persistence list
2017-08-10 09:37:35,460 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 5
2017-08-10 09:37:35,460 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 4 from persistence list
2017-08-10 09:37:35,460 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 4
2017-08-10 09:37:35,461 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:35,461 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329045000 ms
2017-08-10 09:37:40,025 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329060000 ms
2017-08-10 09:37:40,026 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329060000 ms.0 from job set of time 1502329060000 ms
2017-08-10 09:37:40,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 4 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:40,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:40,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:40,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:40,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:40,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 2 tasks
2017-08-10 09:37:40,076 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 8, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:37:40,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 4.0 (TID 9, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:37:40,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 8)
2017-08-10 09:37:40,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 4.0 (TID 9)
2017-08-10 09:37:40,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 100 -> 125
2017-08-10 09:37:40,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 100 -> 125
2017-08-10 09:37:40,217 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_3_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:40,787 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 4.0 (TID 9). 787 bytes result sent to driver
2017-08-10 09:37:40,790 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 4.0 (TID 9) in 712 ms on localhost (1/2)
2017-08-10 09:37:41,116 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 8). 787 bytes result sent to driver
2017-08-10 09:37:41,118 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 8) in 1043 ms on localhost (2/2)
2017-08-10 09:37:41,118 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-08-10 09:37:41,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.044 s
2017-08-10 09:37:41,118 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 4 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.068134 s
2017-08-10 09:37:41,118 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329060000 ms.0 from job set of time 1502329060000 ms
2017-08-10 09:37:41,119 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.118 s for time 1502329060000 ms (execution: 1.092 s)
2017-08-10 09:37:41,119 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 7 from persistence list
2017-08-10 09:37:41,119 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 7
2017-08-10 09:37:41,119 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 6 from persistence list
2017-08-10 09:37:41,120 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 6
2017-08-10 09:37:41,120 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:41,120 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329050000 ms
2017-08-10 09:37:45,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329065000 ms
2017-08-10 09:37:45,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329065000 ms.0 from job set of time 1502329065000 ms
2017-08-10 09:37:45,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:45,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 5 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:45,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 5 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:45,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:45,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:45,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:45,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:45,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:45,042 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:45,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:45,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:45,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 5.0 with 2 tasks
2017-08-10 09:37:45,048 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 5.0 (TID 10, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:37:45,050 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 5.0 (TID 11, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:37:45,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 5.0 (TID 10)
2017-08-10 09:37:45,052 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 5.0 (TID 11)
2017-08-10 09:37:45,054 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 125 -> 150
2017-08-10 09:37:45,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 125 -> 150
2017-08-10 09:37:45,382 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_4_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:45,533 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 5.0 (TID 11). 787 bytes result sent to driver
2017-08-10 09:37:45,534 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 5.0 (TID 11) in 485 ms on localhost (1/2)
2017-08-10 09:37:46,095 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 5.0 (TID 10). 787 bytes result sent to driver
2017-08-10 09:37:46,097 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 5.0 (TID 10) in 1052 ms on localhost (2/2)
2017-08-10 09:37:46,098 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-08-10 09:37:46,098 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 5 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.053 s
2017-08-10 09:37:46,098 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 5 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.068717 s
2017-08-10 09:37:46,098 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329065000 ms.0 from job set of time 1502329065000 ms
2017-08-10 09:37:46,099 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.098 s for time 1502329065000 ms (execution: 1.082 s)
2017-08-10 09:37:46,099 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 9 from persistence list
2017-08-10 09:37:46,099 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 9
2017-08-10 09:37:46,099 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 8 from persistence list
2017-08-10 09:37:46,100 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 8
2017-08-10 09:37:46,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:46,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329055000 ms
2017-08-10 09:37:50,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329070000 ms
2017-08-10 09:37:50,025 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329070000 ms.0 from job set of time 1502329070000 ms
2017-08-10 09:37:50,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:50,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 6 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 6 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:50,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:50,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_6_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:50,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:50,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:50,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 6.0 with 2 tasks
2017-08-10 09:37:50,080 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 6.0 (TID 12, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:37:50,082 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 6.0 (TID 13, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:37:50,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 6.0 (TID 13)
2017-08-10 09:37:50,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 6.0 (TID 12)
2017-08-10 09:37:50,086 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 150 -> 175
2017-08-10 09:37:50,086 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 150 -> 175
2017-08-10 09:37:50,226 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_5_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:50,985 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 6.0 (TID 12). 787 bytes result sent to driver
2017-08-10 09:37:50,987 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 6.0 (TID 12) in 908 ms on localhost (1/2)
2017-08-10 09:37:51,559 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 6.0 (TID 13). 787 bytes result sent to driver
2017-08-10 09:37:51,562 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 6.0 (TID 13) in 1481 ms on localhost (2/2)
2017-08-10 09:37:51,563 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2017-08-10 09:37:51,563 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 6 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.484 s
2017-08-10 09:37:51,563 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 6 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.518105 s
2017-08-10 09:37:51,564 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329070000 ms.0 from job set of time 1502329070000 ms
2017-08-10 09:37:51,564 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 11 from persistence list
2017-08-10 09:37:51,564 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.564 s for time 1502329070000 ms (execution: 1.540 s)
2017-08-10 09:37:51,565 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 11
2017-08-10 09:37:51,565 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 10 from persistence list
2017-08-10 09:37:51,565 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 10
2017-08-10 09:37:51,565 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:51,566 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329060000 ms
2017-08-10 09:37:55,025 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329075000 ms
2017-08-10 09:37:55,026 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329075000 ms.0 from job set of time 1502329075000 ms
2017-08-10 09:37:55,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:37:55,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 7 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:37:55,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 7 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:37:55,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:37:55,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:37:55,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:37:55,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:37:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:37:55,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_7_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:37:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:37:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 7.0 with 2 tasks
2017-08-10 09:37:55,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 7.0 (TID 14, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:37:55,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 7.0 (TID 15, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:37:55,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 7.0 (TID 15)
2017-08-10 09:37:55,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 7.0 (TID 14)
2017-08-10 09:37:55,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 175 -> 200
2017-08-10 09:37:55,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 175 -> 200
2017-08-10 09:37:55,375 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_6_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:37:56,392 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 7.0 (TID 15). 787 bytes result sent to driver
2017-08-10 09:37:56,395 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 7.0 (TID 15) in 1335 ms on localhost (1/2)
2017-08-10 09:37:56,665 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 7.0 (TID 14). 787 bytes result sent to driver
2017-08-10 09:37:56,669 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 7.0 (TID 14) in 1610 ms on localhost (2/2)
2017-08-10 09:37:56,670 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 7 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.611 s
2017-08-10 09:37:56,671 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2017-08-10 09:37:56,672 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 7 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.626714 s
2017-08-10 09:37:56,673 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329075000 ms.0 from job set of time 1502329075000 ms
2017-08-10 09:37:56,673 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 13 from persistence list
2017-08-10 09:37:56,673 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.673 s for time 1502329075000 ms (execution: 1.647 s)
2017-08-10 09:37:56,674 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 13
2017-08-10 09:37:56,674 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 12 from persistence list
2017-08-10 09:37:56,675 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 12
2017-08-10 09:37:56,675 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:37:56,675 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329065000 ms
2017-08-10 09:38:00,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329080000 ms
2017-08-10 09:38:00,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329080000 ms.0 from job set of time 1502329080000 ms
2017-08-10 09:38:00,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 8 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 8 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:00,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:00,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:00,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:00,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_7_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:00,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_8_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 8.0 with 2 tasks
2017-08-10 09:38:00,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 8.0 (TID 16, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:00,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 8.0 (TID 17, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:00,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 8.0 (TID 17)
2017-08-10 09:38:00,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 8.0 (TID 16)
2017-08-10 09:38:00,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 200 -> 225
2017-08-10 09:38:00,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 200 -> 225
2017-08-10 09:38:01,274 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 8.0 (TID 16). 787 bytes result sent to driver
2017-08-10 09:38:01,276 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 8.0 (TID 16) in 1212 ms on localhost (1/2)
2017-08-10 09:38:01,633 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 8.0 (TID 17). 787 bytes result sent to driver
2017-08-10 09:38:01,634 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 8.0 (TID 17) in 1570 ms on localhost (2/2)
2017-08-10 09:38:01,635 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 8 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.573 s
2017-08-10 09:38:01,635 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2017-08-10 09:38:01,635 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 8 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.595577 s
2017-08-10 09:38:01,636 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329080000 ms.0 from job set of time 1502329080000 ms
2017-08-10 09:38:01,636 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.636 s for time 1502329080000 ms (execution: 1.614 s)
2017-08-10 09:38:01,636 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 15 from persistence list
2017-08-10 09:38:01,637 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 15
2017-08-10 09:38:01,637 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 14 from persistence list
2017-08-10 09:38:01,638 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 14
2017-08-10 09:38:01,639 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:01,639 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329070000 ms
2017-08-10 09:38:05,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329085000 ms
2017-08-10 09:38:05,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329085000 ms.0 from job set of time 1502329085000 ms
2017-08-10 09:38:05,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:05,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 9 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:05,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 9 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:05,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:05,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:05,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:05,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:05,033 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_9_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:05,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:05,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:05,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 9.0 with 2 tasks
2017-08-10 09:38:05,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 9.0 (TID 18, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:05,036 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 9.0 (TID 19, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:05,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 9.0 (TID 18)
2017-08-10 09:38:05,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 9.0 (TID 19)
2017-08-10 09:38:05,037 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 225 -> 250
2017-08-10 09:38:05,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 225 -> 250
2017-08-10 09:38:05,123 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_8_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:06,456 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 9.0 (TID 19). 787 bytes result sent to driver
2017-08-10 09:38:06,457 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 9.0 (TID 19) in 1422 ms on localhost (1/2)
2017-08-10 09:38:06,547 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 9.0 (TID 18). 787 bytes result sent to driver
2017-08-10 09:38:06,550 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 9.0 (TID 18) in 1516 ms on localhost (2/2)
2017-08-10 09:38:06,550 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2017-08-10 09:38:06,551 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 9 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.517 s
2017-08-10 09:38:06,551 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 9 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.527295 s
2017-08-10 09:38:06,552 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329085000 ms.0 from job set of time 1502329085000 ms
2017-08-10 09:38:06,552 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 17 from persistence list
2017-08-10 09:38:06,552 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.552 s for time 1502329085000 ms (execution: 1.538 s)
2017-08-10 09:38:06,552 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 17
2017-08-10 09:38:06,552 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 16 from persistence list
2017-08-10 09:38:06,553 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 16
2017-08-10 09:38:06,553 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:06,553 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329075000 ms
2017-08-10 09:38:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329090000 ms
2017-08-10 09:38:10,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329090000 ms.0 from job set of time 1502329090000 ms
2017-08-10 09:38:10,024 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:10,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 10 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:10,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 10 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:10,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:10,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:10,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:10,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:10,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:10,031 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_10_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:10,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:10,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:10,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 10.0 with 2 tasks
2017-08-10 09:38:10,034 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 10.0 (TID 20, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:10,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 10.0 (TID 21, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:10,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 10.0 (TID 20)
2017-08-10 09:38:10,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 10.0 (TID 21)
2017-08-10 09:38:10,036 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 250 -> 275
2017-08-10 09:38:10,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 250 -> 275
2017-08-10 09:38:10,272 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_9_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:10,773 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 10.0 (TID 20). 787 bytes result sent to driver
2017-08-10 09:38:10,774 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 10.0 (TID 20) in 741 ms on localhost (1/2)
2017-08-10 09:38:11,458 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 10.0 (TID 21). 787 bytes result sent to driver
2017-08-10 09:38:11,459 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 10.0 (TID 21) in 1425 ms on localhost (2/2)
2017-08-10 09:38:11,459 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2017-08-10 09:38:11,459 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 10 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.426 s
2017-08-10 09:38:11,460 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 10 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.435389 s
2017-08-10 09:38:11,460 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329090000 ms.0 from job set of time 1502329090000 ms
2017-08-10 09:38:11,460 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.460 s for time 1502329090000 ms (execution: 1.445 s)
2017-08-10 09:38:11,460 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 19 from persistence list
2017-08-10 09:38:11,461 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 19
2017-08-10 09:38:11,461 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 18 from persistence list
2017-08-10 09:38:11,461 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 18
2017-08-10 09:38:11,461 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:11,462 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329080000 ms
2017-08-10 09:38:15,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329095000 ms
2017-08-10 09:38:15,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329095000 ms.0 from job set of time 1502329095000 ms
2017-08-10 09:38:15,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:15,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 11 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:15,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 11 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:15,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:15,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:15,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:15,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:15,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_11_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 11 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 11.0 with 2 tasks
2017-08-10 09:38:15,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 11.0 (TID 22, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:15,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 11.0 (TID 23, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:15,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 11.0 (TID 22)
2017-08-10 09:38:15,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 11.0 (TID 23)
2017-08-10 09:38:15,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 275 -> 300
2017-08-10 09:38:15,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 275 -> 300
2017-08-10 09:38:15,082 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_10_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:15,977 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 11.0 (TID 23). 787 bytes result sent to driver
2017-08-10 09:38:15,979 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 11.0 (TID 23) in 916 ms on localhost (1/2)
2017-08-10 09:38:16,369 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 11.0 (TID 22). 787 bytes result sent to driver
2017-08-10 09:38:16,370 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 11.0 (TID 22) in 1309 ms on localhost (2/2)
2017-08-10 09:38:16,371 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 11 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.309 s
2017-08-10 09:38:16,371 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2017-08-10 09:38:16,371 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 11 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.327583 s
2017-08-10 09:38:16,371 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329095000 ms.0 from job set of time 1502329095000 ms
2017-08-10 09:38:16,372 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.371 s for time 1502329095000 ms (execution: 1.349 s)
2017-08-10 09:38:16,372 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 21 from persistence list
2017-08-10 09:38:16,372 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 21
2017-08-10 09:38:16,372 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 20 from persistence list
2017-08-10 09:38:16,372 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 20
2017-08-10 09:38:16,373 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:16,373 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329085000 ms
2017-08-10 09:38:20,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329100000 ms
2017-08-10 09:38:20,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329100000 ms.0 from job set of time 1502329100000 ms
2017-08-10 09:38:20,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:20,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 12 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 12 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:20,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:20,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_12_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 12.0 with 2 tasks
2017-08-10 09:38:20,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 12.0 (TID 24, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:20,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 12.0 (TID 25, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:20,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 12.0 (TID 25)
2017-08-10 09:38:20,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 12.0 (TID 24)
2017-08-10 09:38:20,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 300 -> 325
2017-08-10 09:38:20,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 300 -> 325
2017-08-10 09:38:20,903 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 12.0 (TID 25). 787 bytes result sent to driver
2017-08-10 09:38:20,907 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 12.0 (TID 25) in 841 ms on localhost (1/2)
2017-08-10 09:38:21,402 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 12.0 (TID 24). 787 bytes result sent to driver
2017-08-10 09:38:21,406 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 12.0 (TID 24) in 1344 ms on localhost (2/2)
2017-08-10 09:38:21,406 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2017-08-10 09:38:21,406 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 12 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.344 s
2017-08-10 09:38:21,406 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 12 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.364911 s
2017-08-10 09:38:21,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329100000 ms.0 from job set of time 1502329100000 ms
2017-08-10 09:38:21,408 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 23 from persistence list
2017-08-10 09:38:21,408 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.407 s for time 1502329100000 ms (execution: 1.385 s)
2017-08-10 09:38:21,408 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 22 from persistence list
2017-08-10 09:38:21,408 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 23
2017-08-10 09:38:21,409 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 22
2017-08-10 09:38:21,409 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:21,409 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329090000 ms
2017-08-10 09:38:25,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329105000 ms
2017-08-10 09:38:25,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329105000 ms.0 from job set of time 1502329105000 ms
2017-08-10 09:38:25,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:25,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 13 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:25,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 13 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:25,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:25,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:25,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:25,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:25,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:25,030 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_13_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:25,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 13 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:25,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:25,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 13.0 with 2 tasks
2017-08-10 09:38:25,032 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 13.0 (TID 26, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:25,033 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 13.0 (TID 27, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:25,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 13.0 (TID 27)
2017-08-10 09:38:25,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 13.0 (TID 26)
2017-08-10 09:38:25,034 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 325 -> 350
2017-08-10 09:38:25,035 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 325 -> 350
2017-08-10 09:38:25,767 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_12_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:26,182 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 13.0 (TID 26). 787 bytes result sent to driver
2017-08-10 09:38:26,184 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 13.0 (TID 26) in 1153 ms on localhost (1/2)
2017-08-10 09:38:26,617 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 13.0 (TID 27). 787 bytes result sent to driver
2017-08-10 09:38:26,618 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 13.0 (TID 27) in 1586 ms on localhost (2/2)
2017-08-10 09:38:26,618 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2017-08-10 09:38:26,619 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 13 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.587 s
2017-08-10 09:38:26,619 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 13 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.595765 s
2017-08-10 09:38:26,619 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329105000 ms.0 from job set of time 1502329105000 ms
2017-08-10 09:38:26,619 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.619 s for time 1502329105000 ms (execution: 1.603 s)
2017-08-10 09:38:26,619 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 25 from persistence list
2017-08-10 09:38:26,620 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 25
2017-08-10 09:38:26,620 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 24 from persistence list
2017-08-10 09:38:26,620 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 24
2017-08-10 09:38:26,620 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:26,620 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329095000 ms
2017-08-10 09:38:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329110000 ms
2017-08-10 09:38:30,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329110000 ms.0 from job set of time 1502329110000 ms
2017-08-10 09:38:30,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 14 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 14 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:30,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:30,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:30,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_14_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 14.0 with 2 tasks
2017-08-10 09:38:30,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 14.0 (TID 28, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:30,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 14.0 (TID 29, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:30,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 14.0 (TID 28)
2017-08-10 09:38:30,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 14.0 (TID 29)
2017-08-10 09:38:30,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 350 -> 375
2017-08-10 09:38:30,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 350 -> 375
2017-08-10 09:38:30,179 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_13_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:31,122 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 14.0 (TID 29). 787 bytes result sent to driver
2017-08-10 09:38:31,123 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 14.0 (TID 29) in 1062 ms on localhost (1/2)
2017-08-10 09:38:31,225 [Executor task launch worker-1] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-10 09:38:31,767 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 14.0 (TID 28). 787 bytes result sent to driver
2017-08-10 09:38:31,769 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 14.0 (TID 28) in 1709 ms on localhost (2/2)
2017-08-10 09:38:31,769 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2017-08-10 09:38:31,769 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 14 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.709 s
2017-08-10 09:38:31,770 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 14 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.729885 s
2017-08-10 09:38:31,770 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329110000 ms.0 from job set of time 1502329110000 ms
2017-08-10 09:38:31,770 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 27 from persistence list
2017-08-10 09:38:31,770 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.770 s for time 1502329110000 ms (execution: 1.750 s)
2017-08-10 09:38:31,770 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 27
2017-08-10 09:38:31,771 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 26 from persistence list
2017-08-10 09:38:31,771 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 26
2017-08-10 09:38:31,771 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:31,771 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329100000 ms
2017-08-10 09:38:35,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329115000 ms
2017-08-10 09:38:35,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329115000 ms.0 from job set of time 1502329115000 ms
2017-08-10 09:38:35,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:35,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 15 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 15 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:35,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:35,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:35,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:35,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:35,035 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_15_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 15 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:35,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 15.0 with 2 tasks
2017-08-10 09:38:35,038 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 15.0 (TID 30, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:35,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 15.0 (TID 31, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:35,040 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 15.0 (TID 30)
2017-08-10 09:38:35,040 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 15.0 (TID 31)
2017-08-10 09:38:35,042 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 375 -> 400
2017-08-10 09:38:35,042 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 375 -> 400
2017-08-10 09:38:35,389 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_14_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:35,924 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 15.0 (TID 30). 787 bytes result sent to driver
2017-08-10 09:38:35,925 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 15.0 (TID 30) in 888 ms on localhost (1/2)
2017-08-10 09:38:36,390 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 15.0 (TID 31). 787 bytes result sent to driver
2017-08-10 09:38:36,392 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 15.0 (TID 31) in 1353 ms on localhost (2/2)
2017-08-10 09:38:36,392 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2017-08-10 09:38:36,392 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 15 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.355 s
2017-08-10 09:38:36,393 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 15 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.366142 s
2017-08-10 09:38:36,393 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329115000 ms.0 from job set of time 1502329115000 ms
2017-08-10 09:38:36,394 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.393 s for time 1502329115000 ms (execution: 1.376 s)
2017-08-10 09:38:36,394 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 29 from persistence list
2017-08-10 09:38:36,394 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 29
2017-08-10 09:38:36,394 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 28 from persistence list
2017-08-10 09:38:36,395 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 28
2017-08-10 09:38:36,395 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:36,395 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329105000 ms
2017-08-10 09:38:40,024 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329120000 ms
2017-08-10 09:38:40,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329120000 ms.0 from job set of time 1502329120000 ms
2017-08-10 09:38:40,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 16 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 16 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:40,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:40,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:40,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:40,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_16_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 16 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 16.0 with 2 tasks
2017-08-10 09:38:40,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 16.0 (TID 32, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:40,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 16.0 (TID 33, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:40,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 16.0 (TID 33)
2017-08-10 09:38:40,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 16.0 (TID 32)
2017-08-10 09:38:40,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 400 -> 425
2017-08-10 09:38:40,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 400 -> 425
2017-08-10 09:38:40,341 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_15_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:41,593 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 16.0 (TID 33). 787 bytes result sent to driver
2017-08-10 09:38:41,595 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 16.0 (TID 33) in 1539 ms on localhost (1/2)
2017-08-10 09:38:41,740 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 16.0 (TID 32). 787 bytes result sent to driver
2017-08-10 09:38:41,742 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 16.0 (TID 32) in 1687 ms on localhost (2/2)
2017-08-10 09:38:41,742 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2017-08-10 09:38:41,742 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 16 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.688 s
2017-08-10 09:38:41,742 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 16 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.704181 s
2017-08-10 09:38:41,743 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329120000 ms.0 from job set of time 1502329120000 ms
2017-08-10 09:38:41,743 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.743 s for time 1502329120000 ms (execution: 1.719 s)
2017-08-10 09:38:41,743 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 31 from persistence list
2017-08-10 09:38:41,744 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 31
2017-08-10 09:38:41,744 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 30 from persistence list
2017-08-10 09:38:41,744 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 30
2017-08-10 09:38:41,744 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:41,744 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329110000 ms
2017-08-10 09:38:45,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329125000 ms
2017-08-10 09:38:45,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329125000 ms.0 from job set of time 1502329125000 ms
2017-08-10 09:38:45,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:45,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 17 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:45,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 17 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:45,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:45,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:45,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:45,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:45,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:45,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_17_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 17 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 17.0 with 2 tasks
2017-08-10 09:38:45,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 17.0 (TID 34, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:45,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 17.0 (TID 35, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:45,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 17.0 (TID 35)
2017-08-10 09:38:45,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 17.0 (TID 34)
2017-08-10 09:38:45,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 425 -> 450
2017-08-10 09:38:45,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 425 -> 450
2017-08-10 09:38:45,359 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_16_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:45,854 [Executor task launch worker-0] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-10 09:38:45,964 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 17.0 (TID 34). 787 bytes result sent to driver
2017-08-10 09:38:45,967 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 17.0 (TID 34) in 912 ms on localhost (1/2)
2017-08-10 09:38:46,733 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 17.0 (TID 35). 787 bytes result sent to driver
2017-08-10 09:38:46,735 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 17.0 (TID 35) in 1679 ms on localhost (2/2)
2017-08-10 09:38:46,735 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2017-08-10 09:38:46,735 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 17 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.680 s
2017-08-10 09:38:46,735 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 17 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.695230 s
2017-08-10 09:38:46,736 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329125000 ms.0 from job set of time 1502329125000 ms
2017-08-10 09:38:46,736 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.736 s for time 1502329125000 ms (execution: 1.716 s)
2017-08-10 09:38:46,736 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 33 from persistence list
2017-08-10 09:38:46,736 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 33
2017-08-10 09:38:46,736 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 32 from persistence list
2017-08-10 09:38:46,737 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 32
2017-08-10 09:38:46,737 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:46,737 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329115000 ms
2017-08-10 09:38:50,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329130000 ms
2017-08-10 09:38:50,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329130000 ms.0 from job set of time 1502329130000 ms
2017-08-10 09:38:50,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:50,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 18 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:50,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 18 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:50,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:50,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:50,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:50,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:50,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:50,032 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_18_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:50,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 18 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:50,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:50,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 18.0 with 2 tasks
2017-08-10 09:38:50,034 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 18.0 (TID 36, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:50,034 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 18.0 (TID 37, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:50,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 18.0 (TID 36)
2017-08-10 09:38:50,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 18.0 (TID 37)
2017-08-10 09:38:50,036 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 450 -> 475
2017-08-10 09:38:50,036 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 450 -> 475
2017-08-10 09:38:50,158 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_17_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:50,796 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 18.0 (TID 36). 787 bytes result sent to driver
2017-08-10 09:38:50,797 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 18.0 (TID 36) in 764 ms on localhost (1/2)
2017-08-10 09:38:51,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 18.0 (TID 37). 787 bytes result sent to driver
2017-08-10 09:38:51,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 18.0 (TID 37) in 1042 ms on localhost (2/2)
2017-08-10 09:38:51,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 18 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.044 s
2017-08-10 09:38:51,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2017-08-10 09:38:51,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 18 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.051505 s
2017-08-10 09:38:51,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329130000 ms.0 from job set of time 1502329130000 ms
2017-08-10 09:38:51,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.078 s for time 1502329130000 ms (execution: 1.063 s)
2017-08-10 09:38:51,078 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 35 from persistence list
2017-08-10 09:38:51,078 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 35
2017-08-10 09:38:51,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 34 from persistence list
2017-08-10 09:38:51,079 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 34
2017-08-10 09:38:51,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:51,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329120000 ms
2017-08-10 09:38:55,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329135000 ms
2017-08-10 09:38:55,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329135000 ms.0 from job set of time 1502329135000 ms
2017-08-10 09:38:55,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:38:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 19 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:38:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 19 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:38:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:38:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:38:55,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 19 (MapPartitionsRDD[39] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:38:55,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:38:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:38:55,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_19_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:55,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 19 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:38:55,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:38:55,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 19.0 with 2 tasks
2017-08-10 09:38:55,032 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 19.0 (TID 38, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:38:55,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 19.0 (TID 39, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:38:55,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 19.0 (TID 38)
2017-08-10 09:38:55,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 19.0 (TID 39)
2017-08-10 09:38:55,034 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 475 -> 500
2017-08-10 09:38:55,034 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 475 -> 500
2017-08-10 09:38:55,400 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_18_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:38:55,623 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 19.0 (TID 38). 787 bytes result sent to driver
2017-08-10 09:38:55,624 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 19.0 (TID 38) in 593 ms on localhost (1/2)
2017-08-10 09:38:56,133 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 19.0 (TID 39). 787 bytes result sent to driver
2017-08-10 09:38:56,135 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 19.0 (TID 39) in 1102 ms on localhost (2/2)
2017-08-10 09:38:56,135 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 19 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.104 s
2017-08-10 09:38:56,135 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 19.0, whose tasks have all completed, from pool 
2017-08-10 09:38:56,135 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 19 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.111852 s
2017-08-10 09:38:56,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329135000 ms.0 from job set of time 1502329135000 ms
2017-08-10 09:38:56,137 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 37 from persistence list
2017-08-10 09:38:56,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.136 s for time 1502329135000 ms (execution: 1.121 s)
2017-08-10 09:38:56,139 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 37
2017-08-10 09:38:56,139 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 36 from persistence list
2017-08-10 09:38:56,139 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 36
2017-08-10 09:38:56,139 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:38:56,140 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329125000 ms
2017-08-10 09:39:00,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329140000 ms
2017-08-10 09:39:00,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329140000 ms.0 from job set of time 1502329140000 ms
2017-08-10 09:39:00,036 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:00,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 20 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:00,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 20 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:00,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:00,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:00,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 20 (MapPartitionsRDD[41] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:00,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:00,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:00,047 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_20_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 20 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 20.0 with 2 tasks
2017-08-10 09:39:00,050 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 20.0 (TID 40, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:00,051 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 20.0 (TID 41, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:00,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 20.0 (TID 40)
2017-08-10 09:39:00,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 20.0 (TID 41)
2017-08-10 09:39:00,053 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 500 -> 525
2017-08-10 09:39:00,053 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 500 -> 525
2017-08-10 09:39:00,507 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_19_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:00,680 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 20.0 (TID 40). 787 bytes result sent to driver
2017-08-10 09:39:00,682 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 20.0 (TID 40) in 633 ms on localhost (1/2)
2017-08-10 09:39:01,009 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 20.0 (TID 41). 787 bytes result sent to driver
2017-08-10 09:39:01,010 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 20.0 (TID 41) in 960 ms on localhost (2/2)
2017-08-10 09:39:01,010 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 20.0, whose tasks have all completed, from pool 
2017-08-10 09:39:01,010 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 20 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.961 s
2017-08-10 09:39:01,010 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 20 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.973905 s
2017-08-10 09:39:01,011 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329140000 ms.0 from job set of time 1502329140000 ms
2017-08-10 09:39:01,011 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.011 s for time 1502329140000 ms (execution: 0.987 s)
2017-08-10 09:39:01,011 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 39 from persistence list
2017-08-10 09:39:01,011 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 39
2017-08-10 09:39:01,011 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 38 from persistence list
2017-08-10 09:39:01,012 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 38
2017-08-10 09:39:01,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:01,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329130000 ms
2017-08-10 09:39:05,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329145000 ms
2017-08-10 09:39:05,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329145000 ms.0 from job set of time 1502329145000 ms
2017-08-10 09:39:05,019 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:05,019 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 21 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:05,019 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 21 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:05,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:05,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:05,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 21 (MapPartitionsRDD[43] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:05,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:05,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:05,025 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_21_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 21 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 21.0 with 2 tasks
2017-08-10 09:39:05,027 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 21.0 (TID 42, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:05,028 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 21.0 (TID 43, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:05,028 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 21.0 (TID 43)
2017-08-10 09:39:05,028 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 21.0 (TID 42)
2017-08-10 09:39:05,029 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 525 -> 550
2017-08-10 09:39:05,029 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 525 -> 550
2017-08-10 09:39:05,406 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_20_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:05,460 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 21.0 (TID 43). 787 bytes result sent to driver
2017-08-10 09:39:05,462 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 21.0 (TID 43) in 434 ms on localhost (1/2)
2017-08-10 09:39:05,754 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 21.0 (TID 42). 787 bytes result sent to driver
2017-08-10 09:39:05,755 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 21.0 (TID 42) in 728 ms on localhost (2/2)
2017-08-10 09:39:05,756 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 21.0, whose tasks have all completed, from pool 
2017-08-10 09:39:05,756 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 21 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.730 s
2017-08-10 09:39:05,756 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 21 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.736904 s
2017-08-10 09:39:05,756 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329145000 ms.0 from job set of time 1502329145000 ms
2017-08-10 09:39:05,756 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.756 s for time 1502329145000 ms (execution: 0.743 s)
2017-08-10 09:39:05,757 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 41 from persistence list
2017-08-10 09:39:05,757 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 41
2017-08-10 09:39:05,757 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 40 from persistence list
2017-08-10 09:39:05,757 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 40
2017-08-10 09:39:05,757 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:05,758 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329135000 ms
2017-08-10 09:39:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329150000 ms
2017-08-10 09:39:10,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329150000 ms.0 from job set of time 1502329150000 ms
2017-08-10 09:39:10,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:10,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 22 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:10,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 22 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:10,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:10,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:10,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 22 (MapPartitionsRDD[45] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:10,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:10,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_22_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 22 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:10,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:10,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 22.0 with 2 tasks
2017-08-10 09:39:10,060 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 22.0 (TID 44, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:10,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 22.0 (TID 45, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:10,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 22.0 (TID 45)
2017-08-10 09:39:10,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 22.0 (TID 44)
2017-08-10 09:39:10,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 550 -> 575
2017-08-10 09:39:10,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 550 -> 575
2017-08-10 09:39:10,279 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_21_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:10,709 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 22.0 (TID 45). 787 bytes result sent to driver
2017-08-10 09:39:10,711 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 22.0 (TID 45) in 650 ms on localhost (1/2)
2017-08-10 09:39:11,268 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 22.0 (TID 44). 787 bytes result sent to driver
2017-08-10 09:39:11,270 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 22.0 (TID 44) in 1210 ms on localhost (2/2)
2017-08-10 09:39:11,270 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 22.0, whose tasks have all completed, from pool 
2017-08-10 09:39:11,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 22 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.213 s
2017-08-10 09:39:11,272 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 22 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.231221 s
2017-08-10 09:39:11,274 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329150000 ms.0 from job set of time 1502329150000 ms
2017-08-10 09:39:11,274 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 43 from persistence list
2017-08-10 09:39:11,274 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.273 s for time 1502329150000 ms (execution: 1.251 s)
2017-08-10 09:39:11,274 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 42 from persistence list
2017-08-10 09:39:11,275 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:11,275 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329140000 ms
2017-08-10 09:39:11,275 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 42
2017-08-10 09:39:11,275 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 43
2017-08-10 09:39:15,024 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329155000 ms
2017-08-10 09:39:15,027 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329155000 ms.0 from job set of time 1502329155000 ms
2017-08-10 09:39:15,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:15,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 23 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:15,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 23 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:15,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 23 (MapPartitionsRDD[47] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:15,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:15,060 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_23_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 23 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 23 (MapPartitionsRDD[47] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 23.0 with 2 tasks
2017-08-10 09:39:15,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 23.0 (TID 46, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:15,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 23.0 (TID 47, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:15,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 23.0 (TID 47)
2017-08-10 09:39:15,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 23.0 (TID 46)
2017-08-10 09:39:15,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 575 -> 600
2017-08-10 09:39:15,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 575 -> 600
2017-08-10 09:39:15,267 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_22_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:16,291 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 23.0 (TID 46). 787 bytes result sent to driver
2017-08-10 09:39:16,293 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 23.0 (TID 46) in 1232 ms on localhost (1/2)
2017-08-10 09:39:16,617 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 23.0 (TID 47). 787 bytes result sent to driver
2017-08-10 09:39:16,618 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 23.0 (TID 47) in 1555 ms on localhost (2/2)
2017-08-10 09:39:16,619 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 23 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.558 s
2017-08-10 09:39:16,619 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 23.0, whose tasks have all completed, from pool 
2017-08-10 09:39:16,619 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 23 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.567878 s
2017-08-10 09:39:16,620 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329155000 ms.0 from job set of time 1502329155000 ms
2017-08-10 09:39:16,620 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.620 s for time 1502329155000 ms (execution: 1.594 s)
2017-08-10 09:39:16,620 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 45 from persistence list
2017-08-10 09:39:16,620 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 45
2017-08-10 09:39:16,621 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 44 from persistence list
2017-08-10 09:39:16,621 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 44
2017-08-10 09:39:16,621 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:16,621 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329145000 ms
2017-08-10 09:39:20,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329160000 ms
2017-08-10 09:39:20,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329160000 ms.0 from job set of time 1502329160000 ms
2017-08-10 09:39:20,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 24 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 24 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 24 (MapPartitionsRDD[49] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:20,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:20,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_24_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 24 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 24.0 with 2 tasks
2017-08-10 09:39:20,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 24.0 (TID 48, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:20,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 24.0 (TID 49, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:20,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 24.0 (TID 49)
2017-08-10 09:39:20,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 24.0 (TID 48)
2017-08-10 09:39:20,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 600 -> 625
2017-08-10 09:39:20,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 600 -> 625
2017-08-10 09:39:20,389 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_23_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:21,201 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 24.0 (TID 48). 787 bytes result sent to driver
2017-08-10 09:39:21,203 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 24.0 (TID 48) in 1139 ms on localhost (1/2)
2017-08-10 09:39:21,434 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 24.0 (TID 49). 787 bytes result sent to driver
2017-08-10 09:39:21,436 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 24.0 (TID 49) in 1371 ms on localhost (2/2)
2017-08-10 09:39:21,437 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 24.0, whose tasks have all completed, from pool 
2017-08-10 09:39:21,437 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 24 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.373 s
2017-08-10 09:39:21,438 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 24 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.393368 s
2017-08-10 09:39:21,438 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329160000 ms.0 from job set of time 1502329160000 ms
2017-08-10 09:39:21,438 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.438 s for time 1502329160000 ms (execution: 1.416 s)
2017-08-10 09:39:21,438 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 47 from persistence list
2017-08-10 09:39:21,439 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 47
2017-08-10 09:39:21,439 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 46 from persistence list
2017-08-10 09:39:21,440 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 46
2017-08-10 09:39:21,440 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:21,440 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329150000 ms
2017-08-10 09:39:25,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329165000 ms
2017-08-10 09:39:25,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329165000 ms.0 from job set of time 1502329165000 ms
2017-08-10 09:39:25,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:25,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 25 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 25 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 25 (MapPartitionsRDD[51] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:25,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:25,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_25_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 25 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 25.0 with 2 tasks
2017-08-10 09:39:25,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 25.0 (TID 50, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:25,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 25.0 (TID 51, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:25,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 25.0 (TID 51)
2017-08-10 09:39:25,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 25.0 (TID 50)
2017-08-10 09:39:25,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 625 -> 650
2017-08-10 09:39:25,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 625 -> 650
2017-08-10 09:39:25,148 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_24_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:25,978 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 25.0 (TID 50). 787 bytes result sent to driver
2017-08-10 09:39:25,980 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 25.0 (TID 50) in 924 ms on localhost (1/2)
2017-08-10 09:39:26,183 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 25.0 (TID 51). 787 bytes result sent to driver
2017-08-10 09:39:26,184 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 25.0 (TID 51) in 1126 ms on localhost (2/2)
2017-08-10 09:39:26,185 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 25.0, whose tasks have all completed, from pool 
2017-08-10 09:39:26,185 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 25 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.129 s
2017-08-10 09:39:26,185 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 25 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.141576 s
2017-08-10 09:39:26,186 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329165000 ms.0 from job set of time 1502329165000 ms
2017-08-10 09:39:26,186 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.186 s for time 1502329165000 ms (execution: 1.165 s)
2017-08-10 09:39:26,186 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 49 from persistence list
2017-08-10 09:39:26,187 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 49
2017-08-10 09:39:26,188 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 48 from persistence list
2017-08-10 09:39:26,190 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 48
2017-08-10 09:39:26,190 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:26,191 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329155000 ms
2017-08-10 09:39:30,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329170000 ms
2017-08-10 09:39:30,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329170000 ms.0 from job set of time 1502329170000 ms
2017-08-10 09:39:30,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:30,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 26 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 26 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:30,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 26 (MapPartitionsRDD[53] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:30,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:30,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:30,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_26_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:30,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 26 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 26 (MapPartitionsRDD[53] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 26.0 with 2 tasks
2017-08-10 09:39:30,035 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 26.0 (TID 52, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:30,036 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 26.0 (TID 53, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:30,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 26.0 (TID 53)
2017-08-10 09:39:30,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 26.0 (TID 52)
2017-08-10 09:39:30,038 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 650 -> 675
2017-08-10 09:39:30,038 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 650 -> 653
2017-08-10 09:39:30,187 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 26.0 (TID 52). 714 bytes result sent to driver
2017-08-10 09:39:30,189 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 26.0 (TID 52) in 155 ms on localhost (1/2)
2017-08-10 09:39:30,293 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_25_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:31,189 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 26.0 (TID 53). 787 bytes result sent to driver
2017-08-10 09:39:31,191 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 26.0 (TID 53) in 1156 ms on localhost (2/2)
2017-08-10 09:39:31,191 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 26.0, whose tasks have all completed, from pool 
2017-08-10 09:39:31,191 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 26 (foreachPartition at streamingProcessNoReg.scala:50) finished in 1.157 s
2017-08-10 09:39:31,191 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 26 finished: foreachPartition at streamingProcessNoReg.scala:50, took 1.164509 s
2017-08-10 09:39:31,192 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329170000 ms.0 from job set of time 1502329170000 ms
2017-08-10 09:39:31,192 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.192 s for time 1502329170000 ms (execution: 1.176 s)
2017-08-10 09:39:31,192 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 51 from persistence list
2017-08-10 09:39:31,193 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 51
2017-08-10 09:39:31,193 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 50 from persistence list
2017-08-10 09:39:31,193 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 50
2017-08-10 09:39:31,193 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:31,193 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329160000 ms
2017-08-10 09:39:35,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329175000 ms
2017-08-10 09:39:35,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329175000 ms.0 from job set of time 1502329175000 ms
2017-08-10 09:39:35,034 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:35,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 27 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 27 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:35,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 27 (MapPartitionsRDD[55] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:35,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:35,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:35,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_27_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:35,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 27 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:35,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 27.0 with 2 tasks
2017-08-10 09:39:35,047 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 27.0 (TID 54, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:35,048 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 27.0 (TID 55, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:35,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 27.0 (TID 54)
2017-08-10 09:39:35,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 27.0 (TID 55)
2017-08-10 09:39:35,050 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:39:35,050 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 675 -> 698
2017-08-10 09:39:35,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 27.0 (TID 54). 714 bytes result sent to driver
2017-08-10 09:39:35,058 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 27.0 (TID 54) in 12 ms on localhost (1/2)
2017-08-10 09:39:35,488 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_26_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:35,736 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 27.0 (TID 55). 787 bytes result sent to driver
2017-08-10 09:39:35,739 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 27.0 (TID 55) in 692 ms on localhost (2/2)
2017-08-10 09:39:35,739 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 27.0, whose tasks have all completed, from pool 
2017-08-10 09:39:35,739 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 27 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.693 s
2017-08-10 09:39:35,740 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 27 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.705187 s
2017-08-10 09:39:35,741 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329175000 ms.0 from job set of time 1502329175000 ms
2017-08-10 09:39:35,741 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 53 from persistence list
2017-08-10 09:39:35,741 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.741 s for time 1502329175000 ms (execution: 0.719 s)
2017-08-10 09:39:35,741 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 53
2017-08-10 09:39:35,742 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 52 from persistence list
2017-08-10 09:39:35,742 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 52
2017-08-10 09:39:35,742 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:35,742 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329165000 ms
2017-08-10 09:39:40,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329180000 ms
2017-08-10 09:39:40,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329180000 ms.0 from job set of time 1502329180000 ms
2017-08-10 09:39:40,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 28 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 28 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 28 (MapPartitionsRDD[57] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:40,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:40,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:40,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_28_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 28 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:40,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 28.0 with 2 tasks
2017-08-10 09:39:40,032 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 28.0 (TID 56, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:40,033 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 28.0 (TID 57, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:40,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 28.0 (TID 57)
2017-08-10 09:39:40,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 28.0 (TID 56)
2017-08-10 09:39:40,034 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:39:40,034 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:39:40,037 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 28.0 (TID 56). 714 bytes result sent to driver
2017-08-10 09:39:40,037 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 28.0 (TID 57). 714 bytes result sent to driver
2017-08-10 09:39:40,039 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 28.0 (TID 57) in 7 ms on localhost (1/2)
2017-08-10 09:39:40,039 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 28.0 (TID 56) in 8 ms on localhost (2/2)
2017-08-10 09:39:40,039 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 28.0, whose tasks have all completed, from pool 
2017-08-10 09:39:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 28 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.008 s
2017-08-10 09:39:40,040 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 28 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.016654 s
2017-08-10 09:39:40,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329180000 ms.0 from job set of time 1502329180000 ms
2017-08-10 09:39:40,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.040 s for time 1502329180000 ms (execution: 0.026 s)
2017-08-10 09:39:40,040 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 55 from persistence list
2017-08-10 09:39:40,041 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 55
2017-08-10 09:39:40,041 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 54 from persistence list
2017-08-10 09:39:40,041 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 54
2017-08-10 09:39:40,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:40,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329170000 ms
2017-08-10 09:39:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329185000 ms
2017-08-10 09:39:45,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329185000 ms.0 from job set of time 1502329185000 ms
2017-08-10 09:39:45,022 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:45,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 29 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:45,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 29 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:45,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:45,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:45,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 29 (MapPartitionsRDD[59] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:45,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:45,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:45,028 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_29_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:45,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 29 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:45,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 29 (MapPartitionsRDD[59] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:45,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 29.0 with 2 tasks
2017-08-10 09:39:45,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 29.0 (TID 58, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:45,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 29.0 (TID 59, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:45,031 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 29.0 (TID 59)
2017-08-10 09:39:45,031 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 29.0 (TID 58)
2017-08-10 09:39:45,032 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:39:45,032 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:39:45,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 29.0 (TID 59). 714 bytes result sent to driver
2017-08-10 09:39:45,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 29.0 (TID 58). 714 bytes result sent to driver
2017-08-10 09:39:45,038 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 29.0 (TID 58) in 9 ms on localhost (1/2)
2017-08-10 09:39:45,038 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 29.0 (TID 59) in 8 ms on localhost (2/2)
2017-08-10 09:39:45,038 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 29.0, whose tasks have all completed, from pool 
2017-08-10 09:39:45,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 29 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.009 s
2017-08-10 09:39:45,039 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 29 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.016194 s
2017-08-10 09:39:45,039 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329185000 ms.0 from job set of time 1502329185000 ms
2017-08-10 09:39:45,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.039 s for time 1502329185000 ms (execution: 0.025 s)
2017-08-10 09:39:45,040 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 57 from persistence list
2017-08-10 09:39:45,040 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 57
2017-08-10 09:39:45,040 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 56 from persistence list
2017-08-10 09:39:45,040 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 56
2017-08-10 09:39:45,040 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:45,040 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329175000 ms
2017-08-10 09:39:50,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329190000 ms
2017-08-10 09:39:50,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329190000 ms.0 from job set of time 1502329190000 ms
2017-08-10 09:39:50,020 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:50,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 30 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:50,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 30 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:50,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:50,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:50,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 30 (MapPartitionsRDD[61] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:50,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:50,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:50,040 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_30_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 30 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 30.0 with 2 tasks
2017-08-10 09:39:50,042 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 30.0 (TID 60, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:50,042 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 30.0 (TID 61, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:50,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 30.0 (TID 60)
2017-08-10 09:39:50,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 30.0 (TID 61)
2017-08-10 09:39:50,043 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:39:50,043 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:39:50,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 30.0 (TID 60). 714 bytes result sent to driver
2017-08-10 09:39:50,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 30.0 (TID 61). 714 bytes result sent to driver
2017-08-10 09:39:50,048 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 30.0 (TID 61) in 6 ms on localhost (1/2)
2017-08-10 09:39:50,049 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 30.0 (TID 60) in 8 ms on localhost (2/2)
2017-08-10 09:39:50,049 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 30.0, whose tasks have all completed, from pool 
2017-08-10 09:39:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 30 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.008 s
2017-08-10 09:39:50,049 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 30 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.028636 s
2017-08-10 09:39:50,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329190000 ms.0 from job set of time 1502329190000 ms
2017-08-10 09:39:50,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.049 s for time 1502329190000 ms (execution: 0.035 s)
2017-08-10 09:39:50,050 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 59 from persistence list
2017-08-10 09:39:50,050 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 59
2017-08-10 09:39:50,050 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 58 from persistence list
2017-08-10 09:39:50,050 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 58
2017-08-10 09:39:50,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:50,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329180000 ms
2017-08-10 09:39:55,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329195000 ms
2017-08-10 09:39:55,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329195000 ms.0 from job set of time 1502329195000 ms
2017-08-10 09:39:55,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:39:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 31 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:39:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 31 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:39:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:39:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:39:55,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 31 (MapPartitionsRDD[63] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:39:55,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:39:55,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:39:55,029 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_31_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:39:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 31 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:39:55,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:39:55,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 31.0 with 2 tasks
2017-08-10 09:39:55,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 31.0 (TID 62, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:39:55,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 31.0 (TID 63, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:39:55,032 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 31.0 (TID 63)
2017-08-10 09:39:55,032 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 31.0 (TID 62)
2017-08-10 09:39:55,033 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:39:55,033 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:39:55,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 31.0 (TID 63). 714 bytes result sent to driver
2017-08-10 09:39:55,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 31.0 (TID 62). 714 bytes result sent to driver
2017-08-10 09:39:55,038 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 31.0 (TID 62) in 8 ms on localhost (1/2)
2017-08-10 09:39:55,039 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 31.0 (TID 63) in 8 ms on localhost (2/2)
2017-08-10 09:39:55,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 31.0, whose tasks have all completed, from pool 
2017-08-10 09:39:55,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 31 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.010 s
2017-08-10 09:39:55,040 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 31 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.017033 s
2017-08-10 09:39:55,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329195000 ms.0 from job set of time 1502329195000 ms
2017-08-10 09:39:55,042 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1502329195000 ms (execution: 0.027 s)
2017-08-10 09:39:55,042 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 61 from persistence list
2017-08-10 09:39:55,042 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 61
2017-08-10 09:39:55,042 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 60 from persistence list
2017-08-10 09:39:55,043 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 60
2017-08-10 09:39:55,043 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:39:55,043 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329185000 ms
2017-08-10 09:40:00,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329200000 ms
2017-08-10 09:40:00,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329200000 ms.0 from job set of time 1502329200000 ms
2017-08-10 09:40:00,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:40:00,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 32 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:40:00,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 32 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:40:00,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:40:00,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:40:00,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 32 (MapPartitionsRDD[65] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:40:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:40:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:40:00,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_32_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 32 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:40:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:40:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 32.0 with 2 tasks
2017-08-10 09:40:00,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 32.0 (TID 64, localhost, partition 0, ANY, 5656 bytes)
2017-08-10 09:40:00,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 32.0 (TID 65, localhost, partition 1, ANY, 5656 bytes)
2017-08-10 09:40:00,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 32.0 (TID 64)
2017-08-10 09:40:00,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 32.0 (TID 65)
2017-08-10 09:40:00,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:40:00,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:40:00,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 32.0 (TID 65). 714 bytes result sent to driver
2017-08-10 09:40:00,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 32.0 (TID 64). 714 bytes result sent to driver
2017-08-10 09:40:00,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 32.0 (TID 65) in 9 ms on localhost (1/2)
2017-08-10 09:40:00,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 32.0 (TID 64) in 11 ms on localhost (2/2)
2017-08-10 09:40:00,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 32.0, whose tasks have all completed, from pool 
2017-08-10 09:40:00,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 32 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.011 s
2017-08-10 09:40:00,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 32 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.024169 s
2017-08-10 09:40:00,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329200000 ms.0 from job set of time 1502329200000 ms
2017-08-10 09:40:00,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 63 from persistence list
2017-08-10 09:40:00,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1502329200000 ms (execution: 0.047 s)
2017-08-10 09:40:00,076 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 63
2017-08-10 09:40:00,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 62 from persistence list
2017-08-10 09:40:00,076 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 62
2017-08-10 09:40:00,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:40:00,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329190000 ms
2017-08-10 09:40:00,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_32_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:00,079 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_27_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:00,081 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_28_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:00,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_29_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:00,086 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_30_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:00,087 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_31_piece0 on 192.168.31.111:51083 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:05,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502329205000 ms
2017-08-10 09:40:05,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502329205000 ms.0 from job set of time 1502329205000 ms
2017-08-10 09:40:05,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNoReg.scala:50
2017-08-10 09:40:05,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 33 (foreachPartition at streamingProcessNoReg.scala:50) with 2 output partitions
2017-08-10 09:40:05,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 33 (foreachPartition at streamingProcessNoReg.scala:50)
2017-08-10 09:40:05,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-10 09:40:05,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-10 09:40:05,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 33 (MapPartitionsRDD[67] at map at streamingProcessNoReg.scala:49), which has no missing parents
2017-08-10 09:40:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33 stored as values in memory (estimated size 3.7 KB, free 413.9 MB)
2017-08-10 09:40:05,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-10 09:40:05,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_33_piece0 in memory on 192.168.31.111:51083 (size: 2.1 KB, free: 413.9 MB)
2017-08-10 09:40:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 33 from broadcast at DAGScheduler.scala:1012
2017-08-10 09:40:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 33 (MapPartitionsRDD[67] at map at streamingProcessNoReg.scala:49)
2017-08-10 09:40:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 33.0 with 2 tasks
2017-08-10 09:40:05,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 33.0 (TID 66, localhost, partition 0, ANY, 5657 bytes)
2017-08-10 09:40:05,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 33.0 (TID 67, localhost, partition 1, ANY, 5657 bytes)
2017-08-10 09:40:05,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 33.0 (TID 66)
2017-08-10 09:40:05,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 33.0 (TID 67)
2017-08-10 09:40:05,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-10 09:40:05,063 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-10 09:40:05,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 33.0 (TID 66). 714 bytes result sent to driver
2017-08-10 09:40:05,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 33.0 (TID 67). 714 bytes result sent to driver
2017-08-10 09:40:05,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 33.0 (TID 66) in 14 ms on localhost (1/2)
2017-08-10 09:40:05,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 33.0 (TID 67) in 12 ms on localhost (2/2)
2017-08-10 09:40:05,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 33.0, whose tasks have all completed, from pool 
2017-08-10 09:40:05,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 33 (foreachPartition at streamingProcessNoReg.scala:50) finished in 0.015 s
2017-08-10 09:40:05,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 33 finished: foreachPartition at streamingProcessNoReg.scala:50, took 0.033046 s
2017-08-10 09:40:05,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502329205000 ms.0 from job set of time 1502329205000 ms
2017-08-10 09:40:05,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1502329205000 ms (execution: 0.053 s)
2017-08-10 09:40:05,074 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 65 from persistence list
2017-08-10 09:40:05,074 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 65
2017-08-10 09:40:05,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 64 from persistence list
2017-08-10 09:40:05,075 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 64
2017-08-10 09:40:05,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-10 09:40:05,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502329195000 ms
2017-08-11 18:42:57,124 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-11 18:42:57,718 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-11 18:42:58,205 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-11 18:42:58,207 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-11 18:42:58,209 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-11 18:42:58,210 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-11 18:42:58,212 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-11 18:43:00,043 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 55891.
2017-08-11 18:43:00,095 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-11 18:43:00,126 [main] ERROR [org.apache.spark.SparkContext] - Error initializing SparkContext.
java.lang.IllegalArgumentException: System memory 259522560 must be at least 471859200. Please increase heap size using the --driver-memory option or spark.driver.memory in Spark configuration.
	at org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:212) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:194) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:308) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:165) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:259) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:423) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:836) [spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84) [spark-streaming_2.11-2.0.0.jar:2.0.0]
	at cn.datapark.process.education.process.streamingProcessNew$.main(streamingProcessNew.scala:36) [classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew.main(streamingProcessNew.scala) [classes/:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_111]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_111]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_111]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_111]
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144) [idea_rt.jar:na]
2017-08-11 18:43:00,159 [main] INFO  [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2017-08-11 18:43:00,162 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2017-08-11 18:43:44,826 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-11 18:43:45,056 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-11 18:43:45,338 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-11 18:43:45,338 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-11 18:43:45,339 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-11 18:43:45,339 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-11 18:43:45,340 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-11 18:43:46,085 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 55996.
2017-08-11 18:43:46,105 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-11 18:43:46,146 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-11 18:43:46,164 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-65c186db-1184-4f49-879d-dbcc04c676b3
2017-08-11 18:43:46,202 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-11 18:43:46,272 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-11 18:43:46,449 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2583ms
2017-08-11 18:43:46,614 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-11 18:43:46,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@17ea53a{/jobs,null,AVAILABLE}
2017-08-11 18:43:46,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@cb0951{/jobs/json,null,AVAILABLE}
2017-08-11 18:43:46,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1f22f18{/jobs/job,null,AVAILABLE}
2017-08-11 18:43:46,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7f474e{/jobs/job/json,null,AVAILABLE}
2017-08-11 18:43:46,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7471ce{/stages,null,AVAILABLE}
2017-08-11 18:43:46,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@276e50{/stages/json,null,AVAILABLE}
2017-08-11 18:43:46,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e70ea3{/stages/stage,null,AVAILABLE}
2017-08-11 18:43:46,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fe7086{/stages/stage/json,null,AVAILABLE}
2017-08-11 18:43:46,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15d8c96{/stages/pool,null,AVAILABLE}
2017-08-11 18:43:46,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/stages/pool/json,null,AVAILABLE}
2017-08-11 18:43:46,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/storage,null,AVAILABLE}
2017-08-11 18:43:46,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/storage/json,null,AVAILABLE}
2017-08-11 18:43:46,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/storage/rdd,null,AVAILABLE}
2017-08-11 18:43:46,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/storage/rdd/json,null,AVAILABLE}
2017-08-11 18:43:46,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/environment,null,AVAILABLE}
2017-08-11 18:43:46,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/environment/json,null,AVAILABLE}
2017-08-11 18:43:46,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/executors,null,AVAILABLE}
2017-08-11 18:43:46,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/executors/json,null,AVAILABLE}
2017-08-11 18:43:46,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/executors/threadDump,null,AVAILABLE}
2017-08-11 18:43:46,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/executors/threadDump/json,null,AVAILABLE}
2017-08-11 18:43:46,642 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/static,null,AVAILABLE}
2017-08-11 18:43:46,642 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/,null,AVAILABLE}
2017-08-11 18:43:46,643 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/api,null,AVAILABLE}
2017-08-11 18:43:46,643 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/stages/stage/kill,null,AVAILABLE}
2017-08-11 18:43:46,650 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@f92cc7{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 18:43:46,651 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2785ms
2017-08-11 18:43:46,651 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-11 18:43:46,679 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-11 18:43:46,870 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-11 18:43:46,919 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56017.
2017-08-11 18:43:46,920 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:56017
2017-08-11 18:43:46,939 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 56017)
2017-08-11 18:43:46,941 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:56017 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 56017)
2017-08-11 18:43:46,954 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 56017)
2017-08-11 18:43:47,231 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@109fac1{/metrics/json,null,AVAILABLE}
2017-08-11 18:43:48,242 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-08-11 18:43:48,242 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:43:48,243 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-11 18:43:48,243 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@1acb27a
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@db392f
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-11 18:43:48,244 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-08-11 18:43:48,245 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@1b2c305
2017-08-11 18:43:48,301 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1502448230000
2017-08-11 18:43:48,302 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1502448230000 ms
2017-08-11 18:43:48,303 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-11 18:43:48,305 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cf395{/streaming,null,AVAILABLE}
2017-08-11 18:43:48,305 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6aa04a{/streaming/json,null,AVAILABLE}
2017-08-11 18:43:48,306 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch,null,AVAILABLE}
2017-08-11 18:43:48,306 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@814f43{/streaming/batch/json,null,AVAILABLE}
2017-08-11 18:43:48,308 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1a00961{/static/streaming,null,AVAILABLE}
2017-08-11 18:43:48,310 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-11 18:43:50,211 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448230000 ms
2017-08-11 18:43:50,216 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448230000 ms.0 from job set of time 1502448230000 ms
2017-08-11 18:43:50,266 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:43:50,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:43:50,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:43:50,302 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:43:50,304 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:43:50,322 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:43:50,481 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-11 18:43:50,491 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:43:50,804 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:43:50,807 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:56017 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:43:50,829 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:43:50,834 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49)
2017-08-11 18:43:50,837 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-11 18:43:50,907 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:43:50,911 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:43:50,924 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-11 18:43:50,924 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-11 18:43:50,974 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 0 -> 25
2017-08-11 18:43:50,974 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 0 -> 25
2017-08-11 18:43:51,196 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-08-11 18:43:51,525 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-08-11 18:43:55,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448235000 ms
2017-08-11 18:44:00,024 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448240000 ms
2017-08-11 18:44:02,109 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-08-11 18:44:02,129 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 11217 ms on localhost (1/2)
2017-08-11 18:44:02,261 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-08-11 18:44:02,269 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 11394 ms on localhost (2/2)
2017-08-11 18:44:02,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcessNew.scala:51) finished in 11.421 s
2017-08-11 18:44:02,271 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-08-11 18:44:02,279 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcessNew.scala:51, took 12.012820 s
2017-08-11 18:44:02,311 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:44:02,313 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448230000 ms.0 from job set of time 1502448230000 ms
2017-08-11 18:44:02,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:44:02,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:44:02,313 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:44:02,314 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:44:02,315 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:44:02,319 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 12.300 s for time 1502448230000 ms (execution: 12.086 s)
2017-08-11 18:44:02,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:44:02,320 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448235000 ms.0 from job set of time 1502448235000 ms
2017-08-11 18:44:02,326 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:44:02,326 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.111:56017 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:44:02,327 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:44:02,327 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49)
2017-08-11 18:44:02,327 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-08-11 18:44:02,330 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:44:02,334 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:44:02,336 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:44:02,336 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-08-11 18:44:02,336 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-08-11 18:44:02,339 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 25 -> 50
2017-08-11 18:44:02,340 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 25 -> 50
2017-08-11 18:44:02,341 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 18:44:02,434 [Executor task launch worker-0] ERROR [org.apache.spark.executor.Executor] - Exception in task 0.0 in stage 1.0 (TID 2)
java.util.regex.PatternSyntaxException: Illegal character range near index 464
55 互动 旗下创意子品牌</p>|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>|更多案例尽在 HUNT:|<a href="http://socialbeta.com.*>案例一周.*?</a>|<h2.*案例一周.*?</h2>|<strong>[\u4e00-\u9fa5]{3}：.*?</strong>|<blockquote>.*?</blockquote>|<p>「案例一周」.*?</p>|<p><strong>「海外案例一周」.*?</p>|<p><strong>本周的精彩案例看点：.*?</p>|<p style="white-space: normal;">.*【案例一周】.*?</p>|<p style="text-align: justify;">.*「案例一周」.*?</p>|<strong>[A-Za-Z]*：[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*【品牌制片厂】</a>是 SocialBeta 推出的.*「别看烂片啦！来看广告啊喂！」.*?</p>|<p><a href=.*>【品牌制片厂】</a>是 SocialBeta 推出的栏目.*我们的口号是：「别看烂片啦！来看广告啊喂！」</p>|<p hiragino="".*>欢迎关注我们的微信公众号：品牌制片厂（brandfilm），每周三 20:46 定时更新。</p>|<strong>本文也将在之后持续更新（在 SocialBeta 微信号后台直接回复「 2017好文 」即可获取）。</strong>|<p style="text-align: justify;">无论你是刚入行不久的新人还是已在广告营销圈打拼多时的行业精英.*赶快收藏起来吧！</p>|<p style="margin: 0px;.*>扫描下方二维码，在【品牌制片厂】公众号.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta：</strong>每日分享最新社交、数字、移动领域营销趋势，搜罗创意的案例，发布各地热门营销职位。欢迎扫一扫，关注微信公众号.*?</p>|<p>.*【案例一周】.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.range(Pattern.java:2655) ~[na:1.8.0_111]
	at java.util.regex.Pattern.clazz(Pattern.java:2562) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2063) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]
2017-08-11 18:44:02,498 [task-result-getter-2] WARN  [org.apache.spark.scheduler.TaskSetManager] - Lost task 0.0 in stage 1.0 (TID 2, localhost): java.util.regex.PatternSyntaxException: Illegal character range near index 464
55 互动 旗下创意子品牌</p>|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>|更多案例尽在 HUNT:|<a href="http://socialbeta.com.*>案例一周.*?</a>|<h2.*案例一周.*?</h2>|<strong>[\u4e00-\u9fa5]{3}：.*?</strong>|<blockquote>.*?</blockquote>|<p>「案例一周」.*?</p>|<p><strong>「海外案例一周」.*?</p>|<p><strong>本周的精彩案例看点：.*?</p>|<p style="white-space: normal;">.*【案例一周】.*?</p>|<p style="text-align: justify;">.*「案例一周」.*?</p>|<strong>[A-Za-Z]*：[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*【品牌制片厂】</a>是 SocialBeta 推出的.*「别看烂片啦！来看广告啊喂！」.*?</p>|<p><a href=.*>【品牌制片厂】</a>是 SocialBeta 推出的栏目.*我们的口号是：「别看烂片啦！来看广告啊喂！」</p>|<p hiragino="".*>欢迎关注我们的微信公众号：品牌制片厂（brandfilm），每周三 20:46 定时更新。</p>|<strong>本文也将在之后持续更新（在 SocialBeta 微信号后台直接回复「 2017好文 」即可获取）。</strong>|<p style="text-align: justify;">无论你是刚入行不久的新人还是已在广告营销圈打拼多时的行业精英.*赶快收藏起来吧！</p>|<p style="margin: 0px;.*>扫描下方二维码，在【品牌制片厂】公众号.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta：</strong>每日分享最新社交、数字、移动领域营销趋势，搜罗创意的案例，发布各地热门营销职位。欢迎扫一扫，关注微信公众号.*?</p>|<p>.*【案例一周】.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.range(Pattern.java:2655)
	at java.util.regex.Pattern.clazz(Pattern.java:2562)
	at java.util.regex.Pattern.sequence(Pattern.java:2063)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2017-08-11 18:44:02,503 [task-result-getter-2] ERROR [org.apache.spark.scheduler.TaskSetManager] - Task 0 in stage 1.0 failed 1 times; aborting job
2017-08-11 18:44:02,507 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Cancelling stage 1
2017-08-11 18:44:02,510 [dispatcher-event-loop-2] INFO  [org.apache.spark.executor.Executor] - Executor is trying to kill task 1.0 in stage 1.0 (TID 3)
2017-08-11 18:44:02,510 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Stage 1 was cancelled
2017-08-11 18:44:02,511 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcessNew.scala:51) failed in 0.182 s
2017-08-11 18:44:02,513 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 failed: foreachPartition at streamingProcessNew.scala:51, took 0.200710 s
2017-08-11 18:44:02,516 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448235000 ms.0 from job set of time 1502448235000 ms
2017-08-11 18:44:02,517 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 7.514 s for time 1502448235000 ms (execution: 0.195 s)
2017-08-11 18:44:02,519 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-08-11 18:44:02,526 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448240000 ms.0 from job set of time 1502448240000 ms
2017-08-11 18:44:02,528 [JobScheduler] ERROR [org.apache.spark.streaming.scheduler.JobScheduler] - Error running job streaming job 1502448235000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 2, localhost): java.util.regex.PatternSyntaxException: Illegal character range near index 464
55 互动 旗下创意子品牌</p>|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>|更多案例尽在 HUNT:|<a href="http://socialbeta.com.*>案例一周.*?</a>|<h2.*案例一周.*?</h2>|<strong>[\u4e00-\u9fa5]{3}：.*?</strong>|<blockquote>.*?</blockquote>|<p>「案例一周」.*?</p>|<p><strong>「海外案例一周」.*?</p>|<p><strong>本周的精彩案例看点：.*?</p>|<p style="white-space: normal;">.*【案例一周】.*?</p>|<p style="text-align: justify;">.*「案例一周」.*?</p>|<strong>[A-Za-Z]*：[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*【品牌制片厂】</a>是 SocialBeta 推出的.*「别看烂片啦！来看广告啊喂！」.*?</p>|<p><a href=.*>【品牌制片厂】</a>是 SocialBeta 推出的栏目.*我们的口号是：「别看烂片啦！来看广告啊喂！」</p>|<p hiragino="".*>欢迎关注我们的微信公众号：品牌制片厂（brandfilm），每周三 20:46 定时更新。</p>|<strong>本文也将在之后持续更新（在 SocialBeta 微信号后台直接回复「 2017好文 」即可获取）。</strong>|<p style="text-align: justify;">无论你是刚入行不久的新人还是已在广告营销圈打拼多时的行业精英.*赶快收藏起来吧！</p>|<p style="margin: 0px;.*>扫描下方二维码，在【品牌制片厂】公众号.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta：</strong>每日分享最新社交、数字、移动领域营销趋势，搜罗创意的案例，发布各地热门营销职位。欢迎扫一扫，关注微信公众号.*?</p>|<p>.*【案例一周】.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.range(Pattern.java:2655)
	at java.util.regex.Pattern.clazz(Pattern.java:2562)
	at java.util.regex.Pattern.sequence(Pattern.java:2063)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[scala-library.jar:na]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[scala-library.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.Option.foreach(Option.scala:257) ~[scala-library.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1911) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:881) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:881) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at scala.util.Try$.apply(Try.scala:192) ~[scala-library.jar:na]
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) ~[scala-library.jar:na]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:244) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_111]
Caused by: java.util.regex.PatternSyntaxException: Illegal character range near index 464
55 互动 旗下创意子品牌</p>|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>|更多案例尽在 HUNT:|<a href="http://socialbeta.com.*>案例一周.*?</a>|<h2.*案例一周.*?</h2>|<strong>[\u4e00-\u9fa5]{3}：.*?</strong>|<blockquote>.*?</blockquote>|<p>「案例一周」.*?</p>|<p><strong>「海外案例一周」.*?</p>|<p><strong>本周的精彩案例看点：.*?</p>|<p style="white-space: normal;">.*【案例一周】.*?</p>|<p style="text-align: justify;">.*「案例一周」.*?</p>|<strong>[A-Za-Z]*：[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*【品牌制片厂】</a>是 SocialBeta 推出的.*「别看烂片啦！来看广告啊喂！」.*?</p>|<p><a href=.*>【品牌制片厂】</a>是 SocialBeta 推出的栏目.*我们的口号是：「别看烂片啦！来看广告啊喂！」</p>|<p hiragino="".*>欢迎关注我们的微信公众号：品牌制片厂（brandfilm），每周三 20:46 定时更新。</p>|<strong>本文也将在之后持续更新（在 SocialBeta 微信号后台直接回复「 2017好文 」即可获取）。</strong>|<p style="text-align: justify;">无论你是刚入行不久的新人还是已在广告营销圈打拼多时的行业精英.*赶快收藏起来吧！</p>|<p style="margin: 0px;.*>扫描下方二维码，在【品牌制片厂】公众号.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta：</strong>每日分享最新社交、数字、移动领域营销趋势，搜罗创意的案例，发布各地热门营销职位。欢迎扫一扫，关注微信公众号.*?</p>|<p>.*【案例一周】.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.range(Pattern.java:2655) ~[na:1.8.0_111]
	at java.util.regex.Pattern.clazz(Pattern.java:2562) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2063) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	... 3 common frames omitted
2017-08-11 18:44:02,530 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:44:02,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:44:02,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:44:02,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:44:02,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:44:02,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:44:02,539 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:44:02,542 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-08-11 18:44:02,543 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:44:02,544 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 18:44:02,545 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:44:02,546 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on 192.168.31.111:56017 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:44:02,546 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:44:02,547 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:49)
2017-08-11 18:44:02,547 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2017-08-11 18:44:02,554 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:44:02,555 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:44:02,556 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2017-08-11 18:44:02,556 [Executor task launch worker-2] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2017-08-11 18:44:02,559 [Executor task launch worker-2] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 50 -> 75
2017-08-11 18:44:02,559 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 50 -> 75
2017-08-11 18:44:02,566 [Thread-0] INFO  [org.apache.spark.streaming.StreamingContext] - Invoking stop(stopGracefully=false) from shutdown hook
2017-08-11 18:44:02,566 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-08-11 18:44:02,567 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-08-11 18:44:02,570 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Stopping JobGenerator immediately
2017-08-11 18:44:02,571 [Thread-0] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Stopped timer for JobGenerator after time 1502448240000
2017-08-11 18:44:02,575 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Stopped JobGenerator
2017-08-11 18:44:02,700 [Executor task launch worker-2] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-11 18:44:02,727 [Executor task launch worker-0] ERROR [org.apache.spark.executor.Executor] - Exception in task 0.0 in stage 2.0 (TID 4)
java.util.regex.PatternSyntaxException: Illegal character range near index 464
55 互动 旗下创意子品牌</p>|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>|更多案例尽在 HUNT:|<a href="http://socialbeta.com.*>案例一周.*?</a>|<h2.*案例一周.*?</h2>|<strong>[\u4e00-\u9fa5]{3}：.*?</strong>|<blockquote>.*?</blockquote>|<p>「案例一周」.*?</p>|<p><strong>「海外案例一周」.*?</p>|<p><strong>本周的精彩案例看点：.*?</p>|<p style="white-space: normal;">.*【案例一周】.*?</p>|<p style="text-align: justify;">.*「案例一周」.*?</p>|<strong>[A-Za-Z]*：[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*【品牌制片厂】</a>是 SocialBeta 推出的.*「别看烂片啦！来看广告啊喂！」.*?</p>|<p><a href=.*>【品牌制片厂】</a>是 SocialBeta 推出的栏目.*我们的口号是：「别看烂片啦！来看广告啊喂！」</p>|<p hiragino="".*>欢迎关注我们的微信公众号：品牌制片厂（brandfilm），每周三 20:46 定时更新。</p>|<strong>本文也将在之后持续更新（在 SocialBeta 微信号后台直接回复「 2017好文 」即可获取）。</strong>|<p style="text-align: justify;">无论你是刚入行不久的新人还是已在广告营销圈打拼多时的行业精英.*赶快收藏起来吧！</p>|<p style="margin: 0px;.*>扫描下方二维码，在【品牌制片厂】公众号.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta：</strong>每日分享最新社交、数字、移动领域营销趋势，搜罗创意的案例，发布各地热门营销职位。欢迎扫一扫，关注微信公众号.*?</p>|<p>.*【案例一周】.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.range(Pattern.java:2655) ~[na:1.8.0_111]
	at java.util.regex.Pattern.clazz(Pattern.java:2562) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2063) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]
2017-08-11 18:44:02,731 [task-result-getter-3] WARN  [org.apache.spark.scheduler.TaskSetManager] - Lost task 0.0 in stage 2.0 (TID 4, localhost): java.util.regex.PatternSyntaxException: Illegal character range near index 464
55 互动 旗下创意子品牌</p>|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>|更多案例尽在 HUNT:|<a href="http://socialbeta.com.*>案例一周.*?</a>|<h2.*案例一周.*?</h2>|<strong>[\u4e00-\u9fa5]{3}：.*?</strong>|<blockquote>.*?</blockquote>|<p>「案例一周」.*?</p>|<p><strong>「海外案例一周」.*?</p>|<p><strong>本周的精彩案例看点：.*?</p>|<p style="white-space: normal;">.*【案例一周】.*?</p>|<p style="text-align: justify;">.*「案例一周」.*?</p>|<strong>[A-Za-Z]*：[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*【品牌制片厂】</a>是 SocialBeta 推出的.*「别看烂片啦！来看广告啊喂！」.*?</p>|<p><a href=.*>【品牌制片厂】</a>是 SocialBeta 推出的栏目.*我们的口号是：「别看烂片啦！来看广告啊喂！」</p>|<p hiragino="".*>欢迎关注我们的微信公众号：品牌制片厂（brandfilm），每周三 20:46 定时更新。</p>|<strong>本文也将在之后持续更新（在 SocialBeta 微信号后台直接回复「 2017好文 」即可获取）。</strong>|<p style="text-align: justify;">无论你是刚入行不久的新人还是已在广告营销圈打拼多时的行业精英.*赶快收藏起来吧！</p>|<p style="margin: 0px;.*>扫描下方二维码，在【品牌制片厂】公众号.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta：</strong>每日分享最新社交、数字、移动领域营销趋势，搜罗创意的案例，发布各地热门营销职位。欢迎扫一扫，关注微信公众号.*?</p>|<p>.*【案例一周】.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.range(Pattern.java:2655)
	at java.util.regex.Pattern.clazz(Pattern.java:2562)
	at java.util.regex.Pattern.sequence(Pattern.java:2063)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2017-08-11 18:44:02,735 [task-result-getter-3] ERROR [org.apache.spark.scheduler.TaskSetManager] - Task 0 in stage 2.0 failed 1 times; aborting job
2017-08-11 18:44:02,735 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Cancelling stage 2
2017-08-11 18:44:02,735 [dispatcher-event-loop-2] INFO  [org.apache.spark.executor.Executor] - Executor is trying to kill task 1.0 in stage 2.0 (TID 5)
2017-08-11 18:44:02,735 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Stage 2 was cancelled
2017-08-11 18:44:02,736 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (foreachPartition at streamingProcessNew.scala:51) failed in 0.187 s
2017-08-11 18:44:02,736 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2 failed: foreachPartition at streamingProcessNew.scala:51, took 0.201530 s
2017-08-11 18:44:02,737 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448240000 ms.0 from job set of time 1502448240000 ms
2017-08-11 18:44:02,737 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.736 s for time 1502448240000 ms (execution: 0.210 s)
2017-08-11 18:44:02,738 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Stopped JobScheduler
2017-08-11 18:44:02,745 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@5cf395{/streaming,null,UNAVAILABLE}
2017-08-11 18:44:02,747 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch,null,UNAVAILABLE}
2017-08-11 18:44:02,749 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1a00961{/static/streaming,null,UNAVAILABLE}
2017-08-11 18:44:02,750 [Thread-0] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext stopped successfully
2017-08-11 18:44:02,750 [Thread-0] INFO  [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2017-08-11 18:44:02,776 [Thread-0] INFO  [org.spark_project.jetty.server.ServerConnector] - Stopped ServerConnector@f92cc7{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 18:44:02,776 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1005f6c{/stages/stage/kill,null,UNAVAILABLE}
2017-08-11 18:44:02,776 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1cec031{/api,null,UNAVAILABLE}
2017-08-11 18:44:02,776 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@13cb11{/,null,UNAVAILABLE}
2017-08-11 18:44:02,776 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@beebb7{/static,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@320ade{/executors/threadDump/json,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@faea88{/executors/threadDump,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@158ed3c{/executors/json,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1366c9b{/executors,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@185fa6b{/environment/json,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@51218e{/environment,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1be7cd5{/storage/rdd/json,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@d6972f{/storage/rdd,null,UNAVAILABLE}
2017-08-11 18:44:02,777 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1648ee9{/storage/json,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@d3b1f5{/storage,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1b3bc3{/stages/pool/json,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@15d8c96{/stages/pool,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@fe7086{/stages/stage/json,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@e70ea3{/stages/stage,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@276e50{/stages/json,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@7471ce{/stages,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@7f474e{/jobs/job/json,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1f22f18{/jobs/job,null,UNAVAILABLE}
2017-08-11 18:44:02,778 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@cb0951{/jobs/json,null,UNAVAILABLE}
2017-08-11 18:44:02,781 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@17ea53a{/jobs,null,UNAVAILABLE}
2017-08-11 18:44:02,783 [Thread-0] INFO  [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://192.168.31.111:4040
2017-08-11 18:44:02,817 [BoneCP-connection-watch-thread] ERROR [com.jolbox.bonecp.CloseThreadMonitor] - Connection obtained from thread [Executor task launch worker-0] was never closed. 
Stack trace of location where connection was obtained follows:
 java.lang.Thread.getStackTrace(Thread.java:1556)
 com.jolbox.bonecp.BoneCP.captureStackTrace(BoneCP.java:572)
 com.jolbox.bonecp.BoneCP.watchConnection(BoneCP.java:561)
 com.jolbox.bonecp.AbstractConnectionStrategy.postConnection(AbstractConnectionStrategy.java:79)
 com.jolbox.bonecp.AbstractConnectionStrategy.getConnection(AbstractConnectionStrategy.java:92)
 com.jolbox.bonecp.BoneCP.getConnection(BoneCP.java:553)
 cn.datapark.process.education.Util.ConnectionPool$.getConnection(ConnectionPool.scala:32)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:53)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
 org.apache.spark.scheduler.Task.run(Task.scala:85)
 org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
 java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 java.lang.Thread.run(Thread.java:745)

2017-08-11 18:44:02,817 [BoneCP-connection-watch-thread] ERROR [com.jolbox.bonecp.CloseThreadMonitor] - Connection obtained from thread [Executor task launch worker-0] was never closed. 
Stack trace of location where connection was obtained follows:
 java.lang.Thread.getStackTrace(Thread.java:1556)
 com.jolbox.bonecp.BoneCP.captureStackTrace(BoneCP.java:572)
 com.jolbox.bonecp.BoneCP.watchConnection(BoneCP.java:561)
 com.jolbox.bonecp.AbstractConnectionStrategy.postConnection(AbstractConnectionStrategy.java:79)
 com.jolbox.bonecp.AbstractConnectionStrategy.getConnection(AbstractConnectionStrategy.java:92)
 com.jolbox.bonecp.BoneCP.getConnection(BoneCP.java:553)
 cn.datapark.process.education.Util.ConnectionPool$.getConnection(ConnectionPool.scala:32)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:53)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
 org.apache.spark.scheduler.Task.run(Task.scala:85)
 org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
 java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 java.lang.Thread.run(Thread.java:745)

2017-08-11 18:44:02,821 [dispatcher-event-loop-0] INFO  [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2017-08-11 18:44:02,840 [Thread-0] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2017-08-11 18:44:02,841 [Thread-0] INFO  [org.apache.spark.storage.BlockManager] - BlockManager stopped
2017-08-11 18:44:02,844 [Thread-0] INFO  [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2017-08-11 18:44:02,846 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2017-08-11 18:44:02,849 [Thread-0] INFO  [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2017-08-11 18:44:02,849 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2017-08-11 18:44:02,850 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\DN\AppData\Local\Temp\spark-cd78a958-096b-4d96-97cf-19e1da2047de
2017-08-11 18:50:48,787 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-11 18:50:48,992 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-11 18:50:49,072 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-11 18:50:49,072 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-11 18:50:49,073 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-11 18:50:49,073 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-11 18:50:49,074 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-11 18:50:49,808 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 56665.
2017-08-11 18:50:49,829 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-11 18:50:49,848 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-11 18:50:49,863 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-2b95574b-c46e-4568-ab4e-685e21bc3d36
2017-08-11 18:50:49,878 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-11 18:50:49,921 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-11 18:50:50,008 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2015ms
2017-08-11 18:50:50,114 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-11 18:50:50,135 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1f22f18{/jobs,null,AVAILABLE}
2017-08-11 18:50:50,135 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7f474e{/jobs/json,null,AVAILABLE}
2017-08-11 18:50:50,135 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7471ce{/jobs/job,null,AVAILABLE}
2017-08-11 18:50:50,135 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@276e50{/jobs/job/json,null,AVAILABLE}
2017-08-11 18:50:50,136 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e70ea3{/stages,null,AVAILABLE}
2017-08-11 18:50:50,136 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fe7086{/stages/json,null,AVAILABLE}
2017-08-11 18:50:50,136 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15d8c96{/stages/stage,null,AVAILABLE}
2017-08-11 18:50:50,136 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/stages/stage/json,null,AVAILABLE}
2017-08-11 18:50:50,136 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/stages/pool,null,AVAILABLE}
2017-08-11 18:50:50,137 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/stages/pool/json,null,AVAILABLE}
2017-08-11 18:50:50,137 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/storage,null,AVAILABLE}
2017-08-11 18:50:50,137 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/storage/json,null,AVAILABLE}
2017-08-11 18:50:50,137 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/storage/rdd,null,AVAILABLE}
2017-08-11 18:50:50,137 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/storage/rdd/json,null,AVAILABLE}
2017-08-11 18:50:50,137 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/environment,null,AVAILABLE}
2017-08-11 18:50:50,138 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/environment/json,null,AVAILABLE}
2017-08-11 18:50:50,138 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/executors,null,AVAILABLE}
2017-08-11 18:50:50,138 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/executors/json,null,AVAILABLE}
2017-08-11 18:50:50,138 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/executors/threadDump,null,AVAILABLE}
2017-08-11 18:50:50,138 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/executors/threadDump/json,null,AVAILABLE}
2017-08-11 18:50:50,144 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/static,null,AVAILABLE}
2017-08-11 18:50:50,144 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/,null,AVAILABLE}
2017-08-11 18:50:50,144 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0fba8{/api,null,AVAILABLE}
2017-08-11 18:50:50,145 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5de5a4{/stages/stage/kill,null,AVAILABLE}
2017-08-11 18:50:50,152 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@1139eab{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 18:50:50,153 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2160ms
2017-08-11 18:50:50,153 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-11 18:50:50,154 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-11 18:50:50,232 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-11 18:50:50,265 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56688.
2017-08-11 18:50:50,266 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:56688
2017-08-11 18:50:50,267 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 56688)
2017-08-11 18:50:50,269 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:56688 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 56688)
2017-08-11 18:50:50,271 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 56688)
2017-08-11 18:50:50,471 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@109fac1{/metrics/json,null,AVAILABLE}
2017-08-11 18:50:50,955 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-08-11 18:50:50,956 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:50:50,956 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-11 18:50:50,957 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-08-11 18:50:50,957 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@3478cf
2017-08-11 18:50:50,957 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-08-11 18:50:50,957 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:50:50,957 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@ca9c1f
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-08-11 18:50:50,958 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@332235
2017-08-11 18:50:50,995 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1502448655000
2017-08-11 18:50:50,995 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1502448655000 ms
2017-08-11 18:50:50,997 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-11 18:50:50,999 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1740dae{/streaming,null,AVAILABLE}
2017-08-11 18:50:50,999 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cf395{/streaming/json,null,AVAILABLE}
2017-08-11 18:50:51,000 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@82cd4f{/streaming/batch,null,AVAILABLE}
2017-08-11 18:50:51,000 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch/json,null,AVAILABLE}
2017-08-11 18:50:51,001 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cf9028{/static/streaming,null,AVAILABLE}
2017-08-11 18:50:51,001 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-11 18:50:55,118 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448655000 ms
2017-08-11 18:50:55,121 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448655000 ms.0 from job set of time 1502448655000 ms
2017-08-11 18:50:55,147 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:50:55,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:50:55,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:50:55,160 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:50:55,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:50:55,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:50:55,226 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-11 18:50:55,233 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:50:55,435 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:50:55,437 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:56688 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:50:55,440 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:50:55,444 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49)
2017-08-11 18:50:55,446 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-11 18:50:55,486 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:50:55,490 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:50:55,496 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-11 18:50:55,496 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-11 18:50:55,523 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 0 -> 25
2017-08-11 18:50:55,523 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 0 -> 25
2017-08-11 18:50:55,591 [Executor task launch worker-1] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-08-11 18:50:55,820 [Executor task launch worker-1] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-08-11 18:50:59,392 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-08-11 18:50:59,407 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 3917 ms on localhost (1/2)
2017-08-11 18:50:59,438 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-08-11 18:50:59,443 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 3977 ms on localhost (2/2)
2017-08-11 18:50:59,444 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcessNew.scala:51) finished in 3.987 s
2017-08-11 18:50:59,444 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-08-11 18:50:59,449 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcessNew.scala:51, took 4.301052 s
2017-08-11 18:50:59,453 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448655000 ms.0 from job set of time 1502448655000 ms
2017-08-11 18:50:59,454 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 4.452 s for time 1502448655000 ms (execution: 4.332 s)
2017-08-11 18:50:59,460 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:50:59,464 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 18:51:00,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448660000 ms
2017-08-11 18:51:00,025 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448660000 ms.0 from job set of time 1502448660000 ms
2017-08-11 18:51:00,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:51:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:51:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:51:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:51:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:51:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:51:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:51:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:51:00,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.111:56688 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:51:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:51:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49)
2017-08-11 18:51:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-08-11 18:51:00,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:51:00,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:51:00,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-08-11 18:51:00,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-08-11 18:51:00,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 25 -> 50
2017-08-11 18:51:00,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 25 -> 50
2017-08-11 18:51:00,169 [Executor task launch worker-0] ERROR [org.apache.spark.executor.Executor] - Exception in task 0.0 in stage 1.0 (TID 2)
java.util.regex.PatternSyntaxException: Illegal character range near index 459
55 互动 旗下创意子品牌|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>|更多案例尽在 HUNT:|<a href="http://socialbeta.com.*>案例一周.*?</a>|<h2.*>案例一周.*?</h2>|<strong>[\u4e00-\u9fa5]{3}：.*?</strong>|<blockquote>.*?</blockquote>|<p>「案例一周」.*?</p>|<p><strong>「海外案例一周」.*?</p>|<p><strong>本周的精彩案例看点：.*?</p>|<p style="white-space: normal;">.*案例一周.*?</p>|<p style="text-align: justify;">.*「案例一周」.*?</p>|<strong>[A-Za-Z]*：[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*【品牌制片厂】</a>是 SocialBeta 推出的.*「别看烂片啦！来看广告啊喂！」.*?</p>|<p><a href=.*>【品牌制片厂】</a>是 SocialBeta 推出的栏目.*我们的口号是：「别看烂片啦！来看广告啊喂！」</p>|<p hiragino="".*>欢迎关注我们的微信公众号：品牌制片厂（brandfilm），每周三 20:46 定时更新。</p>|<strong>本文也将在之后持续更新（在 SocialBeta 微信号后台直接回复「 2017好文 」即可获取）。</strong>|<p style="text-align: justify;">无论你是刚入行不久的新人还是已在广告营销圈打拼多时的行业精英.*赶快收藏起来吧！</p>|<p style="margin: 0px;.*>扫描下方二维码，在【品牌制片厂】公众号.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta：</strong>每日分享最新社交、数字、移动领域营销趋势，搜罗创意的案例，发布各地热门营销职位。欢迎扫一扫，关注微信公众号.*?</p>|<p>.*【案例一周】.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.range(Pattern.java:2655) ~[na:1.8.0_111]
	at java.util.regex.Pattern.clazz(Pattern.java:2562) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2063) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]
2017-08-11 18:51:00,191 [task-result-getter-2] WARN  [org.apache.spark.scheduler.TaskSetManager] - Lost task 0.0 in stage 1.0 (TID 2, localhost): java.util.regex.PatternSyntaxException: Illegal character range near index 459
55 互动 旗下创意子品牌|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>|更多案例尽在 HUNT:|<a href="http://socialbeta.com.*>案例一周.*?</a>|<h2.*>案例一周.*?</h2>|<strong>[\u4e00-\u9fa5]{3}：.*?</strong>|<blockquote>.*?</blockquote>|<p>「案例一周」.*?</p>|<p><strong>「海外案例一周」.*?</p>|<p><strong>本周的精彩案例看点：.*?</p>|<p style="white-space: normal;">.*案例一周.*?</p>|<p style="text-align: justify;">.*「案例一周」.*?</p>|<strong>[A-Za-Z]*：[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*【品牌制片厂】</a>是 SocialBeta 推出的.*「别看烂片啦！来看广告啊喂！」.*?</p>|<p><a href=.*>【品牌制片厂】</a>是 SocialBeta 推出的栏目.*我们的口号是：「别看烂片啦！来看广告啊喂！」</p>|<p hiragino="".*>欢迎关注我们的微信公众号：品牌制片厂（brandfilm），每周三 20:46 定时更新。</p>|<strong>本文也将在之后持续更新（在 SocialBeta 微信号后台直接回复「 2017好文 」即可获取）。</strong>|<p style="text-align: justify;">无论你是刚入行不久的新人还是已在广告营销圈打拼多时的行业精英.*赶快收藏起来吧！</p>|<p style="margin: 0px;.*>扫描下方二维码，在【品牌制片厂】公众号.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta：</strong>每日分享最新社交、数字、移动领域营销趋势，搜罗创意的案例，发布各地热门营销职位。欢迎扫一扫，关注微信公众号.*?</p>|<p>.*【案例一周】.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.range(Pattern.java:2655)
	at java.util.regex.Pattern.clazz(Pattern.java:2562)
	at java.util.regex.Pattern.sequence(Pattern.java:2063)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2017-08-11 18:51:00,197 [task-result-getter-2] ERROR [org.apache.spark.scheduler.TaskSetManager] - Task 0 in stage 1.0 failed 1 times; aborting job
2017-08-11 18:51:00,202 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Cancelling stage 1
2017-08-11 18:51:00,204 [dispatcher-event-loop-2] INFO  [org.apache.spark.executor.Executor] - Executor is trying to kill task 1.0 in stage 1.0 (TID 3)
2017-08-11 18:51:00,204 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Stage 1 was cancelled
2017-08-11 18:51:00,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcessNew.scala:51) failed in 0.140 s
2017-08-11 18:51:00,206 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 failed: foreachPartition at streamingProcessNew.scala:51, took 0.157001 s
2017-08-11 18:51:00,207 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448660000 ms.0 from job set of time 1502448660000 ms
2017-08-11 18:51:00,207 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.207 s for time 1502448660000 ms (execution: 0.182 s)
2017-08-11 18:51:00,208 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-08-11 18:51:00,209 [JobScheduler] ERROR [org.apache.spark.streaming.scheduler.JobScheduler] - Error running job streaming job 1502448660000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 2, localhost): java.util.regex.PatternSyntaxException: Illegal character range near index 459
55 互动 旗下创意子品牌|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>|更多案例尽在 HUNT:|<a href="http://socialbeta.com.*>案例一周.*?</a>|<h2.*>案例一周.*?</h2>|<strong>[\u4e00-\u9fa5]{3}：.*?</strong>|<blockquote>.*?</blockquote>|<p>「案例一周」.*?</p>|<p><strong>「海外案例一周」.*?</p>|<p><strong>本周的精彩案例看点：.*?</p>|<p style="white-space: normal;">.*案例一周.*?</p>|<p style="text-align: justify;">.*「案例一周」.*?</p>|<strong>[A-Za-Z]*：[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*【品牌制片厂】</a>是 SocialBeta 推出的.*「别看烂片啦！来看广告啊喂！」.*?</p>|<p><a href=.*>【品牌制片厂】</a>是 SocialBeta 推出的栏目.*我们的口号是：「别看烂片啦！来看广告啊喂！」</p>|<p hiragino="".*>欢迎关注我们的微信公众号：品牌制片厂（brandfilm），每周三 20:46 定时更新。</p>|<strong>本文也将在之后持续更新（在 SocialBeta 微信号后台直接回复「 2017好文 」即可获取）。</strong>|<p style="text-align: justify;">无论你是刚入行不久的新人还是已在广告营销圈打拼多时的行业精英.*赶快收藏起来吧！</p>|<p style="margin: 0px;.*>扫描下方二维码，在【品牌制片厂】公众号.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta：</strong>每日分享最新社交、数字、移动领域营销趋势，搜罗创意的案例，发布各地热门营销职位。欢迎扫一扫，关注微信公众号.*?</p>|<p>.*【案例一周】.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.range(Pattern.java:2655)
	at java.util.regex.Pattern.clazz(Pattern.java:2562)
	at java.util.regex.Pattern.sequence(Pattern.java:2063)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[scala-library.jar:na]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[scala-library.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.Option.foreach(Option.scala:257) ~[scala-library.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1911) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:881) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:881) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at scala.util.Try$.apply(Try.scala:192) ~[scala-library.jar:na]
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) ~[scala-library.jar:na]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:244) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_111]
Caused by: java.util.regex.PatternSyntaxException: Illegal character range near index 459
55 互动 旗下创意子品牌|<img src="http://img.socialbeta.com/upload/12996-1500373000.png".*?>|<a href="http://hunt.socialbeta.com/".*?>.*?</a>|更多案例尽在 HUNT:|<a href="http://socialbeta.com.*>案例一周.*?</a>|<h2.*>案例一周.*?</h2>|<strong>[\u4e00-\u9fa5]{3}：.*?</strong>|<blockquote>.*?</blockquote>|<p>「案例一周」.*?</p>|<p><strong>「海外案例一周」.*?</p>|<p><strong>本周的精彩案例看点：.*?</p>|<p style="white-space: normal;">.*案例一周.*?</p>|<p style="text-align: justify;">.*「案例一周」.*?</p>|<strong>[A-Za-Z]*：[A-Za-Z]*?</strong>|<p style="white-space: normal;">.*【品牌制片厂】</a>是 SocialBeta 推出的.*「别看烂片啦！来看广告啊喂！」.*?</p>|<p><a href=.*>【品牌制片厂】</a>是 SocialBeta 推出的栏目.*我们的口号是：「别看烂片啦！来看广告啊喂！」</p>|<p hiragino="".*>欢迎关注我们的微信公众号：品牌制片厂（brandfilm），每周三 20:46 定时更新。</p>|<strong>本文也将在之后持续更新（在 SocialBeta 微信号后台直接回复「 2017好文 」即可获取）。</strong>|<p style="text-align: justify;">无论你是刚入行不久的新人还是已在广告营销圈打拼多时的行业精英.*赶快收藏起来吧！</p>|<p style="margin: 0px;.*>扫描下方二维码，在【品牌制片厂】公众号.*?</p>|<img src="http://img.socialbeta.com/upload/4317-1495693498.gif".*?>|<strong>SocialBeta：</strong>每日分享最新社交、数字、移动领域营销趋势，搜罗创意的案例，发布各地热门营销职位。欢迎扫一扫，关注微信公众号.*?</p>|<p>.*【案例一周】.*?</p>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.range(Pattern.java:2655) ~[na:1.8.0_111]
	at java.util.regex.Pattern.clazz(Pattern.java:2562) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2063) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:157) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	... 3 common frames omitted
2017-08-11 18:51:00,219 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-08-11 18:51:00,219 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-08-11 18:51:00,221 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:51:00,222 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 18:51:00,222 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-08-11 18:51:00,224 [Thread-0] INFO  [org.apache.spark.streaming.StreamingContext] - Invoking stop(stopGracefully=false) from shutdown hook
2017-08-11 18:51:00,226 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Stopping JobGenerator immediately
2017-08-11 18:51:00,227 [Thread-0] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Stopped timer for JobGenerator after time 1502448660000
2017-08-11 18:51:00,229 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Stopped JobGenerator
2017-08-11 18:51:00,231 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Stopped JobScheduler
2017-08-11 18:51:00,236 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1740dae{/streaming,null,UNAVAILABLE}
2017-08-11 18:51:00,237 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@82cd4f{/streaming/batch,null,UNAVAILABLE}
2017-08-11 18:51:00,238 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1cf9028{/static/streaming,null,UNAVAILABLE}
2017-08-11 18:51:00,239 [Thread-0] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext stopped successfully
2017-08-11 18:51:00,240 [Thread-0] INFO  [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2017-08-11 18:51:00,244 [Thread-0] INFO  [org.spark_project.jetty.server.ServerConnector] - Stopped ServerConnector@1139eab{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 18:51:00,244 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@5de5a4{/stages/stage/kill,null,UNAVAILABLE}
2017-08-11 18:51:00,244 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@f0fba8{/api,null,UNAVAILABLE}
2017-08-11 18:51:00,244 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1005f6c{/,null,UNAVAILABLE}
2017-08-11 18:51:00,244 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1cec031{/static,null,UNAVAILABLE}
2017-08-11 18:51:00,244 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@13cb11{/executors/threadDump/json,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@beebb7{/executors/threadDump,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@320ade{/executors/json,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@faea88{/executors,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@158ed3c{/environment/json,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1366c9b{/environment,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@185fa6b{/storage/rdd/json,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@51218e{/storage/rdd,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1be7cd5{/storage/json,null,UNAVAILABLE}
2017-08-11 18:51:00,245 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@d6972f{/storage,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1648ee9{/stages/pool/json,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@d3b1f5{/stages/pool,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1b3bc3{/stages/stage/json,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@15d8c96{/stages/stage,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@fe7086{/stages/json,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@e70ea3{/stages,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@276e50{/jobs/job/json,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@7471ce{/jobs/job,null,UNAVAILABLE}
2017-08-11 18:51:00,246 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@7f474e{/jobs/json,null,UNAVAILABLE}
2017-08-11 18:51:00,247 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1f22f18{/jobs,null,UNAVAILABLE}
2017-08-11 18:51:00,249 [Thread-0] INFO  [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://192.168.31.111:4040
2017-08-11 18:51:00,257 [BoneCP-connection-watch-thread] ERROR [com.jolbox.bonecp.CloseThreadMonitor] - Connection obtained from thread [Executor task launch worker-0] was never closed. 
Stack trace of location where connection was obtained follows:
 java.lang.Thread.getStackTrace(Thread.java:1556)
 com.jolbox.bonecp.BoneCP.captureStackTrace(BoneCP.java:572)
 com.jolbox.bonecp.BoneCP.watchConnection(BoneCP.java:561)
 com.jolbox.bonecp.AbstractConnectionStrategy.postConnection(AbstractConnectionStrategy.java:79)
 com.jolbox.bonecp.AbstractConnectionStrategy.getConnection(AbstractConnectionStrategy.java:92)
 com.jolbox.bonecp.BoneCP.getConnection(BoneCP.java:553)
 cn.datapark.process.education.Util.ConnectionPool$.getConnection(ConnectionPool.scala:32)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:53)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
 org.apache.spark.scheduler.Task.run(Task.scala:85)
 org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
 java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 java.lang.Thread.run(Thread.java:745)

2017-08-11 18:51:00,260 [dispatcher-event-loop-2] INFO  [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2017-08-11 18:51:00,270 [Thread-0] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2017-08-11 18:51:00,270 [Thread-0] INFO  [org.apache.spark.storage.BlockManager] - BlockManager stopped
2017-08-11 18:51:00,273 [Thread-0] INFO  [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2017-08-11 18:51:00,276 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2017-08-11 18:51:00,281 [Thread-0] INFO  [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2017-08-11 18:51:00,282 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2017-08-11 18:51:00,283 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\DN\AppData\Local\Temp\spark-e13cad22-6798-4772-ba3f-8a53b9a35d3e
2017-08-11 18:54:53,342 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-11 18:54:53,556 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-11 18:54:53,638 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-11 18:54:53,639 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-11 18:54:53,639 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-11 18:54:53,640 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-11 18:54:53,640 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-11 18:54:54,380 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 57088.
2017-08-11 18:54:54,400 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-11 18:54:54,416 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-11 18:54:54,430 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-e96ec91c-7cc3-4663-adaa-96348eff5153
2017-08-11 18:54:54,443 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-11 18:54:54,484 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-11 18:54:54,565 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2001ms
2017-08-11 18:54:54,653 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-11 18:54:54,670 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1f22f18{/jobs,null,AVAILABLE}
2017-08-11 18:54:54,671 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7f474e{/jobs/json,null,AVAILABLE}
2017-08-11 18:54:54,671 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7471ce{/jobs/job,null,AVAILABLE}
2017-08-11 18:54:54,671 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@276e50{/jobs/job/json,null,AVAILABLE}
2017-08-11 18:54:54,671 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e70ea3{/stages,null,AVAILABLE}
2017-08-11 18:54:54,672 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fe7086{/stages/json,null,AVAILABLE}
2017-08-11 18:54:54,672 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15d8c96{/stages/stage,null,AVAILABLE}
2017-08-11 18:54:54,672 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/stages/stage/json,null,AVAILABLE}
2017-08-11 18:54:54,672 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/stages/pool,null,AVAILABLE}
2017-08-11 18:54:54,672 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/stages/pool/json,null,AVAILABLE}
2017-08-11 18:54:54,673 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/storage,null,AVAILABLE}
2017-08-11 18:54:54,673 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/storage/json,null,AVAILABLE}
2017-08-11 18:54:54,673 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/storage/rdd,null,AVAILABLE}
2017-08-11 18:54:54,673 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/storage/rdd/json,null,AVAILABLE}
2017-08-11 18:54:54,673 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/environment,null,AVAILABLE}
2017-08-11 18:54:54,674 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/environment/json,null,AVAILABLE}
2017-08-11 18:54:54,674 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/executors,null,AVAILABLE}
2017-08-11 18:54:54,674 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/executors/json,null,AVAILABLE}
2017-08-11 18:54:54,674 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/executors/threadDump,null,AVAILABLE}
2017-08-11 18:54:54,674 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/executors/threadDump/json,null,AVAILABLE}
2017-08-11 18:54:54,679 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/static,null,AVAILABLE}
2017-08-11 18:54:54,680 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/,null,AVAILABLE}
2017-08-11 18:54:54,680 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0fba8{/api,null,AVAILABLE}
2017-08-11 18:54:54,680 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5de5a4{/stages/stage/kill,null,AVAILABLE}
2017-08-11 18:54:54,690 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@1139eab{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 18:54:54,690 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2126ms
2017-08-11 18:54:54,690 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-11 18:54:54,691 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-11 18:54:54,766 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-11 18:54:54,809 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57111.
2017-08-11 18:54:54,809 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:57111
2017-08-11 18:54:54,810 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 57111)
2017-08-11 18:54:54,814 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:57111 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 57111)
2017-08-11 18:54:54,816 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 57111)
2017-08-11 18:54:54,999 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@109fac1{/metrics/json,null,AVAILABLE}
2017-08-11 18:54:55,482 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-08-11 18:54:55,482 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:54:55,483 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@e18910
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-08-11 18:54:55,484 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@850a5b
2017-08-11 18:54:55,485 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-08-11 18:54:55,485 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-11 18:54:55,485 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-11 18:54:55,485 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-08-11 18:54:55,485 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5dc86c
2017-08-11 18:54:55,522 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1502448900000
2017-08-11 18:54:55,523 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1502448900000 ms
2017-08-11 18:54:55,523 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-11 18:54:55,525 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1740dae{/streaming,null,AVAILABLE}
2017-08-11 18:54:55,525 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cf395{/streaming/json,null,AVAILABLE}
2017-08-11 18:54:55,525 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@82cd4f{/streaming/batch,null,AVAILABLE}
2017-08-11 18:54:55,526 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch/json,null,AVAILABLE}
2017-08-11 18:54:55,527 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cf9028{/static/streaming,null,AVAILABLE}
2017-08-11 18:54:55,527 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-11 18:55:00,162 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448900000 ms
2017-08-11 18:55:00,166 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448900000 ms.0 from job set of time 1502448900000 ms
2017-08-11 18:55:00,192 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:55:00,204 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:55:00,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:55:00,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:55:00,207 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:55:00,214 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:55:00,273 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-11 18:55:00,280 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:55:00,463 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:55:00,465 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:57111 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:55:00,468 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:55:00,471 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49)
2017-08-11 18:55:00,474 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-11 18:55:00,515 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:55:00,518 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:55:00,524 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-11 18:55:00,524 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-11 18:55:00,549 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 0 -> 25
2017-08-11 18:55:00,549 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 0 -> 25
2017-08-11 18:55:00,639 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-08-11 18:55:00,866 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-08-11 18:55:04,285 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-08-11 18:55:04,301 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 3782 ms on localhost (1/2)
2017-08-11 18:55:04,776 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-08-11 18:55:04,781 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 4287 ms on localhost (2/2)
2017-08-11 18:55:04,783 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcessNew.scala:51) finished in 4.298 s
2017-08-11 18:55:04,784 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-08-11 18:55:04,788 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcessNew.scala:51, took 4.596028 s
2017-08-11 18:55:04,794 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448900000 ms.0 from job set of time 1502448900000 ms
2017-08-11 18:55:04,795 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 4.792 s for time 1502448900000 ms (execution: 4.628 s)
2017-08-11 18:55:04,802 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:55:04,806 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 18:55:05,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448905000 ms
2017-08-11 18:55:05,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448905000 ms.0 from job set of time 1502448905000 ms
2017-08-11 18:55:05,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:55:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:55:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:55:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:55:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:55:05,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:55:05,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:55:05,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:55:05,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.111:57111 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:55:05,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:55:05,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49)
2017-08-11 18:55:05,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-08-11 18:55:05,040 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:55:05,041 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:55:05,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-08-11 18:55:05,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-08-11 18:55:05,045 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 25 -> 50
2017-08-11 18:55:05,045 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 25 -> 50
2017-08-11 18:55:06,173 [Executor task launch worker-0] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-11 18:55:06,544 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 787 bytes result sent to driver
2017-08-11 18:55:06,548 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 1508 ms on localhost (1/2)
2017-08-11 18:55:07,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 787 bytes result sent to driver
2017-08-11 18:55:07,054 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 2016 ms on localhost (2/2)
2017-08-11 18:55:07,054 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-08-11 18:55:07,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcessNew.scala:51) finished in 2.016 s
2017-08-11 18:55:07,055 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: foreachPartition at streamingProcessNew.scala:51, took 2.029054 s
2017-08-11 18:55:07,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448905000 ms.0 from job set of time 1502448905000 ms
2017-08-11 18:55:07,056 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.055 s for time 1502448905000 ms (execution: 2.037 s)
2017-08-11 18:55:07,056 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-08-11 18:55:07,061 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-08-11 18:55:07,062 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-08-11 18:55:07,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:55:07,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 18:55:07,062 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-08-11 18:55:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502448910000 ms
2017-08-11 18:55:10,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502448910000 ms.0 from job set of time 1502448910000 ms
2017-08-11 18:55:10,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 18:55:10,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 18:55:10,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 18:55:10,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 18:55:10,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 18:55:10,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 18:55:10,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 18:55:10,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 18:55:10,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on 192.168.31.111:57111 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:55:10,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2017-08-11 18:55:10,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:49)
2017-08-11 18:55:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2017-08-11 18:55:10,042 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 18:55:10,043 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 18:55:10,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2017-08-11 18:55:10,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2017-08-11 18:55:10,049 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 50 -> 75
2017-08-11 18:55:10,049 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 50 -> 75
2017-08-11 18:55:10,168 [Executor task launch worker-1] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-11 18:55:10,345 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on 192.168.31.111:57111 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 18:55:10,471 [Executor task launch worker-1] ERROR [org.apache.spark.executor.Executor] - Exception in task 1.0 in stage 2.0 (TID 5)
java.util.regex.PatternSyntaxException: Dangling meta character '*' near index 0
*文章为作者独立观点，不代表虎嗅网立场|未来面前，你我还都是孩子，还不去下载 虎嗅App 猛嗅创新！|虎嗅注：本文[来|转]自.*公众号.*?。
^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2123) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:53) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]
2017-08-11 18:55:10,488 [task-result-getter-0] WARN  [org.apache.spark.scheduler.TaskSetManager] - Lost task 1.0 in stage 2.0 (TID 5, localhost): java.util.regex.PatternSyntaxException: Dangling meta character '*' near index 0
*文章为作者独立观点，不代表虎嗅网立场|未来面前，你我还都是孩子，还不去下载 虎嗅App 猛嗅创新！|虎嗅注：本文[来|转]自.*公众号.*?。
^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.sequence(Pattern.java:2123)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:53)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2017-08-11 18:55:10,490 [task-result-getter-0] ERROR [org.apache.spark.scheduler.TaskSetManager] - Task 1 in stage 2.0 failed 1 times; aborting job
2017-08-11 18:55:10,493 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Cancelling stage 2
2017-08-11 18:55:10,496 [dispatcher-event-loop-3] INFO  [org.apache.spark.executor.Executor] - Executor is trying to kill task 0.0 in stage 2.0 (TID 4)
2017-08-11 18:55:10,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Stage 2 was cancelled
2017-08-11 18:55:10,497 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (foreachPartition at streamingProcessNew.scala:51) failed in 0.458 s
2017-08-11 18:55:10,498 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2 failed: foreachPartition at streamingProcessNew.scala:51, took 0.473196 s
2017-08-11 18:55:10,499 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502448910000 ms.0 from job set of time 1502448910000 ms
2017-08-11 18:55:10,499 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.499 s for time 1502448910000 ms (execution: 0.484 s)
2017-08-11 18:55:10,499 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 3 from persistence list
2017-08-11 18:55:10,500 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 3
2017-08-11 18:55:10,500 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2 from persistence list
2017-08-11 18:55:10,501 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2
2017-08-11 18:55:10,501 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 18:55:10,501 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502448900000 ms
2017-08-11 18:55:10,502 [JobScheduler] ERROR [org.apache.spark.streaming.scheduler.JobScheduler] - Error running job streaming job 1502448910000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 5, localhost): java.util.regex.PatternSyntaxException: Dangling meta character '*' near index 0
*文章为作者独立观点，不代表虎嗅网立场|未来面前，你我还都是孩子，还不去下载 虎嗅App 猛嗅创新！|虎嗅注：本文[来|转]自.*公众号.*?。
^
	at java.util.regex.Pattern.error(Pattern.java:1955)
	at java.util.regex.Pattern.sequence(Pattern.java:2123)
	at java.util.regex.Pattern.expr(Pattern.java:1996)
	at java.util.regex.Pattern.compile(Pattern.java:1696)
	at java.util.regex.Pattern.<init>(Pattern.java:1351)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at java.lang.String.replaceAll(String.java:2223)
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:53)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56)
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[scala-library.jar:na]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[scala-library.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.Option.foreach(Option.scala:257) ~[scala-library.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1911) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:881) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:881) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at scala.util.Try$.apply(Try.scala:192) ~[scala-library.jar:na]
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) ~[scala-library.jar:na]
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:244) ~[spark-streaming_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_111]
Caused by: java.util.regex.PatternSyntaxException: Dangling meta character '*' near index 0
*文章为作者独立观点，不代表虎嗅网立场|未来面前，你我还都是孩子，还不去下载 虎嗅App 猛嗅创新！|虎嗅注：本文[来|转]自.*公众号.*?。
^
	at java.util.regex.Pattern.error(Pattern.java:1955) ~[na:1.8.0_111]
	at java.util.regex.Pattern.sequence(Pattern.java:2123) ~[na:1.8.0_111]
	at java.util.regex.Pattern.expr(Pattern.java:1996) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1696) ~[na:1.8.0_111]
	at java.util.regex.Pattern.<init>(Pattern.java:1351) ~[na:1.8.0_111]
	at java.util.regex.Pattern.compile(Pattern.java:1028) ~[na:1.8.0_111]
	at java.lang.String.replaceAll(String.java:2223) ~[na:1.8.0_111]
	at cn.datapark.process.education.reg.replaceContextProcess$.replaceContext(replaceContextProcess.scala:53) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$1.apply(streamingProcessNew.scala:49) ~[classes/:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) ~[scala-library.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library.jar:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:56) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	... 3 common frames omitted
2017-08-11 18:55:10,509 [Thread-0] INFO  [org.apache.spark.streaming.StreamingContext] - Invoking stop(stopGracefully=false) from shutdown hook
2017-08-11 18:55:10,511 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Stopping JobGenerator immediately
2017-08-11 18:55:10,512 [Thread-0] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Stopped timer for JobGenerator after time 1502448910000
2017-08-11 18:55:10,513 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Stopped JobGenerator
2017-08-11 18:55:10,514 [Thread-0] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Stopped JobScheduler
2017-08-11 18:55:10,519 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1740dae{/streaming,null,UNAVAILABLE}
2017-08-11 18:55:10,520 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@82cd4f{/streaming/batch,null,UNAVAILABLE}
2017-08-11 18:55:10,521 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1cf9028{/static/streaming,null,UNAVAILABLE}
2017-08-11 18:55:10,521 [Thread-0] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext stopped successfully
2017-08-11 18:55:10,522 [Thread-0] INFO  [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2017-08-11 18:55:10,525 [Thread-0] INFO  [org.spark_project.jetty.server.ServerConnector] - Stopped ServerConnector@1139eab{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@5de5a4{/stages/stage/kill,null,UNAVAILABLE}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@f0fba8{/api,null,UNAVAILABLE}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1005f6c{/,null,UNAVAILABLE}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1cec031{/static,null,UNAVAILABLE}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@13cb11{/executors/threadDump/json,null,UNAVAILABLE}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@beebb7{/executors/threadDump,null,UNAVAILABLE}
2017-08-11 18:55:10,526 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@320ade{/executors/json,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@faea88{/executors,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@158ed3c{/environment/json,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1366c9b{/environment,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@185fa6b{/storage/rdd/json,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@51218e{/storage/rdd,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1be7cd5{/storage/json,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@d6972f{/storage,null,UNAVAILABLE}
2017-08-11 18:55:10,527 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1648ee9{/stages/pool/json,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@d3b1f5{/stages/pool,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1b3bc3{/stages/stage/json,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@15d8c96{/stages/stage,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@fe7086{/stages/json,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@e70ea3{/stages,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@276e50{/jobs/job/json,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@7471ce{/jobs/job,null,UNAVAILABLE}
2017-08-11 18:55:10,528 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@7f474e{/jobs/json,null,UNAVAILABLE}
2017-08-11 18:55:10,529 [Thread-0] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Stopped o.s.j.s.ServletContextHandler@1f22f18{/jobs,null,UNAVAILABLE}
2017-08-11 18:55:10,530 [Thread-0] INFO  [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://192.168.31.111:4040
2017-08-11 18:55:10,536 [BoneCP-connection-watch-thread] ERROR [com.jolbox.bonecp.CloseThreadMonitor] - Connection obtained from thread [Executor task launch worker-1] was never closed. 
Stack trace of location where connection was obtained follows:
 java.lang.Thread.getStackTrace(Thread.java:1556)
 com.jolbox.bonecp.BoneCP.captureStackTrace(BoneCP.java:572)
 com.jolbox.bonecp.BoneCP.watchConnection(BoneCP.java:561)
 com.jolbox.bonecp.AbstractConnectionStrategy.postConnection(AbstractConnectionStrategy.java:79)
 com.jolbox.bonecp.AbstractConnectionStrategy.getConnection(AbstractConnectionStrategy.java:92)
 com.jolbox.bonecp.BoneCP.getConnection(BoneCP.java:553)
 cn.datapark.process.education.Util.ConnectionPool$.getConnection(ConnectionPool.scala:32)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:53)
 cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
 org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
 org.apache.spark.scheduler.Task.run(Task.scala:85)
 org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
 java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 java.lang.Thread.run(Thread.java:745)

2017-08-11 18:55:10,540 [dispatcher-event-loop-3] INFO  [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2017-08-11 18:55:10,549 [Thread-0] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2017-08-11 18:55:10,550 [Thread-0] INFO  [org.apache.spark.storage.BlockManager] - BlockManager stopped
2017-08-11 18:55:10,551 [Thread-0] INFO  [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2017-08-11 18:55:10,553 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2017-08-11 18:55:10,555 [Thread-0] INFO  [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2017-08-11 18:55:10,556 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2017-08-11 18:55:10,557 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\DN\AppData\Local\Temp\spark-a1e32a3a-2551-4600-b6b3-486ba4bf8dcc
2017-08-11 19:04:35,297 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-11 19:04:35,500 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-11 19:04:35,585 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-11 19:04:35,586 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-11 19:04:35,586 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-11 19:04:35,587 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-11 19:04:35,587 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-11 19:04:36,317 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 58009.
2017-08-11 19:04:36,337 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-11 19:04:36,355 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-11 19:04:36,369 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-a3590255-845b-4aa8-9d7d-97e95eae7dbc
2017-08-11 19:04:36,382 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-11 19:04:36,424 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-11 19:04:36,507 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2205ms
2017-08-11 19:04:36,606 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-11 19:04:36,623 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1f22f18{/jobs,null,AVAILABLE}
2017-08-11 19:04:36,624 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7f474e{/jobs/json,null,AVAILABLE}
2017-08-11 19:04:36,624 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7471ce{/jobs/job,null,AVAILABLE}
2017-08-11 19:04:36,624 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@276e50{/jobs/job/json,null,AVAILABLE}
2017-08-11 19:04:36,624 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e70ea3{/stages,null,AVAILABLE}
2017-08-11 19:04:36,625 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fe7086{/stages/json,null,AVAILABLE}
2017-08-11 19:04:36,625 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15d8c96{/stages/stage,null,AVAILABLE}
2017-08-11 19:04:36,625 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/stages/stage/json,null,AVAILABLE}
2017-08-11 19:04:36,625 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/stages/pool,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/stages/pool/json,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/storage,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/storage/json,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/storage/rdd,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/storage/rdd/json,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/environment,null,AVAILABLE}
2017-08-11 19:04:36,626 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/environment/json,null,AVAILABLE}
2017-08-11 19:04:36,627 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/executors,null,AVAILABLE}
2017-08-11 19:04:36,627 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/executors/json,null,AVAILABLE}
2017-08-11 19:04:36,627 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/executors/threadDump,null,AVAILABLE}
2017-08-11 19:04:36,628 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/executors/threadDump/json,null,AVAILABLE}
2017-08-11 19:04:36,636 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/static,null,AVAILABLE}
2017-08-11 19:04:36,636 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/,null,AVAILABLE}
2017-08-11 19:04:36,637 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0fba8{/api,null,AVAILABLE}
2017-08-11 19:04:36,637 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5de5a4{/stages/stage/kill,null,AVAILABLE}
2017-08-11 19:04:36,644 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@1139eab{HTTP/1.1}{0.0.0.0:4040}
2017-08-11 19:04:36,644 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2343ms
2017-08-11 19:04:36,644 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-11 19:04:36,646 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-11 19:04:36,731 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-11 19:04:36,761 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58030.
2017-08-11 19:04:36,762 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:58030
2017-08-11 19:04:36,764 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 58030)
2017-08-11 19:04:36,767 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:58030 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 58030)
2017-08-11 19:04:36,769 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 58030)
2017-08-11 19:04:36,976 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@109fac1{/metrics/json,null,AVAILABLE}
2017-08-11 19:04:37,471 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-08-11 19:04:37,472 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-11 19:04:37,472 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-11 19:04:37,474 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@1ae3817
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@14bf9cb
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-11 19:04:37,475 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-08-11 19:04:37,476 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@154106c
2017-08-11 19:04:37,516 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1502449480000
2017-08-11 19:04:37,517 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1502449480000 ms
2017-08-11 19:04:37,518 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-11 19:04:37,519 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1740dae{/streaming,null,AVAILABLE}
2017-08-11 19:04:37,520 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cf395{/streaming/json,null,AVAILABLE}
2017-08-11 19:04:37,520 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@82cd4f{/streaming/batch,null,AVAILABLE}
2017-08-11 19:04:37,521 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch/json,null,AVAILABLE}
2017-08-11 19:04:37,522 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cf9028{/static/streaming,null,AVAILABLE}
2017-08-11 19:04:37,522 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-11 19:04:40,211 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449480000 ms
2017-08-11 19:04:40,216 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449480000 ms.0 from job set of time 1502449480000 ms
2017-08-11 19:04:40,253 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:04:40,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:04:40,265 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:04:40,266 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:04:40,268 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:04:40,275 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:04:40,339 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-11 19:04:40,348 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:04:40,554 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:04:40,556 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:04:40,560 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:04:40,563 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:49)
2017-08-11 19:04:40,566 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-11 19:04:40,608 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 19:04:40,612 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 19:04:40,620 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-11 19:04:40,620 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-11 19:04:40,646 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 0 -> 25
2017-08-11 19:04:40,646 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 0 -> 25
2017-08-11 19:04:40,714 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-08-11 19:04:40,980 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-08-11 19:04:44,575 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-08-11 19:04:44,594 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 3981 ms on localhost (1/2)
2017-08-11 19:04:44,677 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-08-11 19:04:44,681 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 4092 ms on localhost (2/2)
2017-08-11 19:04:44,682 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcessNew.scala:51) finished in 4.104 s
2017-08-11 19:04:44,682 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-08-11 19:04:44,687 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcessNew.scala:51, took 4.433350 s
2017-08-11 19:04:44,692 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449480000 ms.0 from job set of time 1502449480000 ms
2017-08-11 19:04:44,693 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 4.690 s for time 1502449480000 ms (execution: 4.477 s)
2017-08-11 19:04:44,700 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:04:44,704 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 19:04:45,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449485000 ms
2017-08-11 19:04:45,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449485000 ms.0 from job set of time 1502449485000 ms
2017-08-11 19:04:45,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:04:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:04:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:04:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:04:45,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:04:45,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:04:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:04:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:04:45,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:04:45,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:04:45,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:49)
2017-08-11 19:04:45,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-08-11 19:04:45,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 19:04:45,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 19:04:45,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-08-11 19:04:45,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-08-11 19:04:45,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 25 -> 50
2017-08-11 19:04:45,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 25 -> 50
2017-08-11 19:04:46,713 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 787 bytes result sent to driver
2017-08-11 19:04:46,720 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 1655 ms on localhost (1/2)
2017-08-11 19:04:47,594 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 787 bytes result sent to driver
2017-08-11 19:04:47,597 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 2535 ms on localhost (2/2)
2017-08-11 19:04:47,597 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-08-11 19:04:47,598 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcessNew.scala:51) finished in 2.535 s
2017-08-11 19:04:47,598 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: foreachPartition at streamingProcessNew.scala:51, took 2.549610 s
2017-08-11 19:04:47,599 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449485000 ms.0 from job set of time 1502449485000 ms
2017-08-11 19:04:47,600 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.599 s for time 1502449485000 ms (execution: 2.575 s)
2017-08-11 19:04:47,602 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-08-11 19:04:47,611 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-08-11 19:04:47,611 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:04:47,611 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-08-11 19:04:47,611 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-08-11 19:04:47,612 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-08-11 19:04:50,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449490000 ms
2017-08-11 19:04:50,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449490000 ms.0 from job set of time 1502449490000 ms
2017-08-11 19:04:50,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:04:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:04:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:04:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:04:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:04:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:04:50,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:04:50,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:04:50,052 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:04:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:04:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:49)
2017-08-11 19:04:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2017-08-11 19:04:50,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5655 bytes)
2017-08-11 19:04:50,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5655 bytes)
2017-08-11 19:04:50,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2017-08-11 19:04:50,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2017-08-11 19:04:50,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 50 -> 75
2017-08-11 19:04:50,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 50 -> 75
2017-08-11 19:04:50,473 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:04:51,386 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2.0 (TID 5). 787 bytes result sent to driver
2017-08-11 19:04:51,389 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2.0 (TID 5) in 1331 ms on localhost (1/2)
2017-08-11 19:04:51,564 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 787 bytes result sent to driver
2017-08-11 19:04:51,567 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 1512 ms on localhost (2/2)
2017-08-11 19:04:51,568 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (foreachPartition at streamingProcessNew.scala:51) finished in 1.512 s
2017-08-11 19:04:51,568 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-08-11 19:04:51,568 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: foreachPartition at streamingProcessNew.scala:51, took 1.530642 s
2017-08-11 19:04:51,569 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449490000 ms.0 from job set of time 1502449490000 ms
2017-08-11 19:04:51,569 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.569 s for time 1502449490000 ms (execution: 1.546 s)
2017-08-11 19:04:51,569 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 3 from persistence list
2017-08-11 19:04:51,570 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 3
2017-08-11 19:04:51,570 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2 from persistence list
2017-08-11 19:04:51,570 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2
2017-08-11 19:04:51,570 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:04:51,570 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449480000 ms
2017-08-11 19:04:55,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449495000 ms
2017-08-11 19:04:55,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449495000 ms.0 from job set of time 1502449495000 ms
2017-08-11 19:04:55,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:04:55,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:04:55,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 3 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:04:55,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:04:55,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:04:55,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:04:55,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:04:55,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:04:55,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:04:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:04:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcessNew.scala:49)
2017-08-11 19:04:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 3.0 with 2 tasks
2017-08-11 19:04:55,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 3.0 (TID 6, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:04:55,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 3.0 (TID 7, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:04:55,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 3.0 (TID 7)
2017-08-11 19:04:55,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 3.0 (TID 6)
2017-08-11 19:04:55,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 75 -> 100
2017-08-11 19:04:55,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 75 -> 100
2017-08-11 19:04:55,394 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:04:55,689 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 3.0 (TID 7). 787 bytes result sent to driver
2017-08-11 19:04:55,692 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 3.0 (TID 7) in 631 ms on localhost (1/2)
2017-08-11 19:04:55,856 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 3.0 (TID 6). 787 bytes result sent to driver
2017-08-11 19:04:55,858 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 3.0 (TID 6) in 799 ms on localhost (2/2)
2017-08-11 19:04:55,858 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-08-11 19:04:55,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 3 (foreachPartition at streamingProcessNew.scala:51) finished in 0.799 s
2017-08-11 19:04:55,859 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: foreachPartition at streamingProcessNew.scala:51, took 0.816545 s
2017-08-11 19:04:55,859 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449495000 ms.0 from job set of time 1502449495000 ms
2017-08-11 19:04:55,860 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.859 s for time 1502449495000 ms (execution: 0.836 s)
2017-08-11 19:04:55,860 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 5 from persistence list
2017-08-11 19:04:55,860 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 5
2017-08-11 19:04:55,860 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 4 from persistence list
2017-08-11 19:04:55,860 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 4
2017-08-11 19:04:55,860 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:04:55,860 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449485000 ms
2017-08-11 19:05:00,026 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449500000 ms
2017-08-11 19:05:00,027 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449500000 ms.0 from job set of time 1502449500000 ms
2017-08-11 19:05:00,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 4 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:00,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:00,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:00,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:00,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 2 tasks
2017-08-11 19:05:00,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 8, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:00,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 4.0 (TID 9, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:00,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 4.0 (TID 9)
2017-08-11 19:05:00,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 8)
2017-08-11 19:05:00,066 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 100 -> 125
2017-08-11 19:05:00,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 100 -> 125
2017-08-11 19:05:01,924 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 4.0 (TID 9). 714 bytes result sent to driver
2017-08-11 19:05:01,926 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 4.0 (TID 9) in 1865 ms on localhost (1/2)
2017-08-11 19:05:04,754 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_3_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:05,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449505000 ms
2017-08-11 19:05:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449510000 ms
2017-08-11 19:05:11,150 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 8). 787 bytes result sent to driver
2017-08-11 19:05:11,152 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 8) in 11093 ms on localhost (2/2)
2017-08-11 19:05:11,152 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-08-11 19:05:11,152 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (foreachPartition at streamingProcessNew.scala:51) finished in 11.093 s
2017-08-11 19:05:11,153 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 4 finished: foreachPartition at streamingProcessNew.scala:51, took 11.105778 s
2017-08-11 19:05:11,154 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449500000 ms.0 from job set of time 1502449500000 ms
2017-08-11 19:05:11,154 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 11.154 s for time 1502449500000 ms (execution: 11.127 s)
2017-08-11 19:05:11,154 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 7 from persistence list
2017-08-11 19:05:11,154 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449505000 ms.0 from job set of time 1502449505000 ms
2017-08-11 19:05:11,154 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 7
2017-08-11 19:05:11,154 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 6 from persistence list
2017-08-11 19:05:11,155 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 6
2017-08-11 19:05:11,155 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:05:11,155 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449490000 ms
2017-08-11 19:05:11,161 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:11,161 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 5 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:11,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 5 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:11,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:11,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:11,162 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:11,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:11,166 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:11,167 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:11,168 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:11,168 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:11,168 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 5.0 with 2 tasks
2017-08-11 19:05:11,170 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 5.0 (TID 10, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:11,171 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 5.0 (TID 11, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:11,171 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 5.0 (TID 11)
2017-08-11 19:05:11,171 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 5.0 (TID 10)
2017-08-11 19:05:11,172 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 125 -> 150
2017-08-11 19:05:11,172 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 125 -> 150
2017-08-11 19:05:12,190 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_4_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:12,223 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 5.0 (TID 11). 787 bytes result sent to driver
2017-08-11 19:05:12,226 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 5.0 (TID 11) in 1056 ms on localhost (1/2)
2017-08-11 19:05:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449515000 ms
2017-08-11 19:05:16,764 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 5.0 (TID 10). 787 bytes result sent to driver
2017-08-11 19:05:16,768 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 5.0 (TID 10) in 5600 ms on localhost (2/2)
2017-08-11 19:05:16,769 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-08-11 19:05:16,769 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 5 (foreachPartition at streamingProcessNew.scala:51) finished in 5.601 s
2017-08-11 19:05:16,770 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 5 finished: foreachPartition at streamingProcessNew.scala:51, took 5.608559 s
2017-08-11 19:05:16,770 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449505000 ms.0 from job set of time 1502449505000 ms
2017-08-11 19:05:16,771 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 11.770 s for time 1502449505000 ms (execution: 5.616 s)
2017-08-11 19:05:16,771 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449510000 ms.0 from job set of time 1502449510000 ms
2017-08-11 19:05:16,771 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 9 from persistence list
2017-08-11 19:05:16,772 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 9
2017-08-11 19:05:16,772 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 8 from persistence list
2017-08-11 19:05:16,772 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 8
2017-08-11 19:05:16,772 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:05:16,773 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449495000 ms
2017-08-11 19:05:16,778 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:16,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 6 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:16,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 6 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:16,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:16,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:16,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:16,780 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:16,786 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:16,789 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_6_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:16,789 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:16,790 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:16,790 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 6.0 with 2 tasks
2017-08-11 19:05:16,791 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 6.0 (TID 12, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:16,792 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 6.0 (TID 13, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:16,792 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 6.0 (TID 12)
2017-08-11 19:05:16,792 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 6.0 (TID 13)
2017-08-11 19:05:16,794 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 150 -> 175
2017-08-11 19:05:16,794 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 150 -> 175
2017-08-11 19:05:17,521 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_5_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:20,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449520000 ms
2017-08-11 19:05:23,762 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 6.0 (TID 13). 787 bytes result sent to driver
2017-08-11 19:05:23,763 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 6.0 (TID 13) in 6972 ms on localhost (1/2)
2017-08-11 19:05:25,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449525000 ms
2017-08-11 19:05:27,903 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 6.0 (TID 12). 787 bytes result sent to driver
2017-08-11 19:05:27,905 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 6.0 (TID 12) in 11115 ms on localhost (2/2)
2017-08-11 19:05:27,905 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 6 (foreachPartition at streamingProcessNew.scala:51) finished in 11.115 s
2017-08-11 19:05:27,905 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2017-08-11 19:05:27,905 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 6 finished: foreachPartition at streamingProcessNew.scala:51, took 11.126572 s
2017-08-11 19:05:27,906 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449510000 ms.0 from job set of time 1502449510000 ms
2017-08-11 19:05:27,906 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 17.906 s for time 1502449510000 ms (execution: 11.135 s)
2017-08-11 19:05:27,906 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 11 from persistence list
2017-08-11 19:05:27,906 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449515000 ms.0 from job set of time 1502449515000 ms
2017-08-11 19:05:27,906 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 11
2017-08-11 19:05:27,906 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 10 from persistence list
2017-08-11 19:05:27,907 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 10
2017-08-11 19:05:27,907 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:05:27,907 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449500000 ms
2017-08-11 19:05:27,913 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:27,913 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 7 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:27,913 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 7 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:27,913 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:27,914 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:27,914 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:27,915 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:27,919 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:27,920 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_7_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:27,920 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:27,920 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:27,920 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 7.0 with 2 tasks
2017-08-11 19:05:27,922 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 7.0 (TID 14, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:27,922 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 7.0 (TID 15, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:27,923 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 7.0 (TID 15)
2017-08-11 19:05:27,923 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 7.0 (TID 14)
2017-08-11 19:05:27,924 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 175 -> 200
2017-08-11 19:05:27,924 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 175 -> 200
2017-08-11 19:05:28,759 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_6_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:30,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449530000 ms
2017-08-11 19:05:34,000 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 7.0 (TID 14). 787 bytes result sent to driver
2017-08-11 19:05:34,002 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 7.0 (TID 14) in 6081 ms on localhost (1/2)
2017-08-11 19:05:35,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449535000 ms
2017-08-11 19:05:36,173 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 7.0 (TID 15). 787 bytes result sent to driver
2017-08-11 19:05:36,176 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 7.0 (TID 15) in 8254 ms on localhost (2/2)
2017-08-11 19:05:36,176 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 7 (foreachPartition at streamingProcessNew.scala:51) finished in 8.255 s
2017-08-11 19:05:36,177 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2017-08-11 19:05:36,177 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 7 finished: foreachPartition at streamingProcessNew.scala:51, took 8.263509 s
2017-08-11 19:05:36,177 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449515000 ms.0 from job set of time 1502449515000 ms
2017-08-11 19:05:36,178 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 21.177 s for time 1502449515000 ms (execution: 8.271 s)
2017-08-11 19:05:36,178 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 13 from persistence list
2017-08-11 19:05:36,178 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449520000 ms.0 from job set of time 1502449520000 ms
2017-08-11 19:05:36,178 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 12 from persistence list
2017-08-11 19:05:36,178 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 12
2017-08-11 19:05:36,178 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 13
2017-08-11 19:05:36,179 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:05:36,179 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449505000 ms
2017-08-11 19:05:36,186 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:36,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 8 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:36,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 8 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:36,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:36,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:36,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:36,189 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:36,193 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:36,194 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_8_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:36,194 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:36,194 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:36,195 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 8.0 with 2 tasks
2017-08-11 19:05:36,196 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 8.0 (TID 16, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:36,197 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 8.0 (TID 17, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:36,197 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 8.0 (TID 17)
2017-08-11 19:05:36,197 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 8.0 (TID 16)
2017-08-11 19:05:36,199 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 200 -> 225
2017-08-11 19:05:36,199 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 200 -> 225
2017-08-11 19:05:37,943 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_7_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449540000 ms
2017-08-11 19:05:41,880 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 8.0 (TID 16). 787 bytes result sent to driver
2017-08-11 19:05:41,882 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 8.0 (TID 16) in 5687 ms on localhost (1/2)
2017-08-11 19:05:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449545000 ms
2017-08-11 19:05:47,277 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 8.0 (TID 17). 787 bytes result sent to driver
2017-08-11 19:05:47,278 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 8.0 (TID 17) in 11082 ms on localhost (2/2)
2017-08-11 19:05:47,279 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2017-08-11 19:05:47,279 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 8 (foreachPartition at streamingProcessNew.scala:51) finished in 11.084 s
2017-08-11 19:05:47,279 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 8 finished: foreachPartition at streamingProcessNew.scala:51, took 11.092670 s
2017-08-11 19:05:47,280 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449520000 ms.0 from job set of time 1502449520000 ms
2017-08-11 19:05:47,280 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 27.280 s for time 1502449520000 ms (execution: 11.102 s)
2017-08-11 19:05:47,280 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 15 from persistence list
2017-08-11 19:05:47,280 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449525000 ms.0 from job set of time 1502449525000 ms
2017-08-11 19:05:47,280 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 15
2017-08-11 19:05:47,280 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 14 from persistence list
2017-08-11 19:05:47,281 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 14
2017-08-11 19:05:47,281 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:05:47,281 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449510000 ms
2017-08-11 19:05:47,286 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:47,287 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 9 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:47,287 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 9 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:47,287 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:47,287 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:47,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:47,289 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:47,292 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:47,293 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_9_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:47,293 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:47,293 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:47,293 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 9.0 with 2 tasks
2017-08-11 19:05:47,294 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 9.0 (TID 18, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:47,295 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 9.0 (TID 19, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:47,295 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 9.0 (TID 18)
2017-08-11 19:05:47,295 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 9.0 (TID 19)
2017-08-11 19:05:47,297 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 225 -> 250
2017-08-11 19:05:47,297 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 225 -> 250
2017-08-11 19:05:47,992 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_8_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:50,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449550000 ms
2017-08-11 19:05:53,405 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 9.0 (TID 18). 787 bytes result sent to driver
2017-08-11 19:05:53,408 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 9.0 (TID 18) in 6114 ms on localhost (1/2)
2017-08-11 19:05:53,688 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 9.0 (TID 19). 787 bytes result sent to driver
2017-08-11 19:05:53,690 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 9.0 (TID 19) in 6395 ms on localhost (2/2)
2017-08-11 19:05:53,690 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2017-08-11 19:05:53,690 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 9 (foreachPartition at streamingProcessNew.scala:51) finished in 6.396 s
2017-08-11 19:05:53,690 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 9 finished: foreachPartition at streamingProcessNew.scala:51, took 6.403461 s
2017-08-11 19:05:53,690 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449525000 ms.0 from job set of time 1502449525000 ms
2017-08-11 19:05:53,691 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 28.690 s for time 1502449525000 ms (execution: 6.410 s)
2017-08-11 19:05:53,691 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 17 from persistence list
2017-08-11 19:05:53,691 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449530000 ms.0 from job set of time 1502449530000 ms
2017-08-11 19:05:53,691 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 17
2017-08-11 19:05:53,691 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 16 from persistence list
2017-08-11 19:05:53,691 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 16
2017-08-11 19:05:53,691 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:05:53,691 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449515000 ms
2017-08-11 19:05:53,696 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:05:53,697 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 10 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:05:53,697 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 10 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:05:53,697 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:05:53,697 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:05:53,698 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:05:53,699 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:05:53,702 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:05:53,703 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_10_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:53,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:05:53,703 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcessNew.scala:49)
2017-08-11 19:05:53,704 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 10.0 with 2 tasks
2017-08-11 19:05:53,704 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 10.0 (TID 20, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:05:53,705 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 10.0 (TID 21, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:05:53,705 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 10.0 (TID 20)
2017-08-11 19:05:53,705 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 10.0 (TID 21)
2017-08-11 19:05:53,706 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 250 -> 275
2017-08-11 19:05:53,706 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 250 -> 275
2017-08-11 19:05:54,765 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_9_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:05:55,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449555000 ms
2017-08-11 19:05:59,491 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 10.0 (TID 20). 787 bytes result sent to driver
2017-08-11 19:05:59,493 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 10.0 (TID 20) in 5789 ms on localhost (1/2)
2017-08-11 19:06:00,025 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449560000 ms
2017-08-11 19:06:00,514 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 10.0 (TID 21). 787 bytes result sent to driver
2017-08-11 19:06:00,518 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 10.0 (TID 21) in 6813 ms on localhost (2/2)
2017-08-11 19:06:00,518 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 10 (foreachPartition at streamingProcessNew.scala:51) finished in 6.814 s
2017-08-11 19:06:00,519 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2017-08-11 19:06:00,519 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 10 finished: foreachPartition at streamingProcessNew.scala:51, took 6.822007 s
2017-08-11 19:06:00,519 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449530000 ms.0 from job set of time 1502449530000 ms
2017-08-11 19:06:00,520 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 30.519 s for time 1502449530000 ms (execution: 6.828 s)
2017-08-11 19:06:00,520 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 19 from persistence list
2017-08-11 19:06:00,520 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449535000 ms.0 from job set of time 1502449535000 ms
2017-08-11 19:06:00,520 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 18 from persistence list
2017-08-11 19:06:00,520 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 19
2017-08-11 19:06:00,521 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 18
2017-08-11 19:06:00,521 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:00,521 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449520000 ms
2017-08-11 19:06:00,525 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:00,526 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 11 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:00,526 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 11 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:00,526 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:00,526 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:00,526 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:00,528 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:00,531 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:00,532 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_11_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:00,533 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 11 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:00,533 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:00,534 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 11.0 with 2 tasks
2017-08-11 19:06:00,535 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 11.0 (TID 22, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:00,536 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 11.0 (TID 23, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:00,536 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 11.0 (TID 23)
2017-08-11 19:06:00,536 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 11.0 (TID 22)
2017-08-11 19:06:00,537 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 275 -> 300
2017-08-11 19:06:00,537 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 275 -> 300
2017-08-11 19:06:02,585 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_10_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:05,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449565000 ms
2017-08-11 19:06:07,539 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 11.0 (TID 23). 787 bytes result sent to driver
2017-08-11 19:06:07,541 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 11.0 (TID 23) in 7006 ms on localhost (1/2)
2017-08-11 19:06:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449570000 ms
2017-08-11 19:06:10,759 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 11.0 (TID 22). 787 bytes result sent to driver
2017-08-11 19:06:10,761 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 11.0 (TID 22) in 10226 ms on localhost (2/2)
2017-08-11 19:06:10,761 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2017-08-11 19:06:10,761 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 11 (foreachPartition at streamingProcessNew.scala:51) finished in 10.227 s
2017-08-11 19:06:10,761 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 11 finished: foreachPartition at streamingProcessNew.scala:51, took 10.235443 s
2017-08-11 19:06:10,762 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449535000 ms.0 from job set of time 1502449535000 ms
2017-08-11 19:06:10,762 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 35.762 s for time 1502449535000 ms (execution: 10.242 s)
2017-08-11 19:06:10,762 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 21 from persistence list
2017-08-11 19:06:10,762 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449540000 ms.0 from job set of time 1502449540000 ms
2017-08-11 19:06:10,762 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 21
2017-08-11 19:06:10,762 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 20 from persistence list
2017-08-11 19:06:10,763 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 20
2017-08-11 19:06:10,763 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:10,763 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449525000 ms
2017-08-11 19:06:10,767 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:10,768 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 12 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:10,768 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 12 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:10,768 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:10,768 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:10,769 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:10,770 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:10,773 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:10,773 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_12_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:10,774 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:10,774 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:10,774 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 12.0 with 2 tasks
2017-08-11 19:06:10,775 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 12.0 (TID 24, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:10,776 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 12.0 (TID 25, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:10,776 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 12.0 (TID 25)
2017-08-11 19:06:10,776 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 12.0 (TID 24)
2017-08-11 19:06:10,777 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 300 -> 325
2017-08-11 19:06:10,777 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 300 -> 325
2017-08-11 19:06:11,739 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_11_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:13,388 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 12.0 (TID 24). 787 bytes result sent to driver
2017-08-11 19:06:13,390 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 12.0 (TID 24) in 2615 ms on localhost (1/2)
2017-08-11 19:06:15,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449575000 ms
2017-08-11 19:06:16,993 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 12.0 (TID 25). 700 bytes result sent to driver
2017-08-11 19:06:16,995 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 12.0 (TID 25) in 6220 ms on localhost (2/2)
2017-08-11 19:06:16,995 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 12 (foreachPartition at streamingProcessNew.scala:51) finished in 6.221 s
2017-08-11 19:06:16,995 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2017-08-11 19:06:16,996 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 12 finished: foreachPartition at streamingProcessNew.scala:51, took 6.227541 s
2017-08-11 19:06:16,996 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449540000 ms.0 from job set of time 1502449540000 ms
2017-08-11 19:06:16,996 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 36.996 s for time 1502449540000 ms (execution: 6.234 s)
2017-08-11 19:06:16,996 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 23 from persistence list
2017-08-11 19:06:16,997 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 22 from persistence list
2017-08-11 19:06:16,997 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:16,997 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 22
2017-08-11 19:06:16,997 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449530000 ms
2017-08-11 19:06:16,996 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449545000 ms.0 from job set of time 1502449545000 ms
2017-08-11 19:06:16,997 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 23
2017-08-11 19:06:17,005 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:17,005 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 13 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:17,005 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 13 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:17,005 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:17,006 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:17,006 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:17,007 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:17,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:17,021 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_13_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:17,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 13 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:17,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:17,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 13.0 with 2 tasks
2017-08-11 19:06:17,023 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 13.0 (TID 26, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:17,023 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 13.0 (TID 27, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:17,024 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 13.0 (TID 26)
2017-08-11 19:06:17,024 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 13.0 (TID 27)
2017-08-11 19:06:17,025 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 325 -> 350
2017-08-11 19:06:17,025 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 325 -> 350
2017-08-11 19:06:17,557 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_12_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:18,210 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 13.0 (TID 26). 787 bytes result sent to driver
2017-08-11 19:06:18,212 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 13.0 (TID 26) in 1190 ms on localhost (1/2)
2017-08-11 19:06:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449580000 ms
2017-08-11 19:06:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449585000 ms
2017-08-11 19:06:25,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 13.0 (TID 27). 787 bytes result sent to driver
2017-08-11 19:06:25,060 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 13.0 (TID 27) in 8037 ms on localhost (2/2)
2017-08-11 19:06:25,060 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2017-08-11 19:06:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 13 (foreachPartition at streamingProcessNew.scala:51) finished in 8.038 s
2017-08-11 19:06:25,061 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 13 finished: foreachPartition at streamingProcessNew.scala:51, took 8.055387 s
2017-08-11 19:06:25,061 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449545000 ms.0 from job set of time 1502449545000 ms
2017-08-11 19:06:25,062 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 25 from persistence list
2017-08-11 19:06:25,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 40.061 s for time 1502449545000 ms (execution: 8.065 s)
2017-08-11 19:06:25,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449550000 ms.0 from job set of time 1502449550000 ms
2017-08-11 19:06:25,062 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 25
2017-08-11 19:06:25,062 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 24 from persistence list
2017-08-11 19:06:25,062 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 24
2017-08-11 19:06:25,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:25,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449535000 ms
2017-08-11 19:06:25,069 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:25,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 14 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:25,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 14 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:25,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:25,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:25,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:25,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:25,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:25,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_14_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:25,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:25,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:25,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 14.0 with 2 tasks
2017-08-11 19:06:25,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 14.0 (TID 28, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:25,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 14.0 (TID 29, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:25,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 14.0 (TID 28)
2017-08-11 19:06:25,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 14.0 (TID 29)
2017-08-11 19:06:25,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 350 -> 375
2017-08-11 19:06:25,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 350 -> 375
2017-08-11 19:06:25,912 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_13_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:26,741 [Executor task launch worker-0] WARN  [cn.datapark.process.education.DB.data2MySQL$] - Error in execution of query()
2017-08-11 19:06:27,436 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 14.0 (TID 28). 787 bytes result sent to driver
2017-08-11 19:06:27,444 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 14.0 (TID 28) in 2368 ms on localhost (1/2)
2017-08-11 19:06:27,714 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 14.0 (TID 29). 787 bytes result sent to driver
2017-08-11 19:06:27,715 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 14.0 (TID 29) in 2638 ms on localhost (2/2)
2017-08-11 19:06:27,715 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 14 (foreachPartition at streamingProcessNew.scala:51) finished in 2.639 s
2017-08-11 19:06:27,716 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2017-08-11 19:06:27,716 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 14 finished: foreachPartition at streamingProcessNew.scala:51, took 2.646759 s
2017-08-11 19:06:27,716 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449550000 ms.0 from job set of time 1502449550000 ms
2017-08-11 19:06:27,716 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 37.716 s for time 1502449550000 ms (execution: 2.654 s)
2017-08-11 19:06:27,716 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 27 from persistence list
2017-08-11 19:06:27,716 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449555000 ms.0 from job set of time 1502449555000 ms
2017-08-11 19:06:27,717 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 27
2017-08-11 19:06:27,717 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 26 from persistence list
2017-08-11 19:06:27,717 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 26
2017-08-11 19:06:27,717 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:27,717 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449540000 ms
2017-08-11 19:06:27,723 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:27,724 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 15 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:27,724 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 15 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:27,724 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:27,724 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:27,725 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:27,726 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:27,729 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:27,729 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_15_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:27,730 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 15 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:27,730 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:27,730 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 15.0 with 2 tasks
2017-08-11 19:06:27,731 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 15.0 (TID 30, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:27,732 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 15.0 (TID 31, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:27,732 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 15.0 (TID 31)
2017-08-11 19:06:27,732 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 15.0 (TID 30)
2017-08-11 19:06:27,733 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 375 -> 400
2017-08-11 19:06:27,733 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 375 -> 400
2017-08-11 19:06:28,696 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_14_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:29,348 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 15.0 (TID 31). 787 bytes result sent to driver
2017-08-11 19:06:29,350 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 15.0 (TID 31) in 1619 ms on localhost (1/2)
2017-08-11 19:06:30,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449590000 ms
2017-08-11 19:06:30,152 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 15.0 (TID 30). 787 bytes result sent to driver
2017-08-11 19:06:30,154 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 15.0 (TID 30) in 2424 ms on localhost (2/2)
2017-08-11 19:06:30,155 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 15 (foreachPartition at streamingProcessNew.scala:51) finished in 2.425 s
2017-08-11 19:06:30,155 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2017-08-11 19:06:30,155 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 15 finished: foreachPartition at streamingProcessNew.scala:51, took 2.431826 s
2017-08-11 19:06:30,156 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449555000 ms.0 from job set of time 1502449555000 ms
2017-08-11 19:06:30,156 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 35.156 s for time 1502449555000 ms (execution: 2.440 s)
2017-08-11 19:06:30,156 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449560000 ms.0 from job set of time 1502449560000 ms
2017-08-11 19:06:30,156 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 29 from persistence list
2017-08-11 19:06:30,157 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 28 from persistence list
2017-08-11 19:06:30,157 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 29
2017-08-11 19:06:30,158 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 28
2017-08-11 19:06:30,158 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:30,158 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449545000 ms
2017-08-11 19:06:30,163 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:30,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 16 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:30,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 16 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:30,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:30,163 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:30,164 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:30,165 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:30,171 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:30,172 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_16_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:30,172 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 16 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:30,172 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:30,173 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 16.0 with 2 tasks
2017-08-11 19:06:30,174 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 16.0 (TID 32, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:30,174 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 16.0 (TID 33, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:30,175 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 16.0 (TID 33)
2017-08-11 19:06:30,175 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 16.0 (TID 32)
2017-08-11 19:06:30,176 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 400 -> 425
2017-08-11 19:06:30,176 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 400 -> 425
2017-08-11 19:06:30,417 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_15_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:33,270 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 16.0 (TID 32). 787 bytes result sent to driver
2017-08-11 19:06:33,272 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 16.0 (TID 32) in 3099 ms on localhost (1/2)
2017-08-11 19:06:34,277 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 16.0 (TID 33). 787 bytes result sent to driver
2017-08-11 19:06:34,279 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 16.0 (TID 33) in 4105 ms on localhost (2/2)
2017-08-11 19:06:34,279 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2017-08-11 19:06:34,279 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 16 (foreachPartition at streamingProcessNew.scala:51) finished in 4.106 s
2017-08-11 19:06:34,279 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 16 finished: foreachPartition at streamingProcessNew.scala:51, took 4.116102 s
2017-08-11 19:06:34,279 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449560000 ms.0 from job set of time 1502449560000 ms
2017-08-11 19:06:34,280 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 34.279 s for time 1502449560000 ms (execution: 4.123 s)
2017-08-11 19:06:34,280 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 31 from persistence list
2017-08-11 19:06:34,280 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449565000 ms.0 from job set of time 1502449565000 ms
2017-08-11 19:06:34,280 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 31
2017-08-11 19:06:34,280 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 30 from persistence list
2017-08-11 19:06:34,281 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 30
2017-08-11 19:06:34,281 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:34,281 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449550000 ms
2017-08-11 19:06:34,287 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:34,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 17 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:34,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 17 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:34,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:34,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:34,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:34,289 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:34,293 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:34,293 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_17_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:34,294 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 17 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:34,294 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:34,294 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 17.0 with 2 tasks
2017-08-11 19:06:34,295 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 17.0 (TID 34, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:34,296 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 17.0 (TID 35, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:34,296 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 17.0 (TID 35)
2017-08-11 19:06:34,296 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 17.0 (TID 34)
2017-08-11 19:06:34,297 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 425 -> 450
2017-08-11 19:06:34,297 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 425 -> 450
2017-08-11 19:06:34,877 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_16_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:35,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449595000 ms
2017-08-11 19:06:36,089 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 17.0 (TID 35). 787 bytes result sent to driver
2017-08-11 19:06:36,090 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 17.0 (TID 35) in 1795 ms on localhost (1/2)
2017-08-11 19:06:36,644 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 17.0 (TID 34). 787 bytes result sent to driver
2017-08-11 19:06:36,647 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 17.0 (TID 34) in 2352 ms on localhost (2/2)
2017-08-11 19:06:36,647 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 17 (foreachPartition at streamingProcessNew.scala:51) finished in 2.353 s
2017-08-11 19:06:36,647 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2017-08-11 19:06:36,647 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 17 finished: foreachPartition at streamingProcessNew.scala:51, took 2.359955 s
2017-08-11 19:06:36,648 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449565000 ms.0 from job set of time 1502449565000 ms
2017-08-11 19:06:36,648 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 31.648 s for time 1502449565000 ms (execution: 2.368 s)
2017-08-11 19:06:36,648 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 33 from persistence list
2017-08-11 19:06:36,648 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449570000 ms.0 from job set of time 1502449570000 ms
2017-08-11 19:06:36,648 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 33
2017-08-11 19:06:36,648 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 32 from persistence list
2017-08-11 19:06:36,649 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 32
2017-08-11 19:06:36,649 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:36,649 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449555000 ms
2017-08-11 19:06:36,654 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:36,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 18 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:36,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 18 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:36,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:36,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:36,656 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:36,657 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:36,659 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:36,660 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_18_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:36,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 18 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:36,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:36,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 18.0 with 2 tasks
2017-08-11 19:06:36,663 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 18.0 (TID 36, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:36,663 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 18.0 (TID 37, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:36,663 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 18.0 (TID 36)
2017-08-11 19:06:36,663 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 18.0 (TID 37)
2017-08-11 19:06:36,665 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 450 -> 475
2017-08-11 19:06:36,665 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 450 -> 475
2017-08-11 19:06:37,313 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_17_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:38,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 18.0 (TID 36). 787 bytes result sent to driver
2017-08-11 19:06:38,046 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 18.0 (TID 36) in 1384 ms on localhost (1/2)
2017-08-11 19:06:38,431 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 18.0 (TID 37). 787 bytes result sent to driver
2017-08-11 19:06:38,433 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 18.0 (TID 37) in 1770 ms on localhost (2/2)
2017-08-11 19:06:38,433 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2017-08-11 19:06:38,433 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 18 (foreachPartition at streamingProcessNew.scala:51) finished in 1.772 s
2017-08-11 19:06:38,434 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 18 finished: foreachPartition at streamingProcessNew.scala:51, took 1.779029 s
2017-08-11 19:06:38,434 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449570000 ms.0 from job set of time 1502449570000 ms
2017-08-11 19:06:38,434 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 28.434 s for time 1502449570000 ms (execution: 1.786 s)
2017-08-11 19:06:38,434 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 35 from persistence list
2017-08-11 19:06:38,434 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449575000 ms.0 from job set of time 1502449575000 ms
2017-08-11 19:06:38,435 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 35
2017-08-11 19:06:38,435 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 34 from persistence list
2017-08-11 19:06:38,435 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 34
2017-08-11 19:06:38,435 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:38,435 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449560000 ms
2017-08-11 19:06:38,440 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:38,441 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 19 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:38,441 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 19 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:38,441 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:38,441 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:38,441 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 19 (MapPartitionsRDD[39] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:38,442 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:38,445 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:38,447 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_19_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:38,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 19 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:38,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:38,448 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 19.0 with 2 tasks
2017-08-11 19:06:38,450 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 19.0 (TID 38, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:38,451 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 19.0 (TID 39, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:38,451 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 19.0 (TID 38)
2017-08-11 19:06:38,451 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 19.0 (TID 39)
2017-08-11 19:06:38,453 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 475 -> 500
2017-08-11 19:06:38,453 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 475 -> 500
2017-08-11 19:06:38,579 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_18_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:39,727 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 19.0 (TID 38). 787 bytes result sent to driver
2017-08-11 19:06:39,729 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 19.0 (TID 38) in 1279 ms on localhost (1/2)
2017-08-11 19:06:40,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449600000 ms
2017-08-11 19:06:40,403 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 19.0 (TID 39). 787 bytes result sent to driver
2017-08-11 19:06:40,404 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 19.0 (TID 39) in 1953 ms on localhost (2/2)
2017-08-11 19:06:40,405 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 19.0, whose tasks have all completed, from pool 
2017-08-11 19:06:40,405 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 19 (foreachPartition at streamingProcessNew.scala:51) finished in 1.956 s
2017-08-11 19:06:40,405 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 19 finished: foreachPartition at streamingProcessNew.scala:51, took 1.964646 s
2017-08-11 19:06:40,405 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449575000 ms.0 from job set of time 1502449575000 ms
2017-08-11 19:06:40,406 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 37 from persistence list
2017-08-11 19:06:40,406 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 25.405 s for time 1502449575000 ms (execution: 1.971 s)
2017-08-11 19:06:40,406 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 37
2017-08-11 19:06:40,406 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449580000 ms.0 from job set of time 1502449580000 ms
2017-08-11 19:06:40,406 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 36 from persistence list
2017-08-11 19:06:40,406 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 36
2017-08-11 19:06:40,406 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:40,407 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449565000 ms
2017-08-11 19:06:40,411 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:40,412 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 20 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:40,412 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 20 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:40,412 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:40,412 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:40,413 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 20 (MapPartitionsRDD[41] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:40,414 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:40,416 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:40,417 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_20_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:40,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 20 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:40,418 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:40,418 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 20.0 with 2 tasks
2017-08-11 19:06:40,419 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 20.0 (TID 40, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:40,419 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 20.0 (TID 41, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:40,420 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 20.0 (TID 40)
2017-08-11 19:06:40,420 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 20.0 (TID 41)
2017-08-11 19:06:40,421 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 500 -> 525
2017-08-11 19:06:40,421 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 500 -> 525
2017-08-11 19:06:40,850 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_19_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:40,969 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 20.0 (TID 40). 787 bytes result sent to driver
2017-08-11 19:06:40,970 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 20.0 (TID 40) in 552 ms on localhost (1/2)
2017-08-11 19:06:42,094 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 20.0 (TID 41). 787 bytes result sent to driver
2017-08-11 19:06:42,096 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 20.0 (TID 41) in 1677 ms on localhost (2/2)
2017-08-11 19:06:42,096 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 20.0, whose tasks have all completed, from pool 
2017-08-11 19:06:42,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 20 (foreachPartition at streamingProcessNew.scala:51) finished in 1.678 s
2017-08-11 19:06:42,096 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 20 finished: foreachPartition at streamingProcessNew.scala:51, took 1.684719 s
2017-08-11 19:06:42,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449580000 ms.0 from job set of time 1502449580000 ms
2017-08-11 19:06:42,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 22.097 s for time 1502449580000 ms (execution: 1.691 s)
2017-08-11 19:06:42,097 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 39 from persistence list
2017-08-11 19:06:42,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449585000 ms.0 from job set of time 1502449585000 ms
2017-08-11 19:06:42,097 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 39
2017-08-11 19:06:42,097 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 38 from persistence list
2017-08-11 19:06:42,098 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 38
2017-08-11 19:06:42,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:42,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449570000 ms
2017-08-11 19:06:42,104 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:42,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 21 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:42,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 21 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:42,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:42,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:42,106 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 21 (MapPartitionsRDD[43] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:42,107 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:42,109 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:42,110 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_21_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:42,111 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 21 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:42,111 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:42,111 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 21.0 with 2 tasks
2017-08-11 19:06:42,112 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 21.0 (TID 42, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:42,112 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 21.0 (TID 43, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:42,113 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 21.0 (TID 43)
2017-08-11 19:06:42,113 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 21.0 (TID 42)
2017-08-11 19:06:42,114 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 525 -> 550
2017-08-11 19:06:42,114 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 525 -> 550
2017-08-11 19:06:42,332 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_20_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:42,729 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 21.0 (TID 43). 787 bytes result sent to driver
2017-08-11 19:06:42,731 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 21.0 (TID 43) in 619 ms on localhost (1/2)
2017-08-11 19:06:43,192 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 21.0 (TID 42). 787 bytes result sent to driver
2017-08-11 19:06:43,193 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 21.0 (TID 42) in 1082 ms on localhost (2/2)
2017-08-11 19:06:43,193 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 21.0, whose tasks have all completed, from pool 
2017-08-11 19:06:43,193 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 21 (foreachPartition at streamingProcessNew.scala:51) finished in 1.082 s
2017-08-11 19:06:43,193 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 21 finished: foreachPartition at streamingProcessNew.scala:51, took 1.088578 s
2017-08-11 19:06:43,194 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449585000 ms.0 from job set of time 1502449585000 ms
2017-08-11 19:06:43,194 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 18.194 s for time 1502449585000 ms (execution: 1.097 s)
2017-08-11 19:06:43,194 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 41 from persistence list
2017-08-11 19:06:43,194 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449590000 ms.0 from job set of time 1502449590000 ms
2017-08-11 19:06:43,194 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 41
2017-08-11 19:06:43,194 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 40 from persistence list
2017-08-11 19:06:43,195 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 40
2017-08-11 19:06:43,195 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:43,195 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449575000 ms
2017-08-11 19:06:43,200 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:43,200 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 22 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:43,201 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 22 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:43,201 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:43,201 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:43,201 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 22 (MapPartitionsRDD[45] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:43,202 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:43,205 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:43,206 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_22_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:43,206 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 22 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:43,206 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:43,206 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 22.0 with 2 tasks
2017-08-11 19:06:43,207 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 22.0 (TID 44, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:43,208 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 22.0 (TID 45, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:43,208 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 22.0 (TID 44)
2017-08-11 19:06:43,208 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 22.0 (TID 45)
2017-08-11 19:06:43,209 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 550 -> 575
2017-08-11 19:06:43,210 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 550 -> 575
2017-08-11 19:06:43,477 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_21_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:44,153 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 22.0 (TID 45). 787 bytes result sent to driver
2017-08-11 19:06:44,155 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 22.0 (TID 45) in 946 ms on localhost (1/2)
2017-08-11 19:06:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449605000 ms
2017-08-11 19:06:47,650 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 22.0 (TID 44). 787 bytes result sent to driver
2017-08-11 19:06:47,651 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 22.0 (TID 44) in 4444 ms on localhost (2/2)
2017-08-11 19:06:47,651 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 22.0, whose tasks have all completed, from pool 
2017-08-11 19:06:47,652 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 22 (foreachPartition at streamingProcessNew.scala:51) finished in 4.445 s
2017-08-11 19:06:47,652 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 22 finished: foreachPartition at streamingProcessNew.scala:51, took 4.451541 s
2017-08-11 19:06:47,652 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449590000 ms.0 from job set of time 1502449590000 ms
2017-08-11 19:06:47,653 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 17.652 s for time 1502449590000 ms (execution: 4.458 s)
2017-08-11 19:06:47,653 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 43 from persistence list
2017-08-11 19:06:47,653 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449595000 ms.0 from job set of time 1502449595000 ms
2017-08-11 19:06:47,653 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 43
2017-08-11 19:06:47,653 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 42 from persistence list
2017-08-11 19:06:47,653 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 42
2017-08-11 19:06:47,653 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:47,653 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449580000 ms
2017-08-11 19:06:47,659 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:47,660 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 23 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:47,660 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 23 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:47,660 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:47,660 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:47,660 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 23 (MapPartitionsRDD[47] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:47,661 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:47,664 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:47,665 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_23_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:47,665 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 23 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:47,665 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 23 (MapPartitionsRDD[47] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:47,666 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 23.0 with 2 tasks
2017-08-11 19:06:47,667 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 23.0 (TID 46, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:47,667 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 23.0 (TID 47, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:47,667 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 23.0 (TID 46)
2017-08-11 19:06:47,667 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 23.0 (TID 47)
2017-08-11 19:06:47,669 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 575 -> 600
2017-08-11 19:06:47,669 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 575 -> 600
2017-08-11 19:06:48,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_22_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:50,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449610000 ms
2017-08-11 19:06:53,006 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 23.0 (TID 46). 787 bytes result sent to driver
2017-08-11 19:06:53,008 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 23.0 (TID 46) in 5342 ms on localhost (1/2)
2017-08-11 19:06:54,348 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 23.0 (TID 47). 787 bytes result sent to driver
2017-08-11 19:06:54,351 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 23.0 (TID 47) in 6684 ms on localhost (2/2)
2017-08-11 19:06:54,351 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 23 (foreachPartition at streamingProcessNew.scala:51) finished in 6.685 s
2017-08-11 19:06:54,351 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 23.0, whose tasks have all completed, from pool 
2017-08-11 19:06:54,351 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 23 finished: foreachPartition at streamingProcessNew.scala:51, took 6.691522 s
2017-08-11 19:06:54,352 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449595000 ms.0 from job set of time 1502449595000 ms
2017-08-11 19:06:54,352 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 19.352 s for time 1502449595000 ms (execution: 6.699 s)
2017-08-11 19:06:54,352 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 45 from persistence list
2017-08-11 19:06:54,353 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449600000 ms.0 from job set of time 1502449600000 ms
2017-08-11 19:06:54,353 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 45
2017-08-11 19:06:54,353 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 44 from persistence list
2017-08-11 19:06:54,354 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 44
2017-08-11 19:06:54,354 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:06:54,354 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449585000 ms
2017-08-11 19:06:54,362 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:06:54,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 24 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:06:54,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 24 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:06:54,363 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:06:54,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:06:54,364 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 24 (MapPartitionsRDD[49] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:06:54,366 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:06:54,370 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:06:54,371 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_24_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:54,371 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 24 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:06:54,372 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at map at streamingProcessNew.scala:49)
2017-08-11 19:06:54,372 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 24.0 with 2 tasks
2017-08-11 19:06:54,374 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 24.0 (TID 48, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:06:54,374 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 24.0 (TID 49, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:06:54,375 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 24.0 (TID 48)
2017-08-11 19:06:54,375 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 24.0 (TID 49)
2017-08-11 19:06:54,376 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 600 -> 625
2017-08-11 19:06:54,376 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 600 -> 625
2017-08-11 19:06:55,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449615000 ms
2017-08-11 19:06:55,387 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_23_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:06:57,945 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 24.0 (TID 49). 787 bytes result sent to driver
2017-08-11 19:06:57,947 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 24.0 (TID 49) in 3573 ms on localhost (1/2)
2017-08-11 19:07:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449620000 ms
2017-08-11 19:07:05,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449625000 ms
2017-08-11 19:07:05,630 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 24.0 (TID 48). 874 bytes result sent to driver
2017-08-11 19:07:05,633 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 24.0 (TID 48) in 11260 ms on localhost (2/2)
2017-08-11 19:07:05,633 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 24.0, whose tasks have all completed, from pool 
2017-08-11 19:07:05,633 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 24 (foreachPartition at streamingProcessNew.scala:51) finished in 11.261 s
2017-08-11 19:07:05,634 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 24 finished: foreachPartition at streamingProcessNew.scala:51, took 11.270865 s
2017-08-11 19:07:05,634 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449600000 ms.0 from job set of time 1502449600000 ms
2017-08-11 19:07:05,634 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 25.634 s for time 1502449600000 ms (execution: 11.282 s)
2017-08-11 19:07:05,634 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 47 from persistence list
2017-08-11 19:07:05,634 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449605000 ms.0 from job set of time 1502449605000 ms
2017-08-11 19:07:05,635 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 47
2017-08-11 19:07:05,635 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 46 from persistence list
2017-08-11 19:07:05,635 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:05,635 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449590000 ms
2017-08-11 19:07:05,635 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 46
2017-08-11 19:07:05,641 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:05,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 25 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:05,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 25 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:05,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:05,642 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:05,643 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 25 (MapPartitionsRDD[51] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:05,644 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:05,650 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:05,651 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_25_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:05,651 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 25 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:05,651 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:05,651 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 25.0 with 2 tasks
2017-08-11 19:07:05,653 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 25.0 (TID 50, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:05,653 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 25.0 (TID 51, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:05,654 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 25.0 (TID 51)
2017-08-11 19:07:05,654 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 25.0 (TID 50)
2017-08-11 19:07:05,656 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 625 -> 650
2017-08-11 19:07:05,656 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 625 -> 650
2017-08-11 19:07:10,006 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_24_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:10,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449630000 ms
2017-08-11 19:07:11,510 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 25.0 (TID 51). 787 bytes result sent to driver
2017-08-11 19:07:11,514 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 25.0 (TID 51) in 5861 ms on localhost (1/2)
2017-08-11 19:07:11,522 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 25.0 (TID 50). 874 bytes result sent to driver
2017-08-11 19:07:11,526 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 25.0 (TID 50) in 5874 ms on localhost (2/2)
2017-08-11 19:07:11,526 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 25 (foreachPartition at streamingProcessNew.scala:51) finished in 5.874 s
2017-08-11 19:07:11,526 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 25.0, whose tasks have all completed, from pool 
2017-08-11 19:07:11,527 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 25 finished: foreachPartition at streamingProcessNew.scala:51, took 5.884944 s
2017-08-11 19:07:11,527 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449605000 ms.0 from job set of time 1502449605000 ms
2017-08-11 19:07:11,527 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 26.527 s for time 1502449605000 ms (execution: 5.893 s)
2017-08-11 19:07:11,527 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 49 from persistence list
2017-08-11 19:07:11,527 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449610000 ms.0 from job set of time 1502449610000 ms
2017-08-11 19:07:11,528 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 48 from persistence list
2017-08-11 19:07:11,528 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 49
2017-08-11 19:07:11,528 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 48
2017-08-11 19:07:11,528 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:11,528 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449595000 ms
2017-08-11 19:07:11,536 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:11,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 26 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:11,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 26 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:11,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:11,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:11,537 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 26 (MapPartitionsRDD[53] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:11,538 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:11,542 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:11,543 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_26_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:11,543 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 26 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:11,544 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 26 (MapPartitionsRDD[53] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:11,544 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 26.0 with 2 tasks
2017-08-11 19:07:11,545 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 26.0 (TID 52, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:11,546 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 26.0 (TID 53, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:11,546 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 26.0 (TID 53)
2017-08-11 19:07:11,547 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 26.0 (TID 52)
2017-08-11 19:07:11,548 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 650 -> 675
2017-08-11 19:07:11,549 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 650 -> 653
2017-08-11 19:07:11,679 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 26.0 (TID 52). 714 bytes result sent to driver
2017-08-11 19:07:11,681 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 26.0 (TID 52) in 137 ms on localhost (1/2)
2017-08-11 19:07:15,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449635000 ms
2017-08-11 19:07:19,552 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_25_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449640000 ms
2017-08-11 19:07:22,498 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 26.0 (TID 53). 787 bytes result sent to driver
2017-08-11 19:07:22,500 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 26.0 (TID 53) in 10955 ms on localhost (2/2)
2017-08-11 19:07:22,501 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 26.0, whose tasks have all completed, from pool 
2017-08-11 19:07:22,501 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 26 (foreachPartition at streamingProcessNew.scala:51) finished in 10.957 s
2017-08-11 19:07:22,501 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 26 finished: foreachPartition at streamingProcessNew.scala:51, took 10.964820 s
2017-08-11 19:07:22,502 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449610000 ms.0 from job set of time 1502449610000 ms
2017-08-11 19:07:22,502 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 32.502 s for time 1502449610000 ms (execution: 10.975 s)
2017-08-11 19:07:22,502 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 51 from persistence list
2017-08-11 19:07:22,502 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449615000 ms.0 from job set of time 1502449615000 ms
2017-08-11 19:07:22,502 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 51
2017-08-11 19:07:22,502 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 50 from persistence list
2017-08-11 19:07:22,503 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 50
2017-08-11 19:07:22,503 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:22,503 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449600000 ms
2017-08-11 19:07:22,509 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:22,509 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 27 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:22,509 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 27 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:22,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:22,577 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:22,578 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 27 (MapPartitionsRDD[55] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:22,582 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:22,592 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27_piece0 stored as bytes in memory (estimated size 2.2 KB, free 413.9 MB)
2017-08-11 19:07:22,594 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_27_piece0 in memory on 192.168.31.111:58030 (size: 2.2 KB, free: 413.9 MB)
2017-08-11 19:07:22,595 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 27 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:22,596 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:22,596 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 27.0 with 2 tasks
2017-08-11 19:07:22,600 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 27.0 (TID 54, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:22,602 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 27.0 (TID 55, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:22,603 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 27.0 (TID 54)
2017-08-11 19:07:22,603 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 27.0 (TID 55)
2017-08-11 19:07:22,607 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:22,608 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 675 -> 698
2017-08-11 19:07:22,611 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 27.0 (TID 54). 714 bytes result sent to driver
2017-08-11 19:07:22,613 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 27.0 (TID 54) in 15 ms on localhost (1/2)
2017-08-11 19:07:25,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449645000 ms
2017-08-11 19:07:26,269 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 27.0 (TID 55). 714 bytes result sent to driver
2017-08-11 19:07:26,275 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 27.0 (TID 55) in 3675 ms on localhost (2/2)
2017-08-11 19:07:26,276 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 27.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 27 (foreachPartition at streamingProcessNew.scala:51) finished in 3.679 s
2017-08-11 19:07:26,277 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 27 finished: foreachPartition at streamingProcessNew.scala:51, took 3.767312 s
2017-08-11 19:07:26,278 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449615000 ms.0 from job set of time 1502449615000 ms
2017-08-11 19:07:26,279 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 31.278 s for time 1502449615000 ms (execution: 3.776 s)
2017-08-11 19:07:26,279 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 53 from persistence list
2017-08-11 19:07:26,279 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449620000 ms.0 from job set of time 1502449620000 ms
2017-08-11 19:07:26,280 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 53
2017-08-11 19:07:26,280 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 52 from persistence list
2017-08-11 19:07:26,281 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 52
2017-08-11 19:07:26,281 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,281 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449605000 ms
2017-08-11 19:07:26,298 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:26,300 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 28 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:26,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 28 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:26,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:26,302 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:26,303 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 28 (MapPartitionsRDD[57] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:26,306 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:26,314 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:26,316 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_28_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,318 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 28 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:26,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:26,319 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 28.0 with 2 tasks
2017-08-11 19:07:26,323 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 28.0 (TID 56, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:26,325 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 28.0 (TID 57, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:26,325 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 28.0 (TID 56)
2017-08-11 19:07:26,325 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 28.0 (TID 57)
2017-08-11 19:07:26,328 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:26,328 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:26,331 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 28.0 (TID 56). 714 bytes result sent to driver
2017-08-11 19:07:26,331 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 28.0 (TID 57). 714 bytes result sent to driver
2017-08-11 19:07:26,333 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 28.0 (TID 57) in 10 ms on localhost (1/2)
2017-08-11 19:07:26,334 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 28.0 (TID 56) in 14 ms on localhost (2/2)
2017-08-11 19:07:26,334 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 28.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,334 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 28 (foreachPartition at streamingProcessNew.scala:51) finished in 0.014 s
2017-08-11 19:07:26,334 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 28 finished: foreachPartition at streamingProcessNew.scala:51, took 0.035879 s
2017-08-11 19:07:26,335 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449620000 ms.0 from job set of time 1502449620000 ms
2017-08-11 19:07:26,335 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 26.335 s for time 1502449620000 ms (execution: 0.056 s)
2017-08-11 19:07:26,335 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 55 from persistence list
2017-08-11 19:07:26,335 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449625000 ms.0 from job set of time 1502449625000 ms
2017-08-11 19:07:26,335 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 55
2017-08-11 19:07:26,336 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 54 from persistence list
2017-08-11 19:07:26,336 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 54
2017-08-11 19:07:26,336 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,336 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449610000 ms
2017-08-11 19:07:26,341 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:26,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 29 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:26,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 29 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:26,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:26,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:26,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 29 (MapPartitionsRDD[59] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:26,344 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:26,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:26,347 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_29_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,348 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 29 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:26,348 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 29 (MapPartitionsRDD[59] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:26,348 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 29.0 with 2 tasks
2017-08-11 19:07:26,349 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 29.0 (TID 58, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:26,350 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 29.0 (TID 59, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:26,350 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 29.0 (TID 59)
2017-08-11 19:07:26,350 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 29.0 (TID 58)
2017-08-11 19:07:26,351 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:26,351 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:26,355 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 29.0 (TID 58). 714 bytes result sent to driver
2017-08-11 19:07:26,355 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 29.0 (TID 59). 714 bytes result sent to driver
2017-08-11 19:07:26,357 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 29.0 (TID 58) in 9 ms on localhost (1/2)
2017-08-11 19:07:26,357 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 29.0 (TID 59) in 8 ms on localhost (2/2)
2017-08-11 19:07:26,357 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 29.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,357 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 29 (foreachPartition at streamingProcessNew.scala:51) finished in 0.009 s
2017-08-11 19:07:26,358 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 29 finished: foreachPartition at streamingProcessNew.scala:51, took 0.016259 s
2017-08-11 19:07:26,358 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449625000 ms.0 from job set of time 1502449625000 ms
2017-08-11 19:07:26,358 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 21.358 s for time 1502449625000 ms (execution: 0.023 s)
2017-08-11 19:07:26,358 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449630000 ms.0 from job set of time 1502449630000 ms
2017-08-11 19:07:26,360 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 57 from persistence list
2017-08-11 19:07:26,360 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 56 from persistence list
2017-08-11 19:07:26,361 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 57
2017-08-11 19:07:26,361 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,361 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449615000 ms
2017-08-11 19:07:26,362 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 56
2017-08-11 19:07:26,364 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:26,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 30 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:26,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 30 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:26,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:26,365 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:26,366 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 30 (MapPartitionsRDD[61] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:26,367 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:26,374 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:26,375 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_26_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,375 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_30_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,376 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 30 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:26,376 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_27_piece0 on 192.168.31.111:58030 in memory (size: 2.2 KB, free: 413.9 MB)
2017-08-11 19:07:26,376 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:26,376 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 30.0 with 2 tasks
2017-08-11 19:07:26,378 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 30.0 (TID 60, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:26,378 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_28_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,379 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 30.0 (TID 61, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:26,379 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 30.0 (TID 60)
2017-08-11 19:07:26,379 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 30.0 (TID 61)
2017-08-11 19:07:26,380 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_29_piece0 on 192.168.31.111:58030 in memory (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,380 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:26,380 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:26,384 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 30.0 (TID 61). 714 bytes result sent to driver
2017-08-11 19:07:26,384 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 30.0 (TID 60). 714 bytes result sent to driver
2017-08-11 19:07:26,386 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 30.0 (TID 61) in 8 ms on localhost (1/2)
2017-08-11 19:07:26,386 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 30.0 (TID 60) in 9 ms on localhost (2/2)
2017-08-11 19:07:26,386 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 30.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,386 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 30 (foreachPartition at streamingProcessNew.scala:51) finished in 0.009 s
2017-08-11 19:07:26,386 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 30 finished: foreachPartition at streamingProcessNew.scala:51, took 0.021912 s
2017-08-11 19:07:26,387 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449630000 ms.0 from job set of time 1502449630000 ms
2017-08-11 19:07:26,387 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 16.387 s for time 1502449630000 ms (execution: 0.029 s)
2017-08-11 19:07:26,387 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 59 from persistence list
2017-08-11 19:07:26,387 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449635000 ms.0 from job set of time 1502449635000 ms
2017-08-11 19:07:26,387 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 59
2017-08-11 19:07:26,388 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 58 from persistence list
2017-08-11 19:07:26,388 [block-manager-slave-async-thread-pool-6] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 58
2017-08-11 19:07:26,388 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,388 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449620000 ms
2017-08-11 19:07:26,393 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:26,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 31 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:26,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 31 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:26,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:26,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:26,394 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 31 (MapPartitionsRDD[63] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:26,395 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:26,398 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:26,399 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_31_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 31 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:26,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:26,399 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 31.0 with 2 tasks
2017-08-11 19:07:26,400 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 31.0 (TID 62, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:26,401 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 31.0 (TID 63, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:26,401 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 31.0 (TID 63)
2017-08-11 19:07:26,401 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 31.0 (TID 62)
2017-08-11 19:07:26,402 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:26,402 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:26,405 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 31.0 (TID 63). 714 bytes result sent to driver
2017-08-11 19:07:26,405 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 31.0 (TID 62). 714 bytes result sent to driver
2017-08-11 19:07:26,407 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 31.0 (TID 63) in 6 ms on localhost (1/2)
2017-08-11 19:07:26,407 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 31.0 (TID 62) in 7 ms on localhost (2/2)
2017-08-11 19:07:26,407 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 31.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,408 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 31 (foreachPartition at streamingProcessNew.scala:51) finished in 0.009 s
2017-08-11 19:07:26,408 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 31 finished: foreachPartition at streamingProcessNew.scala:51, took 0.014434 s
2017-08-11 19:07:26,408 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449635000 ms.0 from job set of time 1502449635000 ms
2017-08-11 19:07:26,408 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 11.408 s for time 1502449635000 ms (execution: 0.021 s)
2017-08-11 19:07:26,409 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449640000 ms.0 from job set of time 1502449640000 ms
2017-08-11 19:07:26,409 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 61 from persistence list
2017-08-11 19:07:26,409 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 60 from persistence list
2017-08-11 19:07:26,409 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 61
2017-08-11 19:07:26,409 [block-manager-slave-async-thread-pool-7] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 60
2017-08-11 19:07:26,409 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,410 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449625000 ms
2017-08-11 19:07:26,414 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:26,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 32 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:26,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 32 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:26,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:26,416 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:26,416 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 32 (MapPartitionsRDD[65] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:26,417 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:26,419 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:26,419 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_32_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:26,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 32 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:26,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:26,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 32.0 with 2 tasks
2017-08-11 19:07:26,421 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 32.0 (TID 64, localhost, partition 0, ANY, 5656 bytes)
2017-08-11 19:07:26,421 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 32.0 (TID 65, localhost, partition 1, ANY, 5656 bytes)
2017-08-11 19:07:26,422 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 32.0 (TID 65)
2017-08-11 19:07:26,422 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 32.0 (TID 64)
2017-08-11 19:07:26,423 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:26,423 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:26,426 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 32.0 (TID 64). 714 bytes result sent to driver
2017-08-11 19:07:26,426 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 32.0 (TID 65). 714 bytes result sent to driver
2017-08-11 19:07:26,427 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 32.0 (TID 65) in 6 ms on localhost (1/2)
2017-08-11 19:07:26,428 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 32.0 (TID 64) in 8 ms on localhost (2/2)
2017-08-11 19:07:26,428 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 32.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,428 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 32 (foreachPartition at streamingProcessNew.scala:51) finished in 0.008 s
2017-08-11 19:07:26,428 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 32 finished: foreachPartition at streamingProcessNew.scala:51, took 0.013367 s
2017-08-11 19:07:26,429 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449640000 ms.0 from job set of time 1502449640000 ms
2017-08-11 19:07:26,429 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 6.428 s for time 1502449640000 ms (execution: 0.019 s)
2017-08-11 19:07:26,429 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 63 from persistence list
2017-08-11 19:07:26,429 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449645000 ms.0 from job set of time 1502449645000 ms
2017-08-11 19:07:26,429 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 63
2017-08-11 19:07:26,429 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 62 from persistence list
2017-08-11 19:07:26,429 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 62
2017-08-11 19:07:26,429 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,429 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449630000 ms
2017-08-11 19:07:26,434 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:26,434 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 33 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:26,434 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 33 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:26,434 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:26,434 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:26,435 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 33 (MapPartitionsRDD[67] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:26,435 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:26,438 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33_piece0 stored as bytes in memory (estimated size 2.2 KB, free 413.9 MB)
2017-08-11 19:07:26,438 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_33_piece0 in memory on 192.168.31.111:58030 (size: 2.2 KB, free: 413.9 MB)
2017-08-11 19:07:26,439 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 33 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:26,439 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 33 (MapPartitionsRDD[67] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:26,439 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 33.0 with 2 tasks
2017-08-11 19:07:26,440 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 33.0 (TID 66, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:26,441 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 33.0 (TID 67, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:26,441 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 33.0 (TID 67)
2017-08-11 19:07:26,441 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 33.0 (TID 66)
2017-08-11 19:07:26,442 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:26,442 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:26,445 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 33.0 (TID 67). 714 bytes result sent to driver
2017-08-11 19:07:26,445 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 33.0 (TID 66). 714 bytes result sent to driver
2017-08-11 19:07:26,446 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 33.0 (TID 66) in 6 ms on localhost (1/2)
2017-08-11 19:07:26,446 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 33.0 (TID 67) in 6 ms on localhost (2/2)
2017-08-11 19:07:26,446 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 33.0, whose tasks have all completed, from pool 
2017-08-11 19:07:26,447 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 33 (foreachPartition at streamingProcessNew.scala:51) finished in 0.006 s
2017-08-11 19:07:26,447 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 33 finished: foreachPartition at streamingProcessNew.scala:51, took 0.012780 s
2017-08-11 19:07:26,447 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449645000 ms.0 from job set of time 1502449645000 ms
2017-08-11 19:07:26,447 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 65 from persistence list
2017-08-11 19:07:26,447 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.447 s for time 1502449645000 ms (execution: 0.018 s)
2017-08-11 19:07:26,448 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 65
2017-08-11 19:07:26,448 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 64 from persistence list
2017-08-11 19:07:26,448 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 64
2017-08-11 19:07:26,448 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:26,448 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449635000 ms
2017-08-11 19:07:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449650000 ms
2017-08-11 19:07:30,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449650000 ms.0 from job set of time 1502449650000 ms
2017-08-11 19:07:30,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:30,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 34 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 34 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:30,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 34 (MapPartitionsRDD[69] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_34 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_34_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:30,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_34_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 34 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 34 (MapPartitionsRDD[69] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 34.0 with 2 tasks
2017-08-11 19:07:30,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 34.0 (TID 68, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:30,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 34.0 (TID 69, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:30,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 34.0 (TID 68)
2017-08-11 19:07:30,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 34.0 (TID 69)
2017-08-11 19:07:30,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:30,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:30,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 34.0 (TID 68). 714 bytes result sent to driver
2017-08-11 19:07:30,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 34.0 (TID 69). 714 bytes result sent to driver
2017-08-11 19:07:30,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 34.0 (TID 68) in 11 ms on localhost (1/2)
2017-08-11 19:07:30,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 34.0 (TID 69) in 11 ms on localhost (2/2)
2017-08-11 19:07:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 34 (foreachPartition at streamingProcessNew.scala:51) finished in 0.012 s
2017-08-11 19:07:30,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 34.0, whose tasks have all completed, from pool 
2017-08-11 19:07:30,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 34 finished: foreachPartition at streamingProcessNew.scala:51, took 0.025855 s
2017-08-11 19:07:30,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449650000 ms.0 from job set of time 1502449650000 ms
2017-08-11 19:07:30,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1502449650000 ms (execution: 0.045 s)
2017-08-11 19:07:30,066 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 67 from persistence list
2017-08-11 19:07:30,067 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 67
2017-08-11 19:07:30,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 66 from persistence list
2017-08-11 19:07:30,067 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 66
2017-08-11 19:07:30,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:30,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449640000 ms
2017-08-11 19:07:35,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449655000 ms
2017-08-11 19:07:35,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449655000 ms.0 from job set of time 1502449655000 ms
2017-08-11 19:07:35,020 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:35,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 35 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:35,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 35 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:35,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:35,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:35,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 35 (MapPartitionsRDD[71] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:35,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_35 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:35,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_35_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:35,026 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_35_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:35,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 35 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:35,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 35 (MapPartitionsRDD[71] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:35,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 35.0 with 2 tasks
2017-08-11 19:07:35,028 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 35.0 (TID 70, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:35,028 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 35.0 (TID 71, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:35,028 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 35.0 (TID 70)
2017-08-11 19:07:35,028 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 35.0 (TID 71)
2017-08-11 19:07:35,029 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:35,029 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:35,032 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 35.0 (TID 71). 714 bytes result sent to driver
2017-08-11 19:07:35,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 35.0 (TID 70). 714 bytes result sent to driver
2017-08-11 19:07:35,034 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 35.0 (TID 71) in 6 ms on localhost (1/2)
2017-08-11 19:07:35,035 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 35.0 (TID 70) in 8 ms on localhost (2/2)
2017-08-11 19:07:35,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 35 (foreachPartition at streamingProcessNew.scala:51) finished in 0.009 s
2017-08-11 19:07:35,035 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 35.0, whose tasks have all completed, from pool 
2017-08-11 19:07:35,035 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 35 finished: foreachPartition at streamingProcessNew.scala:51, took 0.015128 s
2017-08-11 19:07:35,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449655000 ms.0 from job set of time 1502449655000 ms
2017-08-11 19:07:35,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.036 s for time 1502449655000 ms (execution: 0.022 s)
2017-08-11 19:07:35,036 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 69 from persistence list
2017-08-11 19:07:35,036 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 69
2017-08-11 19:07:35,036 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 68 from persistence list
2017-08-11 19:07:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:35,037 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 68
2017-08-11 19:07:35,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449645000 ms
2017-08-11 19:07:40,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449660000 ms
2017-08-11 19:07:40,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449660000 ms.0 from job set of time 1502449660000 ms
2017-08-11 19:07:40,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:40,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 36 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 36 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:40,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 36 (MapPartitionsRDD[73] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:40,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_36 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_36_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:40,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_36_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:40,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 36 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 36 (MapPartitionsRDD[73] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 36.0 with 2 tasks
2017-08-11 19:07:40,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 36.0 (TID 72, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:40,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 36.0 (TID 73, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:40,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 36.0 (TID 73)
2017-08-11 19:07:40,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 36.0 (TID 72)
2017-08-11 19:07:40,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:40,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:40,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 36.0 (TID 73). 714 bytes result sent to driver
2017-08-11 19:07:40,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 36.0 (TID 72). 714 bytes result sent to driver
2017-08-11 19:07:40,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 36.0 (TID 73) in 17 ms on localhost (1/2)
2017-08-11 19:07:40,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 36.0 (TID 72) in 21 ms on localhost (2/2)
2017-08-11 19:07:40,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 36.0, whose tasks have all completed, from pool 
2017-08-11 19:07:40,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 36 (foreachPartition at streamingProcessNew.scala:51) finished in 0.021 s
2017-08-11 19:07:40,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 36 finished: foreachPartition at streamingProcessNew.scala:51, took 0.039014 s
2017-08-11 19:07:40,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449660000 ms.0 from job set of time 1502449660000 ms
2017-08-11 19:07:40,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1502449660000 ms (execution: 0.060 s)
2017-08-11 19:07:40,081 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 71 from persistence list
2017-08-11 19:07:40,082 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 71
2017-08-11 19:07:40,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 70 from persistence list
2017-08-11 19:07:40,083 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 70
2017-08-11 19:07:40,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:40,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449650000 ms
2017-08-11 19:07:45,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449665000 ms
2017-08-11 19:07:45,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449665000 ms.0 from job set of time 1502449665000 ms
2017-08-11 19:07:45,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:45,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 37 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:45,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 37 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:45,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:45,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:45,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 37 (MapPartitionsRDD[75] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:45,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_37 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:45,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_37_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:45,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_37_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 37 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 37 (MapPartitionsRDD[75] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 37.0 with 2 tasks
2017-08-11 19:07:45,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 37.0 (TID 74, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:45,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 37.0 (TID 75, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:45,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 37.0 (TID 75)
2017-08-11 19:07:45,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 37.0 (TID 74)
2017-08-11 19:07:45,056 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:45,056 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:45,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 37.0 (TID 74). 714 bytes result sent to driver
2017-08-11 19:07:45,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 37.0 (TID 75). 714 bytes result sent to driver
2017-08-11 19:07:45,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 37.0 (TID 74) in 9 ms on localhost (1/2)
2017-08-11 19:07:45,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 37.0 (TID 75) in 9 ms on localhost (2/2)
2017-08-11 19:07:45,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 37.0, whose tasks have all completed, from pool 
2017-08-11 19:07:45,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 37 (foreachPartition at streamingProcessNew.scala:51) finished in 0.010 s
2017-08-11 19:07:45,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 37 finished: foreachPartition at streamingProcessNew.scala:51, took 0.021704 s
2017-08-11 19:07:45,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449665000 ms.0 from job set of time 1502449665000 ms
2017-08-11 19:07:45,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1502449665000 ms (execution: 0.041 s)
2017-08-11 19:07:45,064 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 73 from persistence list
2017-08-11 19:07:45,065 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 73
2017-08-11 19:07:45,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 72 from persistence list
2017-08-11 19:07:45,066 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 72
2017-08-11 19:07:45,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:45,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449655000 ms
2017-08-11 19:07:50,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449670000 ms
2017-08-11 19:07:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449670000 ms.0 from job set of time 1502449670000 ms
2017-08-11 19:07:50,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 38 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 38 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 38 (MapPartitionsRDD[77] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_38 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_38_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:50,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_38_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 38 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 38 (MapPartitionsRDD[77] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 38.0 with 2 tasks
2017-08-11 19:07:50,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 38.0 (TID 76, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:50,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 38.0 (TID 77, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:50,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 38.0 (TID 76)
2017-08-11 19:07:50,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 38.0 (TID 77)
2017-08-11 19:07:50,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:50,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:50,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 38.0 (TID 77). 714 bytes result sent to driver
2017-08-11 19:07:50,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 38.0 (TID 76). 714 bytes result sent to driver
2017-08-11 19:07:50,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 38.0 (TID 76) in 11 ms on localhost (1/2)
2017-08-11 19:07:50,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 38.0 (TID 77) in 10 ms on localhost (2/2)
2017-08-11 19:07:50,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 38.0, whose tasks have all completed, from pool 
2017-08-11 19:07:50,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 38 (foreachPartition at streamingProcessNew.scala:51) finished in 0.011 s
2017-08-11 19:07:50,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 38 finished: foreachPartition at streamingProcessNew.scala:51, took 0.028136 s
2017-08-11 19:07:50,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449670000 ms.0 from job set of time 1502449670000 ms
2017-08-11 19:07:50,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1502449670000 ms (execution: 0.050 s)
2017-08-11 19:07:50,071 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 75 from persistence list
2017-08-11 19:07:50,071 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 75
2017-08-11 19:07:50,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 74 from persistence list
2017-08-11 19:07:50,072 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 74
2017-08-11 19:07:50,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:50,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449660000 ms
2017-08-11 19:07:55,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449675000 ms
2017-08-11 19:07:55,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449675000 ms.0 from job set of time 1502449675000 ms
2017-08-11 19:07:55,024 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:07:55,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 39 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:07:55,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 39 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:07:55,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:07:55,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:07:55,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 39 (MapPartitionsRDD[79] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:07:55,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_39 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:07:55,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_39_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:07:55,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_39_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:07:55,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 39 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:07:55,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 39 (MapPartitionsRDD[79] at map at streamingProcessNew.scala:49)
2017-08-11 19:07:55,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 39.0 with 2 tasks
2017-08-11 19:07:55,033 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 39.0 (TID 78, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:07:55,034 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 39.0 (TID 79, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:07:55,034 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 39.0 (TID 78)
2017-08-11 19:07:55,034 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 39.0 (TID 79)
2017-08-11 19:07:55,035 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:07:55,036 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:07:55,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 39.0 (TID 79). 714 bytes result sent to driver
2017-08-11 19:07:55,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 39.0 (TID 78). 714 bytes result sent to driver
2017-08-11 19:07:55,040 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 39.0 (TID 78) in 8 ms on localhost (1/2)
2017-08-11 19:07:55,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 39.0 (TID 79) in 7 ms on localhost (2/2)
2017-08-11 19:07:55,041 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 39.0, whose tasks have all completed, from pool 
2017-08-11 19:07:55,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 39 (foreachPartition at streamingProcessNew.scala:51) finished in 0.009 s
2017-08-11 19:07:55,041 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 39 finished: foreachPartition at streamingProcessNew.scala:51, took 0.016317 s
2017-08-11 19:07:55,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449675000 ms.0 from job set of time 1502449675000 ms
2017-08-11 19:07:55,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1502449675000 ms (execution: 0.025 s)
2017-08-11 19:07:55,041 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 77 from persistence list
2017-08-11 19:07:55,042 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 77
2017-08-11 19:07:55,042 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 76 from persistence list
2017-08-11 19:07:55,042 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 76
2017-08-11 19:07:55,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:07:55,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449665000 ms
2017-08-11 19:08:00,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449680000 ms
2017-08-11 19:08:00,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449680000 ms.0 from job set of time 1502449680000 ms
2017-08-11 19:08:00,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:08:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 40 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:08:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 40 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:08:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:08:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:08:00,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 40 (MapPartitionsRDD[81] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:08:00,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_40 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:08:00,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_40_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-08-11 19:08:00,039 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_40_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:08:00,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 40 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:08:00,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 40 (MapPartitionsRDD[81] at map at streamingProcessNew.scala:49)
2017-08-11 19:08:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 40.0 with 2 tasks
2017-08-11 19:08:00,043 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 40.0 (TID 80, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:08:00,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 40.0 (TID 81, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:08:00,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 40.0 (TID 81)
2017-08-11 19:08:00,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 40.0 (TID 80)
2017-08-11 19:08:00,046 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:08:00,046 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:08:00,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 40.0 (TID 80). 714 bytes result sent to driver
2017-08-11 19:08:00,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 40.0 (TID 81). 714 bytes result sent to driver
2017-08-11 19:08:00,052 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 40.0 (TID 80) in 10 ms on localhost (1/2)
2017-08-11 19:08:00,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 40.0 (TID 81) in 9 ms on localhost (2/2)
2017-08-11 19:08:00,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 40.0, whose tasks have all completed, from pool 
2017-08-11 19:08:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 40 (foreachPartition at streamingProcessNew.scala:51) finished in 0.011 s
2017-08-11 19:08:00,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 40 finished: foreachPartition at streamingProcessNew.scala:51, took 0.021904 s
2017-08-11 19:08:00,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449680000 ms.0 from job set of time 1502449680000 ms
2017-08-11 19:08:00,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.053 s for time 1502449680000 ms (execution: 0.034 s)
2017-08-11 19:08:00,053 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 79 from persistence list
2017-08-11 19:08:00,053 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 79
2017-08-11 19:08:00,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 78 from persistence list
2017-08-11 19:08:00,054 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 78
2017-08-11 19:08:00,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:08:00,055 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449670000 ms
2017-08-11 19:08:05,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449685000 ms
2017-08-11 19:08:05,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449685000 ms.0 from job set of time 1502449685000 ms
2017-08-11 19:08:05,035 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:08:05,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 41 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:08:05,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 41 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:08:05,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:08:05,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:08:05,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 41 (MapPartitionsRDD[83] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:08:05,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_41 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-08-11 19:08:05,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_41_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.8 MB)
2017-08-11 19:08:05,044 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_41_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:08:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 41 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:08:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 41 (MapPartitionsRDD[83] at map at streamingProcessNew.scala:49)
2017-08-11 19:08:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 41.0 with 2 tasks
2017-08-11 19:08:05,046 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 41.0 (TID 82, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:08:05,047 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 41.0 (TID 83, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:08:05,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 41.0 (TID 83)
2017-08-11 19:08:05,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 41.0 (TID 82)
2017-08-11 19:08:05,049 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:08:05,050 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:08:05,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 41.0 (TID 82). 714 bytes result sent to driver
2017-08-11 19:08:05,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 41.0 (TID 83). 801 bytes result sent to driver
2017-08-11 19:08:05,056 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 41.0 (TID 83) in 9 ms on localhost (1/2)
2017-08-11 19:08:05,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 41.0 (TID 82) in 12 ms on localhost (2/2)
2017-08-11 19:08:05,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 41.0, whose tasks have all completed, from pool 
2017-08-11 19:08:05,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 41 (foreachPartition at streamingProcessNew.scala:51) finished in 0.012 s
2017-08-11 19:08:05,057 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 41 finished: foreachPartition at streamingProcessNew.scala:51, took 0.021698 s
2017-08-11 19:08:05,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449685000 ms.0 from job set of time 1502449685000 ms
2017-08-11 19:08:05,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.058 s for time 1502449685000 ms (execution: 0.035 s)
2017-08-11 19:08:05,058 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 81 from persistence list
2017-08-11 19:08:05,058 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 81
2017-08-11 19:08:05,058 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 80 from persistence list
2017-08-11 19:08:05,058 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 80
2017-08-11 19:08:05,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:08:05,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449675000 ms
2017-08-11 19:08:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449690000 ms
2017-08-11 19:08:10,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449690000 ms.0 from job set of time 1502449690000 ms
2017-08-11 19:08:10,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:08:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 42 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:08:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 42 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:08:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:08:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:08:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 42 (MapPartitionsRDD[85] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:08:10,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_42 stored as values in memory (estimated size 3.9 KB, free 413.8 MB)
2017-08-11 19:08:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_42_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.8 MB)
2017-08-11 19:08:10,047 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_42_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:08:10,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 42 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:08:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 42 (MapPartitionsRDD[85] at map at streamingProcessNew.scala:49)
2017-08-11 19:08:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 42.0 with 2 tasks
2017-08-11 19:08:10,050 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 42.0 (TID 84, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:08:10,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 42.0 (TID 85, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:08:10,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 42.0 (TID 84)
2017-08-11 19:08:10,053 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:08:10,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 42.0 (TID 85)
2017-08-11 19:08:10,056 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:08:10,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 42.0 (TID 84). 714 bytes result sent to driver
2017-08-11 19:08:10,060 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 42.0 (TID 84) in 11 ms on localhost (1/2)
2017-08-11 19:08:10,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 42.0 (TID 85). 714 bytes result sent to driver
2017-08-11 19:08:10,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 42.0 (TID 85) in 14 ms on localhost (2/2)
2017-08-11 19:08:10,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 42.0, whose tasks have all completed, from pool 
2017-08-11 19:08:10,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 42 (foreachPartition at streamingProcessNew.scala:51) finished in 0.016 s
2017-08-11 19:08:10,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 42 finished: foreachPartition at streamingProcessNew.scala:51, took 0.025798 s
2017-08-11 19:08:10,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449690000 ms.0 from job set of time 1502449690000 ms
2017-08-11 19:08:10,066 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 83 from persistence list
2017-08-11 19:08:10,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1502449690000 ms (execution: 0.045 s)
2017-08-11 19:08:10,066 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 83
2017-08-11 19:08:10,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 82 from persistence list
2017-08-11 19:08:10,067 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 82
2017-08-11 19:08:10,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:08:10,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449680000 ms
2017-08-11 19:08:15,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449695000 ms
2017-08-11 19:08:15,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449695000 ms.0 from job set of time 1502449695000 ms
2017-08-11 19:08:15,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:08:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 43 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:08:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 43 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:08:15,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:08:15,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:08:15,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 43 (MapPartitionsRDD[87] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:08:15,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_43 stored as values in memory (estimated size 3.9 KB, free 413.8 MB)
2017-08-11 19:08:15,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_43_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.8 MB)
2017-08-11 19:08:15,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_43_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:08:15,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 43 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:08:15,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 43 (MapPartitionsRDD[87] at map at streamingProcessNew.scala:49)
2017-08-11 19:08:15,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 43.0 with 2 tasks
2017-08-11 19:08:15,041 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 43.0 (TID 86, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:08:15,041 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 43.0 (TID 87, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:08:15,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 43.0 (TID 87)
2017-08-11 19:08:15,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 43.0 (TID 86)
2017-08-11 19:08:15,043 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:08:15,045 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:08:15,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 43.0 (TID 87). 714 bytes result sent to driver
2017-08-11 19:08:15,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 43.0 (TID 86). 714 bytes result sent to driver
2017-08-11 19:08:15,049 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 43.0 (TID 87) in 8 ms on localhost (1/2)
2017-08-11 19:08:15,049 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 43.0 (TID 86) in 10 ms on localhost (2/2)
2017-08-11 19:08:15,050 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 43.0, whose tasks have all completed, from pool 
2017-08-11 19:08:15,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 43 (foreachPartition at streamingProcessNew.scala:51) finished in 0.011 s
2017-08-11 19:08:15,050 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 43 finished: foreachPartition at streamingProcessNew.scala:51, took 0.020363 s
2017-08-11 19:08:15,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449695000 ms.0 from job set of time 1502449695000 ms
2017-08-11 19:08:15,051 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.050 s for time 1502449695000 ms (execution: 0.031 s)
2017-08-11 19:08:15,051 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 85 from persistence list
2017-08-11 19:08:15,051 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 85
2017-08-11 19:08:15,051 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 84 from persistence list
2017-08-11 19:08:15,052 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 84
2017-08-11 19:08:15,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:08:15,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449685000 ms
2017-08-11 19:08:20,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1502449700000 ms
2017-08-11 19:08:20,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1502449700000 ms.0 from job set of time 1502449700000 ms
2017-08-11 19:08:20,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:51
2017-08-11 19:08:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 44 (foreachPartition at streamingProcessNew.scala:51) with 2 output partitions
2017-08-11 19:08:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 44 (foreachPartition at streamingProcessNew.scala:51)
2017-08-11 19:08:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-11 19:08:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-11 19:08:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 44 (MapPartitionsRDD[89] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-11 19:08:20,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_44 stored as values in memory (estimated size 3.9 KB, free 413.8 MB)
2017-08-11 19:08:20,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_44_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.8 MB)
2017-08-11 19:08:20,033 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_44_piece0 in memory on 192.168.31.111:58030 (size: 2.1 KB, free: 413.9 MB)
2017-08-11 19:08:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 44 from broadcast at DAGScheduler.scala:1012
2017-08-11 19:08:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 44 (MapPartitionsRDD[89] at map at streamingProcessNew.scala:49)
2017-08-11 19:08:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 44.0 with 2 tasks
2017-08-11 19:08:20,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 44.0 (TID 88, localhost, partition 0, ANY, 5657 bytes)
2017-08-11 19:08:20,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 44.0 (TID 89, localhost, partition 1, ANY, 5657 bytes)
2017-08-11 19:08:20,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 44.0 (TID 89)
2017-08-11 19:08:20,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 44.0 (TID 88)
2017-08-11 19:08:20,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 653 is the same as ending offset skipping test07 0
2017-08-11 19:08:20,040 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 698 is the same as ending offset skipping test07 1
2017-08-11 19:08:20,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 44.0 (TID 88). 714 bytes result sent to driver
2017-08-11 19:08:20,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 44.0 (TID 89). 714 bytes result sent to driver
2017-08-11 19:08:20,046 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 44.0 (TID 88) in 9 ms on localhost (1/2)
2017-08-11 19:08:20,047 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 44.0 (TID 89) in 8 ms on localhost (2/2)
2017-08-11 19:08:20,047 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 44.0, whose tasks have all completed, from pool 
2017-08-11 19:08:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 44 (foreachPartition at streamingProcessNew.scala:51) finished in 0.010 s
2017-08-11 19:08:20,047 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 44 finished: foreachPartition at streamingProcessNew.scala:51, took 0.020885 s
2017-08-11 19:08:20,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1502449700000 ms.0 from job set of time 1502449700000 ms
2017-08-11 19:08:20,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1502449700000 ms (execution: 0.029 s)
2017-08-11 19:08:20,048 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 87 from persistence list
2017-08-11 19:08:20,049 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 87
2017-08-11 19:08:20,050 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 86 from persistence list
2017-08-11 19:08:20,050 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 86
2017-08-11 19:08:20,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-11 19:08:20,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1502449690000 ms
