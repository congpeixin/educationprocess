2017-08-18 12:13:10,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 277.0 (TID 555)
2017-08-18 12:13:10,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 277.0 (TID 554)
2017-08-18 12:13:10,089 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:13:10,089 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:13:10,093 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 277.0 (TID 555). 714 bytes result sent to driver
2017-08-18 12:13:10,094 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 277.0 (TID 554). 714 bytes result sent to driver
2017-08-18 12:13:10,096 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 277.0 (TID 555) in 26 ms on localhost (1/2)
2017-08-18 12:13:10,096 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 277.0 (TID 554) in 30 ms on localhost (2/2)
2017-08-18 12:13:10,096 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 277.0, whose tasks have all completed, from pool 
2017-08-18 12:13:10,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 277 (foreachPartition at streamingProcessNew.scala:50) finished in 0.030 s
2017-08-18 12:13:10,096 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 277 finished: foreachPartition at streamingProcessNew.scala:50, took 0.048659 s
2017-08-18 12:13:10,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029590000 ms.0 from job set of time 1503029590000 ms
2017-08-18 12:13:10,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.097 s for time 1503029590000 ms (execution: 0.076 s)
2017-08-18 12:13:10,097 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 553 from persistence list
2017-08-18 12:13:10,097 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 553
2017-08-18 12:13:10,098 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 552 from persistence list
2017-08-18 12:13:10,098 [block-manager-slave-async-thread-pool-26] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 552
2017-08-18 12:13:10,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:13:10,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029580000 ms
2017-08-18 12:13:15,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029595000 ms
2017-08-18 12:13:15,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029595000 ms.0 from job set of time 1503029595000 ms
2017-08-18 12:13:15,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:13:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 278 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:13:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 278 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:13:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:13:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:13:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 278 (MapPartitionsRDD[557] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:13:15,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_278 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:13:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_278_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-18 12:13:15,064 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_278_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:13:15,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 278 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:13:15,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 278 (MapPartitionsRDD[557] at map at streamingProcessNew.scala:49)
2017-08-18 12:13:15,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 278.0 with 2 tasks
2017-08-18 12:13:15,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 278.0 (TID 556, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:13:15,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 278.0 (TID 557, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:13:15,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 278.0 (TID 557)
2017-08-18 12:13:15,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 278.0 (TID 556)
2017-08-18 12:13:15,084 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_263_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:13:15,084 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:13:15,085 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:13:15,087 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_264_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:13:15,089 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_265_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:13:15,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 278.0 (TID 557). 787 bytes result sent to driver
2017-08-18 12:13:15,089 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 278.0 (TID 556). 787 bytes result sent to driver
2017-08-18 12:13:15,093 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_266_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:13:15,094 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 278.0 (TID 557) in 26 ms on localhost (1/2)
2017-08-18 12:13:15,094 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 278.0 (TID 556) in 29 ms on localhost (2/2)
2017-08-18 12:13:15,094 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 278.0, whose tasks have all completed, from pool 
2017-08-18 12:13:15,094 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 278 (foreachPartition at streamingProcessNew.scala:50) finished in 0.029 s
2017-08-18 12:13:15,095 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_267_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:13:15,096 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 278 finished: foreachPartition at streamingProcessNew.scala:50, took 0.042699 s
2017-08-18 12:13:15,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029595000 ms.0 from job set of time 1503029595000 ms
2017-08-18 12:13:15,097 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 555 from persistence list
2017-08-18 12:13:15,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.097 s for time 1503029595000 ms (execution: 0.076 s)
2017-08-18 12:13:15,097 [block-manager-slave-async-thread-pool-26] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 555
2017-08-18 12:13:15,097 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 554 from persistence list
2017-08-18 12:13:15,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:13:15,098 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 554
2017-08-18 12:13:15,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029585000 ms
2017-08-18 12:13:15,099 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_268_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:13:15,102 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_269_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:13:15,105 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_270_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:13:15,106 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_271_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:13:15,109 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_272_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:13:15,111 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_273_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:13:15,114 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_274_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:13:15,115 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_275_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:13:15,117 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_276_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:13:15,118 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_277_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:13:20,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029600000 ms
2017-08-18 12:13:20,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029600000 ms.0 from job set of time 1503029600000 ms
2017-08-18 12:13:20,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:13:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 279 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:13:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 279 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:13:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:13:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:13:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 279 (MapPartitionsRDD[559] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:13:20,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_279 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:13:20,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_279_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:13:20,037 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_279_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:13:20,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 279 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:13:20,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 279 (MapPartitionsRDD[559] at map at streamingProcessNew.scala:49)
2017-08-18 12:13:20,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 279.0 with 2 tasks
2017-08-18 12:13:20,040 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 279.0 (TID 558, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:13:20,041 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 279.0 (TID 559, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:13:20,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 279.0 (TID 558)
2017-08-18 12:13:20,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 279.0 (TID 559)
2017-08-18 12:13:20,043 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:13:20,043 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:13:20,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 279.0 (TID 558). 714 bytes result sent to driver
2017-08-18 12:13:20,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 279.0 (TID 559). 714 bytes result sent to driver
2017-08-18 12:13:20,049 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 279.0 (TID 559) in 8 ms on localhost (1/2)
2017-08-18 12:13:20,049 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 279.0 (TID 558) in 11 ms on localhost (2/2)
2017-08-18 12:13:20,050 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 279.0, whose tasks have all completed, from pool 
2017-08-18 12:13:20,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 279 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:13:20,050 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 279 finished: foreachPartition at streamingProcessNew.scala:50, took 0.023683 s
2017-08-18 12:13:20,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029600000 ms.0 from job set of time 1503029600000 ms
2017-08-18 12:13:20,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.050 s for time 1503029600000 ms (execution: 0.035 s)
2017-08-18 12:13:20,050 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 557 from persistence list
2017-08-18 12:13:20,051 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 557
2017-08-18 12:13:20,051 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 556 from persistence list
2017-08-18 12:13:20,051 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 556
2017-08-18 12:13:20,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:13:20,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029590000 ms
2017-08-18 12:13:25,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029605000 ms
2017-08-18 12:13:25,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029605000 ms.0 from job set of time 1503029605000 ms
2017-08-18 12:13:25,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:13:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 280 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:13:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 280 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:13:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:13:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:13:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 280 (MapPartitionsRDD[561] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:13:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_280 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:13:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_280_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:13:25,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_280_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:13:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 280 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:13:25,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 280 (MapPartitionsRDD[561] at map at streamingProcessNew.scala:49)
2017-08-18 12:13:25,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 280.0 with 2 tasks
2017-08-18 12:13:25,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 280.0 (TID 560, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:13:25,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 280.0 (TID 561, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:13:25,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 280.0 (TID 561)
2017-08-18 12:13:25,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 280.0 (TID 560)
2017-08-18 12:13:25,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:13:25,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:13:25,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 280.0 (TID 561). 714 bytes result sent to driver
2017-08-18 12:13:25,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 280.0 (TID 560). 714 bytes result sent to driver
2017-08-18 12:13:25,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 280.0 (TID 561) in 7 ms on localhost (1/2)
2017-08-18 12:13:25,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 280.0 (TID 560) in 10 ms on localhost (2/2)
2017-08-18 12:13:25,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 280.0, whose tasks have all completed, from pool 
2017-08-18 12:13:25,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 280 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:13:25,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 280 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025046 s
2017-08-18 12:13:25,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029605000 ms.0 from job set of time 1503029605000 ms
2017-08-18 12:13:25,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1503029605000 ms (execution: 0.053 s)
2017-08-18 12:13:25,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 559 from persistence list
2017-08-18 12:13:25,075 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 559
2017-08-18 12:13:25,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 558 from persistence list
2017-08-18 12:13:25,075 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 558
2017-08-18 12:13:25,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:13:25,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029595000 ms
2017-08-18 12:13:30,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029610000 ms
2017-08-18 12:13:30,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029610000 ms.0 from job set of time 1503029610000 ms
2017-08-18 12:13:30,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:13:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 281 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:13:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 281 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:13:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:13:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:13:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 281 (MapPartitionsRDD[563] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:13:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_281 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:13:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_281_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:13:30,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_281_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:13:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 281 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:13:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 281 (MapPartitionsRDD[563] at map at streamingProcessNew.scala:49)
2017-08-18 12:13:30,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 281.0 with 2 tasks
2017-08-18 12:13:30,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 281.0 (TID 562, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:13:30,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 281.0 (TID 563, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:13:30,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 281.0 (TID 563)
2017-08-18 12:13:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 281.0 (TID 562)
2017-08-18 12:13:30,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:13:30,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:13:30,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 281.0 (TID 562). 714 bytes result sent to driver
2017-08-18 12:13:30,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 281.0 (TID 563). 714 bytes result sent to driver
2017-08-18 12:13:30,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 281.0 (TID 562) in 14 ms on localhost (1/2)
2017-08-18 12:13:30,084 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 281.0 (TID 563) in 12 ms on localhost (2/2)
2017-08-18 12:13:30,084 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 281.0, whose tasks have all completed, from pool 
2017-08-18 12:13:30,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 281 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:13:30,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 281 finished: foreachPartition at streamingProcessNew.scala:50, took 0.031005 s
2017-08-18 12:13:30,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029610000 ms.0 from job set of time 1503029610000 ms
2017-08-18 12:13:30,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1503029610000 ms (execution: 0.063 s)
2017-08-18 12:13:30,085 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 561 from persistence list
2017-08-18 12:13:30,086 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 561
2017-08-18 12:13:30,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 560 from persistence list
2017-08-18 12:13:30,086 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 560
2017-08-18 12:13:30,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:13:30,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029600000 ms
2017-08-18 12:13:35,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029615000 ms
2017-08-18 12:13:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029615000 ms.0 from job set of time 1503029615000 ms
2017-08-18 12:13:35,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:13:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 282 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:13:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 282 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:13:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:13:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:13:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 282 (MapPartitionsRDD[565] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:13:35,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_282 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:13:35,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_282_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:13:35,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_282_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:13:35,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 282 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:13:35,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 282 (MapPartitionsRDD[565] at map at streamingProcessNew.scala:49)
2017-08-18 12:13:35,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 282.0 with 2 tasks
2017-08-18 12:13:35,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 282.0 (TID 564, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:13:35,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 282.0 (TID 565, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:13:35,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 282.0 (TID 565)
2017-08-18 12:13:35,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 282.0 (TID 564)
2017-08-18 12:13:35,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:13:35,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:13:35,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 282.0 (TID 564). 714 bytes result sent to driver
2017-08-18 12:13:35,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 282.0 (TID 565). 714 bytes result sent to driver
2017-08-18 12:13:35,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 282.0 (TID 564) in 12 ms on localhost (1/2)
2017-08-18 12:13:35,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 282.0 (TID 565) in 10 ms on localhost (2/2)
2017-08-18 12:13:35,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 282.0, whose tasks have all completed, from pool 
2017-08-18 12:13:35,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 282 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:13:35,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 282 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024033 s
2017-08-18 12:13:35,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029615000 ms.0 from job set of time 1503029615000 ms
2017-08-18 12:13:35,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1503029615000 ms (execution: 0.061 s)
2017-08-18 12:13:35,082 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 563 from persistence list
2017-08-18 12:13:35,082 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 563
2017-08-18 12:13:35,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 562 from persistence list
2017-08-18 12:13:35,083 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 562
2017-08-18 12:13:35,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:13:35,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029605000 ms
2017-08-18 12:13:40,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029620000 ms
2017-08-18 12:13:40,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029620000 ms.0 from job set of time 1503029620000 ms
2017-08-18 12:13:40,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:13:40,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 283 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:13:40,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 283 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:13:40,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:13:40,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:13:40,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 283 (MapPartitionsRDD[567] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:13:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_283 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:13:40,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_283_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:13:40,033 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_283_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:13:40,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 283 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:13:40,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 283 (MapPartitionsRDD[567] at map at streamingProcessNew.scala:49)
2017-08-18 12:13:40,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 283.0 with 2 tasks
2017-08-18 12:13:40,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 283.0 (TID 566, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:13:40,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 283.0 (TID 567, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:13:40,037 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 283.0 (TID 567)
2017-08-18 12:13:40,037 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 283.0 (TID 566)
2017-08-18 12:13:40,039 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:13:40,039 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:13:40,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 283.0 (TID 566). 714 bytes result sent to driver
2017-08-18 12:13:40,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 283.0 (TID 567). 714 bytes result sent to driver
2017-08-18 12:13:40,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 283.0 (TID 566) in 10 ms on localhost (1/2)
2017-08-18 12:13:40,044 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 283.0 (TID 567) in 8 ms on localhost (2/2)
2017-08-18 12:13:40,045 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 283.0, whose tasks have all completed, from pool 
2017-08-18 12:13:40,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 283 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:13:40,046 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 283 finished: foreachPartition at streamingProcessNew.scala:50, took 0.018889 s
2017-08-18 12:13:40,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029620000 ms.0 from job set of time 1503029620000 ms
2017-08-18 12:13:40,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.046 s for time 1503029620000 ms (execution: 0.032 s)
2017-08-18 12:13:40,046 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 565 from persistence list
2017-08-18 12:13:40,047 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 565
2017-08-18 12:13:40,047 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 564 from persistence list
2017-08-18 12:13:40,047 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 564
2017-08-18 12:13:40,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:13:40,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029610000 ms
2017-08-18 12:13:45,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029625000 ms
2017-08-18 12:13:45,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029625000 ms.0 from job set of time 1503029625000 ms
2017-08-18 12:13:45,063 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:13:45,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 284 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:13:45,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 284 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:13:45,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:13:45,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:13:45,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 284 (MapPartitionsRDD[569] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:13:45,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_284 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:13:45,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_284_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:13:45,086 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_284_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:13:45,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 284 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:13:45,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 284 (MapPartitionsRDD[569] at map at streamingProcessNew.scala:49)
2017-08-18 12:13:45,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 284.0 with 2 tasks
2017-08-18 12:13:45,094 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 284.0 (TID 568, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:13:45,095 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 284.0 (TID 569, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:13:45,096 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 284.0 (TID 568)
2017-08-18 12:13:45,096 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 284.0 (TID 569)
2017-08-18 12:13:45,102 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:13:45,102 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:13:45,108 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 284.0 (TID 568). 714 bytes result sent to driver
2017-08-18 12:13:45,108 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 284.0 (TID 569). 714 bytes result sent to driver
2017-08-18 12:13:45,111 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 284.0 (TID 568) in 21 ms on localhost (1/2)
2017-08-18 12:13:45,111 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 284.0 (TID 569) in 17 ms on localhost (2/2)
2017-08-18 12:13:45,111 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 284.0, whose tasks have all completed, from pool 
2017-08-18 12:13:45,111 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 284 (foreachPartition at streamingProcessNew.scala:50) finished in 0.022 s
2017-08-18 12:13:45,112 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 284 finished: foreachPartition at streamingProcessNew.scala:50, took 0.048758 s
2017-08-18 12:13:45,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029625000 ms.0 from job set of time 1503029625000 ms
2017-08-18 12:13:45,113 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.112 s for time 1503029625000 ms (execution: 0.091 s)
2017-08-18 12:13:45,113 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 567 from persistence list
2017-08-18 12:13:45,113 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 567
2017-08-18 12:13:45,113 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 566 from persistence list
2017-08-18 12:13:45,114 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 566
2017-08-18 12:13:45,114 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:13:45,114 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029615000 ms
2017-08-18 12:13:50,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029630000 ms
2017-08-18 12:13:50,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029630000 ms.0 from job set of time 1503029630000 ms
2017-08-18 12:13:50,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:13:50,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 285 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:13:50,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 285 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:13:50,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:13:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:13:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 285 (MapPartitionsRDD[571] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:13:50,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_285 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:13:50,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_285_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:13:50,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_285_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:13:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 285 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:13:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 285 (MapPartitionsRDD[571] at map at streamingProcessNew.scala:49)
2017-08-18 12:13:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 285.0 with 2 tasks
2017-08-18 12:13:50,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 285.0 (TID 570, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:13:50,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 285.0 (TID 571, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:13:50,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 285.0 (TID 571)
2017-08-18 12:13:50,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 285.0 (TID 570)
2017-08-18 12:13:50,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:13:50,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:13:50,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 285.0 (TID 570). 714 bytes result sent to driver
2017-08-18 12:13:50,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 285.0 (TID 571). 714 bytes result sent to driver
2017-08-18 12:13:50,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 285.0 (TID 571) in 10 ms on localhost (1/2)
2017-08-18 12:13:50,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 285.0 (TID 570) in 12 ms on localhost (2/2)
2017-08-18 12:13:50,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 285.0, whose tasks have all completed, from pool 
2017-08-18 12:13:50,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 285 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:13:50,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 285 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024139 s
2017-08-18 12:13:50,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029630000 ms.0 from job set of time 1503029630000 ms
2017-08-18 12:13:50,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1503029630000 ms (execution: 0.060 s)
2017-08-18 12:13:50,084 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 569 from persistence list
2017-08-18 12:13:50,084 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 569
2017-08-18 12:13:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 568 from persistence list
2017-08-18 12:13:50,085 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 568
2017-08-18 12:13:50,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:13:50,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029620000 ms
2017-08-18 12:13:55,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029635000 ms
2017-08-18 12:13:55,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029635000 ms.0 from job set of time 1503029635000 ms
2017-08-18 12:13:55,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:13:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 286 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:13:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 286 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:13:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:13:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:13:55,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 286 (MapPartitionsRDD[573] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:13:55,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_286 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:13:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_286_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:13:55,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_286_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:13:55,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 286 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:13:55,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 286 (MapPartitionsRDD[573] at map at streamingProcessNew.scala:49)
2017-08-18 12:13:55,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 286.0 with 2 tasks
2017-08-18 12:13:55,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 286.0 (TID 572, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:13:55,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 286.0 (TID 573, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:13:55,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 286.0 (TID 572)
2017-08-18 12:13:55,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 286.0 (TID 573)
2017-08-18 12:13:55,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:13:55,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:13:55,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 286.0 (TID 573). 714 bytes result sent to driver
2017-08-18 12:13:55,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 286.0 (TID 572). 714 bytes result sent to driver
2017-08-18 12:13:55,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 286.0 (TID 573) in 7 ms on localhost (1/2)
2017-08-18 12:13:55,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 286.0 (TID 572) in 10 ms on localhost (2/2)
2017-08-18 12:13:55,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 286.0, whose tasks have all completed, from pool 
2017-08-18 12:13:55,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 286 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:13:55,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 286 finished: foreachPartition at streamingProcessNew.scala:50, took 0.026197 s
2017-08-18 12:13:55,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029635000 ms.0 from job set of time 1503029635000 ms
2017-08-18 12:13:55,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1503029635000 ms (execution: 0.057 s)
2017-08-18 12:13:55,078 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 571 from persistence list
2017-08-18 12:13:55,078 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 571
2017-08-18 12:13:55,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 570 from persistence list
2017-08-18 12:13:55,079 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 570
2017-08-18 12:13:55,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:13:55,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029625000 ms
2017-08-18 12:14:00,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029640000 ms
2017-08-18 12:14:00,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029640000 ms.0 from job set of time 1503029640000 ms
2017-08-18 12:14:00,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:14:00,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 287 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:14:00,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 287 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:14:00,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:14:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:14:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 287 (MapPartitionsRDD[575] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:14:00,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_287 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:14:00,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_287_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:14:00,037 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_287_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:14:00,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 287 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:14:00,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 287 (MapPartitionsRDD[575] at map at streamingProcessNew.scala:49)
2017-08-18 12:14:00,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 287.0 with 2 tasks
2017-08-18 12:14:00,041 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 287.0 (TID 574, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:14:00,041 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 287.0 (TID 575, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:14:00,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 287.0 (TID 574)
2017-08-18 12:14:00,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 287.0 (TID 575)
2017-08-18 12:14:00,043 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:14:00,043 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:14:00,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 287.0 (TID 575). 714 bytes result sent to driver
2017-08-18 12:14:00,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 287.0 (TID 574). 714 bytes result sent to driver
2017-08-18 12:14:00,048 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 287.0 (TID 574) in 9 ms on localhost (1/2)
2017-08-18 12:14:00,048 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 287.0 (TID 575) in 7 ms on localhost (2/2)
2017-08-18 12:14:00,048 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 287.0, whose tasks have all completed, from pool 
2017-08-18 12:14:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 287 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:14:00,048 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 287 finished: foreachPartition at streamingProcessNew.scala:50, took 0.018377 s
2017-08-18 12:14:00,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029640000 ms.0 from job set of time 1503029640000 ms
2017-08-18 12:14:00,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1503029640000 ms (execution: 0.031 s)
2017-08-18 12:14:00,049 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 573 from persistence list
2017-08-18 12:14:00,049 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 573
2017-08-18 12:14:00,049 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 572 from persistence list
2017-08-18 12:14:00,049 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 572
2017-08-18 12:14:00,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:14:00,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029630000 ms
2017-08-18 12:14:05,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029645000 ms
2017-08-18 12:14:05,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029645000 ms.0 from job set of time 1503029645000 ms
2017-08-18 12:14:05,062 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:14:05,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 288 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:14:05,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 288 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:14:05,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:14:05,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:14:05,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 288 (MapPartitionsRDD[577] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:14:05,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_288 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:14:05,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_288_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:14:05,086 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_288_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:14:05,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 288 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:14:05,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 288 (MapPartitionsRDD[577] at map at streamingProcessNew.scala:49)
2017-08-18 12:14:05,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 288.0 with 2 tasks
2017-08-18 12:14:05,090 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 288.0 (TID 576, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:14:05,091 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 288.0 (TID 577, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:14:05,091 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 288.0 (TID 577)
2017-08-18 12:14:05,091 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 288.0 (TID 576)
2017-08-18 12:14:05,094 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:14:05,094 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:14:05,099 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 288.0 (TID 577). 714 bytes result sent to driver
2017-08-18 12:14:05,099 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 288.0 (TID 576). 714 bytes result sent to driver
2017-08-18 12:14:05,102 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 288.0 (TID 577) in 12 ms on localhost (1/2)
2017-08-18 12:14:05,102 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 288.0 (TID 576) in 15 ms on localhost (2/2)
2017-08-18 12:14:05,102 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 288.0, whose tasks have all completed, from pool 
2017-08-18 12:14:05,102 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 288 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:14:05,103 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 288 finished: foreachPartition at streamingProcessNew.scala:50, took 0.040710 s
2017-08-18 12:14:05,103 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029645000 ms.0 from job set of time 1503029645000 ms
2017-08-18 12:14:05,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.103 s for time 1503029645000 ms (execution: 0.082 s)
2017-08-18 12:14:05,104 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 575 from persistence list
2017-08-18 12:14:05,104 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 575
2017-08-18 12:14:05,104 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 574 from persistence list
2017-08-18 12:14:05,105 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 574
2017-08-18 12:14:05,105 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:14:05,105 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029635000 ms
2017-08-18 12:14:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029650000 ms
2017-08-18 12:14:10,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029650000 ms.0 from job set of time 1503029650000 ms
2017-08-18 12:14:10,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:14:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 289 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:14:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 289 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:14:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:14:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:14:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 289 (MapPartitionsRDD[579] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:14:10,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_289 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:14:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_289_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:14:10,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_289_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:14:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 289 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:14:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 289 (MapPartitionsRDD[579] at map at streamingProcessNew.scala:49)
2017-08-18 12:14:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 289.0 with 2 tasks
2017-08-18 12:14:10,081 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 289.0 (TID 578, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:14:10,082 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 289.0 (TID 579, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:14:10,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 289.0 (TID 579)
2017-08-18 12:14:10,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 289.0 (TID 578)
2017-08-18 12:14:10,086 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:14:10,086 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:14:10,091 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 289.0 (TID 578). 714 bytes result sent to driver
2017-08-18 12:14:10,091 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 289.0 (TID 579). 714 bytes result sent to driver
2017-08-18 12:14:10,093 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 289.0 (TID 578) in 16 ms on localhost (1/2)
2017-08-18 12:14:10,094 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 289.0 (TID 579) in 12 ms on localhost (2/2)
2017-08-18 12:14:10,094 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 289.0, whose tasks have all completed, from pool 
2017-08-18 12:14:10,094 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 289 (foreachPartition at streamingProcessNew.scala:50) finished in 0.017 s
2017-08-18 12:14:10,094 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 289 finished: foreachPartition at streamingProcessNew.scala:50, took 0.033982 s
2017-08-18 12:14:10,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029650000 ms.0 from job set of time 1503029650000 ms
2017-08-18 12:14:10,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.095 s for time 1503029650000 ms (execution: 0.074 s)
2017-08-18 12:14:10,095 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 577 from persistence list
2017-08-18 12:14:10,096 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 577
2017-08-18 12:14:10,096 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 576 from persistence list
2017-08-18 12:14:10,096 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 576
2017-08-18 12:14:10,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:14:10,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029640000 ms
2017-08-18 12:14:15,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029655000 ms
2017-08-18 12:14:15,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029655000 ms.0 from job set of time 1503029655000 ms
2017-08-18 12:14:15,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:14:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 290 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:14:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 290 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:14:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:14:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:14:15,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 290 (MapPartitionsRDD[581] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:14:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_290 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:14:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_290_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:14:15,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_290_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:14:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 290 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:14:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 290 (MapPartitionsRDD[581] at map at streamingProcessNew.scala:49)
2017-08-18 12:14:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 290.0 with 2 tasks
2017-08-18 12:14:15,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 290.0 (TID 580, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:14:15,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 290.0 (TID 581, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:14:15,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 290.0 (TID 580)
2017-08-18 12:14:15,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 290.0 (TID 581)
2017-08-18 12:14:15,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:14:15,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:14:15,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 290.0 (TID 581). 714 bytes result sent to driver
2017-08-18 12:14:15,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 290.0 (TID 580). 801 bytes result sent to driver
2017-08-18 12:14:15,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 290.0 (TID 581) in 10 ms on localhost (1/2)
2017-08-18 12:14:15,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 290.0 (TID 580) in 12 ms on localhost (2/2)
2017-08-18 12:14:15,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 290.0, whose tasks have all completed, from pool 
2017-08-18 12:14:15,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 290 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:14:15,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 290 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024090 s
2017-08-18 12:14:15,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029655000 ms.0 from job set of time 1503029655000 ms
2017-08-18 12:14:15,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1503029655000 ms (execution: 0.055 s)
2017-08-18 12:14:15,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 579 from persistence list
2017-08-18 12:14:15,078 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 579
2017-08-18 12:14:15,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 578 from persistence list
2017-08-18 12:14:15,078 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 578
2017-08-18 12:14:15,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:14:15,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029645000 ms
2017-08-18 12:14:20,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029660000 ms
2017-08-18 12:14:20,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029660000 ms.0 from job set of time 1503029660000 ms
2017-08-18 12:14:20,028 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:14:20,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 291 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:14:20,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 291 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:14:20,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:14:20,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:14:20,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 291 (MapPartitionsRDD[583] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:14:20,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_291 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:14:20,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_291_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:14:20,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_291_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:14:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 291 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:14:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 291 (MapPartitionsRDD[583] at map at streamingProcessNew.scala:49)
2017-08-18 12:14:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 291.0 with 2 tasks
2017-08-18 12:14:20,039 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 291.0 (TID 582, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:14:20,040 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 291.0 (TID 583, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:14:20,040 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 291.0 (TID 583)
2017-08-18 12:14:20,040 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 291.0 (TID 582)
2017-08-18 12:14:20,042 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:14:20,042 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:14:20,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 291.0 (TID 583). 714 bytes result sent to driver
2017-08-18 12:14:20,045 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 291.0 (TID 582). 714 bytes result sent to driver
2017-08-18 12:14:20,047 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 291.0 (TID 583) in 8 ms on localhost (1/2)
2017-08-18 12:14:20,047 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 291.0 (TID 582) in 11 ms on localhost (2/2)
2017-08-18 12:14:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 291 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:14:20,047 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 291.0, whose tasks have all completed, from pool 
2017-08-18 12:14:20,047 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 291 finished: foreachPartition at streamingProcessNew.scala:50, took 0.019414 s
2017-08-18 12:14:20,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029660000 ms.0 from job set of time 1503029660000 ms
2017-08-18 12:14:20,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1503029660000 ms (execution: 0.031 s)
2017-08-18 12:14:20,048 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 581 from persistence list
2017-08-18 12:14:20,048 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 581
2017-08-18 12:14:20,048 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 580 from persistence list
2017-08-18 12:14:20,049 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 580
2017-08-18 12:14:20,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:14:20,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029650000 ms
2017-08-18 12:14:25,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029665000 ms
2017-08-18 12:14:25,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029665000 ms.0 from job set of time 1503029665000 ms
2017-08-18 12:14:25,064 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:14:25,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 292 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:14:25,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 292 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:14:25,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:14:25,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:14:25,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 292 (MapPartitionsRDD[585] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:14:25,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_292 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:14:25,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_292_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:14:25,089 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_292_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:14:25,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 292 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:14:25,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 292 (MapPartitionsRDD[585] at map at streamingProcessNew.scala:49)
2017-08-18 12:14:25,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 292.0 with 2 tasks
2017-08-18 12:14:25,096 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 292.0 (TID 584, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:14:25,097 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 292.0 (TID 585, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:14:25,098 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 292.0 (TID 584)
2017-08-18 12:14:25,098 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 292.0 (TID 585)
2017-08-18 12:14:25,109 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:14:25,109 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:14:25,118 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 292.0 (TID 584). 801 bytes result sent to driver
2017-08-18 12:14:25,118 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 292.0 (TID 585). 714 bytes result sent to driver
2017-08-18 12:14:25,124 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 292.0 (TID 585) in 27 ms on localhost (1/2)
2017-08-18 12:14:25,124 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 292.0 (TID 584) in 32 ms on localhost (2/2)
2017-08-18 12:14:25,125 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 292.0, whose tasks have all completed, from pool 
2017-08-18 12:14:25,125 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 292 (foreachPartition at streamingProcessNew.scala:50) finished in 0.033 s
2017-08-18 12:14:25,125 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 292 finished: foreachPartition at streamingProcessNew.scala:50, took 0.061088 s
2017-08-18 12:14:25,126 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029665000 ms.0 from job set of time 1503029665000 ms
2017-08-18 12:14:25,126 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.126 s for time 1503029665000 ms (execution: 0.105 s)
2017-08-18 12:14:25,126 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 583 from persistence list
2017-08-18 12:14:25,127 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 583
2017-08-18 12:14:25,127 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 582 from persistence list
2017-08-18 12:14:25,127 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 582
2017-08-18 12:14:25,127 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:14:25,127 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029655000 ms
2017-08-18 12:14:30,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029670000 ms
2017-08-18 12:14:30,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029670000 ms.0 from job set of time 1503029670000 ms
2017-08-18 12:14:30,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:14:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 293 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:14:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 293 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:14:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:14:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:14:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 293 (MapPartitionsRDD[587] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:14:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_293 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:14:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_293_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-18 12:14:30,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_293_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:14:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 293 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:14:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 293 (MapPartitionsRDD[587] at map at streamingProcessNew.scala:49)
2017-08-18 12:14:30,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 293.0 with 2 tasks
2017-08-18 12:14:30,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 293.0 (TID 586, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:14:30,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 293.0 (TID 587, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:14:30,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 293.0 (TID 586)
2017-08-18 12:14:30,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 293.0 (TID 587)
2017-08-18 12:14:30,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:14:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:14:30,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 293.0 (TID 586). 714 bytes result sent to driver
2017-08-18 12:14:30,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 293.0 (TID 587). 714 bytes result sent to driver
2017-08-18 12:14:30,079 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 293.0 (TID 586) in 13 ms on localhost (1/2)
2017-08-18 12:14:30,079 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 293.0 (TID 587) in 10 ms on localhost (2/2)
2017-08-18 12:14:30,079 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 293.0, whose tasks have all completed, from pool 
2017-08-18 12:14:30,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 293 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:14:30,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 293 finished: foreachPartition at streamingProcessNew.scala:50, took 0.026807 s
2017-08-18 12:14:30,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029670000 ms.0 from job set of time 1503029670000 ms
2017-08-18 12:14:30,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1503029670000 ms (execution: 0.060 s)
2017-08-18 12:14:30,080 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 585 from persistence list
2017-08-18 12:14:30,081 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 585
2017-08-18 12:14:30,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 584 from persistence list
2017-08-18 12:14:30,081 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 584
2017-08-18 12:14:30,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:14:30,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029660000 ms
2017-08-18 12:14:35,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029675000 ms
2017-08-18 12:14:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029675000 ms.0 from job set of time 1503029675000 ms
2017-08-18 12:14:35,065 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:14:35,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_293_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:14:35,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 294 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:14:35,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 294 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:14:35,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:14:35,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:14:35,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 294 (MapPartitionsRDD[589] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:14:35,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_278_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:14:35,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_279_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:14:35,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_280_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:14:35,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_294 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:14:35,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_281_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:14:35,081 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_282_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:14:35,084 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_283_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:14:35,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_294_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:14:35,086 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_294_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:14:35,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 294 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:14:35,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 294 (MapPartitionsRDD[589] at map at streamingProcessNew.scala:49)
2017-08-18 12:14:35,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 294.0 with 2 tasks
2017-08-18 12:14:35,090 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_284_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:14:35,090 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 294.0 (TID 588, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:14:35,090 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 294.0 (TID 589, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:14:35,091 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 294.0 (TID 589)
2017-08-18 12:14:35,091 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 294.0 (TID 588)
2017-08-18 12:14:35,092 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_285_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:14:35,094 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_286_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:14:35,096 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:14:35,096 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:14:35,098 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_287_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:14:35,101 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_288_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:14:35,101 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 294.0 (TID 589). 714 bytes result sent to driver
2017-08-18 12:14:35,101 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 294.0 (TID 588). 714 bytes result sent to driver
2017-08-18 12:14:35,102 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_289_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:14:35,103 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 294.0 (TID 589) in 13 ms on localhost (1/2)
2017-08-18 12:14:35,103 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 294.0 (TID 588) in 15 ms on localhost (2/2)
2017-08-18 12:14:35,103 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 294.0, whose tasks have all completed, from pool 
2017-08-18 12:14:35,103 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 294 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:14:35,103 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 294 finished: foreachPartition at streamingProcessNew.scala:50, took 0.038072 s
2017-08-18 12:14:35,103 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_290_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:14:35,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029675000 ms.0 from job set of time 1503029675000 ms
2017-08-18 12:14:35,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.104 s for time 1503029675000 ms (execution: 0.083 s)
2017-08-18 12:14:35,104 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 587 from persistence list
2017-08-18 12:14:35,104 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 587
2017-08-18 12:14:35,104 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 586 from persistence list
2017-08-18 12:14:35,104 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_291_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:14:35,105 [block-manager-slave-async-thread-pool-28] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 586
2017-08-18 12:14:35,105 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:14:35,105 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029665000 ms
2017-08-18 12:14:35,105 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_292_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:14:40,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029680000 ms
2017-08-18 12:14:40,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029680000 ms.0 from job set of time 1503029680000 ms
2017-08-18 12:14:40,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:14:40,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 295 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:14:40,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 295 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:14:40,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:14:40,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:14:40,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 295 (MapPartitionsRDD[591] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:14:40,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_295 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:14:40,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_295_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:14:40,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_295_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:14:40,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 295 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:14:40,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 295 (MapPartitionsRDD[591] at map at streamingProcessNew.scala:49)
2017-08-18 12:14:40,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 295.0 with 2 tasks
2017-08-18 12:14:40,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 295.0 (TID 590, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:14:40,038 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 295.0 (TID 591, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:14:40,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 295.0 (TID 591)
2017-08-18 12:14:40,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 295.0 (TID 590)
2017-08-18 12:14:40,040 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:14:40,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:14:40,043 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 295.0 (TID 590). 714 bytes result sent to driver
2017-08-18 12:14:40,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 295.0 (TID 591). 714 bytes result sent to driver
2017-08-18 12:14:40,045 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 295.0 (TID 590) in 9 ms on localhost (1/2)
2017-08-18 12:14:40,045 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 295.0 (TID 591) in 7 ms on localhost (2/2)
2017-08-18 12:14:40,045 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 295.0, whose tasks have all completed, from pool 
2017-08-18 12:14:40,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 295 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:14:40,046 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 295 finished: foreachPartition at streamingProcessNew.scala:50, took 0.018310 s
2017-08-18 12:14:40,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029680000 ms.0 from job set of time 1503029680000 ms
2017-08-18 12:14:40,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.046 s for time 1503029680000 ms (execution: 0.032 s)
2017-08-18 12:14:40,046 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 589 from persistence list
2017-08-18 12:14:40,046 [block-manager-slave-async-thread-pool-28] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 589
2017-08-18 12:14:40,047 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 588 from persistence list
2017-08-18 12:14:40,047 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 588
2017-08-18 12:14:40,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:14:40,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029670000 ms
2017-08-18 12:14:45,031 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029685000 ms
2017-08-18 12:14:45,032 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029685000 ms.0 from job set of time 1503029685000 ms
2017-08-18 12:14:45,065 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:14:45,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 296 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:14:45,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 296 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:14:45,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:14:45,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:14:45,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 296 (MapPartitionsRDD[593] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:14:45,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_296 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:14:45,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_296_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:14:45,079 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_296_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:14:45,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 296 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:14:45,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 296 (MapPartitionsRDD[593] at map at streamingProcessNew.scala:49)
2017-08-18 12:14:45,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 296.0 with 2 tasks
2017-08-18 12:14:45,081 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 296.0 (TID 592, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:14:45,082 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 296.0 (TID 593, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:14:45,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 296.0 (TID 593)
2017-08-18 12:14:45,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 296.0 (TID 592)
2017-08-18 12:14:45,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:14:45,084 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:14:45,088 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 296.0 (TID 593). 714 bytes result sent to driver
2017-08-18 12:14:45,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 296.0 (TID 592). 714 bytes result sent to driver
2017-08-18 12:14:45,091 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 296.0 (TID 593) in 8 ms on localhost (1/2)
2017-08-18 12:14:45,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 296.0 (TID 592) in 11 ms on localhost (2/2)
2017-08-18 12:14:45,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 296.0, whose tasks have all completed, from pool 
2017-08-18 12:14:45,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 296 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:14:45,091 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 296 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025743 s
2017-08-18 12:14:45,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029685000 ms.0 from job set of time 1503029685000 ms
2017-08-18 12:14:45,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1503029685000 ms (execution: 0.060 s)
2017-08-18 12:14:45,092 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 591 from persistence list
2017-08-18 12:14:45,092 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 591
2017-08-18 12:14:45,092 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 590 from persistence list
2017-08-18 12:14:45,093 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 590
2017-08-18 12:14:45,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:14:45,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029675000 ms
2017-08-18 12:14:50,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029690000 ms
2017-08-18 12:14:50,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029690000 ms.0 from job set of time 1503029690000 ms
2017-08-18 12:14:50,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:14:50,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 297 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:14:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 297 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:14:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:14:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:14:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 297 (MapPartitionsRDD[595] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:14:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_297 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:14:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_297_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:14:50,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_297_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:14:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 297 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:14:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 297 (MapPartitionsRDD[595] at map at streamingProcessNew.scala:49)
2017-08-18 12:14:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 297.0 with 2 tasks
2017-08-18 12:14:50,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 297.0 (TID 594, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:14:50,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 297.0 (TID 595, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:14:50,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 297.0 (TID 594)
2017-08-18 12:14:50,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 297.0 (TID 595)
2017-08-18 12:14:50,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:14:50,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:14:50,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 297.0 (TID 594). 714 bytes result sent to driver
2017-08-18 12:14:50,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 297.0 (TID 595). 714 bytes result sent to driver
2017-08-18 12:14:50,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 297.0 (TID 594) in 14 ms on localhost (1/2)
2017-08-18 12:14:50,070 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 297.0 (TID 595) in 11 ms on localhost (2/2)
2017-08-18 12:14:50,070 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 297.0, whose tasks have all completed, from pool 
2017-08-18 12:14:50,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 297 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:14:50,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 297 finished: foreachPartition at streamingProcessNew.scala:50, took 0.028510 s
2017-08-18 12:14:50,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029690000 ms.0 from job set of time 1503029690000 ms
2017-08-18 12:14:50,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1503029690000 ms (execution: 0.051 s)
2017-08-18 12:14:50,071 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 593 from persistence list
2017-08-18 12:14:50,072 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 593
2017-08-18 12:14:50,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 592 from persistence list
2017-08-18 12:14:50,073 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 592
2017-08-18 12:14:50,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:14:50,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029680000 ms
2017-08-18 12:14:55,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029695000 ms
2017-08-18 12:14:55,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029695000 ms.0 from job set of time 1503029695000 ms
2017-08-18 12:14:55,028 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:14:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 298 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:14:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 298 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:14:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:14:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:14:55,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 298 (MapPartitionsRDD[597] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:14:55,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_298 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:14:55,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_298_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:14:55,038 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_298_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:14:55,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 298 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:14:55,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 298 (MapPartitionsRDD[597] at map at streamingProcessNew.scala:49)
2017-08-18 12:14:55,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 298.0 with 2 tasks
2017-08-18 12:14:55,042 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 298.0 (TID 596, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:14:55,042 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 298.0 (TID 597, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:14:55,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 298.0 (TID 597)
2017-08-18 12:14:55,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 298.0 (TID 596)
2017-08-18 12:14:55,045 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:14:55,045 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:14:55,049 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 298.0 (TID 596). 714 bytes result sent to driver
2017-08-18 12:14:55,049 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 298.0 (TID 597). 714 bytes result sent to driver
2017-08-18 12:14:55,051 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 298.0 (TID 596) in 11 ms on localhost (1/2)
2017-08-18 12:14:55,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 298.0 (TID 597) in 9 ms on localhost (2/2)
2017-08-18 12:14:55,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 298.0, whose tasks have all completed, from pool 
2017-08-18 12:14:55,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 298 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:14:55,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 298 finished: foreachPartition at streamingProcessNew.scala:50, took 0.023359 s
2017-08-18 12:14:55,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029695000 ms.0 from job set of time 1503029695000 ms
2017-08-18 12:14:55,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.052 s for time 1503029695000 ms (execution: 0.039 s)
2017-08-18 12:14:55,053 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 595 from persistence list
2017-08-18 12:14:55,053 [block-manager-slave-async-thread-pool-28] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 595
2017-08-18 12:14:55,053 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 594 from persistence list
2017-08-18 12:14:55,053 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 594
2017-08-18 12:14:55,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:14:55,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029685000 ms
2017-08-18 12:15:00,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029700000 ms
2017-08-18 12:15:00,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029700000 ms.0 from job set of time 1503029700000 ms
2017-08-18 12:15:00,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:15:00,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 299 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:15:00,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 299 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:15:00,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:15:00,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:15:00,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 299 (MapPartitionsRDD[599] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:15:00,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_299 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:15:00,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_299_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:15:00,036 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_299_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:15:00,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 299 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:15:00,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 299 (MapPartitionsRDD[599] at map at streamingProcessNew.scala:49)
2017-08-18 12:15:00,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 299.0 with 2 tasks
2017-08-18 12:15:00,039 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 299.0 (TID 598, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:15:00,039 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 299.0 (TID 599, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:15:00,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 299.0 (TID 599)
2017-08-18 12:15:00,040 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 299.0 (TID 598)
2017-08-18 12:15:00,041 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:15:00,042 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:15:00,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 299.0 (TID 599). 714 bytes result sent to driver
2017-08-18 12:15:00,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 299.0 (TID 598). 714 bytes result sent to driver
2017-08-18 12:15:00,046 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 299.0 (TID 599) in 7 ms on localhost (1/2)
2017-08-18 12:15:00,046 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 299.0 (TID 598) in 9 ms on localhost (2/2)
2017-08-18 12:15:00,047 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 299.0, whose tasks have all completed, from pool 
2017-08-18 12:15:00,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 299 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:15:00,047 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 299 finished: foreachPartition at streamingProcessNew.scala:50, took 0.018213 s
2017-08-18 12:15:00,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029700000 ms.0 from job set of time 1503029700000 ms
2017-08-18 12:15:00,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.047 s for time 1503029700000 ms (execution: 0.031 s)
2017-08-18 12:15:00,048 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 597 from persistence list
2017-08-18 12:15:00,048 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 596 from persistence list
2017-08-18 12:15:00,048 [block-manager-slave-async-thread-pool-28] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 597
2017-08-18 12:15:00,049 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 596
2017-08-18 12:15:00,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:15:00,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029690000 ms
2017-08-18 12:15:05,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029705000 ms
2017-08-18 12:15:05,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029705000 ms.0 from job set of time 1503029705000 ms
2017-08-18 12:15:05,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:15:05,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 300 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:15:05,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 300 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:15:05,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:15:05,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:15:05,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 300 (MapPartitionsRDD[601] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:15:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_300 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:15:05,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_300_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:15:05,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_300_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:15:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 300 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:15:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 300 (MapPartitionsRDD[601] at map at streamingProcessNew.scala:49)
2017-08-18 12:15:05,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 300.0 with 2 tasks
2017-08-18 12:15:05,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 300.0 (TID 600, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:15:05,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 300.0 (TID 601, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:15:05,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 300.0 (TID 600)
2017-08-18 12:15:05,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 300.0 (TID 601)
2017-08-18 12:15:05,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:15:05,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:15:05,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 300.0 (TID 601). 714 bytes result sent to driver
2017-08-18 12:15:05,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 300.0 (TID 600). 714 bytes result sent to driver
2017-08-18 12:15:05,070 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 300.0 (TID 600) in 10 ms on localhost (1/2)
2017-08-18 12:15:05,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 300.0 (TID 601) in 8 ms on localhost (2/2)
2017-08-18 12:15:05,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 300.0, whose tasks have all completed, from pool 
2017-08-18 12:15:05,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 300 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:15:05,071 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 300 finished: foreachPartition at streamingProcessNew.scala:50, took 0.022928 s
2017-08-18 12:15:05,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029705000 ms.0 from job set of time 1503029705000 ms
2017-08-18 12:15:05,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1503029705000 ms (execution: 0.048 s)
2017-08-18 12:15:05,071 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 599 from persistence list
2017-08-18 12:15:05,072 [block-manager-slave-async-thread-pool-28] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 599
2017-08-18 12:15:05,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 598 from persistence list
2017-08-18 12:15:05,072 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 598
2017-08-18 12:15:05,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:15:05,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029695000 ms
2017-08-18 12:15:10,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029710000 ms
2017-08-18 12:15:10,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029710000 ms.0 from job set of time 1503029710000 ms
2017-08-18 12:15:10,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:15:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 301 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:15:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 301 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:15:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:15:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:15:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 301 (MapPartitionsRDD[603] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:15:10,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_301 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:15:10,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_301_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:15:10,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_301_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:15:10,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 301 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:15:10,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 301 (MapPartitionsRDD[603] at map at streamingProcessNew.scala:49)
2017-08-18 12:15:10,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 301.0 with 2 tasks
2017-08-18 12:15:10,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 301.0 (TID 602, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:15:10,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 301.0 (TID 603, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:15:10,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 301.0 (TID 602)
2017-08-18 12:15:10,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 301.0 (TID 603)
2017-08-18 12:15:10,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:15:10,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:15:10,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 301.0 (TID 603). 714 bytes result sent to driver
2017-08-18 12:15:10,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 301.0 (TID 602). 714 bytes result sent to driver
2017-08-18 12:15:10,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 301.0 (TID 603) in 10 ms on localhost (1/2)
2017-08-18 12:15:10,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 301.0 (TID 602) in 12 ms on localhost (2/2)
2017-08-18 12:15:10,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 301.0, whose tasks have all completed, from pool 
2017-08-18 12:15:10,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 301 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:15:10,084 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 301 finished: foreachPartition at streamingProcessNew.scala:50, took 0.029710 s
2017-08-18 12:15:10,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029710000 ms.0 from job set of time 1503029710000 ms
2017-08-18 12:15:10,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1503029710000 ms (execution: 0.061 s)
2017-08-18 12:15:10,085 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 601 from persistence list
2017-08-18 12:15:10,085 [block-manager-slave-async-thread-pool-28] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 601
2017-08-18 12:15:10,085 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 600 from persistence list
2017-08-18 12:15:10,085 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 600
2017-08-18 12:15:10,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:15:10,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029700000 ms
2017-08-18 12:15:15,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029715000 ms
2017-08-18 12:15:15,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029715000 ms.0 from job set of time 1503029715000 ms
2017-08-18 12:15:15,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:15:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 302 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:15:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 302 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:15:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:15:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:15:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 302 (MapPartitionsRDD[605] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:15:15,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_302 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:15:15,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_302_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:15:15,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_302_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:15:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 302 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:15:15,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 302 (MapPartitionsRDD[605] at map at streamingProcessNew.scala:49)
2017-08-18 12:15:15,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 302.0 with 2 tasks
2017-08-18 12:15:15,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 302.0 (TID 604, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:15:15,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 302.0 (TID 605, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:15:15,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 302.0 (TID 604)
2017-08-18 12:15:15,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 302.0 (TID 605)
2017-08-18 12:15:15,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:15:15,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:15:15,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 302.0 (TID 605). 714 bytes result sent to driver
2017-08-18 12:15:15,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 302.0 (TID 604). 714 bytes result sent to driver
2017-08-18 12:15:15,083 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 302.0 (TID 605) in 9 ms on localhost (1/2)
2017-08-18 12:15:15,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 302.0 (TID 604) in 13 ms on localhost (2/2)
2017-08-18 12:15:15,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 302.0, whose tasks have all completed, from pool 
2017-08-18 12:15:15,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 302 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:15:15,084 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 302 finished: foreachPartition at streamingProcessNew.scala:50, took 0.029613 s
2017-08-18 12:15:15,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029715000 ms.0 from job set of time 1503029715000 ms
2017-08-18 12:15:15,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1503029715000 ms (execution: 0.063 s)
2017-08-18 12:15:15,085 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 603 from persistence list
2017-08-18 12:15:15,085 [block-manager-slave-async-thread-pool-28] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 603
2017-08-18 12:15:15,085 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 602 from persistence list
2017-08-18 12:15:15,085 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 602
2017-08-18 12:15:15,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:15:15,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029705000 ms
2017-08-18 12:15:20,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029720000 ms
2017-08-18 12:15:20,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029720000 ms.0 from job set of time 1503029720000 ms
2017-08-18 12:15:20,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:15:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 303 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:15:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 303 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:15:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:15:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:15:20,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 303 (MapPartitionsRDD[607] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:15:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_303 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:15:20,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_303_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:15:20,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_303_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:15:20,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 303 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:15:20,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 303 (MapPartitionsRDD[607] at map at streamingProcessNew.scala:49)
2017-08-18 12:15:20,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 303.0 with 2 tasks
2017-08-18 12:15:20,037 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 303.0 (TID 606, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:15:20,038 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 303.0 (TID 607, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:15:20,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 303.0 (TID 606)
2017-08-18 12:15:20,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 303.0 (TID 607)
2017-08-18 12:15:20,041 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:15:20,041 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:15:20,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 303.0 (TID 607). 714 bytes result sent to driver
2017-08-18 12:15:20,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 303.0 (TID 606). 714 bytes result sent to driver
2017-08-18 12:15:20,045 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 303.0 (TID 606) in 10 ms on localhost (1/2)
2017-08-18 12:15:20,045 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 303.0 (TID 607) in 7 ms on localhost (2/2)
2017-08-18 12:15:20,045 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 303.0, whose tasks have all completed, from pool 
2017-08-18 12:15:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 303 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:15:20,046 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 303 finished: foreachPartition at streamingProcessNew.scala:50, took 0.019374 s
2017-08-18 12:15:20,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029720000 ms.0 from job set of time 1503029720000 ms
2017-08-18 12:15:20,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.046 s for time 1503029720000 ms (execution: 0.031 s)
2017-08-18 12:15:20,046 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 605 from persistence list
2017-08-18 12:15:20,047 [block-manager-slave-async-thread-pool-28] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 605
2017-08-18 12:15:20,047 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 604 from persistence list
2017-08-18 12:15:20,047 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 604
2017-08-18 12:15:20,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:15:20,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029710000 ms
2017-08-18 12:15:25,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029725000 ms
2017-08-18 12:15:25,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029725000 ms.0 from job set of time 1503029725000 ms
2017-08-18 12:15:25,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:15:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 304 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:15:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 304 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:15:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:15:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:15:25,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 304 (MapPartitionsRDD[609] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:15:25,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_304 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:15:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_304_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:15:25,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_304_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:15:25,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 304 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:15:25,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 304 (MapPartitionsRDD[609] at map at streamingProcessNew.scala:49)
2017-08-18 12:15:25,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 304.0 with 2 tasks
2017-08-18 12:15:25,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 304.0 (TID 608, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:15:25,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 304.0 (TID 609, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:15:25,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 304.0 (TID 609)
2017-08-18 12:15:25,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 304.0 (TID 608)
2017-08-18 12:15:25,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:15:25,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:15:25,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 304.0 (TID 609). 801 bytes result sent to driver
2017-08-18 12:15:25,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 304.0 (TID 608). 714 bytes result sent to driver
2017-08-18 12:15:25,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 304.0 (TID 608) in 10 ms on localhost (1/2)
2017-08-18 12:15:25,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 304.0 (TID 609) in 8 ms on localhost (2/2)
2017-08-18 12:15:25,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 304.0, whose tasks have all completed, from pool 
2017-08-18 12:15:25,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 304 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:15:25,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 304 finished: foreachPartition at streamingProcessNew.scala:50, took 0.027681 s
2017-08-18 12:15:25,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029725000 ms.0 from job set of time 1503029725000 ms
2017-08-18 12:15:25,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1503029725000 ms (execution: 0.055 s)
2017-08-18 12:15:25,076 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 607 from persistence list
2017-08-18 12:15:25,076 [block-manager-slave-async-thread-pool-28] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 607
2017-08-18 12:15:25,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 606 from persistence list
2017-08-18 12:15:25,077 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 606
2017-08-18 12:15:25,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:15:25,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029715000 ms
2017-08-18 12:15:30,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029730000 ms
2017-08-18 12:15:30,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029730000 ms.0 from job set of time 1503029730000 ms
2017-08-18 12:15:30,064 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:15:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 305 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:15:30,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 305 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:15:30,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:15:30,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:15:30,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 305 (MapPartitionsRDD[611] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:15:30,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_305 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:15:30,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_305_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:15:30,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_305_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:15:30,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 305 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:15:30,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 305 (MapPartitionsRDD[611] at map at streamingProcessNew.scala:49)
2017-08-18 12:15:30,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 305.0 with 2 tasks
2017-08-18 12:15:30,080 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 305.0 (TID 610, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:15:30,080 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 305.0 (TID 611, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:15:30,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 305.0 (TID 611)
2017-08-18 12:15:30,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 305.0 (TID 610)
2017-08-18 12:15:30,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:15:30,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:15:30,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 305.0 (TID 611). 714 bytes result sent to driver
2017-08-18 12:15:30,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 305.0 (TID 610). 714 bytes result sent to driver
2017-08-18 12:15:30,087 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 305.0 (TID 610) in 9 ms on localhost (1/2)
2017-08-18 12:15:30,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 305.0 (TID 611) in 7 ms on localhost (2/2)
2017-08-18 12:15:30,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 305.0, whose tasks have all completed, from pool 
2017-08-18 12:15:30,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 305 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:15:30,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 305 finished: foreachPartition at streamingProcessNew.scala:50, took 0.023377 s
2017-08-18 12:15:30,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029730000 ms.0 from job set of time 1503029730000 ms
2017-08-18 12:15:30,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1503029730000 ms (execution: 0.065 s)
2017-08-18 12:15:30,089 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 609 from persistence list
2017-08-18 12:15:30,089 [block-manager-slave-async-thread-pool-28] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 609
2017-08-18 12:15:30,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 608 from persistence list
2017-08-18 12:15:30,090 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 608
2017-08-18 12:15:30,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:15:30,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029720000 ms
2017-08-18 12:15:35,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029735000 ms
2017-08-18 12:15:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029735000 ms.0 from job set of time 1503029735000 ms
2017-08-18 12:15:35,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:15:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 306 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:15:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 306 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:15:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:15:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:15:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 306 (MapPartitionsRDD[613] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:15:35,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_306 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:15:35,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_306_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:15:35,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_306_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:15:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 306 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:15:35,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 306 (MapPartitionsRDD[613] at map at streamingProcessNew.scala:49)
2017-08-18 12:15:35,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 306.0 with 2 tasks
2017-08-18 12:15:35,073 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 306.0 (TID 612, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:15:35,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 306.0 (TID 613, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:15:35,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 306.0 (TID 613)
2017-08-18 12:15:35,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 306.0 (TID 612)
2017-08-18 12:15:35,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:15:35,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:15:35,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 306.0 (TID 613). 801 bytes result sent to driver
2017-08-18 12:15:35,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 306.0 (TID 612). 714 bytes result sent to driver
2017-08-18 12:15:35,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 306.0 (TID 613) in 11 ms on localhost (1/2)
2017-08-18 12:15:35,085 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 306.0 (TID 612) in 14 ms on localhost (2/2)
2017-08-18 12:15:35,085 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 306.0, whose tasks have all completed, from pool 
2017-08-18 12:15:35,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 306 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:15:35,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 306 finished: foreachPartition at streamingProcessNew.scala:50, took 0.028077 s
2017-08-18 12:15:35,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029735000 ms.0 from job set of time 1503029735000 ms
2017-08-18 12:15:35,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.086 s for time 1503029735000 ms (execution: 0.065 s)
2017-08-18 12:15:35,087 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 611 from persistence list
2017-08-18 12:15:35,087 [block-manager-slave-async-thread-pool-28] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 611
2017-08-18 12:15:35,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 610 from persistence list
2017-08-18 12:15:35,088 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 610
2017-08-18 12:15:35,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:15:35,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029725000 ms
2017-08-18 12:15:40,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029740000 ms
2017-08-18 12:15:40,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029740000 ms.0 from job set of time 1503029740000 ms
2017-08-18 12:15:40,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:15:40,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 307 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:15:40,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 307 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:15:40,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:15:40,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:15:40,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 307 (MapPartitionsRDD[615] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:15:40,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_307 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:15:40,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_307_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:15:40,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_307_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:15:40,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 307 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:15:40,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 307 (MapPartitionsRDD[615] at map at streamingProcessNew.scala:49)
2017-08-18 12:15:40,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 307.0 with 2 tasks
2017-08-18 12:15:40,034 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 307.0 (TID 614, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:15:40,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 307.0 (TID 615, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:15:40,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 307.0 (TID 614)
2017-08-18 12:15:40,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 307.0 (TID 615)
2017-08-18 12:15:40,037 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:15:40,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:15:40,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 307.0 (TID 615). 714 bytes result sent to driver
2017-08-18 12:15:40,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 307.0 (TID 614). 714 bytes result sent to driver
2017-08-18 12:15:40,043 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 307.0 (TID 615) in 8 ms on localhost (1/2)
2017-08-18 12:15:40,044 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 307.0 (TID 614) in 11 ms on localhost (2/2)
2017-08-18 12:15:40,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 307 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:15:40,044 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 307.0, whose tasks have all completed, from pool 
2017-08-18 12:15:40,044 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 307 finished: foreachPartition at streamingProcessNew.scala:50, took 0.019121 s
2017-08-18 12:15:40,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029740000 ms.0 from job set of time 1503029740000 ms
2017-08-18 12:15:40,045 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 613 from persistence list
2017-08-18 12:15:40,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.044 s for time 1503029740000 ms (execution: 0.031 s)
2017-08-18 12:15:40,045 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 612 from persistence list
2017-08-18 12:15:40,045 [block-manager-slave-async-thread-pool-28] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 613
2017-08-18 12:15:40,045 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 612
2017-08-18 12:15:40,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:15:40,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029730000 ms
2017-08-18 12:15:45,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029745000 ms
2017-08-18 12:15:45,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029745000 ms.0 from job set of time 1503029745000 ms
2017-08-18 12:15:45,056 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:15:45,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 308 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:15:45,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 308 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:15:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:15:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:15:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 308 (MapPartitionsRDD[617] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:15:45,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_308 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:15:45,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_308_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:15:45,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_308_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:15:45,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 308 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:15:45,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 308 (MapPartitionsRDD[617] at map at streamingProcessNew.scala:49)
2017-08-18 12:15:45,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 308.0 with 2 tasks
2017-08-18 12:15:45,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 308.0 (TID 616, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:15:45,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 308.0 (TID 617, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:15:45,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 308.0 (TID 616)
2017-08-18 12:15:45,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 308.0 (TID 617)
2017-08-18 12:15:45,081 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:15:45,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:15:45,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 308.0 (TID 616). 714 bytes result sent to driver
2017-08-18 12:15:45,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 308.0 (TID 617). 714 bytes result sent to driver
2017-08-18 12:15:45,086 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 308.0 (TID 616) in 12 ms on localhost (1/2)
2017-08-18 12:15:45,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 308.0 (TID 617) in 10 ms on localhost (2/2)
2017-08-18 12:15:45,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 308.0, whose tasks have all completed, from pool 
2017-08-18 12:15:45,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 308 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:15:45,087 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 308 finished: foreachPartition at streamingProcessNew.scala:50, took 0.030928 s
2017-08-18 12:15:45,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029745000 ms.0 from job set of time 1503029745000 ms
2017-08-18 12:15:45,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1503029745000 ms (execution: 0.067 s)
2017-08-18 12:15:45,088 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 615 from persistence list
2017-08-18 12:15:45,088 [block-manager-slave-async-thread-pool-28] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 615
2017-08-18 12:15:45,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 614 from persistence list
2017-08-18 12:15:45,089 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 614
2017-08-18 12:15:45,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:15:45,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029735000 ms
2017-08-18 12:15:50,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029750000 ms
2017-08-18 12:15:50,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029750000 ms.0 from job set of time 1503029750000 ms
2017-08-18 12:15:50,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:15:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 309 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:15:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 309 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:15:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:15:50,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:15:50,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 309 (MapPartitionsRDD[619] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:15:50,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_309 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:15:50,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_309_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:15:50,085 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_294_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:15:50,085 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_309_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:15:50,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 309 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:15:50,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 309 (MapPartitionsRDD[619] at map at streamingProcessNew.scala:49)
2017-08-18 12:15:50,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 309.0 with 2 tasks
2017-08-18 12:15:50,087 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_295_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:15:50,089 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 309.0 (TID 618, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:15:50,090 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 309.0 (TID 619, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:15:50,091 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 309.0 (TID 618)
2017-08-18 12:15:50,091 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 309.0 (TID 619)
2017-08-18 12:15:50,091 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_296_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:15:50,094 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_297_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:15:50,095 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:15:50,096 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:15:50,097 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_298_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:15:50,101 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_299_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:15:50,101 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 309.0 (TID 618). 714 bytes result sent to driver
2017-08-18 12:15:50,101 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 309.0 (TID 619). 714 bytes result sent to driver
2017-08-18 12:15:50,104 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_300_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:15:50,104 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 309.0 (TID 618) in 17 ms on localhost (1/2)
2017-08-18 12:15:50,105 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 309.0 (TID 619) in 15 ms on localhost (2/2)
2017-08-18 12:15:50,105 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 309.0, whose tasks have all completed, from pool 
2017-08-18 12:15:50,106 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 309 (foreachPartition at streamingProcessNew.scala:50) finished in 0.019 s
2017-08-18 12:15:50,106 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_301_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:15:50,106 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 309 finished: foreachPartition at streamingProcessNew.scala:50, took 0.048559 s
2017-08-18 12:15:50,108 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029750000 ms.0 from job set of time 1503029750000 ms
2017-08-18 12:15:50,108 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 617 from persistence list
2017-08-18 12:15:50,108 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.108 s for time 1503029750000 ms (execution: 0.086 s)
2017-08-18 12:15:50,109 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 617
2017-08-18 12:15:50,109 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 616 from persistence list
2017-08-18 12:15:50,109 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_302_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:15:50,110 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:15:50,111 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029740000 ms
2017-08-18 12:15:50,111 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 616
2017-08-18 12:15:50,113 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_303_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:15:50,114 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_304_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:15:50,117 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_305_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:15:50,119 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_306_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:15:50,120 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_307_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:15:50,121 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_308_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:15:55,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029755000 ms
2017-08-18 12:15:55,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029755000 ms.0 from job set of time 1503029755000 ms
2017-08-18 12:15:55,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:15:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 310 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:15:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 310 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:15:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:15:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:15:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 310 (MapPartitionsRDD[621] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:15:55,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_310 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:15:55,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_310_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:15:55,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_310_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:15:55,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 310 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:15:55,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 310 (MapPartitionsRDD[621] at map at streamingProcessNew.scala:49)
2017-08-18 12:15:55,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 310.0 with 2 tasks
2017-08-18 12:15:55,088 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 310.0 (TID 620, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:15:55,089 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 310.0 (TID 621, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:15:55,090 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 310.0 (TID 621)
2017-08-18 12:15:55,090 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 310.0 (TID 620)
2017-08-18 12:15:55,096 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:15:55,096 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:15:55,099 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 310.0 (TID 620). 714 bytes result sent to driver
2017-08-18 12:15:55,099 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 310.0 (TID 621). 714 bytes result sent to driver
2017-08-18 12:15:55,101 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 310.0 (TID 620) in 15 ms on localhost (1/2)
2017-08-18 12:15:55,101 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 310.0 (TID 621) in 12 ms on localhost (2/2)
2017-08-18 12:15:55,102 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 310.0, whose tasks have all completed, from pool 
2017-08-18 12:15:55,102 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 310 (foreachPartition at streamingProcessNew.scala:50) finished in 0.017 s
2017-08-18 12:15:55,102 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 310 finished: foreachPartition at streamingProcessNew.scala:50, took 0.040446 s
2017-08-18 12:15:55,102 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029755000 ms.0 from job set of time 1503029755000 ms
2017-08-18 12:15:55,103 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.102 s for time 1503029755000 ms (execution: 0.079 s)
2017-08-18 12:15:55,103 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 619 from persistence list
2017-08-18 12:15:55,103 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 619
2017-08-18 12:15:55,103 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 618 from persistence list
2017-08-18 12:15:55,103 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 618
2017-08-18 12:15:55,103 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:15:55,104 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029745000 ms
2017-08-18 12:16:00,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029760000 ms
2017-08-18 12:16:00,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029760000 ms.0 from job set of time 1503029760000 ms
2017-08-18 12:16:00,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:16:00,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 311 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:16:00,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 311 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:16:00,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:16:00,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:16:00,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 311 (MapPartitionsRDD[623] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:16:00,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_311 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:16:00,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_311_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:16:00,035 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_311_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:16:00,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 311 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:16:00,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 311 (MapPartitionsRDD[623] at map at streamingProcessNew.scala:49)
2017-08-18 12:16:00,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 311.0 with 2 tasks
2017-08-18 12:16:00,039 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 311.0 (TID 622, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:16:00,039 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 311.0 (TID 623, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:16:00,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 311.0 (TID 623)
2017-08-18 12:16:00,040 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 311.0 (TID 622)
2017-08-18 12:16:00,042 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:16:00,042 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:16:00,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 311.0 (TID 623). 714 bytes result sent to driver
2017-08-18 12:16:00,045 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 311.0 (TID 622). 714 bytes result sent to driver
2017-08-18 12:16:00,047 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 311.0 (TID 622) in 11 ms on localhost (1/2)
2017-08-18 12:16:00,047 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 311.0 (TID 623) in 8 ms on localhost (2/2)
2017-08-18 12:16:00,047 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 311.0, whose tasks have all completed, from pool 
2017-08-18 12:16:00,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 311 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:16:00,048 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 311 finished: foreachPartition at streamingProcessNew.scala:50, took 0.020978 s
2017-08-18 12:16:00,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029760000 ms.0 from job set of time 1503029760000 ms
2017-08-18 12:16:00,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1503029760000 ms (execution: 0.032 s)
2017-08-18 12:16:00,048 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 621 from persistence list
2017-08-18 12:16:00,049 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 621
2017-08-18 12:16:00,049 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 620 from persistence list
2017-08-18 12:16:00,049 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 620
2017-08-18 12:16:00,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:16:00,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029750000 ms
2017-08-18 12:16:05,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029765000 ms
2017-08-18 12:16:05,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029765000 ms.0 from job set of time 1503029765000 ms
2017-08-18 12:16:05,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:16:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 312 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:16:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 312 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:16:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:16:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:16:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 312 (MapPartitionsRDD[625] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:16:05,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_312 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:16:05,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_312_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:16:05,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_312_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:16:05,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 312 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:16:05,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 312 (MapPartitionsRDD[625] at map at streamingProcessNew.scala:49)
2017-08-18 12:16:05,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 312.0 with 2 tasks
2017-08-18 12:16:05,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 312.0 (TID 624, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:16:05,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 312.0 (TID 625, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:16:05,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 312.0 (TID 625)
2017-08-18 12:16:05,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 312.0 (TID 624)
2017-08-18 12:16:05,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:16:05,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:16:05,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 312.0 (TID 624). 714 bytes result sent to driver
2017-08-18 12:16:05,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 312.0 (TID 625). 714 bytes result sent to driver
2017-08-18 12:16:05,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 312.0 (TID 624) in 11 ms on localhost (1/2)
2017-08-18 12:16:05,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 312.0 (TID 625) in 9 ms on localhost (2/2)
2017-08-18 12:16:05,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 312.0, whose tasks have all completed, from pool 
2017-08-18 12:16:05,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 312 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:16:05,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 312 finished: foreachPartition at streamingProcessNew.scala:50, took 0.023147 s
2017-08-18 12:16:05,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029765000 ms.0 from job set of time 1503029765000 ms
2017-08-18 12:16:05,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1503029765000 ms (execution: 0.060 s)
2017-08-18 12:16:05,082 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 623 from persistence list
2017-08-18 12:16:05,082 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 623
2017-08-18 12:16:05,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 622 from persistence list
2017-08-18 12:16:05,083 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 622
2017-08-18 12:16:05,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:16:05,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029755000 ms
2017-08-18 12:16:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029770000 ms
2017-08-18 12:16:10,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029770000 ms.0 from job set of time 1503029770000 ms
2017-08-18 12:16:10,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:16:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 313 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:16:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 313 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:16:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:16:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:16:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 313 (MapPartitionsRDD[627] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:16:10,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_313 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:16:10,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_313_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:16:10,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_313_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:16:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 313 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:16:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 313 (MapPartitionsRDD[627] at map at streamingProcessNew.scala:49)
2017-08-18 12:16:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 313.0 with 2 tasks
2017-08-18 12:16:10,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 313.0 (TID 626, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:16:10,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 313.0 (TID 627, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:16:10,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 313.0 (TID 627)
2017-08-18 12:16:10,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 313.0 (TID 626)
2017-08-18 12:16:10,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:16:10,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:16:10,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 313.0 (TID 627). 801 bytes result sent to driver
2017-08-18 12:16:10,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 313.0 (TID 626). 801 bytes result sent to driver
2017-08-18 12:16:10,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 313.0 (TID 626) in 12 ms on localhost (1/2)
2017-08-18 12:16:10,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 313.0 (TID 627) in 9 ms on localhost (2/2)
2017-08-18 12:16:10,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 313.0, whose tasks have all completed, from pool 
2017-08-18 12:16:10,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 313 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:16:10,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 313 finished: foreachPartition at streamingProcessNew.scala:50, took 0.028089 s
2017-08-18 12:16:10,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029770000 ms.0 from job set of time 1503029770000 ms
2017-08-18 12:16:10,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1503029770000 ms (execution: 0.057 s)
2017-08-18 12:16:10,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 625 from persistence list
2017-08-18 12:16:10,080 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 625
2017-08-18 12:16:10,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 624 from persistence list
2017-08-18 12:16:10,080 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 624
2017-08-18 12:16:10,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:16:10,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029760000 ms
2017-08-18 12:16:15,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029775000 ms
2017-08-18 12:16:15,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029775000 ms.0 from job set of time 1503029775000 ms
2017-08-18 12:16:15,056 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:16:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 314 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:16:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 314 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:16:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:16:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:16:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 314 (MapPartitionsRDD[629] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:16:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_314 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:16:15,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_314_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:16:15,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_314_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:16:15,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 314 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:16:15,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 314 (MapPartitionsRDD[629] at map at streamingProcessNew.scala:49)
2017-08-18 12:16:15,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 314.0 with 2 tasks
2017-08-18 12:16:15,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 314.0 (TID 628, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:16:15,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 314.0 (TID 629, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:16:15,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 314.0 (TID 629)
2017-08-18 12:16:15,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 314.0 (TID 628)
2017-08-18 12:16:15,081 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:16:15,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:16:15,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 314.0 (TID 629). 714 bytes result sent to driver
2017-08-18 12:16:15,087 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 314.0 (TID 628). 714 bytes result sent to driver
2017-08-18 12:16:15,090 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 314.0 (TID 628) in 16 ms on localhost (1/2)
2017-08-18 12:16:15,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 314.0 (TID 629) in 13 ms on localhost (2/2)
2017-08-18 12:16:15,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 314.0, whose tasks have all completed, from pool 
2017-08-18 12:16:15,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 314 (foreachPartition at streamingProcessNew.scala:50) finished in 0.017 s
2017-08-18 12:16:15,091 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 314 finished: foreachPartition at streamingProcessNew.scala:50, took 0.035141 s
2017-08-18 12:16:15,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029775000 ms.0 from job set of time 1503029775000 ms
2017-08-18 12:16:15,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1503029775000 ms (execution: 0.070 s)
2017-08-18 12:16:15,092 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 627 from persistence list
2017-08-18 12:16:15,093 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 627
2017-08-18 12:16:15,093 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 626 from persistence list
2017-08-18 12:16:15,094 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 626
2017-08-18 12:16:15,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:16:15,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029765000 ms
2017-08-18 12:16:20,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029780000 ms
2017-08-18 12:16:20,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029780000 ms.0 from job set of time 1503029780000 ms
2017-08-18 12:16:20,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:16:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 315 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:16:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 315 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:16:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:16:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:16:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 315 (MapPartitionsRDD[631] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:16:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_315 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:16:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_315_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:16:20,036 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_315_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:16:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 315 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:16:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 315 (MapPartitionsRDD[631] at map at streamingProcessNew.scala:49)
2017-08-18 12:16:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 315.0 with 2 tasks
2017-08-18 12:16:20,039 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 315.0 (TID 630, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:16:20,040 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 315.0 (TID 631, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:16:20,040 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 315.0 (TID 630)
2017-08-18 12:16:20,040 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 315.0 (TID 631)
2017-08-18 12:16:20,042 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:16:20,042 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:16:20,045 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 315.0 (TID 631). 714 bytes result sent to driver
2017-08-18 12:16:20,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 315.0 (TID 630). 714 bytes result sent to driver
2017-08-18 12:16:20,047 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 315.0 (TID 631) in 8 ms on localhost (1/2)
2017-08-18 12:16:20,047 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 315.0 (TID 630) in 10 ms on localhost (2/2)
2017-08-18 12:16:20,047 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 315.0, whose tasks have all completed, from pool 
2017-08-18 12:16:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 315 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:16:20,047 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 315 finished: foreachPartition at streamingProcessNew.scala:50, took 0.021181 s
2017-08-18 12:16:20,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029780000 ms.0 from job set of time 1503029780000 ms
2017-08-18 12:16:20,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1503029780000 ms (execution: 0.033 s)
2017-08-18 12:16:20,048 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 629 from persistence list
2017-08-18 12:16:20,048 [block-manager-slave-async-thread-pool-27] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 629
2017-08-18 12:16:20,048 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 628 from persistence list
2017-08-18 12:16:20,049 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 628
2017-08-18 12:16:20,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:16:20,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029770000 ms
2017-08-18 12:16:25,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029785000 ms
2017-08-18 12:16:25,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029785000 ms.0 from job set of time 1503029785000 ms
2017-08-18 12:16:25,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:16:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 316 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:16:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 316 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:16:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:16:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:16:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 316 (MapPartitionsRDD[633] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:16:25,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_316 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:16:25,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_316_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:16:25,080 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_316_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:16:25,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 316 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:16:25,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 316 (MapPartitionsRDD[633] at map at streamingProcessNew.scala:49)
2017-08-18 12:16:25,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 316.0 with 2 tasks
2017-08-18 12:16:25,087 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 316.0 (TID 632, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:16:25,088 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 316.0 (TID 633, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:16:25,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 316.0 (TID 632)
2017-08-18 12:16:25,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 316.0 (TID 633)
2017-08-18 12:16:25,094 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:16:25,094 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:16:25,099 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 316.0 (TID 632). 714 bytes result sent to driver
2017-08-18 12:16:25,099 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 316.0 (TID 633). 714 bytes result sent to driver
2017-08-18 12:16:25,102 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 316.0 (TID 632) in 19 ms on localhost (1/2)
2017-08-18 12:16:25,102 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 316.0 (TID 633) in 15 ms on localhost (2/2)
2017-08-18 12:16:25,102 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 316.0, whose tasks have all completed, from pool 
2017-08-18 12:16:25,102 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 316 (foreachPartition at streamingProcessNew.scala:50) finished in 0.020 s
2017-08-18 12:16:25,102 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 316 finished: foreachPartition at streamingProcessNew.scala:50, took 0.043706 s
2017-08-18 12:16:25,103 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029785000 ms.0 from job set of time 1503029785000 ms
2017-08-18 12:16:25,103 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.103 s for time 1503029785000 ms (execution: 0.082 s)
2017-08-18 12:16:25,103 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 631 from persistence list
2017-08-18 12:16:25,104 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 631
2017-08-18 12:16:25,104 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 630 from persistence list
2017-08-18 12:16:25,104 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 630
2017-08-18 12:16:25,104 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:16:25,104 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029775000 ms
2017-08-18 12:16:30,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029790000 ms
2017-08-18 12:16:30,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029790000 ms.0 from job set of time 1503029790000 ms
2017-08-18 12:16:30,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:16:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 317 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:16:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 317 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:16:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:16:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:16:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 317 (MapPartitionsRDD[635] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:16:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_317 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:16:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_317_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:16:30,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_317_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:16:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 317 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:16:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 317 (MapPartitionsRDD[635] at map at streamingProcessNew.scala:49)
2017-08-18 12:16:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 317.0 with 2 tasks
2017-08-18 12:16:30,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 317.0 (TID 634, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:16:30,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 317.0 (TID 635, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:16:30,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 317.0 (TID 635)
2017-08-18 12:16:30,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 317.0 (TID 634)
2017-08-18 12:16:30,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:16:30,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:16:30,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 317.0 (TID 634). 714 bytes result sent to driver
2017-08-18 12:16:30,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 317.0 (TID 635). 714 bytes result sent to driver
2017-08-18 12:16:30,073 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 317.0 (TID 634) in 12 ms on localhost (1/2)
2017-08-18 12:16:30,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 317.0 (TID 635) in 9 ms on localhost (2/2)
2017-08-18 12:16:30,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 317.0, whose tasks have all completed, from pool 
2017-08-18 12:16:30,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 317 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:16:30,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 317 finished: foreachPartition at streamingProcessNew.scala:50, took 0.023028 s
2017-08-18 12:16:30,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029790000 ms.0 from job set of time 1503029790000 ms
2017-08-18 12:16:30,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1503029790000 ms (execution: 0.050 s)
2017-08-18 12:16:30,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 633 from persistence list
2017-08-18 12:16:30,075 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 633
2017-08-18 12:16:30,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 632 from persistence list
2017-08-18 12:16:30,075 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 632
2017-08-18 12:16:30,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:16:30,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029780000 ms
2017-08-18 12:16:35,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029795000 ms
2017-08-18 12:16:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029795000 ms.0 from job set of time 1503029795000 ms
2017-08-18 12:16:35,056 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:16:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 318 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:16:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 318 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:16:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:16:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:16:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 318 (MapPartitionsRDD[637] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:16:35,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_318 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:16:35,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_318_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:16:35,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_318_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:16:35,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 318 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:16:35,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 318 (MapPartitionsRDD[637] at map at streamingProcessNew.scala:49)
2017-08-18 12:16:35,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 318.0 with 2 tasks
2017-08-18 12:16:35,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 318.0 (TID 636, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:16:35,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 318.0 (TID 637, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:16:35,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 318.0 (TID 637)
2017-08-18 12:16:35,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 318.0 (TID 636)
2017-08-18 12:16:35,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:16:35,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:16:35,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 318.0 (TID 637). 714 bytes result sent to driver
2017-08-18 12:16:35,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 318.0 (TID 636). 714 bytes result sent to driver
2017-08-18 12:16:35,083 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 318.0 (TID 636) in 14 ms on localhost (1/2)
2017-08-18 12:16:35,083 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 318.0 (TID 637) in 10 ms on localhost (2/2)
2017-08-18 12:16:35,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 318 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:16:35,083 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 318.0, whose tasks have all completed, from pool 
2017-08-18 12:16:35,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 318 finished: foreachPartition at streamingProcessNew.scala:50, took 0.026745 s
2017-08-18 12:16:35,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029795000 ms.0 from job set of time 1503029795000 ms
2017-08-18 12:16:35,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1503029795000 ms (execution: 0.063 s)
2017-08-18 12:16:35,084 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 635 from persistence list
2017-08-18 12:16:35,085 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 635
2017-08-18 12:16:35,085 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 634 from persistence list
2017-08-18 12:16:35,085 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 634
2017-08-18 12:16:35,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:16:35,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029785000 ms
2017-08-18 12:16:40,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029800000 ms
2017-08-18 12:16:40,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029800000 ms.0 from job set of time 1503029800000 ms
2017-08-18 12:16:40,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:16:40,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 319 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:16:40,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 319 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:16:40,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:16:40,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:16:40,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 319 (MapPartitionsRDD[639] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:16:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_319 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:16:40,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_319_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:16:40,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_319_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:16:40,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 319 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:16:40,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 319 (MapPartitionsRDD[639] at map at streamingProcessNew.scala:49)
2017-08-18 12:16:40,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 319.0 with 2 tasks
2017-08-18 12:16:40,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 319.0 (TID 638, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:16:40,039 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 319.0 (TID 639, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:16:40,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 319.0 (TID 639)
2017-08-18 12:16:40,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 319.0 (TID 638)
2017-08-18 12:16:40,041 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:16:40,041 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:16:40,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 319.0 (TID 638). 714 bytes result sent to driver
2017-08-18 12:16:40,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 319.0 (TID 639). 714 bytes result sent to driver
2017-08-18 12:16:40,045 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 319.0 (TID 638) in 9 ms on localhost (1/2)
2017-08-18 12:16:40,046 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 319.0 (TID 639) in 7 ms on localhost (2/2)
2017-08-18 12:16:40,046 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 319.0, whose tasks have all completed, from pool 
2017-08-18 12:16:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 319 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:16:40,046 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 319 finished: foreachPartition at streamingProcessNew.scala:50, took 0.019602 s
2017-08-18 12:16:40,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029800000 ms.0 from job set of time 1503029800000 ms
2017-08-18 12:16:40,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.046 s for time 1503029800000 ms (execution: 0.030 s)
2017-08-18 12:16:40,047 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 637 from persistence list
2017-08-18 12:16:40,047 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 637
2017-08-18 12:16:40,047 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 636 from persistence list
2017-08-18 12:16:40,047 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 636
2017-08-18 12:16:40,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:16:40,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029790000 ms
2017-08-18 12:16:45,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029805000 ms
2017-08-18 12:16:45,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029805000 ms.0 from job set of time 1503029805000 ms
2017-08-18 12:16:45,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:16:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 320 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:16:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 320 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:16:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:16:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:16:45,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 320 (MapPartitionsRDD[641] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:16:45,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_320 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:16:45,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_320_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:16:45,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_320_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:16:45,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 320 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:16:45,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 320 (MapPartitionsRDD[641] at map at streamingProcessNew.scala:49)
2017-08-18 12:16:45,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 320.0 with 2 tasks
2017-08-18 12:16:45,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 320.0 (TID 640, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:16:45,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 320.0 (TID 641, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:16:45,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 320.0 (TID 641)
2017-08-18 12:16:45,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 320.0 (TID 640)
2017-08-18 12:16:45,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:16:45,080 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:16:45,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 320.0 (TID 640). 714 bytes result sent to driver
2017-08-18 12:16:45,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 320.0 (TID 641). 714 bytes result sent to driver
2017-08-18 12:16:45,086 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 320.0 (TID 640) in 13 ms on localhost (1/2)
2017-08-18 12:16:45,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 320.0 (TID 641) in 10 ms on localhost (2/2)
2017-08-18 12:16:45,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 320.0, whose tasks have all completed, from pool 
2017-08-18 12:16:45,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 320 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:16:45,087 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 320 finished: foreachPartition at streamingProcessNew.scala:50, took 0.032256 s
2017-08-18 12:16:45,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029805000 ms.0 from job set of time 1503029805000 ms
2017-08-18 12:16:45,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1503029805000 ms (execution: 0.067 s)
2017-08-18 12:16:45,088 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 639 from persistence list
2017-08-18 12:16:45,088 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 639
2017-08-18 12:16:45,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 638 from persistence list
2017-08-18 12:16:45,089 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 638
2017-08-18 12:16:45,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:16:45,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029795000 ms
2017-08-18 12:16:50,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029810000 ms
2017-08-18 12:16:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029810000 ms.0 from job set of time 1503029810000 ms
2017-08-18 12:16:50,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:16:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 321 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:16:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 321 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:16:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:16:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:16:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 321 (MapPartitionsRDD[643] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:16:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_321 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:16:50,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_321_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:16:50,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_321_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:16:50,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 321 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:16:50,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 321 (MapPartitionsRDD[643] at map at streamingProcessNew.scala:49)
2017-08-18 12:16:50,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 321.0 with 2 tasks
2017-08-18 12:16:50,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 321.0 (TID 642, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:16:50,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 321.0 (TID 643, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:16:50,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 321.0 (TID 643)
2017-08-18 12:16:50,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 321.0 (TID 642)
2017-08-18 12:16:50,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:16:50,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:16:50,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 321.0 (TID 643). 714 bytes result sent to driver
2017-08-18 12:16:50,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 321.0 (TID 642). 714 bytes result sent to driver
2017-08-18 12:16:50,079 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 321.0 (TID 642) in 11 ms on localhost (1/2)
2017-08-18 12:16:50,079 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 321.0 (TID 643) in 10 ms on localhost (2/2)
2017-08-18 12:16:50,079 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 321.0, whose tasks have all completed, from pool 
2017-08-18 12:16:50,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 321 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:16:50,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 321 finished: foreachPartition at streamingProcessNew.scala:50, took 0.022352 s
2017-08-18 12:16:50,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029810000 ms.0 from job set of time 1503029810000 ms
2017-08-18 12:16:50,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1503029810000 ms (execution: 0.059 s)
2017-08-18 12:16:50,080 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 641 from persistence list
2017-08-18 12:16:50,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 640 from persistence list
2017-08-18 12:16:50,080 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 641
2017-08-18 12:16:50,081 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 640
2017-08-18 12:16:50,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:16:50,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029800000 ms
2017-08-18 12:16:55,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029815000 ms
2017-08-18 12:16:55,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029815000 ms.0 from job set of time 1503029815000 ms
2017-08-18 12:16:55,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:16:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 322 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:16:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 322 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:16:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:16:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:16:55,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 322 (MapPartitionsRDD[645] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:16:55,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_322 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:16:55,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_322_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:16:55,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_322_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:16:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 322 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:16:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 322 (MapPartitionsRDD[645] at map at streamingProcessNew.scala:49)
2017-08-18 12:16:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 322.0 with 2 tasks
2017-08-18 12:16:55,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 322.0 (TID 644, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:16:55,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 322.0 (TID 645, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:16:55,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 322.0 (TID 645)
2017-08-18 12:16:55,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 322.0 (TID 644)
2017-08-18 12:16:55,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:16:55,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:16:55,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 322.0 (TID 644). 714 bytes result sent to driver
2017-08-18 12:16:55,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 322.0 (TID 645). 714 bytes result sent to driver
2017-08-18 12:16:55,089 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 322.0 (TID 644) in 14 ms on localhost (1/2)
2017-08-18 12:16:55,089 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 322.0 (TID 645) in 11 ms on localhost (2/2)
2017-08-18 12:16:55,089 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 322.0, whose tasks have all completed, from pool 
2017-08-18 12:16:55,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 322 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:16:55,090 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 322 finished: foreachPartition at streamingProcessNew.scala:50, took 0.031267 s
2017-08-18 12:16:55,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029815000 ms.0 from job set of time 1503029815000 ms
2017-08-18 12:16:55,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1503029815000 ms (execution: 0.070 s)
2017-08-18 12:16:55,091 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 643 from persistence list
2017-08-18 12:16:55,092 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 643
2017-08-18 12:16:55,092 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 642 from persistence list
2017-08-18 12:16:55,092 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 642
2017-08-18 12:16:55,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:16:55,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029805000 ms
2017-08-18 12:17:00,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029820000 ms
2017-08-18 12:17:00,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029820000 ms.0 from job set of time 1503029820000 ms
2017-08-18 12:17:00,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:17:00,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 323 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:17:00,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 323 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:17:00,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:17:00,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:17:00,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 323 (MapPartitionsRDD[647] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:17:00,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_323 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:17:00,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_323_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:17:00,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_323_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:17:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 323 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:17:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 323 (MapPartitionsRDD[647] at map at streamingProcessNew.scala:49)
2017-08-18 12:17:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 323.0 with 2 tasks
2017-08-18 12:17:00,034 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 323.0 (TID 646, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:17:00,034 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 323.0 (TID 647, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:17:00,034 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 323.0 (TID 647)
2017-08-18 12:17:00,034 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 323.0 (TID 646)
2017-08-18 12:17:00,036 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:17:00,036 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:17:00,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 323.0 (TID 647). 714 bytes result sent to driver
2017-08-18 12:17:00,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 323.0 (TID 646). 714 bytes result sent to driver
2017-08-18 12:17:00,041 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 323.0 (TID 646) in 9 ms on localhost (1/2)
2017-08-18 12:17:00,041 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 323.0 (TID 647) in 7 ms on localhost (2/2)
2017-08-18 12:17:00,041 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 323.0, whose tasks have all completed, from pool 
2017-08-18 12:17:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 323 (foreachPartition at streamingProcessNew.scala:50) finished in 0.009 s
2017-08-18 12:17:00,042 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 323 finished: foreachPartition at streamingProcessNew.scala:50, took 0.018230 s
2017-08-18 12:17:00,042 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029820000 ms.0 from job set of time 1503029820000 ms
2017-08-18 12:17:00,042 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.042 s for time 1503029820000 ms (execution: 0.029 s)
2017-08-18 12:17:00,042 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 645 from persistence list
2017-08-18 12:17:00,043 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 645
2017-08-18 12:17:00,043 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 644 from persistence list
2017-08-18 12:17:00,043 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 644
2017-08-18 12:17:00,043 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:17:00,043 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029810000 ms
2017-08-18 12:17:05,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029825000 ms
2017-08-18 12:17:05,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029825000 ms.0 from job set of time 1503029825000 ms
2017-08-18 12:17:05,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:17:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 324 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:17:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 324 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:17:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:17:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:17:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 324 (MapPartitionsRDD[649] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:17:05,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_324 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:17:05,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_324_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-18 12:17:05,086 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_324_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:17:05,086 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_309_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:17:05,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 324 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:17:05,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 324 (MapPartitionsRDD[649] at map at streamingProcessNew.scala:49)
2017-08-18 12:17:05,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 324.0 with 2 tasks
2017-08-18 12:17:05,089 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_310_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:17:05,090 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 324.0 (TID 648, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:17:05,092 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 324.0 (TID 649, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:17:05,092 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 324.0 (TID 649)
2017-08-18 12:17:05,092 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 324.0 (TID 648)
2017-08-18 12:17:05,095 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:17:05,095 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:17:05,096 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_311_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:17:05,100 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_312_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:17:05,100 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 324.0 (TID 649). 714 bytes result sent to driver
2017-08-18 12:17:05,100 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 324.0 (TID 648). 714 bytes result sent to driver
2017-08-18 12:17:05,102 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_313_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:17:05,104 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 324.0 (TID 649) in 14 ms on localhost (1/2)
2017-08-18 12:17:05,105 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 324.0 (TID 648) in 16 ms on localhost (2/2)
2017-08-18 12:17:05,105 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 324.0, whose tasks have all completed, from pool 
2017-08-18 12:17:05,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 324 (foreachPartition at streamingProcessNew.scala:50) finished in 0.018 s
2017-08-18 12:17:05,106 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_314_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:17:05,106 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 324 finished: foreachPartition at streamingProcessNew.scala:50, took 0.047739 s
2017-08-18 12:17:05,107 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029825000 ms.0 from job set of time 1503029825000 ms
2017-08-18 12:17:05,108 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 647 from persistence list
2017-08-18 12:17:05,108 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.107 s for time 1503029825000 ms (execution: 0.086 s)
2017-08-18 12:17:05,108 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 647
2017-08-18 12:17:05,108 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 646 from persistence list
2017-08-18 12:17:05,110 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:17:05,111 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029815000 ms
2017-08-18 12:17:05,111 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_315_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:17:05,111 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 646
2017-08-18 12:17:05,113 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_316_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:17:05,117 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_317_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:17:05,119 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_318_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:17:05,121 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_319_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:17:05,125 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_320_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:17:05,126 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_321_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:17:05,127 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_322_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:17:05,128 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_323_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:17:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029830000 ms
2017-08-18 12:17:10,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029830000 ms.0 from job set of time 1503029830000 ms
2017-08-18 12:17:10,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:17:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 325 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:17:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 325 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:17:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:17:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:17:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 325 (MapPartitionsRDD[651] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:17:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_325 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:17:10,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_325_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:17:10,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_325_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:17:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 325 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:17:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 325 (MapPartitionsRDD[651] at map at streamingProcessNew.scala:49)
2017-08-18 12:17:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 325.0 with 2 tasks
2017-08-18 12:17:10,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 325.0 (TID 650, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:17:10,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 325.0 (TID 651, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:17:10,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 325.0 (TID 651)
2017-08-18 12:17:10,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 325.0 (TID 650)
2017-08-18 12:17:10,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:17:10,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:17:10,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 325.0 (TID 650). 714 bytes result sent to driver
2017-08-18 12:17:10,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 325.0 (TID 651). 714 bytes result sent to driver
2017-08-18 12:17:10,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 325.0 (TID 650) in 11 ms on localhost (1/2)
2017-08-18 12:17:10,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 325.0 (TID 651) in 8 ms on localhost (2/2)
2017-08-18 12:17:10,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 325.0, whose tasks have all completed, from pool 
2017-08-18 12:17:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 325 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:17:10,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 325 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025198 s
2017-08-18 12:17:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029830000 ms.0 from job set of time 1503029830000 ms
2017-08-18 12:17:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1503029830000 ms (execution: 0.054 s)
2017-08-18 12:17:10,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 649 from persistence list
2017-08-18 12:17:10,075 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 649
2017-08-18 12:17:10,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 648 from persistence list
2017-08-18 12:17:10,076 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 648
2017-08-18 12:17:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:17:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029820000 ms
2017-08-18 12:17:15,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029835000 ms
2017-08-18 12:17:15,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029835000 ms.0 from job set of time 1503029835000 ms
2017-08-18 12:17:15,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:17:15,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 326 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:17:15,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 326 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:17:15,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:17:15,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:17:15,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 326 (MapPartitionsRDD[653] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:17:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_326 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:17:15,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_326_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:17:15,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_326_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:17:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 326 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:17:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 326 (MapPartitionsRDD[653] at map at streamingProcessNew.scala:49)
2017-08-18 12:17:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 326.0 with 2 tasks
2017-08-18 12:17:15,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 326.0 (TID 652, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:17:15,064 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 326.0 (TID 653, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:17:15,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 326.0 (TID 653)
2017-08-18 12:17:15,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 326.0 (TID 652)
2017-08-18 12:17:15,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:17:15,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:17:15,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 326.0 (TID 653). 714 bytes result sent to driver
2017-08-18 12:17:15,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 326.0 (TID 652). 714 bytes result sent to driver
2017-08-18 12:17:15,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 326.0 (TID 652) in 12 ms on localhost (1/2)
2017-08-18 12:17:15,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 326.0 (TID 653) in 10 ms on localhost (2/2)
2017-08-18 12:17:15,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 326.0, whose tasks have all completed, from pool 
2017-08-18 12:17:15,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 326 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:17:15,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 326 finished: foreachPartition at streamingProcessNew.scala:50, took 0.022963 s
2017-08-18 12:17:15,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029835000 ms.0 from job set of time 1503029835000 ms
2017-08-18 12:17:15,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1503029835000 ms (execution: 0.052 s)
2017-08-18 12:17:15,074 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 651 from persistence list
2017-08-18 12:17:15,075 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 651
2017-08-18 12:17:15,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 650 from persistence list
2017-08-18 12:17:15,075 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 650
2017-08-18 12:17:15,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:17:15,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029825000 ms
2017-08-18 12:17:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029840000 ms
2017-08-18 12:17:20,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029840000 ms.0 from job set of time 1503029840000 ms
2017-08-18 12:17:20,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:17:20,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 327 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:17:20,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 327 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:17:20,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:17:20,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:17:20,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 327 (MapPartitionsRDD[655] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:17:20,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_327 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:17:20,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_327_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:17:20,031 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_327_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:17:20,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 327 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:17:20,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 327 (MapPartitionsRDD[655] at map at streamingProcessNew.scala:49)
2017-08-18 12:17:20,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 327.0 with 2 tasks
2017-08-18 12:17:20,034 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 327.0 (TID 654, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:17:20,034 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 327.0 (TID 655, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:17:20,034 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 327.0 (TID 654)
2017-08-18 12:17:20,034 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 327.0 (TID 655)
2017-08-18 12:17:20,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:17:20,038 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:17:20,040 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 327.0 (TID 655). 714 bytes result sent to driver
2017-08-18 12:17:20,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 327.0 (TID 654). 714 bytes result sent to driver
2017-08-18 12:17:20,042 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 327.0 (TID 655) in 8 ms on localhost (1/2)
2017-08-18 12:17:20,043 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 327.0 (TID 654) in 11 ms on localhost (2/2)
2017-08-18 12:17:20,043 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 327.0, whose tasks have all completed, from pool 
2017-08-18 12:17:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 327 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:17:20,044 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 327 finished: foreachPartition at streamingProcessNew.scala:50, took 0.020171 s
2017-08-18 12:17:20,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029840000 ms.0 from job set of time 1503029840000 ms
2017-08-18 12:17:20,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.044 s for time 1503029840000 ms (execution: 0.031 s)
2017-08-18 12:17:20,044 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 653 from persistence list
2017-08-18 12:17:20,045 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 653
2017-08-18 12:17:20,045 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 652 from persistence list
2017-08-18 12:17:20,045 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 652
2017-08-18 12:17:20,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:17:20,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029830000 ms
2017-08-18 12:17:25,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029845000 ms
2017-08-18 12:17:25,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029845000 ms.0 from job set of time 1503029845000 ms
2017-08-18 12:17:25,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:17:25,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 328 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:17:25,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 328 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:17:25,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:17:25,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:17:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 328 (MapPartitionsRDD[657] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:17:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_328 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:17:25,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_328_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:17:25,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_328_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:17:25,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 328 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:17:25,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 328 (MapPartitionsRDD[657] at map at streamingProcessNew.scala:49)
2017-08-18 12:17:25,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 328.0 with 2 tasks
2017-08-18 12:17:25,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 328.0 (TID 656, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:17:25,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 328.0 (TID 657, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:17:25,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 328.0 (TID 657)
2017-08-18 12:17:25,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 328.0 (TID 656)
2017-08-18 12:17:25,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:17:25,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:17:25,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 328.0 (TID 657). 714 bytes result sent to driver
2017-08-18 12:17:25,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 328.0 (TID 656). 714 bytes result sent to driver
2017-08-18 12:17:25,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 328.0 (TID 656) in 9 ms on localhost (1/2)
2017-08-18 12:17:25,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 328.0 (TID 657) in 7 ms on localhost (2/2)
2017-08-18 12:17:25,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 328.0, whose tasks have all completed, from pool 
2017-08-18 12:17:25,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 328 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:17:25,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 328 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025103 s
2017-08-18 12:17:25,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029845000 ms.0 from job set of time 1503029845000 ms
2017-08-18 12:17:25,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1503029845000 ms (execution: 0.057 s)
2017-08-18 12:17:25,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 655 from persistence list
2017-08-18 12:17:25,079 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 655
2017-08-18 12:17:25,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 654 from persistence list
2017-08-18 12:17:25,079 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 654
2017-08-18 12:17:25,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:17:25,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029835000 ms
2017-08-18 12:17:30,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029850000 ms
2017-08-18 12:17:30,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029850000 ms.0 from job set of time 1503029850000 ms
2017-08-18 12:17:30,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:17:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 329 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:17:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 329 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:17:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:17:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:17:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 329 (MapPartitionsRDD[659] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:17:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_329 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:17:30,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_329_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:17:30,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_329_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:17:30,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 329 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:17:30,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 329 (MapPartitionsRDD[659] at map at streamingProcessNew.scala:49)
2017-08-18 12:17:30,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 329.0 with 2 tasks
2017-08-18 12:17:30,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 329.0 (TID 658, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:17:30,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 329.0 (TID 659, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:17:30,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 329.0 (TID 659)
2017-08-18 12:17:30,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 329.0 (TID 658)
2017-08-18 12:17:30,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:17:30,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:17:30,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 329.0 (TID 659). 714 bytes result sent to driver
2017-08-18 12:17:30,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 329.0 (TID 658). 714 bytes result sent to driver
2017-08-18 12:17:30,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 329.0 (TID 659) in 7 ms on localhost (1/2)
2017-08-18 12:17:30,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 329.0 (TID 658) in 12 ms on localhost (2/2)
2017-08-18 12:17:30,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 329.0, whose tasks have all completed, from pool 
2017-08-18 12:17:30,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 329 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:17:30,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 329 finished: foreachPartition at streamingProcessNew.scala:50, took 0.028470 s
2017-08-18 12:17:30,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029850000 ms.0 from job set of time 1503029850000 ms
2017-08-18 12:17:30,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1503029850000 ms (execution: 0.062 s)
2017-08-18 12:17:30,084 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 657 from persistence list
2017-08-18 12:17:30,084 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 657
2017-08-18 12:17:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 656 from persistence list
2017-08-18 12:17:30,084 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 656
2017-08-18 12:17:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:17:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029840000 ms
2017-08-18 12:17:35,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029855000 ms
2017-08-18 12:17:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029855000 ms.0 from job set of time 1503029855000 ms
2017-08-18 12:17:35,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:17:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 330 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:17:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 330 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:17:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:17:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:17:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 330 (MapPartitionsRDD[661] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:17:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_330 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:17:35,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_330_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:17:35,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_330_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:17:35,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 330 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:17:35,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 330 (MapPartitionsRDD[661] at map at streamingProcessNew.scala:49)
2017-08-18 12:17:35,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 330.0 with 2 tasks
2017-08-18 12:17:35,085 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 330.0 (TID 660, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:17:35,086 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 330.0 (TID 661, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:17:35,087 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 330.0 (TID 660)
2017-08-18 12:17:35,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 330.0 (TID 661)
2017-08-18 12:17:35,091 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:17:35,091 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:17:35,097 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 330.0 (TID 661). 714 bytes result sent to driver
2017-08-18 12:17:35,097 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 330.0 (TID 660). 714 bytes result sent to driver
2017-08-18 12:17:35,100 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 330.0 (TID 661) in 14 ms on localhost (1/2)
2017-08-18 12:17:35,100 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 330.0 (TID 660) in 19 ms on localhost (2/2)
2017-08-18 12:17:35,101 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 330.0, whose tasks have all completed, from pool 
2017-08-18 12:17:35,101 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 330 (foreachPartition at streamingProcessNew.scala:50) finished in 0.021 s
2017-08-18 12:17:35,101 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 330 finished: foreachPartition at streamingProcessNew.scala:50, took 0.043623 s
2017-08-18 12:17:35,102 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029855000 ms.0 from job set of time 1503029855000 ms
2017-08-18 12:17:35,102 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.102 s for time 1503029855000 ms (execution: 0.082 s)
2017-08-18 12:17:35,102 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 659 from persistence list
2017-08-18 12:17:35,103 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 659
2017-08-18 12:17:35,103 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 658 from persistence list
2017-08-18 12:17:35,104 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 658
2017-08-18 12:17:35,104 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:17:35,104 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029845000 ms
2017-08-18 12:17:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029860000 ms
2017-08-18 12:17:40,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029860000 ms.0 from job set of time 1503029860000 ms
2017-08-18 12:17:40,024 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:17:40,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 331 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:17:40,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 331 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:17:40,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:17:40,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:17:40,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 331 (MapPartitionsRDD[663] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:17:40,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_331 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:17:40,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_331_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:17:40,031 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_331_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:17:40,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 331 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:17:40,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 331 (MapPartitionsRDD[663] at map at streamingProcessNew.scala:49)
2017-08-18 12:17:40,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 331.0 with 2 tasks
2017-08-18 12:17:40,034 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 331.0 (TID 662, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:17:40,035 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 331.0 (TID 663, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:17:40,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 331.0 (TID 662)
2017-08-18 12:17:40,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 331.0 (TID 663)
2017-08-18 12:17:40,037 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:17:40,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:17:40,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 331.0 (TID 662). 714 bytes result sent to driver
2017-08-18 12:17:40,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 331.0 (TID 663). 714 bytes result sent to driver
2017-08-18 12:17:40,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 331.0 (TID 662) in 12 ms on localhost (1/2)
2017-08-18 12:17:40,044 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 331.0 (TID 663) in 10 ms on localhost (2/2)
2017-08-18 12:17:40,044 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 331.0, whose tasks have all completed, from pool 
2017-08-18 12:17:40,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 331 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:17:40,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 331 finished: foreachPartition at streamingProcessNew.scala:50, took 0.020399 s
2017-08-18 12:17:40,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029860000 ms.0 from job set of time 1503029860000 ms
2017-08-18 12:17:40,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.045 s for time 1503029860000 ms (execution: 0.032 s)
2017-08-18 12:17:40,045 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 661 from persistence list
2017-08-18 12:17:40,045 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 661
2017-08-18 12:17:40,046 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 660 from persistence list
2017-08-18 12:17:40,046 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 660
2017-08-18 12:17:40,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:17:40,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029850000 ms
2017-08-18 12:17:45,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029865000 ms
2017-08-18 12:17:45,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029865000 ms.0 from job set of time 1503029865000 ms
2017-08-18 12:17:45,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:17:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 332 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:17:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 332 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:17:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:17:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:17:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 332 (MapPartitionsRDD[665] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:17:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_332 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:17:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_332_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:17:45,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_332_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:17:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 332 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:17:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 332 (MapPartitionsRDD[665] at map at streamingProcessNew.scala:49)
2017-08-18 12:17:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 332.0 with 2 tasks
2017-08-18 12:17:45,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 332.0 (TID 664, localhost, partition 0, ANY, 5657 bytes)
2017-08-18 12:17:45,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 332.0 (TID 665, localhost, partition 1, ANY, 5657 bytes)
2017-08-18 12:17:45,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 332.0 (TID 665)
2017-08-18 12:17:45,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 332.0 (TID 664)
2017-08-18 12:17:45,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:17:45,066 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:17:45,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 332.0 (TID 664). 714 bytes result sent to driver
2017-08-18 12:17:45,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 332.0 (TID 665). 714 bytes result sent to driver
2017-08-18 12:17:45,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 332.0 (TID 664) in 11 ms on localhost (1/2)
2017-08-18 12:17:45,072 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 332.0 (TID 665) in 9 ms on localhost (2/2)
2017-08-18 12:17:45,072 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 332.0, whose tasks have all completed, from pool 
2017-08-18 12:17:45,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 332 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:17:45,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 332 finished: foreachPartition at streamingProcessNew.scala:50, took 0.020617 s
2017-08-18 12:17:45,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029865000 ms.0 from job set of time 1503029865000 ms
2017-08-18 12:17:45,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1503029865000 ms (execution: 0.050 s)
2017-08-18 12:17:45,073 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 663 from persistence list
2017-08-18 12:17:45,073 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 662 from persistence list
2017-08-18 12:17:45,073 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 663
2017-08-18 12:17:45,074 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 662
2017-08-18 12:17:45,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:17:45,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029855000 ms
2017-08-18 12:17:50,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029870000 ms
2017-08-18 12:17:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029870000 ms.0 from job set of time 1503029870000 ms
2017-08-18 12:17:50,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:17:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 333 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:17:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 333 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:17:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:17:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:17:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 333 (MapPartitionsRDD[667] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:17:50,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_333 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:17:50,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_333_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:17:50,085 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_333_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:17:50,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 333 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:17:50,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 333 (MapPartitionsRDD[667] at map at streamingProcessNew.scala:49)
2017-08-18 12:17:50,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 333.0 with 2 tasks
2017-08-18 12:17:50,093 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 333.0 (TID 666, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:17:50,094 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 333.0 (TID 667, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:17:50,094 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 333.0 (TID 667)
2017-08-18 12:17:50,094 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 333.0 (TID 666)
2017-08-18 12:17:50,098 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:17:50,098 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:17:50,103 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 333.0 (TID 666). 714 bytes result sent to driver
2017-08-18 12:17:50,103 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 333.0 (TID 667). 714 bytes result sent to driver
2017-08-18 12:17:50,107 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 333.0 (TID 667) in 13 ms on localhost (1/2)
2017-08-18 12:17:50,107 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 333.0 (TID 666) in 20 ms on localhost (2/2)
2017-08-18 12:17:50,107 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 333.0, whose tasks have all completed, from pool 
2017-08-18 12:17:50,107 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 333 (foreachPartition at streamingProcessNew.scala:50) finished in 0.020 s
2017-08-18 12:17:50,108 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 333 finished: foreachPartition at streamingProcessNew.scala:50, took 0.047734 s
2017-08-18 12:17:50,109 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029870000 ms.0 from job set of time 1503029870000 ms
2017-08-18 12:17:50,109 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.108 s for time 1503029870000 ms (execution: 0.087 s)
2017-08-18 12:17:50,109 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 665 from persistence list
2017-08-18 12:17:50,110 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 665
2017-08-18 12:17:50,110 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 664 from persistence list
2017-08-18 12:17:50,110 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 664
2017-08-18 12:17:50,110 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:17:50,110 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029860000 ms
2017-08-18 12:17:55,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029875000 ms
2017-08-18 12:17:55,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029875000 ms.0 from job set of time 1503029875000 ms
2017-08-18 12:17:55,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:17:55,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 334 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:17:55,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 334 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:17:55,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:17:55,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:17:55,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 334 (MapPartitionsRDD[669] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:17:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_334 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:17:55,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_334_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:17:55,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_334_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:17:55,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 334 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:17:55,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 334 (MapPartitionsRDD[669] at map at streamingProcessNew.scala:49)
2017-08-18 12:17:55,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 334.0 with 2 tasks
2017-08-18 12:17:55,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 334.0 (TID 668, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:17:55,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 334.0 (TID 669, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:17:55,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 334.0 (TID 668)
2017-08-18 12:17:55,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 334.0 (TID 669)
2017-08-18 12:17:55,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:17:55,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:17:55,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 334.0 (TID 668). 801 bytes result sent to driver
2017-08-18 12:17:55,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 334.0 (TID 669). 801 bytes result sent to driver
2017-08-18 12:17:55,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 334.0 (TID 668) in 10 ms on localhost (1/2)
2017-08-18 12:17:55,079 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 334.0 (TID 669) in 9 ms on localhost (2/2)
2017-08-18 12:17:55,079 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 334.0, whose tasks have all completed, from pool 
2017-08-18 12:17:55,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 334 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:17:55,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 334 finished: foreachPartition at streamingProcessNew.scala:50, took 0.027024 s
2017-08-18 12:17:55,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029875000 ms.0 from job set of time 1503029875000 ms
2017-08-18 12:17:55,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1503029875000 ms (execution: 0.059 s)
2017-08-18 12:17:55,080 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 667 from persistence list
2017-08-18 12:17:55,080 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 667
2017-08-18 12:17:55,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 666 from persistence list
2017-08-18 12:17:55,080 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 666
2017-08-18 12:17:55,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:17:55,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029865000 ms
2017-08-18 12:18:00,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029880000 ms
2017-08-18 12:18:00,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029880000 ms.0 from job set of time 1503029880000 ms
2017-08-18 12:18:00,028 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:18:00,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 335 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:18:00,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 335 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:18:00,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:18:00,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:18:00,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 335 (MapPartitionsRDD[671] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:18:00,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_335 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:18:00,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_335_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:18:00,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_335_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:18:00,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 335 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:18:00,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 335 (MapPartitionsRDD[671] at map at streamingProcessNew.scala:49)
2017-08-18 12:18:00,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 335.0 with 2 tasks
2017-08-18 12:18:00,040 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 335.0 (TID 670, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:18:00,041 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 335.0 (TID 671, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:18:00,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 335.0 (TID 670)
2017-08-18 12:18:00,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 335.0 (TID 671)
2017-08-18 12:18:00,043 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:18:00,043 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:18:00,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 335.0 (TID 670). 714 bytes result sent to driver
2017-08-18 12:18:00,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 335.0 (TID 671). 714 bytes result sent to driver
2017-08-18 12:18:00,048 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 335.0 (TID 670) in 11 ms on localhost (1/2)
2017-08-18 12:18:00,049 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 335.0 (TID 671) in 8 ms on localhost (2/2)
2017-08-18 12:18:00,049 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 335.0, whose tasks have all completed, from pool 
2017-08-18 12:18:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 335 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:18:00,050 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 335 finished: foreachPartition at streamingProcessNew.scala:50, took 0.021634 s
2017-08-18 12:18:00,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029880000 ms.0 from job set of time 1503029880000 ms
2017-08-18 12:18:00,051 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.050 s for time 1503029880000 ms (execution: 0.035 s)
2017-08-18 12:18:00,051 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 669 from persistence list
2017-08-18 12:18:00,051 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 669
2017-08-18 12:18:00,051 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 668 from persistence list
2017-08-18 12:18:00,052 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 668
2017-08-18 12:18:00,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:18:00,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029870000 ms
2017-08-18 12:18:05,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029885000 ms
2017-08-18 12:18:05,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029885000 ms.0 from job set of time 1503029885000 ms
2017-08-18 12:18:05,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:18:05,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 336 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:18:05,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 336 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:18:05,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:18:05,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:18:05,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 336 (MapPartitionsRDD[673] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:18:05,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_336 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:18:05,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_336_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:18:05,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_336_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:18:05,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 336 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:18:05,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 336 (MapPartitionsRDD[673] at map at streamingProcessNew.scala:49)
2017-08-18 12:18:05,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 336.0 with 2 tasks
2017-08-18 12:18:05,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 336.0 (TID 672, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:18:05,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 336.0 (TID 673, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:18:05,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 336.0 (TID 672)
2017-08-18 12:18:05,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 336.0 (TID 673)
2017-08-18 12:18:05,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:18:05,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:18:05,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 336.0 (TID 672). 714 bytes result sent to driver
2017-08-18 12:18:05,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 336.0 (TID 673). 714 bytes result sent to driver
2017-08-18 12:18:05,062 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 336.0 (TID 672) in 10 ms on localhost (1/2)
2017-08-18 12:18:05,062 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 336.0 (TID 673) in 7 ms on localhost (2/2)
2017-08-18 12:18:05,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 336.0, whose tasks have all completed, from pool 
2017-08-18 12:18:05,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 336 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:18:05,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 336 finished: foreachPartition at streamingProcessNew.scala:50, took 0.021424 s
2017-08-18 12:18:05,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029885000 ms.0 from job set of time 1503029885000 ms
2017-08-18 12:18:05,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1503029885000 ms (execution: 0.042 s)
2017-08-18 12:18:05,063 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 671 from persistence list
2017-08-18 12:18:05,064 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 671
2017-08-18 12:18:05,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 670 from persistence list
2017-08-18 12:18:05,064 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 670
2017-08-18 12:18:05,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:18:05,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029875000 ms
2017-08-18 12:18:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029890000 ms
2017-08-18 12:18:10,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029890000 ms.0 from job set of time 1503029890000 ms
2017-08-18 12:18:10,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:18:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 337 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:18:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 337 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:18:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:18:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:18:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 337 (MapPartitionsRDD[675] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:18:10,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_337 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:18:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_337_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:18:10,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_337_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:18:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 337 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:18:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 337 (MapPartitionsRDD[675] at map at streamingProcessNew.scala:49)
2017-08-18 12:18:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 337.0 with 2 tasks
2017-08-18 12:18:10,080 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 337.0 (TID 674, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:18:10,080 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 337.0 (TID 675, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:18:10,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 337.0 (TID 675)
2017-08-18 12:18:10,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 337.0 (TID 674)
2017-08-18 12:18:10,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:18:10,085 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:18:10,089 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 337.0 (TID 674). 714 bytes result sent to driver
2017-08-18 12:18:10,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 337.0 (TID 675). 714 bytes result sent to driver
2017-08-18 12:18:10,091 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 337.0 (TID 674) in 14 ms on localhost (1/2)
2017-08-18 12:18:10,092 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 337.0 (TID 675) in 12 ms on localhost (2/2)
2017-08-18 12:18:10,092 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 337.0, whose tasks have all completed, from pool 
2017-08-18 12:18:10,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 337 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:18:10,092 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 337 finished: foreachPartition at streamingProcessNew.scala:50, took 0.031446 s
2017-08-18 12:18:10,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029890000 ms.0 from job set of time 1503029890000 ms
2017-08-18 12:18:10,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.093 s for time 1503029890000 ms (execution: 0.072 s)
2017-08-18 12:18:10,093 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 673 from persistence list
2017-08-18 12:18:10,094 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 673
2017-08-18 12:18:10,094 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 672 from persistence list
2017-08-18 12:18:10,094 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 672
2017-08-18 12:18:10,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:18:10,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029880000 ms
2017-08-18 12:18:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029895000 ms
2017-08-18 12:18:15,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029895000 ms.0 from job set of time 1503029895000 ms
2017-08-18 12:18:15,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:18:15,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 338 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:18:15,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 338 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:18:15,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:18:15,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:18:15,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 338 (MapPartitionsRDD[677] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:18:15,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_338 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:18:15,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_338_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:18:15,032 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_338_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:18:15,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 338 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:18:15,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 338 (MapPartitionsRDD[677] at map at streamingProcessNew.scala:49)
2017-08-18 12:18:15,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 338.0 with 2 tasks
2017-08-18 12:18:15,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 338.0 (TID 676, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:18:15,036 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 338.0 (TID 677, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:18:15,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 338.0 (TID 676)
2017-08-18 12:18:15,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 338.0 (TID 677)
2017-08-18 12:18:15,038 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:18:15,038 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:18:15,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 338.0 (TID 676). 714 bytes result sent to driver
2017-08-18 12:18:15,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 338.0 (TID 677). 714 bytes result sent to driver
2017-08-18 12:18:15,043 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 338.0 (TID 677) in 7 ms on localhost (1/2)
2017-08-18 12:18:15,044 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 338.0 (TID 676) in 10 ms on localhost (2/2)
2017-08-18 12:18:15,044 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 338.0, whose tasks have all completed, from pool 
2017-08-18 12:18:15,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 338 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:18:15,044 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 338 finished: foreachPartition at streamingProcessNew.scala:50, took 0.018720 s
2017-08-18 12:18:15,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029895000 ms.0 from job set of time 1503029895000 ms
2017-08-18 12:18:15,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.044 s for time 1503029895000 ms (execution: 0.031 s)
2017-08-18 12:18:15,045 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 675 from persistence list
2017-08-18 12:18:15,045 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 675
2017-08-18 12:18:15,045 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 674 from persistence list
2017-08-18 12:18:15,045 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 674
2017-08-18 12:18:15,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:18:15,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029885000 ms
2017-08-18 12:18:20,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029900000 ms
2017-08-18 12:18:20,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029900000 ms.0 from job set of time 1503029900000 ms
2017-08-18 12:18:20,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:18:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 339 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:18:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 339 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:18:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:18:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:18:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 339 (MapPartitionsRDD[679] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:18:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_339 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:18:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_339_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-18 12:18:20,036 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_339_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:18:20,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 339 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:18:20,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 339 (MapPartitionsRDD[679] at map at streamingProcessNew.scala:49)
2017-08-18 12:18:20,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 339.0 with 2 tasks
2017-08-18 12:18:20,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 339.0 (TID 678, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:18:20,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 339.0 (TID 679, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:18:20,040 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 339.0 (TID 678)
2017-08-18 12:18:20,040 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 339.0 (TID 679)
2017-08-18 12:18:20,054 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:18:20,055 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:18:20,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_324_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:18:20,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 339.0 (TID 679). 714 bytes result sent to driver
2017-08-18 12:18:20,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 339.0 (TID 678). 787 bytes result sent to driver
2017-08-18 12:18:20,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_325_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:18:20,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 339.0 (TID 679) in 24 ms on localhost (1/2)
2017-08-18 12:18:20,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 339.0 (TID 678) in 25 ms on localhost (2/2)
2017-08-18 12:18:20,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 339.0, whose tasks have all completed, from pool 
2017-08-18 12:18:20,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 339 (foreachPartition at streamingProcessNew.scala:50) finished in 0.028 s
2017-08-18 12:18:20,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 339 finished: foreachPartition at streamingProcessNew.scala:50, took 0.038635 s
2017-08-18 12:18:20,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029900000 ms.0 from job set of time 1503029900000 ms
2017-08-18 12:18:20,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_326_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:18:20,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1503029900000 ms (execution: 0.051 s)
2017-08-18 12:18:20,067 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 677 from persistence list
2017-08-18 12:18:20,067 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 677
2017-08-18 12:18:20,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 676 from persistence list
2017-08-18 12:18:20,069 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 676
2017-08-18 12:18:20,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:18:20,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029890000 ms
2017-08-18 12:18:20,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_327_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:18:20,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_328_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:18:20,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_329_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:18:20,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_330_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:18:20,079 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_331_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:18:20,084 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_332_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:18:20,087 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_333_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:18:20,089 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_334_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:18:20,093 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_335_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:18:20,094 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_336_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:18:20,095 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_337_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:18:20,096 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_338_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:18:25,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029905000 ms
2017-08-18 12:18:25,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029905000 ms.0 from job set of time 1503029905000 ms
2017-08-18 12:18:25,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:18:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 340 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:18:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 340 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:18:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:18:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:18:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 340 (MapPartitionsRDD[681] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:18:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_340 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:18:25,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_340_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:18:25,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_340_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:18:25,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 340 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:18:25,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 340 (MapPartitionsRDD[681] at map at streamingProcessNew.scala:49)
2017-08-18 12:18:25,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 340.0 with 2 tasks
2017-08-18 12:18:25,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 340.0 (TID 680, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:18:25,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 340.0 (TID 681, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:18:25,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 340.0 (TID 681)
2017-08-18 12:18:25,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 340.0 (TID 680)
2017-08-18 12:18:25,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:18:25,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:18:25,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 340.0 (TID 681). 714 bytes result sent to driver
2017-08-18 12:18:25,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 340.0 (TID 680). 714 bytes result sent to driver
2017-08-18 12:18:25,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 340.0 (TID 681) in 8 ms on localhost (1/2)
2017-08-18 12:18:25,078 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 340.0 (TID 680) in 10 ms on localhost (2/2)
2017-08-18 12:18:25,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 340.0, whose tasks have all completed, from pool 
2017-08-18 12:18:25,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 340 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:18:25,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 340 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024187 s
2017-08-18 12:18:25,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029905000 ms.0 from job set of time 1503029905000 ms
2017-08-18 12:18:25,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1503029905000 ms (execution: 0.057 s)
2017-08-18 12:18:25,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 679 from persistence list
2017-08-18 12:18:25,080 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 679
2017-08-18 12:18:25,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 678 from persistence list
2017-08-18 12:18:25,080 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 678
2017-08-18 12:18:25,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:18:25,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029895000 ms
2017-08-18 12:18:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029910000 ms
2017-08-18 12:18:30,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029910000 ms.0 from job set of time 1503029910000 ms
2017-08-18 12:18:30,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:18:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 341 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:18:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 341 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:18:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:18:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:18:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 341 (MapPartitionsRDD[683] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:18:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_341 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:18:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_341_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:18:30,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_341_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:18:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 341 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:18:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 341 (MapPartitionsRDD[683] at map at streamingProcessNew.scala:49)
2017-08-18 12:18:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 341.0 with 2 tasks
2017-08-18 12:18:30,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 341.0 (TID 682, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:18:30,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 341.0 (TID 683, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:18:30,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 341.0 (TID 682)
2017-08-18 12:18:30,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 341.0 (TID 683)
2017-08-18 12:18:30,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:18:30,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:18:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 341.0 (TID 683). 714 bytes result sent to driver
2017-08-18 12:18:30,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 341.0 (TID 682). 714 bytes result sent to driver
2017-08-18 12:18:30,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 341.0 (TID 683) in 10 ms on localhost (1/2)
2017-08-18 12:18:30,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 341.0 (TID 682) in 13 ms on localhost (2/2)
2017-08-18 12:18:30,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 341.0, whose tasks have all completed, from pool 
2017-08-18 12:18:30,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 341 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:18:30,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 341 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025216 s
2017-08-18 12:18:30,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029910000 ms.0 from job set of time 1503029910000 ms
2017-08-18 12:18:30,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1503029910000 ms (execution: 0.056 s)
2017-08-18 12:18:30,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 681 from persistence list
2017-08-18 12:18:30,077 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 681
2017-08-18 12:18:30,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 680 from persistence list
2017-08-18 12:18:30,078 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 680
2017-08-18 12:18:30,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:18:30,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029900000 ms
2017-08-18 12:18:35,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029915000 ms
2017-08-18 12:18:35,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029915000 ms.0 from job set of time 1503029915000 ms
2017-08-18 12:18:35,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:18:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 342 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:18:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 342 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:18:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:18:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:18:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 342 (MapPartitionsRDD[685] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:18:35,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_342 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:18:35,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_342_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:18:35,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_342_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:18:35,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 342 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:18:35,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 342 (MapPartitionsRDD[685] at map at streamingProcessNew.scala:49)
2017-08-18 12:18:35,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 342.0 with 2 tasks
2017-08-18 12:18:35,079 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 342.0 (TID 684, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:18:35,079 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 342.0 (TID 685, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:18:35,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 342.0 (TID 685)
2017-08-18 12:18:35,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 342.0 (TID 684)
2017-08-18 12:18:35,083 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:18:35,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:18:35,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 342.0 (TID 685). 714 bytes result sent to driver
2017-08-18 12:18:35,087 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 342.0 (TID 684). 714 bytes result sent to driver
2017-08-18 12:18:35,090 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 342.0 (TID 685) in 10 ms on localhost (1/2)
2017-08-18 12:18:35,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 342.0 (TID 684) in 14 ms on localhost (2/2)
2017-08-18 12:18:35,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 342.0, whose tasks have all completed, from pool 
2017-08-18 12:18:35,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 342 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:18:35,090 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 342 finished: foreachPartition at streamingProcessNew.scala:50, took 0.030652 s
2017-08-18 12:18:35,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029915000 ms.0 from job set of time 1503029915000 ms
2017-08-18 12:18:35,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.091 s for time 1503029915000 ms (execution: 0.069 s)
2017-08-18 12:18:35,091 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 683 from persistence list
2017-08-18 12:18:35,091 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 683
2017-08-18 12:18:35,091 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 682 from persistence list
2017-08-18 12:18:35,092 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 682
2017-08-18 12:18:35,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:18:35,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029905000 ms
2017-08-18 12:18:40,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029920000 ms
2017-08-18 12:18:40,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029920000 ms.0 from job set of time 1503029920000 ms
2017-08-18 12:18:40,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:18:40,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 343 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:18:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 343 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:18:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:18:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:18:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 343 (MapPartitionsRDD[687] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:18:40,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_343 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:18:40,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_343_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:18:40,036 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_343_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:18:40,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 343 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:18:40,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 343 (MapPartitionsRDD[687] at map at streamingProcessNew.scala:49)
2017-08-18 12:18:40,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 343.0 with 2 tasks
2017-08-18 12:18:40,041 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 343.0 (TID 686, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:18:40,042 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 343.0 (TID 687, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:18:40,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 343.0 (TID 686)
2017-08-18 12:18:40,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 343.0 (TID 687)
2017-08-18 12:18:40,044 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:18:40,044 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:18:40,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 343.0 (TID 687). 714 bytes result sent to driver
2017-08-18 12:18:40,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 343.0 (TID 686). 714 bytes result sent to driver
2017-08-18 12:18:40,049 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 343.0 (TID 687) in 6 ms on localhost (1/2)
2017-08-18 12:18:40,049 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 343.0 (TID 686) in 12 ms on localhost (2/2)
2017-08-18 12:18:40,049 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 343.0, whose tasks have all completed, from pool 
2017-08-18 12:18:40,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 343 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:18:40,049 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 343 finished: foreachPartition at streamingProcessNew.scala:50, took 0.020155 s
2017-08-18 12:18:40,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029920000 ms.0 from job set of time 1503029920000 ms
2017-08-18 12:18:40,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.050 s for time 1503029920000 ms (execution: 0.033 s)
2017-08-18 12:18:40,050 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 685 from persistence list
2017-08-18 12:18:40,050 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 685
2017-08-18 12:18:40,050 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 684 from persistence list
2017-08-18 12:18:40,051 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 684
2017-08-18 12:18:40,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:18:40,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029910000 ms
2017-08-18 12:18:45,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029925000 ms
2017-08-18 12:18:45,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029925000 ms.0 from job set of time 1503029925000 ms
2017-08-18 12:18:45,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:18:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 344 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:18:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 344 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:18:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:18:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:18:45,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 344 (MapPartitionsRDD[689] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:18:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_344 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:18:45,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_344_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:18:45,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_344_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:18:45,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 344 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:18:45,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 344 (MapPartitionsRDD[689] at map at streamingProcessNew.scala:49)
2017-08-18 12:18:45,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 344.0 with 2 tasks
2017-08-18 12:18:45,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 344.0 (TID 688, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:18:45,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 344.0 (TID 689, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:18:45,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 344.0 (TID 689)
2017-08-18 12:18:45,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 344.0 (TID 688)
2017-08-18 12:18:45,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:18:45,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:18:45,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 344.0 (TID 689). 714 bytes result sent to driver
2017-08-18 12:18:45,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 344.0 (TID 688). 714 bytes result sent to driver
2017-08-18 12:18:45,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 344.0 (TID 689) in 10 ms on localhost (1/2)
2017-08-18 12:18:45,078 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 344.0 (TID 688) in 12 ms on localhost (2/2)
2017-08-18 12:18:45,078 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 344.0, whose tasks have all completed, from pool 
2017-08-18 12:18:45,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 344 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:18:45,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 344 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024652 s
2017-08-18 12:18:45,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029925000 ms.0 from job set of time 1503029925000 ms
2017-08-18 12:18:45,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1503029925000 ms (execution: 0.058 s)
2017-08-18 12:18:45,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 687 from persistence list
2017-08-18 12:18:45,080 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 687
2017-08-18 12:18:45,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 686 from persistence list
2017-08-18 12:18:45,080 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 686
2017-08-18 12:18:45,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:18:45,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029915000 ms
2017-08-18 12:18:50,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029930000 ms
2017-08-18 12:18:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029930000 ms.0 from job set of time 1503029930000 ms
2017-08-18 12:18:50,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:18:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 345 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:18:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 345 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:18:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:18:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:18:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 345 (MapPartitionsRDD[691] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:18:50,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_345 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:18:50,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_345_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:18:50,075 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_345_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:18:50,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 345 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:18:50,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 345 (MapPartitionsRDD[691] at map at streamingProcessNew.scala:49)
2017-08-18 12:18:50,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 345.0 with 2 tasks
2017-08-18 12:18:50,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 345.0 (TID 690, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:18:50,080 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 345.0 (TID 691, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:18:50,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 345.0 (TID 691)
2017-08-18 12:18:50,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 345.0 (TID 690)
2017-08-18 12:18:50,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:18:50,084 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:18:50,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 345.0 (TID 690). 714 bytes result sent to driver
2017-08-18 12:18:50,088 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 345.0 (TID 691). 714 bytes result sent to driver
2017-08-18 12:18:50,089 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 345.0 (TID 691) in 9 ms on localhost (1/2)
2017-08-18 12:18:50,089 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 345.0 (TID 690) in 12 ms on localhost (2/2)
2017-08-18 12:18:50,089 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 345.0, whose tasks have all completed, from pool 
2017-08-18 12:18:50,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 345 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:18:50,090 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 345 finished: foreachPartition at streamingProcessNew.scala:50, took 0.030576 s
2017-08-18 12:18:50,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029930000 ms.0 from job set of time 1503029930000 ms
2017-08-18 12:18:50,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1503029930000 ms (execution: 0.069 s)
2017-08-18 12:18:50,090 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 689 from persistence list
2017-08-18 12:18:50,091 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 689
2017-08-18 12:18:50,091 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 688 from persistence list
2017-08-18 12:18:50,091 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 688
2017-08-18 12:18:50,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:18:50,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029920000 ms
2017-08-18 12:18:55,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029935000 ms
2017-08-18 12:18:55,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029935000 ms.0 from job set of time 1503029935000 ms
2017-08-18 12:18:55,062 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:18:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 346 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:18:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 346 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:18:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:18:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:18:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 346 (MapPartitionsRDD[693] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:18:55,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_346 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:18:55,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_346_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:18:55,076 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_346_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:18:55,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 346 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:18:55,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 346 (MapPartitionsRDD[693] at map at streamingProcessNew.scala:49)
2017-08-18 12:18:55,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 346.0 with 2 tasks
2017-08-18 12:18:55,081 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 346.0 (TID 692, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:18:55,082 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 346.0 (TID 693, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:18:55,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 346.0 (TID 693)
2017-08-18 12:18:55,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 346.0 (TID 692)
2017-08-18 12:18:55,086 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:18:55,086 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:18:55,092 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 346.0 (TID 692). 714 bytes result sent to driver
2017-08-18 12:18:55,092 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 346.0 (TID 693). 714 bytes result sent to driver
2017-08-18 12:18:55,094 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 346.0 (TID 692) in 16 ms on localhost (1/2)
2017-08-18 12:18:55,095 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 346.0 (TID 693) in 14 ms on localhost (2/2)
2017-08-18 12:18:55,095 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 346.0, whose tasks have all completed, from pool 
2017-08-18 12:18:55,095 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 346 (foreachPartition at streamingProcessNew.scala:50) finished in 0.017 s
2017-08-18 12:18:55,095 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 346 finished: foreachPartition at streamingProcessNew.scala:50, took 0.033233 s
2017-08-18 12:18:55,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029935000 ms.0 from job set of time 1503029935000 ms
2017-08-18 12:18:55,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.096 s for time 1503029935000 ms (execution: 0.076 s)
2017-08-18 12:18:55,096 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 691 from persistence list
2017-08-18 12:18:55,096 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 691
2017-08-18 12:18:55,097 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 690 from persistence list
2017-08-18 12:18:55,097 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 690
2017-08-18 12:18:55,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:18:55,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029925000 ms
2017-08-18 12:19:00,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029940000 ms
2017-08-18 12:19:00,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029940000 ms.0 from job set of time 1503029940000 ms
2017-08-18 12:19:00,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:19:00,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 347 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:19:00,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 347 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:19:00,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:19:00,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:19:00,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 347 (MapPartitionsRDD[695] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:19:00,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_347 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:19:00,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_347_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:19:00,034 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_347_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:19:00,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 347 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:19:00,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 347 (MapPartitionsRDD[695] at map at streamingProcessNew.scala:49)
2017-08-18 12:19:00,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 347.0 with 2 tasks
2017-08-18 12:19:00,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 347.0 (TID 694, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:19:00,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 347.0 (TID 695, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:19:00,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 347.0 (TID 695)
2017-08-18 12:19:00,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 347.0 (TID 694)
2017-08-18 12:19:00,039 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:19:00,039 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:19:00,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 347.0 (TID 694). 714 bytes result sent to driver
2017-08-18 12:19:00,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 347.0 (TID 695). 714 bytes result sent to driver
2017-08-18 12:19:00,046 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 347.0 (TID 694) in 11 ms on localhost (1/2)
2017-08-18 12:19:00,046 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 347.0 (TID 695) in 9 ms on localhost (2/2)
2017-08-18 12:19:00,046 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 347.0, whose tasks have all completed, from pool 
2017-08-18 12:19:00,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 347 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:19:00,047 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 347 finished: foreachPartition at streamingProcessNew.scala:50, took 0.020399 s
2017-08-18 12:19:00,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029940000 ms.0 from job set of time 1503029940000 ms
2017-08-18 12:19:00,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.047 s for time 1503029940000 ms (execution: 0.034 s)
2017-08-18 12:19:00,047 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 693 from persistence list
2017-08-18 12:19:00,048 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 693
2017-08-18 12:19:00,048 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 692 from persistence list
2017-08-18 12:19:00,049 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 692
2017-08-18 12:19:00,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:19:00,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029930000 ms
2017-08-18 12:19:05,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029945000 ms
2017-08-18 12:19:05,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029945000 ms.0 from job set of time 1503029945000 ms
2017-08-18 12:19:05,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:19:05,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 348 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:19:05,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 348 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:19:05,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:19:05,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:19:05,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 348 (MapPartitionsRDD[697] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:19:05,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_348 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:19:05,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_348_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:19:05,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_348_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:19:05,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 348 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:19:05,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 348 (MapPartitionsRDD[697] at map at streamingProcessNew.scala:49)
2017-08-18 12:19:05,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 348.0 with 2 tasks
2017-08-18 12:19:05,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 348.0 (TID 696, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:19:05,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 348.0 (TID 697, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:19:05,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 348.0 (TID 696)
2017-08-18 12:19:05,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 348.0 (TID 697)
2017-08-18 12:19:05,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:19:05,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:19:05,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 348.0 (TID 697). 714 bytes result sent to driver
2017-08-18 12:19:05,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 348.0 (TID 696). 714 bytes result sent to driver
2017-08-18 12:19:05,085 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 348.0 (TID 697) in 14 ms on localhost (1/2)
2017-08-18 12:19:05,085 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 348.0 (TID 696) in 20 ms on localhost (2/2)
2017-08-18 12:19:05,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 348 (foreachPartition at streamingProcessNew.scala:50) finished in 0.021 s
2017-08-18 12:19:05,086 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 348.0, whose tasks have all completed, from pool 
2017-08-18 12:19:05,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 348 finished: foreachPartition at streamingProcessNew.scala:50, took 0.037026 s
2017-08-18 12:19:05,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029945000 ms.0 from job set of time 1503029945000 ms
2017-08-18 12:19:05,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1503029945000 ms (execution: 0.065 s)
2017-08-18 12:19:05,088 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 695 from persistence list
2017-08-18 12:19:05,089 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 695
2017-08-18 12:19:05,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 694 from persistence list
2017-08-18 12:19:05,090 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 694
2017-08-18 12:19:05,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:19:05,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029935000 ms
2017-08-18 12:19:10,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029950000 ms
2017-08-18 12:19:10,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029950000 ms.0 from job set of time 1503029950000 ms
2017-08-18 12:19:10,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:19:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 349 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:19:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 349 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:19:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:19:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:19:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 349 (MapPartitionsRDD[699] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:19:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_349 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:19:10,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_349_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:19:10,060 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_349_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:19:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 349 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:19:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 349 (MapPartitionsRDD[699] at map at streamingProcessNew.scala:49)
2017-08-18 12:19:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 349.0 with 2 tasks
2017-08-18 12:19:10,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 349.0 (TID 698, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:19:10,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 349.0 (TID 699, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:19:10,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 349.0 (TID 698)
2017-08-18 12:19:10,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 349.0 (TID 699)
2017-08-18 12:19:10,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:19:10,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:19:10,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 349.0 (TID 699). 714 bytes result sent to driver
2017-08-18 12:19:10,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 349.0 (TID 698). 714 bytes result sent to driver
2017-08-18 12:19:10,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 349.0 (TID 699) in 8 ms on localhost (1/2)
2017-08-18 12:19:10,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 349.0 (TID 698) in 12 ms on localhost (2/2)
2017-08-18 12:19:10,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 349.0, whose tasks have all completed, from pool 
2017-08-18 12:19:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 349 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:19:10,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 349 finished: foreachPartition at streamingProcessNew.scala:50, took 0.026283 s
2017-08-18 12:19:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029950000 ms.0 from job set of time 1503029950000 ms
2017-08-18 12:19:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1503029950000 ms (execution: 0.056 s)
2017-08-18 12:19:10,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 697 from persistence list
2017-08-18 12:19:10,075 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 697
2017-08-18 12:19:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 696 from persistence list
2017-08-18 12:19:10,076 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 696
2017-08-18 12:19:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:19:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029940000 ms
2017-08-18 12:19:15,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029955000 ms
2017-08-18 12:19:15,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029955000 ms.0 from job set of time 1503029955000 ms
2017-08-18 12:19:15,109 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:19:15,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 350 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:19:15,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 350 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:19:15,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:19:15,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:19:15,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 350 (MapPartitionsRDD[701] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:19:15,116 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_350 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:19:15,122 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_350_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:19:15,123 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_350_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:19:15,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 350 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:19:15,124 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 350 (MapPartitionsRDD[701] at map at streamingProcessNew.scala:49)
2017-08-18 12:19:15,124 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 350.0 with 2 tasks
2017-08-18 12:19:15,128 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 350.0 (TID 700, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:19:15,128 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 350.0 (TID 701, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:19:15,129 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 350.0 (TID 701)
2017-08-18 12:19:15,129 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 350.0 (TID 700)
2017-08-18 12:19:15,132 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:19:15,132 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:19:15,137 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 350.0 (TID 700). 714 bytes result sent to driver
2017-08-18 12:19:15,137 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 350.0 (TID 701). 714 bytes result sent to driver
2017-08-18 12:19:15,139 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 350.0 (TID 700) in 15 ms on localhost (1/2)
2017-08-18 12:19:15,139 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 350.0 (TID 701) in 11 ms on localhost (2/2)
2017-08-18 12:19:15,139 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 350 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:19:15,139 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 350.0, whose tasks have all completed, from pool 
2017-08-18 12:19:15,140 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 350 finished: foreachPartition at streamingProcessNew.scala:50, took 0.030946 s
2017-08-18 12:19:15,140 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029955000 ms.0 from job set of time 1503029955000 ms
2017-08-18 12:19:15,140 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.140 s for time 1503029955000 ms (execution: 0.069 s)
2017-08-18 12:19:15,141 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 699 from persistence list
2017-08-18 12:19:15,141 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 699
2017-08-18 12:19:15,141 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 698 from persistence list
2017-08-18 12:19:15,141 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 698
2017-08-18 12:19:15,141 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:19:15,141 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029945000 ms
2017-08-18 12:19:20,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029960000 ms
2017-08-18 12:19:20,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029960000 ms.0 from job set of time 1503029960000 ms
2017-08-18 12:19:20,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:19:20,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 351 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:19:20,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 351 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:19:20,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:19:20,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:19:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 351 (MapPartitionsRDD[703] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:19:20,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_351 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:19:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_351_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:19:20,036 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_351_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:19:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 351 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:19:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 351 (MapPartitionsRDD[703] at map at streamingProcessNew.scala:49)
2017-08-18 12:19:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 351.0 with 2 tasks
2017-08-18 12:19:20,038 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 351.0 (TID 702, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:19:20,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 351.0 (TID 703, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:19:20,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 351.0 (TID 702)
2017-08-18 12:19:20,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 351.0 (TID 703)
2017-08-18 12:19:20,041 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:19:20,041 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:19:20,045 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 351.0 (TID 703). 714 bytes result sent to driver
2017-08-18 12:19:20,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 351.0 (TID 702). 714 bytes result sent to driver
2017-08-18 12:19:20,047 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 351.0 (TID 703) in 9 ms on localhost (1/2)
2017-08-18 12:19:20,047 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 351.0 (TID 702) in 10 ms on localhost (2/2)
2017-08-18 12:19:20,047 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 351.0, whose tasks have all completed, from pool 
2017-08-18 12:19:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 351 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:19:20,048 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 351 finished: foreachPartition at streamingProcessNew.scala:50, took 0.018812 s
2017-08-18 12:19:20,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029960000 ms.0 from job set of time 1503029960000 ms
2017-08-18 12:19:20,048 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 701 from persistence list
2017-08-18 12:19:20,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1503029960000 ms (execution: 0.033 s)
2017-08-18 12:19:20,049 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 700 from persistence list
2017-08-18 12:19:20,049 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 701
2017-08-18 12:19:20,049 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 700
2017-08-18 12:19:20,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:19:20,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029950000 ms
2017-08-18 12:19:25,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029965000 ms
2017-08-18 12:19:25,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029965000 ms.0 from job set of time 1503029965000 ms
2017-08-18 12:19:25,071 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:19:25,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 352 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:19:25,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 352 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:19:25,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:19:25,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:19:25,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 352 (MapPartitionsRDD[705] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:19:25,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_352 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:19:25,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_352_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:19:25,080 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_352_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:19:25,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 352 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:19:25,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 352 (MapPartitionsRDD[705] at map at streamingProcessNew.scala:49)
2017-08-18 12:19:25,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 352.0 with 2 tasks
2017-08-18 12:19:25,083 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 352.0 (TID 704, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:19:25,084 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 352.0 (TID 705, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:19:25,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 352.0 (TID 705)
2017-08-18 12:19:25,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 352.0 (TID 704)
2017-08-18 12:19:25,086 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:19:25,086 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:19:25,090 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 352.0 (TID 705). 714 bytes result sent to driver
2017-08-18 12:19:25,090 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 352.0 (TID 704). 714 bytes result sent to driver
2017-08-18 12:19:25,092 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 352.0 (TID 705) in 8 ms on localhost (1/2)
2017-08-18 12:19:25,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 352.0 (TID 704) in 11 ms on localhost (2/2)
2017-08-18 12:19:25,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 352.0, whose tasks have all completed, from pool 
2017-08-18 12:19:25,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 352 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:19:25,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 352 finished: foreachPartition at streamingProcessNew.scala:50, took 0.020948 s
2017-08-18 12:19:25,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029965000 ms.0 from job set of time 1503029965000 ms
2017-08-18 12:19:25,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.093 s for time 1503029965000 ms (execution: 0.070 s)
2017-08-18 12:19:25,093 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 703 from persistence list
2017-08-18 12:19:25,094 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 703
2017-08-18 12:19:25,094 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 702 from persistence list
2017-08-18 12:19:25,094 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 702
2017-08-18 12:19:25,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:19:25,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029955000 ms
2017-08-18 12:19:30,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029970000 ms
2017-08-18 12:19:30,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029970000 ms.0 from job set of time 1503029970000 ms
2017-08-18 12:19:30,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:19:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 353 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:19:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 353 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:19:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:19:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:19:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 353 (MapPartitionsRDD[707] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:19:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_353 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:19:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_353_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:19:30,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_353_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:19:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 353 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:19:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 353 (MapPartitionsRDD[707] at map at streamingProcessNew.scala:49)
2017-08-18 12:19:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 353.0 with 2 tasks
2017-08-18 12:19:30,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 353.0 (TID 706, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:19:30,070 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 353.0 (TID 707, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:19:30,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 353.0 (TID 706)
2017-08-18 12:19:30,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 353.0 (TID 707)
2017-08-18 12:19:30,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:19:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:19:30,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 353.0 (TID 706). 714 bytes result sent to driver
2017-08-18 12:19:30,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 353.0 (TID 707). 714 bytes result sent to driver
2017-08-18 12:19:30,080 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 353.0 (TID 706) in 14 ms on localhost (1/2)
2017-08-18 12:19:30,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 353.0 (TID 707) in 11 ms on localhost (2/2)
2017-08-18 12:19:30,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 353.0, whose tasks have all completed, from pool 
2017-08-18 12:19:30,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 353 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:19:30,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 353 finished: foreachPartition at streamingProcessNew.scala:50, took 0.027557 s
2017-08-18 12:19:30,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029970000 ms.0 from job set of time 1503029970000 ms
2017-08-18 12:19:30,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1503029970000 ms (execution: 0.061 s)
2017-08-18 12:19:30,082 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 705 from persistence list
2017-08-18 12:19:30,082 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 705
2017-08-18 12:19:30,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 704 from persistence list
2017-08-18 12:19:30,083 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 704
2017-08-18 12:19:30,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:19:30,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029960000 ms
2017-08-18 12:19:35,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029975000 ms
2017-08-18 12:19:35,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029975000 ms.0 from job set of time 1503029975000 ms
2017-08-18 12:19:35,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:19:35,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 354 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:19:35,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 354 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:19:35,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:19:35,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:19:35,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 354 (MapPartitionsRDD[709] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:19:35,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_354 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:19:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_354_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-18 12:19:35,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_354_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:19:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 354 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:19:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 354 (MapPartitionsRDD[709] at map at streamingProcessNew.scala:49)
2017-08-18 12:19:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 354.0 with 2 tasks
2017-08-18 12:19:35,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 354.0 (TID 708, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:19:35,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 354.0 (TID 709, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:19:35,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 354.0 (TID 708)
2017-08-18 12:19:35,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 354.0 (TID 709)
2017-08-18 12:19:35,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:19:35,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:19:35,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 354.0 (TID 708). 714 bytes result sent to driver
2017-08-18 12:19:35,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 354.0 (TID 709). 714 bytes result sent to driver
2017-08-18 12:19:35,070 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 354.0 (TID 708) in 10 ms on localhost (1/2)
2017-08-18 12:19:35,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 354.0 (TID 709) in 7 ms on localhost (2/2)
2017-08-18 12:19:35,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 354.0, whose tasks have all completed, from pool 
2017-08-18 12:19:35,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 354 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:19:35,071 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 354 finished: foreachPartition at streamingProcessNew.scala:50, took 0.022978 s
2017-08-18 12:19:35,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029975000 ms.0 from job set of time 1503029975000 ms
2017-08-18 12:19:35,072 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 707 from persistence list
2017-08-18 12:19:35,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1503029975000 ms (execution: 0.047 s)
2017-08-18 12:19:35,072 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 707
2017-08-18 12:19:35,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 706 from persistence list
2017-08-18 12:19:35,072 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 706
2017-08-18 12:19:35,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:19:35,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029965000 ms
2017-08-18 12:19:40,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029980000 ms
2017-08-18 12:19:40,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029980000 ms.0 from job set of time 1503029980000 ms
2017-08-18 12:19:40,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:19:40,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 355 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:19:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 355 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:19:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:19:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:19:40,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 355 (MapPartitionsRDD[711] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:19:40,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_354_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:19:40,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_339_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:19:40,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_340_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:19:40,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_355 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:19:40,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_341_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:19:40,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_342_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:19:40,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_355_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:19:40,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_355_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:19:40,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 355 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:19:40,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_343_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:19:40,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 355 (MapPartitionsRDD[711] at map at streamingProcessNew.scala:49)
2017-08-18 12:19:40,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 355.0 with 2 tasks
2017-08-18 12:19:40,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 355.0 (TID 710, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:19:40,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_344_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:19:40,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 355.0 (TID 711, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:19:40,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 355.0 (TID 710)
2017-08-18 12:19:40,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 355.0 (TID 711)
2017-08-18 12:19:40,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:19:40,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:19:40,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_345_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:19:40,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 355.0 (TID 710). 714 bytes result sent to driver
2017-08-18 12:19:40,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 355.0 (TID 711). 714 bytes result sent to driver
2017-08-18 12:19:40,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 355.0 (TID 710) in 17 ms on localhost (1/2)
2017-08-18 12:19:40,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 355.0 (TID 711) in 14 ms on localhost (2/2)
2017-08-18 12:19:40,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 355.0, whose tasks have all completed, from pool 
2017-08-18 12:19:40,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 355 (foreachPartition at streamingProcessNew.scala:50) finished in 0.017 s
2017-08-18 12:19:40,088 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_346_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:19:40,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 355 finished: foreachPartition at streamingProcessNew.scala:50, took 0.034854 s
2017-08-18 12:19:40,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029980000 ms.0 from job set of time 1503029980000 ms
2017-08-18 12:19:40,090 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 709 from persistence list
2017-08-18 12:19:40,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1503029980000 ms (execution: 0.071 s)
2017-08-18 12:19:40,090 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 709
2017-08-18 12:19:40,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 708 from persistence list
2017-08-18 12:19:40,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:19:40,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029970000 ms
2017-08-18 12:19:40,091 [block-manager-slave-async-thread-pool-30] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 708
2017-08-18 12:19:40,093 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_347_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:19:40,097 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_348_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:19:40,098 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_349_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:19:40,099 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_350_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:19:40,100 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_351_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:19:40,101 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_352_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:19:40,102 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_353_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:19:45,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029985000 ms
2017-08-18 12:19:45,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029985000 ms.0 from job set of time 1503029985000 ms
2017-08-18 12:19:45,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:19:45,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 356 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:19:45,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 356 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:19:45,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:19:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:19:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 356 (MapPartitionsRDD[713] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:19:45,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_356 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:19:45,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_356_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:19:45,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_356_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:19:45,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 356 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:19:45,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 356 (MapPartitionsRDD[713] at map at streamingProcessNew.scala:49)
2017-08-18 12:19:45,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 356.0 with 2 tasks
2017-08-18 12:19:45,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 356.0 (TID 712, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:19:45,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 356.0 (TID 713, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:19:45,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 356.0 (TID 713)
2017-08-18 12:19:45,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 356.0 (TID 712)
2017-08-18 12:19:45,083 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:19:45,083 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:19:45,089 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 356.0 (TID 713). 714 bytes result sent to driver
2017-08-18 12:19:45,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 356.0 (TID 712). 714 bytes result sent to driver
2017-08-18 12:19:45,093 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 356.0 (TID 713) in 20 ms on localhost (1/2)
2017-08-18 12:19:45,094 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 356.0 (TID 712) in 24 ms on localhost (2/2)
2017-08-18 12:19:45,094 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 356 (foreachPartition at streamingProcessNew.scala:50) finished in 0.025 s
2017-08-18 12:19:45,094 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 356.0, whose tasks have all completed, from pool 
2017-08-18 12:19:45,094 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 356 finished: foreachPartition at streamingProcessNew.scala:50, took 0.047150 s
2017-08-18 12:19:45,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029985000 ms.0 from job set of time 1503029985000 ms
2017-08-18 12:19:45,096 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 711 from persistence list
2017-08-18 12:19:45,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.095 s for time 1503029985000 ms (execution: 0.076 s)
2017-08-18 12:19:45,096 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 711
2017-08-18 12:19:45,096 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 710 from persistence list
2017-08-18 12:19:45,097 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 710
2017-08-18 12:19:45,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:19:45,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029975000 ms
2017-08-18 12:19:50,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029990000 ms
2017-08-18 12:19:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029990000 ms.0 from job set of time 1503029990000 ms
2017-08-18 12:19:50,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:19:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 357 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:19:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 357 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:19:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:19:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:19:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 357 (MapPartitionsRDD[715] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:19:50,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_357 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:19:50,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_357_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:19:50,085 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_357_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:19:50,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 357 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:19:50,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 357 (MapPartitionsRDD[715] at map at streamingProcessNew.scala:49)
2017-08-18 12:19:50,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 357.0 with 2 tasks
2017-08-18 12:19:50,090 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 357.0 (TID 714, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:19:50,091 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 357.0 (TID 715, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:19:50,091 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 357.0 (TID 715)
2017-08-18 12:19:50,091 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 357.0 (TID 714)
2017-08-18 12:19:50,094 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:19:50,094 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:19:50,099 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 357.0 (TID 715). 714 bytes result sent to driver
2017-08-18 12:19:50,099 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 357.0 (TID 714). 714 bytes result sent to driver
2017-08-18 12:19:50,102 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 357.0 (TID 715) in 12 ms on localhost (1/2)
2017-08-18 12:19:50,103 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 357.0 (TID 714) in 16 ms on localhost (2/2)
2017-08-18 12:19:50,103 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 357.0, whose tasks have all completed, from pool 
2017-08-18 12:19:50,103 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 357 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:19:50,104 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 357 finished: foreachPartition at streamingProcessNew.scala:50, took 0.044304 s
2017-08-18 12:19:50,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029990000 ms.0 from job set of time 1503029990000 ms
2017-08-18 12:19:50,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.104 s for time 1503029990000 ms (execution: 0.084 s)
2017-08-18 12:19:50,104 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 713 from persistence list
2017-08-18 12:19:50,105 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 713
2017-08-18 12:19:50,105 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 712 from persistence list
2017-08-18 12:19:50,105 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 712
2017-08-18 12:19:50,105 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:19:50,106 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029980000 ms
2017-08-18 12:19:55,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503029995000 ms
2017-08-18 12:19:55,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503029995000 ms.0 from job set of time 1503029995000 ms
2017-08-18 12:19:55,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:19:55,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 358 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:19:55,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 358 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:19:55,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:19:55,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:19:55,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 358 (MapPartitionsRDD[717] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:19:55,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_358 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:19:55,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_358_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:19:55,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_358_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:19:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 358 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:19:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 358 (MapPartitionsRDD[717] at map at streamingProcessNew.scala:49)
2017-08-18 12:19:55,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 358.0 with 2 tasks
2017-08-18 12:19:55,078 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 358.0 (TID 716, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:19:55,079 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 358.0 (TID 717, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:19:55,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 358.0 (TID 716)
2017-08-18 12:19:55,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 358.0 (TID 717)
2017-08-18 12:19:55,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:19:55,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:19:55,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 358.0 (TID 717). 714 bytes result sent to driver
2017-08-18 12:19:55,086 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 358.0 (TID 716). 714 bytes result sent to driver
2017-08-18 12:19:55,089 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 358.0 (TID 717) in 11 ms on localhost (1/2)
2017-08-18 12:19:55,089 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 358.0 (TID 716) in 14 ms on localhost (2/2)
2017-08-18 12:19:55,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 358.0, whose tasks have all completed, from pool 
2017-08-18 12:19:55,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 358 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:19:55,090 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 358 finished: foreachPartition at streamingProcessNew.scala:50, took 0.030194 s
2017-08-18 12:19:55,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503029995000 ms.0 from job set of time 1503029995000 ms
2017-08-18 12:19:55,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.091 s for time 1503029995000 ms (execution: 0.070 s)
2017-08-18 12:19:55,091 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 715 from persistence list
2017-08-18 12:19:55,091 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 715
2017-08-18 12:19:55,092 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 714 from persistence list
2017-08-18 12:19:55,092 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 714
2017-08-18 12:19:55,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:19:55,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029985000 ms
2017-08-18 12:20:00,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030000000 ms
2017-08-18 12:20:00,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030000000 ms.0 from job set of time 1503030000000 ms
2017-08-18 12:20:00,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:20:00,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 359 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:20:00,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 359 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:20:00,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:20:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:20:00,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 359 (MapPartitionsRDD[719] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:20:00,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_359 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:20:00,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_359_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:20:00,040 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_359_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:20:00,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 359 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:20:00,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 359 (MapPartitionsRDD[719] at map at streamingProcessNew.scala:49)
2017-08-18 12:20:00,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 359.0 with 2 tasks
2017-08-18 12:20:00,045 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 359.0 (TID 718, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:20:00,045 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 359.0 (TID 719, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:20:00,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 359.0 (TID 719)
2017-08-18 12:20:00,045 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 359.0 (TID 718)
2017-08-18 12:20:00,047 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:20:00,047 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:20:00,052 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 359.0 (TID 718). 714 bytes result sent to driver
2017-08-18 12:20:00,052 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 359.0 (TID 719). 714 bytes result sent to driver
2017-08-18 12:20:00,053 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 359.0 (TID 718) in 12 ms on localhost (1/2)
2017-08-18 12:20:00,054 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 359.0 (TID 719) in 8 ms on localhost (2/2)
2017-08-18 12:20:00,054 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 359.0, whose tasks have all completed, from pool 
2017-08-18 12:20:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 359 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:20:00,054 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 359 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024089 s
2017-08-18 12:20:00,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030000000 ms.0 from job set of time 1503030000000 ms
2017-08-18 12:20:00,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.054 s for time 1503030000000 ms (execution: 0.038 s)
2017-08-18 12:20:00,055 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 717 from persistence list
2017-08-18 12:20:00,055 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 717
2017-08-18 12:20:00,055 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 716 from persistence list
2017-08-18 12:20:00,055 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 716
2017-08-18 12:20:00,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:20:00,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029990000 ms
2017-08-18 12:20:02,410 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_359_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:20:02,411 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_358_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:20:02,412 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_357_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:20:02,414 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_356_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:20:02,415 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_355_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:20:02,416 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_0_piece0 on 192.168.31.111:60809 in memory (size: 25.2 KB, free: 413.9 MB)
2017-08-18 12:20:05,024 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030005000 ms
2017-08-18 12:20:05,025 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030005000 ms.0 from job set of time 1503030005000 ms
2017-08-18 12:20:05,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:20:05,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 360 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:20:05,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 360 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:20:05,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:20:05,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:20:05,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 360 (MapPartitionsRDD[721] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:20:05,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_360 stored as values in memory (estimated size 34.5 KB, free 413.9 MB)
2017-08-18 12:20:05,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_360_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.9 MB)
2017-08-18 12:20:05,073 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_360_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:20:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 360 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:20:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 360 (MapPartitionsRDD[721] at map at streamingProcessNew.scala:49)
2017-08-18 12:20:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 360.0 with 2 tasks
2017-08-18 12:20:05,078 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 360.0 (TID 720, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:20:05,079 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 360.0 (TID 721, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:20:05,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 360.0 (TID 721)
2017-08-18 12:20:05,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 360.0 (TID 720)
2017-08-18 12:20:05,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:20:05,083 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:20:05,088 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 360.0 (TID 721). 714 bytes result sent to driver
2017-08-18 12:20:05,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 360.0 (TID 720). 714 bytes result sent to driver
2017-08-18 12:20:05,091 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 360.0 (TID 720) in 15 ms on localhost (1/2)
2017-08-18 12:20:05,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 360.0 (TID 721) in 13 ms on localhost (2/2)
2017-08-18 12:20:05,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 360.0, whose tasks have all completed, from pool 
2017-08-18 12:20:05,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 360 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:20:05,092 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 360 finished: foreachPartition at streamingProcessNew.scala:50, took 0.031231 s
2017-08-18 12:20:05,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030005000 ms.0 from job set of time 1503030005000 ms
2017-08-18 12:20:05,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1503030005000 ms (execution: 0.067 s)
2017-08-18 12:20:05,093 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 719 from persistence list
2017-08-18 12:20:05,093 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 719
2017-08-18 12:20:05,093 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 718 from persistence list
2017-08-18 12:20:05,094 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 718
2017-08-18 12:20:05,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:20:05,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503029995000 ms
2017-08-18 12:20:10,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030010000 ms
2017-08-18 12:20:10,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030010000 ms.0 from job set of time 1503030010000 ms
2017-08-18 12:20:10,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:20:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 361 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:20:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 361 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:20:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:20:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:20:10,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 361 (MapPartitionsRDD[723] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:20:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_361 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:20:10,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_361_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:20:10,087 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_361_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:20:10,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 361 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:20:10,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 361 (MapPartitionsRDD[723] at map at streamingProcessNew.scala:49)
2017-08-18 12:20:10,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 361.0 with 2 tasks
2017-08-18 12:20:10,095 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 361.0 (TID 722, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:20:10,096 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 361.0 (TID 723, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:20:10,097 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 361.0 (TID 722)
2017-08-18 12:20:10,097 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 361.0 (TID 723)
2017-08-18 12:20:10,100 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:20:10,100 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:20:10,106 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 361.0 (TID 723). 714 bytes result sent to driver
2017-08-18 12:20:10,106 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 361.0 (TID 722). 714 bytes result sent to driver
2017-08-18 12:20:10,109 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 361.0 (TID 723) in 13 ms on localhost (1/2)
2017-08-18 12:20:10,110 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 361.0 (TID 722) in 20 ms on localhost (2/2)
2017-08-18 12:20:10,110 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 361.0, whose tasks have all completed, from pool 
2017-08-18 12:20:10,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 361 (foreachPartition at streamingProcessNew.scala:50) finished in 0.020 s
2017-08-18 12:20:10,111 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 361 finished: foreachPartition at streamingProcessNew.scala:50, took 0.049060 s
2017-08-18 12:20:10,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030010000 ms.0 from job set of time 1503030010000 ms
2017-08-18 12:20:10,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.112 s for time 1503030010000 ms (execution: 0.090 s)
2017-08-18 12:20:10,112 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 721 from persistence list
2017-08-18 12:20:10,112 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 721
2017-08-18 12:20:10,113 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 720 from persistence list
2017-08-18 12:20:10,113 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 720
2017-08-18 12:20:10,113 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:20:10,113 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030000000 ms
2017-08-18 12:20:15,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030015000 ms
2017-08-18 12:20:15,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030015000 ms.0 from job set of time 1503030015000 ms
2017-08-18 12:20:15,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:20:15,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 362 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:20:15,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 362 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:20:15,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:20:15,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:20:15,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 362 (MapPartitionsRDD[725] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:20:15,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_362 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:20:15,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_362_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:20:15,048 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_362_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:20:15,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 362 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:20:15,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 362 (MapPartitionsRDD[725] at map at streamingProcessNew.scala:49)
2017-08-18 12:20:15,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 362.0 with 2 tasks
2017-08-18 12:20:15,050 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 362.0 (TID 724, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:20:15,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 362.0 (TID 725, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:20:15,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 362.0 (TID 725)
2017-08-18 12:20:15,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 362.0 (TID 724)
2017-08-18 12:20:15,053 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:20:15,053 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:20:15,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 362.0 (TID 725). 714 bytes result sent to driver
2017-08-18 12:20:15,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 362.0 (TID 724). 714 bytes result sent to driver
2017-08-18 12:20:15,058 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 362.0 (TID 725) in 7 ms on localhost (1/2)
2017-08-18 12:20:15,059 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 362.0 (TID 724) in 9 ms on localhost (2/2)
2017-08-18 12:20:15,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 362 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:20:15,059 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 362.0, whose tasks have all completed, from pool 
2017-08-18 12:20:15,059 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 362 finished: foreachPartition at streamingProcessNew.scala:50, took 0.021653 s
2017-08-18 12:20:15,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030015000 ms.0 from job set of time 1503030015000 ms
2017-08-18 12:20:15,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.059 s for time 1503030015000 ms (execution: 0.041 s)
2017-08-18 12:20:15,060 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 723 from persistence list
2017-08-18 12:20:15,060 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 723
2017-08-18 12:20:15,060 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 722 from persistence list
2017-08-18 12:20:15,060 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 722
2017-08-18 12:20:15,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:20:15,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030005000 ms
2017-08-18 12:20:20,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030020000 ms
2017-08-18 12:20:20,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030020000 ms.0 from job set of time 1503030020000 ms
2017-08-18 12:20:20,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:20:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 363 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:20:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 363 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:20:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:20:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:20:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 363 (MapPartitionsRDD[727] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:20:20,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_363 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:20:20,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_363_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:20:20,043 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_363_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:20:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 363 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:20:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 363 (MapPartitionsRDD[727] at map at streamingProcessNew.scala:49)
2017-08-18 12:20:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 363.0 with 2 tasks
2017-08-18 12:20:20,046 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 363.0 (TID 726, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:20:20,047 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 363.0 (TID 727, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:20:20,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 363.0 (TID 726)
2017-08-18 12:20:20,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 363.0 (TID 727)
2017-08-18 12:20:20,049 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:20:20,049 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:20:20,052 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 363.0 (TID 726). 714 bytes result sent to driver
2017-08-18 12:20:20,052 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 363.0 (TID 727). 714 bytes result sent to driver
2017-08-18 12:20:20,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 363.0 (TID 727) in 10 ms on localhost (1/2)
2017-08-18 12:20:20,057 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 363.0 (TID 726) in 13 ms on localhost (2/2)
2017-08-18 12:20:20,057 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 363.0, whose tasks have all completed, from pool 
2017-08-18 12:20:20,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 363 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:20:20,057 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 363 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024015 s
2017-08-18 12:20:20,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030020000 ms.0 from job set of time 1503030020000 ms
2017-08-18 12:20:20,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.057 s for time 1503030020000 ms (execution: 0.039 s)
2017-08-18 12:20:20,058 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 725 from persistence list
2017-08-18 12:20:20,058 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 725
2017-08-18 12:20:20,058 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 724 from persistence list
2017-08-18 12:20:20,059 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 724
2017-08-18 12:20:20,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:20:20,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030010000 ms
2017-08-18 12:20:25,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030025000 ms
2017-08-18 12:20:25,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030025000 ms.0 from job set of time 1503030025000 ms
2017-08-18 12:20:25,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:20:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 364 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:20:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 364 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:20:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:20:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:20:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 364 (MapPartitionsRDD[729] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:20:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_364 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:20:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_364_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:20:25,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_364_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:20:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 364 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:20:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 364 (MapPartitionsRDD[729] at map at streamingProcessNew.scala:49)
2017-08-18 12:20:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 364.0 with 2 tasks
2017-08-18 12:20:25,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 364.0 (TID 728, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:20:25,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 364.0 (TID 729, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:20:25,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 364.0 (TID 729)
2017-08-18 12:20:25,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 364.0 (TID 728)
2017-08-18 12:20:25,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:20:25,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:20:25,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 364.0 (TID 728). 714 bytes result sent to driver
2017-08-18 12:20:25,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 364.0 (TID 729). 714 bytes result sent to driver
2017-08-18 12:20:25,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 364.0 (TID 728) in 15 ms on localhost (1/2)
2017-08-18 12:20:25,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 364.0 (TID 729) in 12 ms on localhost (2/2)
2017-08-18 12:20:25,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 364.0, whose tasks have all completed, from pool 
2017-08-18 12:20:25,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 364 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:20:25,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 364 finished: foreachPartition at streamingProcessNew.scala:50, took 0.029270 s
2017-08-18 12:20:25,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030025000 ms.0 from job set of time 1503030025000 ms
2017-08-18 12:20:25,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1503030025000 ms (execution: 0.059 s)
2017-08-18 12:20:25,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 727 from persistence list
2017-08-18 12:20:25,079 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 727
2017-08-18 12:20:25,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 726 from persistence list
2017-08-18 12:20:25,080 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 726
2017-08-18 12:20:25,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:20:25,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030015000 ms
2017-08-18 12:20:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030030000 ms
2017-08-18 12:20:30,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030030000 ms.0 from job set of time 1503030030000 ms
2017-08-18 12:20:30,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:20:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 365 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:20:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 365 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:20:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:20:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:20:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 365 (MapPartitionsRDD[731] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:20:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_365 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:20:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_365_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:20:30,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_365_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:20:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 365 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:20:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 365 (MapPartitionsRDD[731] at map at streamingProcessNew.scala:49)
2017-08-18 12:20:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 365.0 with 2 tasks
2017-08-18 12:20:30,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 365.0 (TID 730, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:20:30,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 365.0 (TID 731, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:20:30,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 365.0 (TID 730)
2017-08-18 12:20:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 365.0 (TID 731)
2017-08-18 12:20:30,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:20:30,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:20:30,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 365.0 (TID 731). 714 bytes result sent to driver
2017-08-18 12:20:30,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 365.0 (TID 730). 714 bytes result sent to driver
2017-08-18 12:20:30,081 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 365.0 (TID 731) in 9 ms on localhost (1/2)
2017-08-18 12:20:30,081 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 365.0 (TID 730) in 12 ms on localhost (2/2)
2017-08-18 12:20:30,081 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 365.0, whose tasks have all completed, from pool 
2017-08-18 12:20:30,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 365 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:20:30,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 365 finished: foreachPartition at streamingProcessNew.scala:50, took 0.028514 s
2017-08-18 12:20:30,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030030000 ms.0 from job set of time 1503030030000 ms
2017-08-18 12:20:30,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1503030030000 ms (execution: 0.062 s)
2017-08-18 12:20:30,082 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 729 from persistence list
2017-08-18 12:20:30,083 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 729
2017-08-18 12:20:30,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 728 from persistence list
2017-08-18 12:20:30,083 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 728
2017-08-18 12:20:30,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:20:30,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030020000 ms
2017-08-18 12:20:35,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030035000 ms
2017-08-18 12:20:35,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030035000 ms.0 from job set of time 1503030035000 ms
2017-08-18 12:20:35,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:20:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 366 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:20:35,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 366 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:20:35,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:20:35,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:20:35,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 366 (MapPartitionsRDD[733] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:20:35,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_366 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:20:35,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_366_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:20:35,086 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_366_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:20:35,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 366 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:20:35,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 366 (MapPartitionsRDD[733] at map at streamingProcessNew.scala:49)
2017-08-18 12:20:35,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 366.0 with 2 tasks
2017-08-18 12:20:35,091 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 366.0 (TID 732, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:20:35,091 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 366.0 (TID 733, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:20:35,092 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 366.0 (TID 732)
2017-08-18 12:20:35,092 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 366.0 (TID 733)
2017-08-18 12:20:35,095 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:20:35,095 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:20:35,100 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 366.0 (TID 732). 714 bytes result sent to driver
2017-08-18 12:20:35,100 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 366.0 (TID 733). 714 bytes result sent to driver
2017-08-18 12:20:35,102 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 366.0 (TID 732) in 14 ms on localhost (1/2)
2017-08-18 12:20:35,103 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 366.0 (TID 733) in 12 ms on localhost (2/2)
2017-08-18 12:20:35,103 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 366.0, whose tasks have all completed, from pool 
2017-08-18 12:20:35,103 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 366 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:20:35,104 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 366 finished: foreachPartition at streamingProcessNew.scala:50, took 0.043763 s
2017-08-18 12:20:35,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030035000 ms.0 from job set of time 1503030035000 ms
2017-08-18 12:20:35,105 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.104 s for time 1503030035000 ms (execution: 0.082 s)
2017-08-18 12:20:35,105 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 731 from persistence list
2017-08-18 12:20:35,105 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 731
2017-08-18 12:20:35,105 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 730 from persistence list
2017-08-18 12:20:35,106 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 730
2017-08-18 12:20:35,106 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:20:35,106 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030025000 ms
2017-08-18 12:20:40,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030040000 ms
2017-08-18 12:20:40,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030040000 ms.0 from job set of time 1503030040000 ms
2017-08-18 12:20:40,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:20:40,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 367 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:20:40,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 367 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:20:40,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:20:40,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:20:40,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 367 (MapPartitionsRDD[735] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:20:40,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_367 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:20:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_367_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:20:40,047 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_367_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:20:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 367 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:20:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 367 (MapPartitionsRDD[735] at map at streamingProcessNew.scala:49)
2017-08-18 12:20:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 367.0 with 2 tasks
2017-08-18 12:20:40,050 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 367.0 (TID 734, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:20:40,050 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 367.0 (TID 735, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:20:40,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 367.0 (TID 735)
2017-08-18 12:20:40,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 367.0 (TID 734)
2017-08-18 12:20:40,052 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:20:40,052 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:20:40,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 367.0 (TID 735). 714 bytes result sent to driver
2017-08-18 12:20:40,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 367.0 (TID 734). 714 bytes result sent to driver
2017-08-18 12:20:40,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 367.0 (TID 735) in 7 ms on localhost (1/2)
2017-08-18 12:20:40,057 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 367.0 (TID 734) in 9 ms on localhost (2/2)
2017-08-18 12:20:40,057 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 367.0, whose tasks have all completed, from pool 
2017-08-18 12:20:40,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 367 (foreachPartition at streamingProcessNew.scala:50) finished in 0.009 s
2017-08-18 12:20:40,058 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 367 finished: foreachPartition at streamingProcessNew.scala:50, took 0.020128 s
2017-08-18 12:20:40,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030040000 ms.0 from job set of time 1503030040000 ms
2017-08-18 12:20:40,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.058 s for time 1503030040000 ms (execution: 0.039 s)
2017-08-18 12:20:40,059 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 733 from persistence list
2017-08-18 12:20:40,059 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 733
2017-08-18 12:20:40,059 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 732 from persistence list
2017-08-18 12:20:40,059 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 732
2017-08-18 12:20:40,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:20:40,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030030000 ms
2017-08-18 12:20:45,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030045000 ms
2017-08-18 12:20:45,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030045000 ms.0 from job set of time 1503030045000 ms
2017-08-18 12:20:45,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:20:45,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 368 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:20:45,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 368 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:20:45,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:20:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:20:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 368 (MapPartitionsRDD[737] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:20:45,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_368 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:20:45,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_368_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:20:45,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_368_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:20:45,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 368 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:20:45,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 368 (MapPartitionsRDD[737] at map at streamingProcessNew.scala:49)
2017-08-18 12:20:45,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 368.0 with 2 tasks
2017-08-18 12:20:45,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 368.0 (TID 736, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:20:45,080 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 368.0 (TID 737, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:20:45,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 368.0 (TID 737)
2017-08-18 12:20:45,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 368.0 (TID 736)
2017-08-18 12:20:45,085 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:20:45,085 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:20:45,091 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 368.0 (TID 736). 714 bytes result sent to driver
2017-08-18 12:20:45,091 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 368.0 (TID 737). 714 bytes result sent to driver
2017-08-18 12:20:45,094 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 368.0 (TID 737) in 14 ms on localhost (1/2)
2017-08-18 12:20:45,095 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 368.0 (TID 736) in 20 ms on localhost (2/2)
2017-08-18 12:20:45,095 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 368.0, whose tasks have all completed, from pool 
2017-08-18 12:20:45,095 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 368 (foreachPartition at streamingProcessNew.scala:50) finished in 0.020 s
2017-08-18 12:20:45,096 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 368 finished: foreachPartition at streamingProcessNew.scala:50, took 0.040019 s
2017-08-18 12:20:45,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030045000 ms.0 from job set of time 1503030045000 ms
2017-08-18 12:20:45,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.097 s for time 1503030045000 ms (execution: 0.075 s)
2017-08-18 12:20:45,097 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 735 from persistence list
2017-08-18 12:20:45,098 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 735
2017-08-18 12:20:45,098 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 734 from persistence list
2017-08-18 12:20:45,098 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 734
2017-08-18 12:20:45,099 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:20:45,099 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030035000 ms
2017-08-18 12:20:50,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030050000 ms
2017-08-18 12:20:50,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030050000 ms.0 from job set of time 1503030050000 ms
2017-08-18 12:20:50,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:20:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 369 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:20:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 369 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:20:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:20:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:20:50,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 369 (MapPartitionsRDD[739] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:20:50,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_369 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:20:50,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_369_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:20:50,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_369_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:20:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 369 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:20:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 369 (MapPartitionsRDD[739] at map at streamingProcessNew.scala:49)
2017-08-18 12:20:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 369.0 with 2 tasks
2017-08-18 12:20:50,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 369.0 (TID 738, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:20:50,073 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 369.0 (TID 739, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:20:50,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 369.0 (TID 738)
2017-08-18 12:20:50,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 369.0 (TID 739)
2017-08-18 12:20:50,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:20:50,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:20:50,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 369.0 (TID 738). 714 bytes result sent to driver
2017-08-18 12:20:50,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 369.0 (TID 739). 714 bytes result sent to driver
2017-08-18 12:20:50,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 369.0 (TID 738) in 13 ms on localhost (1/2)
2017-08-18 12:20:50,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 369.0 (TID 739) in 10 ms on localhost (2/2)
2017-08-18 12:20:50,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 369.0, whose tasks have all completed, from pool 
2017-08-18 12:20:50,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 369 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:20:50,084 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 369 finished: foreachPartition at streamingProcessNew.scala:50, took 0.026908 s
2017-08-18 12:20:50,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030050000 ms.0 from job set of time 1503030050000 ms
2017-08-18 12:20:50,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1503030050000 ms (execution: 0.063 s)
2017-08-18 12:20:50,085 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 737 from persistence list
2017-08-18 12:20:50,086 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 737
2017-08-18 12:20:50,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 736 from persistence list
2017-08-18 12:20:50,086 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 736
2017-08-18 12:20:50,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:20:50,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030040000 ms
2017-08-18 12:20:55,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030055000 ms
2017-08-18 12:20:55,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030055000 ms.0 from job set of time 1503030055000 ms
2017-08-18 12:20:55,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:20:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 370 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:20:55,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 370 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:20:55,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:20:55,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:20:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 370 (MapPartitionsRDD[741] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:20:55,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_370 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:20:55,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_370_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:20:55,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_370_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:20:55,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 370 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:20:55,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 370 (MapPartitionsRDD[741] at map at streamingProcessNew.scala:49)
2017-08-18 12:20:55,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 370.0 with 2 tasks
2017-08-18 12:20:55,080 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 370.0 (TID 740, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:20:55,081 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 370.0 (TID 741, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:20:55,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 370.0 (TID 741)
2017-08-18 12:20:55,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 370.0 (TID 740)
2017-08-18 12:20:55,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:20:55,084 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:20:55,090 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 370.0 (TID 740). 714 bytes result sent to driver
2017-08-18 12:20:55,090 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 370.0 (TID 741). 714 bytes result sent to driver
2017-08-18 12:20:55,093 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 370.0 (TID 741) in 12 ms on localhost (1/2)
2017-08-18 12:20:55,093 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 370.0 (TID 740) in 16 ms on localhost (2/2)
2017-08-18 12:20:55,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 370 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:20:55,093 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 370.0, whose tasks have all completed, from pool 
2017-08-18 12:20:55,094 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 370 finished: foreachPartition at streamingProcessNew.scala:50, took 0.036273 s
2017-08-18 12:20:55,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030055000 ms.0 from job set of time 1503030055000 ms
2017-08-18 12:20:55,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.094 s for time 1503030055000 ms (execution: 0.074 s)
2017-08-18 12:20:55,094 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 739 from persistence list
2017-08-18 12:20:55,095 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 739
2017-08-18 12:20:55,095 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 738 from persistence list
2017-08-18 12:20:55,095 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 738
2017-08-18 12:20:55,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:20:55,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030045000 ms
2017-08-18 12:21:00,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030060000 ms
2017-08-18 12:21:00,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030060000 ms.0 from job set of time 1503030060000 ms
2017-08-18 12:21:00,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:21:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 371 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:21:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 371 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:21:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:21:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:21:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 371 (MapPartitionsRDD[743] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:21:00,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_371 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:21:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_371_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:21:00,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_371_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:21:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 371 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:21:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 371 (MapPartitionsRDD[743] at map at streamingProcessNew.scala:49)
2017-08-18 12:21:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 371.0 with 2 tasks
2017-08-18 12:21:00,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 371.0 (TID 742, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:21:00,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 371.0 (TID 743, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:21:00,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 371.0 (TID 742)
2017-08-18 12:21:00,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 371.0 (TID 743)
2017-08-18 12:21:00,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:21:00,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:21:00,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 371.0 (TID 743). 714 bytes result sent to driver
2017-08-18 12:21:00,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 371.0 (TID 742). 714 bytes result sent to driver
2017-08-18 12:21:00,062 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 371.0 (TID 743) in 8 ms on localhost (1/2)
2017-08-18 12:21:00,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 371.0 (TID 742) in 10 ms on localhost (2/2)
2017-08-18 12:21:00,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 371.0, whose tasks have all completed, from pool 
2017-08-18 12:21:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 371 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:21:00,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 371 finished: foreachPartition at streamingProcessNew.scala:50, took 0.022946 s
2017-08-18 12:21:00,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030060000 ms.0 from job set of time 1503030060000 ms
2017-08-18 12:21:00,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1503030060000 ms (execution: 0.043 s)
2017-08-18 12:21:00,063 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 741 from persistence list
2017-08-18 12:21:00,064 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 741
2017-08-18 12:21:00,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 740 from persistence list
2017-08-18 12:21:00,064 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 740
2017-08-18 12:21:00,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:21:00,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030050000 ms
2017-08-18 12:21:05,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030065000 ms
2017-08-18 12:21:05,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030065000 ms.0 from job set of time 1503030065000 ms
2017-08-18 12:21:05,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:21:05,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 372 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:21:05,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 372 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:21:05,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:21:05,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:21:05,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 372 (MapPartitionsRDD[745] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:21:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_372 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:21:05,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_372_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:21:05,085 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_372_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:21:05,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 372 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:21:05,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 372 (MapPartitionsRDD[745] at map at streamingProcessNew.scala:49)
2017-08-18 12:21:05,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 372.0 with 2 tasks
2017-08-18 12:21:05,090 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 372.0 (TID 744, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:21:05,091 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 372.0 (TID 745, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:21:05,091 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 372.0 (TID 745)
2017-08-18 12:21:05,091 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 372.0 (TID 744)
2017-08-18 12:21:05,095 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:21:05,095 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:21:05,100 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 372.0 (TID 745). 714 bytes result sent to driver
2017-08-18 12:21:05,100 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 372.0 (TID 744). 714 bytes result sent to driver
2017-08-18 12:21:05,103 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 372.0 (TID 744) in 16 ms on localhost (1/2)
2017-08-18 12:21:05,103 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 372.0 (TID 745) in 13 ms on localhost (2/2)
2017-08-18 12:21:05,104 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 372.0, whose tasks have all completed, from pool 
2017-08-18 12:21:05,104 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 372 (foreachPartition at streamingProcessNew.scala:50) finished in 0.017 s
2017-08-18 12:21:05,104 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 372 finished: foreachPartition at streamingProcessNew.scala:50, took 0.042395 s
2017-08-18 12:21:05,105 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030065000 ms.0 from job set of time 1503030065000 ms
2017-08-18 12:21:05,105 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.105 s for time 1503030065000 ms (execution: 0.084 s)
2017-08-18 12:21:05,105 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 743 from persistence list
2017-08-18 12:21:05,106 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 743
2017-08-18 12:21:05,106 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 742 from persistence list
2017-08-18 12:21:05,106 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 742
2017-08-18 12:21:05,106 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:21:05,106 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030055000 ms
2017-08-18 12:21:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030070000 ms
2017-08-18 12:21:10,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030070000 ms.0 from job set of time 1503030070000 ms
2017-08-18 12:21:10,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:21:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 373 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:21:10,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 373 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:21:10,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:21:10,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:21:10,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 373 (MapPartitionsRDD[747] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:21:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_373 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:21:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_373_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:21:10,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_373_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:21:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 373 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:21:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 373 (MapPartitionsRDD[747] at map at streamingProcessNew.scala:49)
2017-08-18 12:21:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 373.0 with 2 tasks
2017-08-18 12:21:10,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 373.0 (TID 746, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:21:10,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 373.0 (TID 747, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:21:10,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 373.0 (TID 747)
2017-08-18 12:21:10,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 373.0 (TID 746)
2017-08-18 12:21:10,063 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:21:10,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:21:10,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 373.0 (TID 746). 714 bytes result sent to driver
2017-08-18 12:21:10,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 373.0 (TID 747). 714 bytes result sent to driver
2017-08-18 12:21:10,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 373.0 (TID 746) in 12 ms on localhost (1/2)
2017-08-18 12:21:10,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 373.0 (TID 747) in 10 ms on localhost (2/2)
2017-08-18 12:21:10,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 373.0, whose tasks have all completed, from pool 
2017-08-18 12:21:10,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 373 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:21:10,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 373 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024382 s
2017-08-18 12:21:10,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030070000 ms.0 from job set of time 1503030070000 ms
2017-08-18 12:21:10,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1503030070000 ms (execution: 0.049 s)
2017-08-18 12:21:10,071 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 745 from persistence list
2017-08-18 12:21:10,072 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 745
2017-08-18 12:21:10,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 744 from persistence list
2017-08-18 12:21:10,072 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 744
2017-08-18 12:21:10,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:21:10,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030060000 ms
2017-08-18 12:21:15,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030075000 ms
2017-08-18 12:21:15,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030075000 ms.0 from job set of time 1503030075000 ms
2017-08-18 12:21:15,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:21:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 374 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:21:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 374 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:21:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:21:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:21:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 374 (MapPartitionsRDD[749] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:21:15,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_374 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:21:15,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_374_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:21:15,079 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_374_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:21:15,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 374 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:21:15,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 374 (MapPartitionsRDD[749] at map at streamingProcessNew.scala:49)
2017-08-18 12:21:15,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 374.0 with 2 tasks
2017-08-18 12:21:15,084 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 374.0 (TID 748, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:21:15,085 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 374.0 (TID 749, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:21:15,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 374.0 (TID 749)
2017-08-18 12:21:15,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 374.0 (TID 748)
2017-08-18 12:21:15,089 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:21:15,089 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:21:15,094 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 374.0 (TID 748). 714 bytes result sent to driver
2017-08-18 12:21:15,094 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 374.0 (TID 749). 714 bytes result sent to driver
2017-08-18 12:21:15,097 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 374.0 (TID 748) in 16 ms on localhost (1/2)
2017-08-18 12:21:15,097 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 374.0 (TID 749) in 13 ms on localhost (2/2)
2017-08-18 12:21:15,097 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 374.0, whose tasks have all completed, from pool 
2017-08-18 12:21:15,097 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 374 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:21:15,098 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 374 finished: foreachPartition at streamingProcessNew.scala:50, took 0.039608 s
2017-08-18 12:21:15,098 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030075000 ms.0 from job set of time 1503030075000 ms
2017-08-18 12:21:15,099 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.098 s for time 1503030075000 ms (execution: 0.077 s)
2017-08-18 12:21:15,099 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 747 from persistence list
2017-08-18 12:21:15,099 [block-manager-slave-async-thread-pool-31] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 747
2017-08-18 12:21:15,100 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 746 from persistence list
2017-08-18 12:21:15,100 [block-manager-slave-async-thread-pool-32] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 746
2017-08-18 12:21:15,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:21:15,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030065000 ms
2017-08-18 12:21:20,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030080000 ms
2017-08-18 12:21:20,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030080000 ms.0 from job set of time 1503030080000 ms
2017-08-18 12:21:20,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:21:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 375 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:21:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 375 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:21:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:21:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:21:20,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 375 (MapPartitionsRDD[751] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:21:20,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_375 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:21:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_375_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:21:20,045 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_375_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:21:20,047 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_360_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:21:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 375 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:21:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 375 (MapPartitionsRDD[751] at map at streamingProcessNew.scala:49)
2017-08-18 12:21:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 375.0 with 2 tasks
2017-08-18 12:21:20,050 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 375.0 (TID 750, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:21:20,051 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 375.0 (TID 751, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:21:20,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_361_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:21:20,052 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 375.0 (TID 750)
2017-08-18 12:21:20,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 375.0 (TID 751)
2017-08-18 12:21:20,054 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:21:20,055 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:21:20,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_362_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:21:20,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 375.0 (TID 751). 714 bytes result sent to driver
2017-08-18 12:21:20,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 375.0 (TID 750). 714 bytes result sent to driver
2017-08-18 12:21:20,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_363_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:21:20,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 375.0 (TID 751) in 12 ms on localhost (1/2)
2017-08-18 12:21:20,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 375.0 (TID 750) in 15 ms on localhost (2/2)
2017-08-18 12:21:20,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 375.0, whose tasks have all completed, from pool 
2017-08-18 12:21:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 375 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:21:20,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 375 finished: foreachPartition at streamingProcessNew.scala:50, took 0.033798 s
2017-08-18 12:21:20,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030080000 ms.0 from job set of time 1503030080000 ms
2017-08-18 12:21:20,065 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 749 from persistence list
2017-08-18 12:21:20,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1503030080000 ms (execution: 0.049 s)
2017-08-18 12:21:20,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_364_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:21:20,068 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 749
2017-08-18 12:21:20,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 748 from persistence list
2017-08-18 12:21:20,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:21:20,069 [block-manager-slave-async-thread-pool-33] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 748
2017-08-18 12:21:20,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030070000 ms
2017-08-18 12:21:20,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_365_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:21:20,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_366_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:21:20,076 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_367_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:21:20,080 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_368_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:21:20,083 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_369_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:21:20,085 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_370_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:21:20,086 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_371_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:21:20,087 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_372_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:21:20,088 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_373_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:21:20,089 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_374_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:21:25,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030085000 ms
2017-08-18 12:21:25,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030085000 ms.0 from job set of time 1503030085000 ms
2017-08-18 12:21:25,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:21:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 376 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:21:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 376 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:21:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:21:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:21:25,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 376 (MapPartitionsRDD[753] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:21:25,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_376 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:21:25,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_376_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:21:25,082 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_376_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:21:25,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 376 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:21:25,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 376 (MapPartitionsRDD[753] at map at streamingProcessNew.scala:49)
2017-08-18 12:21:25,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 376.0 with 2 tasks
2017-08-18 12:21:25,092 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 376.0 (TID 752, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:21:25,094 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 376.0 (TID 753, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:21:25,094 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 376.0 (TID 753)
2017-08-18 12:21:25,094 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 376.0 (TID 752)
2017-08-18 12:21:25,099 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:21:25,099 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:21:25,104 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 376.0 (TID 753). 714 bytes result sent to driver
2017-08-18 12:21:25,104 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 376.0 (TID 752). 714 bytes result sent to driver
2017-08-18 12:21:25,107 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 376.0 (TID 753) in 13 ms on localhost (1/2)
2017-08-18 12:21:25,107 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 376.0 (TID 752) in 19 ms on localhost (2/2)
2017-08-18 12:21:25,107 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 376.0, whose tasks have all completed, from pool 
2017-08-18 12:21:25,107 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 376 (foreachPartition at streamingProcessNew.scala:50) finished in 0.022 s
2017-08-18 12:21:25,108 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 376 finished: foreachPartition at streamingProcessNew.scala:50, took 0.047232 s
2017-08-18 12:21:25,108 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030085000 ms.0 from job set of time 1503030085000 ms
2017-08-18 12:21:25,109 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 751 from persistence list
2017-08-18 12:21:25,109 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.108 s for time 1503030085000 ms (execution: 0.086 s)
2017-08-18 12:21:25,109 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 751
2017-08-18 12:21:25,109 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 750 from persistence list
2017-08-18 12:21:25,110 [block-manager-slave-async-thread-pool-34] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 750
2017-08-18 12:21:25,110 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:21:25,110 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030075000 ms
2017-08-18 12:21:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030090000 ms
2017-08-18 12:21:30,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030090000 ms.0 from job set of time 1503030090000 ms
2017-08-18 12:21:30,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:21:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 377 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:21:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 377 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:21:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:21:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:21:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 377 (MapPartitionsRDD[755] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:21:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_377 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:21:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_377_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:21:30,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_377_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:21:30,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 377 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:21:30,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 377 (MapPartitionsRDD[755] at map at streamingProcessNew.scala:49)
2017-08-18 12:21:30,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 377.0 with 2 tasks
2017-08-18 12:21:30,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 377.0 (TID 754, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:21:30,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 377.0 (TID 755, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:21:30,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 377.0 (TID 755)
2017-08-18 12:21:30,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 377.0 (TID 754)
2017-08-18 12:21:30,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:21:30,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:21:30,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 377.0 (TID 754). 714 bytes result sent to driver
2017-08-18 12:21:30,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 377.0 (TID 755). 714 bytes result sent to driver
2017-08-18 12:21:30,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 377.0 (TID 754) in 18 ms on localhost (1/2)
2017-08-18 12:21:30,085 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 377.0 (TID 755) in 12 ms on localhost (2/2)
2017-08-18 12:21:30,086 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 377.0, whose tasks have all completed, from pool 
2017-08-18 12:21:30,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 377 (foreachPartition at streamingProcessNew.scala:50) finished in 0.019 s
2017-08-18 12:21:30,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 377 finished: foreachPartition at streamingProcessNew.scala:50, took 0.033019 s
2017-08-18 12:21:30,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030090000 ms.0 from job set of time 1503030090000 ms
2017-08-18 12:21:30,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1503030090000 ms (execution: 0.066 s)
2017-08-18 12:21:30,087 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 753 from persistence list
2017-08-18 12:21:30,088 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 753
2017-08-18 12:21:30,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 752 from persistence list
2017-08-18 12:21:30,088 [block-manager-slave-async-thread-pool-34] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 752
2017-08-18 12:21:30,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:21:30,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030080000 ms
2017-08-18 12:21:35,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030095000 ms
2017-08-18 12:21:35,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030095000 ms.0 from job set of time 1503030095000 ms
2017-08-18 12:21:35,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:21:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 378 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:21:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 378 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:21:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:21:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:21:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 378 (MapPartitionsRDD[757] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:21:35,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_378 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:21:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_378_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:21:35,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_378_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:21:35,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 378 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:21:35,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 378 (MapPartitionsRDD[757] at map at streamingProcessNew.scala:49)
2017-08-18 12:21:35,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 378.0 with 2 tasks
2017-08-18 12:21:35,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 378.0 (TID 756, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:21:35,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 378.0 (TID 757, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:21:35,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 378.0 (TID 756)
2017-08-18 12:21:35,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 378.0 (TID 757)
2017-08-18 12:21:35,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:21:35,080 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:21:35,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 378.0 (TID 757). 714 bytes result sent to driver
2017-08-18 12:21:35,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 378.0 (TID 756). 714 bytes result sent to driver
2017-08-18 12:21:35,086 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 378.0 (TID 757) in 11 ms on localhost (1/2)
2017-08-18 12:21:35,087 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 378.0 (TID 756) in 16 ms on localhost (2/2)
2017-08-18 12:21:35,087 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 378.0, whose tasks have all completed, from pool 
2017-08-18 12:21:35,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 378 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:21:35,087 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 378 finished: foreachPartition at streamingProcessNew.scala:50, took 0.029144 s
2017-08-18 12:21:35,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030095000 ms.0 from job set of time 1503030095000 ms
2017-08-18 12:21:35,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1503030095000 ms (execution: 0.068 s)
2017-08-18 12:21:35,088 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 755 from persistence list
2017-08-18 12:21:35,088 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 755
2017-08-18 12:21:35,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 754 from persistence list
2017-08-18 12:21:35,089 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 754
2017-08-18 12:21:35,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:21:35,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030085000 ms
2017-08-18 12:21:40,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030100000 ms
2017-08-18 12:21:40,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030100000 ms.0 from job set of time 1503030100000 ms
2017-08-18 12:21:40,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:21:40,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 379 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:21:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 379 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:21:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:21:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:21:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 379 (MapPartitionsRDD[759] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:21:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_379 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:21:40,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_379_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:21:40,045 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_379_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:21:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 379 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:21:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 379 (MapPartitionsRDD[759] at map at streamingProcessNew.scala:49)
2017-08-18 12:21:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 379.0 with 2 tasks
2017-08-18 12:21:40,049 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 379.0 (TID 758, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:21:40,049 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 379.0 (TID 759, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:21:40,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 379.0 (TID 759)
2017-08-18 12:21:40,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 379.0 (TID 758)
2017-08-18 12:21:40,052 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:21:40,052 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:21:40,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 379.0 (TID 758). 714 bytes result sent to driver
2017-08-18 12:21:40,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 379.0 (TID 759). 714 bytes result sent to driver
2017-08-18 12:21:40,058 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 379.0 (TID 758) in 10 ms on localhost (1/2)
2017-08-18 12:21:40,058 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 379.0 (TID 759) in 9 ms on localhost (2/2)
2017-08-18 12:21:40,058 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 379.0, whose tasks have all completed, from pool 
2017-08-18 12:21:40,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 379 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:21:40,058 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 379 finished: foreachPartition at streamingProcessNew.scala:50, took 0.020200 s
2017-08-18 12:21:40,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030100000 ms.0 from job set of time 1503030100000 ms
2017-08-18 12:21:40,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.059 s for time 1503030100000 ms (execution: 0.038 s)
2017-08-18 12:21:40,059 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 757 from persistence list
2017-08-18 12:21:40,060 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 757
2017-08-18 12:21:40,060 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 756 from persistence list
2017-08-18 12:21:40,060 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 756
2017-08-18 12:21:40,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:21:40,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030090000 ms
2017-08-18 12:21:45,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030105000 ms
2017-08-18 12:21:45,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030105000 ms.0 from job set of time 1503030105000 ms
2017-08-18 12:21:45,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:21:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 380 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:21:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 380 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:21:45,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:21:45,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:21:45,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 380 (MapPartitionsRDD[761] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:21:45,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_380 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:21:45,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_380_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:21:45,077 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_380_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:21:45,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 380 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:21:45,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 380 (MapPartitionsRDD[761] at map at streamingProcessNew.scala:49)
2017-08-18 12:21:45,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 380.0 with 2 tasks
2017-08-18 12:21:45,081 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 380.0 (TID 760, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:21:45,082 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 380.0 (TID 761, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:21:45,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 380.0 (TID 761)
2017-08-18 12:21:45,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 380.0 (TID 760)
2017-08-18 12:21:45,085 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:21:45,085 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:21:45,090 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 380.0 (TID 761). 801 bytes result sent to driver
2017-08-18 12:21:45,090 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 380.0 (TID 760). 801 bytes result sent to driver
2017-08-18 12:21:45,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 380.0 (TID 760) in 13 ms on localhost (1/2)
2017-08-18 12:21:45,093 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 380.0 (TID 761) in 12 ms on localhost (2/2)
2017-08-18 12:21:45,093 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 380.0, whose tasks have all completed, from pool 
2017-08-18 12:21:45,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 380 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:21:45,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 380 finished: foreachPartition at streamingProcessNew.scala:50, took 0.034142 s
2017-08-18 12:21:45,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030105000 ms.0 from job set of time 1503030105000 ms
2017-08-18 12:21:45,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.094 s for time 1503030105000 ms (execution: 0.072 s)
2017-08-18 12:21:45,094 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 759 from persistence list
2017-08-18 12:21:45,095 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 759
2017-08-18 12:21:45,095 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 758 from persistence list
2017-08-18 12:21:45,095 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 758
2017-08-18 12:21:45,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:21:45,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030095000 ms
2017-08-18 12:21:50,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030110000 ms
2017-08-18 12:21:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030110000 ms.0 from job set of time 1503030110000 ms
2017-08-18 12:21:50,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:21:50,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 381 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:21:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 381 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:21:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:21:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:21:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 381 (MapPartitionsRDD[763] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:21:50,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_381 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:21:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_381_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:21:50,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_381_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:21:50,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 381 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:21:50,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 381 (MapPartitionsRDD[763] at map at streamingProcessNew.scala:49)
2017-08-18 12:21:50,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 381.0 with 2 tasks
2017-08-18 12:21:50,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 381.0 (TID 762, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:21:50,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 381.0 (TID 763, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:21:50,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 381.0 (TID 762)
2017-08-18 12:21:50,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 381.0 (TID 763)
2017-08-18 12:21:50,080 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:21:50,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:21:50,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 381.0 (TID 762). 714 bytes result sent to driver
2017-08-18 12:21:50,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 381.0 (TID 763). 714 bytes result sent to driver
2017-08-18 12:21:50,086 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 381.0 (TID 762) in 14 ms on localhost (1/2)
2017-08-18 12:21:50,087 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 381.0 (TID 763) in 12 ms on localhost (2/2)
2017-08-18 12:21:50,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 381 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:21:50,087 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 381.0, whose tasks have all completed, from pool 
2017-08-18 12:21:50,087 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 381 finished: foreachPartition at streamingProcessNew.scala:50, took 0.028287 s
2017-08-18 12:21:50,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030110000 ms.0 from job set of time 1503030110000 ms
2017-08-18 12:21:50,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1503030110000 ms (execution: 0.067 s)
2017-08-18 12:21:50,088 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 761 from persistence list
2017-08-18 12:21:50,088 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 761
2017-08-18 12:21:50,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 760 from persistence list
2017-08-18 12:21:50,088 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 760
2017-08-18 12:21:50,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:21:50,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030100000 ms
2017-08-18 12:21:55,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030115000 ms
2017-08-18 12:21:55,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030115000 ms.0 from job set of time 1503030115000 ms
2017-08-18 12:21:55,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:21:55,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 382 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:21:55,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 382 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:21:55,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:21:55,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:21:55,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 382 (MapPartitionsRDD[765] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:21:55,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_382 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:21:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_382_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:21:55,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_382_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:21:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 382 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:21:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 382 (MapPartitionsRDD[765] at map at streamingProcessNew.scala:49)
2017-08-18 12:21:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 382.0 with 2 tasks
2017-08-18 12:21:55,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 382.0 (TID 764, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:21:55,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 382.0 (TID 765, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:21:55,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 382.0 (TID 765)
2017-08-18 12:21:55,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 382.0 (TID 764)
2017-08-18 12:21:55,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:21:55,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:21:55,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 382.0 (TID 765). 714 bytes result sent to driver
2017-08-18 12:21:55,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 382.0 (TID 764). 714 bytes result sent to driver
2017-08-18 12:21:55,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 382.0 (TID 764) in 15 ms on localhost (1/2)
2017-08-18 12:21:55,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 382.0 (TID 765) in 12 ms on localhost (2/2)
2017-08-18 12:21:55,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 382.0, whose tasks have all completed, from pool 
2017-08-18 12:21:55,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 382 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:21:55,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 382 finished: foreachPartition at streamingProcessNew.scala:50, took 0.029434 s
2017-08-18 12:21:55,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030115000 ms.0 from job set of time 1503030115000 ms
2017-08-18 12:21:55,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1503030115000 ms (execution: 0.057 s)
2017-08-18 12:21:55,078 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 763 from persistence list
2017-08-18 12:21:55,078 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 763
2017-08-18 12:21:55,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 762 from persistence list
2017-08-18 12:21:55,079 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 762
2017-08-18 12:21:55,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:21:55,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030105000 ms
2017-08-18 12:22:00,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030120000 ms
2017-08-18 12:22:00,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030120000 ms.0 from job set of time 1503030120000 ms
2017-08-18 12:22:00,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:22:00,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 383 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:22:00,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 383 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:22:00,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:22:00,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:22:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 383 (MapPartitionsRDD[767] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:22:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_383 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:22:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_383_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:22:00,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_383_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:22:00,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 383 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:22:00,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 383 (MapPartitionsRDD[767] at map at streamingProcessNew.scala:49)
2017-08-18 12:22:00,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 383.0 with 2 tasks
2017-08-18 12:22:00,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 383.0 (TID 766, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:22:00,060 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 383.0 (TID 767, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:22:00,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 383.0 (TID 767)
2017-08-18 12:22:00,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 383.0 (TID 766)
2017-08-18 12:22:00,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:22:00,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:22:00,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 383.0 (TID 767). 714 bytes result sent to driver
2017-08-18 12:22:00,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 383.0 (TID 766). 714 bytes result sent to driver
2017-08-18 12:22:00,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 383.0 (TID 767) in 8 ms on localhost (1/2)
2017-08-18 12:22:00,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 383.0 (TID 766) in 12 ms on localhost (2/2)
2017-08-18 12:22:00,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 383.0, whose tasks have all completed, from pool 
2017-08-18 12:22:00,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 383 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:22:00,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 383 finished: foreachPartition at streamingProcessNew.scala:50, took 0.020867 s
2017-08-18 12:22:00,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030120000 ms.0 from job set of time 1503030120000 ms
2017-08-18 12:22:00,068 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 765 from persistence list
2017-08-18 12:22:00,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1503030120000 ms (execution: 0.046 s)
2017-08-18 12:22:00,069 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 765
2017-08-18 12:22:00,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 764 from persistence list
2017-08-18 12:22:00,069 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 764
2017-08-18 12:22:00,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:22:00,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030110000 ms
2017-08-18 12:22:05,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030125000 ms
2017-08-18 12:22:05,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030125000 ms.0 from job set of time 1503030125000 ms
2017-08-18 12:22:05,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:22:05,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 384 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:22:05,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 384 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:22:05,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:22:05,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:22:05,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 384 (MapPartitionsRDD[769] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:22:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_384 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:22:05,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_384_piece0 stored as bytes in memory (estimated size 25.2 KB, free 413.3 MB)
2017-08-18 12:22:05,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_384_piece0 in memory on 192.168.31.111:60809 (size: 25.2 KB, free: 413.7 MB)
2017-08-18 12:22:05,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 384 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:22:05,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 384 (MapPartitionsRDD[769] at map at streamingProcessNew.scala:49)
2017-08-18 12:22:05,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 384.0 with 2 tasks
2017-08-18 12:22:05,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 384.0 (TID 768, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:22:05,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 384.0 (TID 769, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:22:05,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 384.0 (TID 769)
2017-08-18 12:22:05,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 384.0 (TID 768)
2017-08-18 12:22:05,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:22:05,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:22:05,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 384.0 (TID 769). 714 bytes result sent to driver
2017-08-18 12:22:05,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 384.0 (TID 768). 714 bytes result sent to driver
2017-08-18 12:22:05,079 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 384.0 (TID 769) in 11 ms on localhost (1/2)
2017-08-18 12:22:05,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 384.0 (TID 768) in 13 ms on localhost (2/2)
2017-08-18 12:22:05,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 384.0, whose tasks have all completed, from pool 
2017-08-18 12:22:05,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 384 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:22:05,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 384 finished: foreachPartition at streamingProcessNew.scala:50, took 0.028759 s
2017-08-18 12:22:05,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030125000 ms.0 from job set of time 1503030125000 ms
2017-08-18 12:22:05,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1503030125000 ms (execution: 0.059 s)
2017-08-18 12:22:05,081 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 767 from persistence list
2017-08-18 12:22:05,081 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 767
2017-08-18 12:22:05,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 766 from persistence list
2017-08-18 12:22:05,082 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 766
2017-08-18 12:22:05,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:22:05,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030115000 ms
2017-08-18 12:22:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030130000 ms
2017-08-18 12:22:10,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030130000 ms.0 from job set of time 1503030130000 ms
2017-08-18 12:22:10,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:22:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 385 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:22:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 385 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:22:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:22:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:22:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 385 (MapPartitionsRDD[771] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:22:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_385 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:22:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_385_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:22:10,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_385_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:22:10,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 385 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:22:10,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 385 (MapPartitionsRDD[771] at map at streamingProcessNew.scala:49)
2017-08-18 12:22:10,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 385.0 with 2 tasks
2017-08-18 12:22:10,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 385.0 (TID 770, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:22:10,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 385.0 (TID 771, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:22:10,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 385.0 (TID 771)
2017-08-18 12:22:10,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 385.0 (TID 770)
2017-08-18 12:22:10,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:22:10,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:22:10,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 385.0 (TID 770). 714 bytes result sent to driver
2017-08-18 12:22:10,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 385.0 (TID 771). 714 bytes result sent to driver
2017-08-18 12:22:10,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 385.0 (TID 770) in 17 ms on localhost (1/2)
2017-08-18 12:22:10,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 385.0 (TID 771) in 13 ms on localhost (2/2)
2017-08-18 12:22:10,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 385.0, whose tasks have all completed, from pool 
2017-08-18 12:22:10,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 385 (foreachPartition at streamingProcessNew.scala:50) finished in 0.018 s
2017-08-18 12:22:10,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 385 finished: foreachPartition at streamingProcessNew.scala:50, took 0.034101 s
2017-08-18 12:22:10,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030130000 ms.0 from job set of time 1503030130000 ms
2017-08-18 12:22:10,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1503030130000 ms (execution: 0.064 s)
2017-08-18 12:22:10,084 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 769 from persistence list
2017-08-18 12:22:10,085 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 769
2017-08-18 12:22:10,085 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 768 from persistence list
2017-08-18 12:22:10,085 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 768
2017-08-18 12:22:10,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:22:10,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030120000 ms
2017-08-18 12:22:15,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030135000 ms
2017-08-18 12:22:15,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030135000 ms.0 from job set of time 1503030135000 ms
2017-08-18 12:22:15,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:22:15,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 386 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:22:15,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 386 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:22:15,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:22:15,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:22:15,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 386 (MapPartitionsRDD[773] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:22:15,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_386 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:22:15,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_386_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:22:15,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_386_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:22:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 386 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:22:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 386 (MapPartitionsRDD[773] at map at streamingProcessNew.scala:49)
2017-08-18 12:22:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 386.0 with 2 tasks
2017-08-18 12:22:15,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 386.0 (TID 772, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:22:15,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 386.0 (TID 773, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:22:15,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 386.0 (TID 772)
2017-08-18 12:22:15,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 386.0 (TID 773)
2017-08-18 12:22:15,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:22:15,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:22:15,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 386.0 (TID 772). 714 bytes result sent to driver
2017-08-18 12:22:15,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 386.0 (TID 773). 714 bytes result sent to driver
2017-08-18 12:22:15,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 386.0 (TID 772) in 14 ms on localhost (1/2)
2017-08-18 12:22:15,078 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 386.0 (TID 773) in 11 ms on localhost (2/2)
2017-08-18 12:22:15,078 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 386.0, whose tasks have all completed, from pool 
2017-08-18 12:22:15,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 386 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:22:15,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 386 finished: foreachPartition at streamingProcessNew.scala:50, took 0.027394 s
2017-08-18 12:22:15,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030135000 ms.0 from job set of time 1503030135000 ms
2017-08-18 12:22:15,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1503030135000 ms (execution: 0.056 s)
2017-08-18 12:22:15,080 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 771 from persistence list
2017-08-18 12:22:15,080 [block-manager-slave-async-thread-pool-34] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 771
2017-08-18 12:22:15,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 770 from persistence list
2017-08-18 12:22:15,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:22:15,081 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 770
2017-08-18 12:22:15,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030125000 ms
2017-08-18 12:22:20,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030140000 ms
2017-08-18 12:22:20,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030140000 ms.0 from job set of time 1503030140000 ms
2017-08-18 12:22:20,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:22:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 387 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:22:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 387 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:22:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:22:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:22:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 387 (MapPartitionsRDD[775] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:22:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_387 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:22:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_387_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:22:20,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_387_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:22:20,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 387 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:22:20,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 387 (MapPartitionsRDD[775] at map at streamingProcessNew.scala:49)
2017-08-18 12:22:20,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 387.0 with 2 tasks
2017-08-18 12:22:20,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 387.0 (TID 774, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:22:20,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 387.0 (TID 775, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:22:20,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 387.0 (TID 775)
2017-08-18 12:22:20,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 387.0 (TID 774)
2017-08-18 12:22:20,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:22:20,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:22:20,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 387.0 (TID 774). 714 bytes result sent to driver
2017-08-18 12:22:20,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 387.0 (TID 775). 714 bytes result sent to driver
2017-08-18 12:22:20,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 387.0 (TID 774) in 11 ms on localhost (1/2)
2017-08-18 12:22:20,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 387.0 (TID 775) in 10 ms on localhost (2/2)
2017-08-18 12:22:20,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 387.0, whose tasks have all completed, from pool 
2017-08-18 12:22:20,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 387 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:22:20,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 387 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024987 s
2017-08-18 12:22:20,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030140000 ms.0 from job set of time 1503030140000 ms
2017-08-18 12:22:20,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1503030140000 ms (execution: 0.045 s)
2017-08-18 12:22:20,069 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 773 from persistence list
2017-08-18 12:22:20,070 [block-manager-slave-async-thread-pool-34] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 773
2017-08-18 12:22:20,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 772 from persistence list
2017-08-18 12:22:20,070 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 772
2017-08-18 12:22:20,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:22:20,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030130000 ms
2017-08-18 12:22:25,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030145000 ms
2017-08-18 12:22:25,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030145000 ms.0 from job set of time 1503030145000 ms
2017-08-18 12:22:25,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:22:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 388 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:22:25,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 388 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:22:25,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:22:25,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:22:25,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 388 (MapPartitionsRDD[777] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:22:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_388 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:22:25,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_388_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:22:25,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_388_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:22:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 388 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:22:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 388 (MapPartitionsRDD[777] at map at streamingProcessNew.scala:49)
2017-08-18 12:22:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 388.0 with 2 tasks
2017-08-18 12:22:25,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 388.0 (TID 776, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:22:25,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 388.0 (TID 777, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:22:25,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 388.0 (TID 777)
2017-08-18 12:22:25,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 388.0 (TID 776)
2017-08-18 12:22:25,063 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:22:25,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:22:25,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 388.0 (TID 776). 714 bytes result sent to driver
2017-08-18 12:22:25,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 388.0 (TID 777). 714 bytes result sent to driver
2017-08-18 12:22:25,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 388.0 (TID 776) in 11 ms on localhost (1/2)
2017-08-18 12:22:25,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 388.0 (TID 777) in 8 ms on localhost (2/2)
2017-08-18 12:22:25,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 388.0, whose tasks have all completed, from pool 
2017-08-18 12:22:25,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 388 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:22:25,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 388 finished: foreachPartition at streamingProcessNew.scala:50, took 0.023095 s
2017-08-18 12:22:25,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030145000 ms.0 from job set of time 1503030145000 ms
2017-08-18 12:22:25,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1503030145000 ms (execution: 0.047 s)
2017-08-18 12:22:25,069 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 775 from persistence list
2017-08-18 12:22:25,070 [block-manager-slave-async-thread-pool-34] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 775
2017-08-18 12:22:25,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 774 from persistence list
2017-08-18 12:22:25,070 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 774
2017-08-18 12:22:25,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:22:25,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030135000 ms
2017-08-18 12:22:30,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030150000 ms
2017-08-18 12:22:30,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030150000 ms.0 from job set of time 1503030150000 ms
2017-08-18 12:22:30,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:22:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 389 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:22:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 389 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:22:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:22:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:22:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 389 (MapPartitionsRDD[779] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:22:30,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_389 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:22:30,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_389_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:22:30,079 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_389_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:22:30,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 389 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:22:30,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 389 (MapPartitionsRDD[779] at map at streamingProcessNew.scala:49)
2017-08-18 12:22:30,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 389.0 with 2 tasks
2017-08-18 12:22:30,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 389.0 (TID 778, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:22:30,085 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 389.0 (TID 779, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:22:30,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 389.0 (TID 778)
2017-08-18 12:22:30,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 389.0 (TID 779)
2017-08-18 12:22:30,089 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:22:30,089 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:22:30,094 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 389.0 (TID 778). 714 bytes result sent to driver
2017-08-18 12:22:30,094 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 389.0 (TID 779). 714 bytes result sent to driver
2017-08-18 12:22:30,097 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 389.0 (TID 778) in 15 ms on localhost (1/2)
2017-08-18 12:22:30,097 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 389.0 (TID 779) in 13 ms on localhost (2/2)
2017-08-18 12:22:30,097 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 389.0, whose tasks have all completed, from pool 
2017-08-18 12:22:30,097 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 389 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:22:30,098 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 389 finished: foreachPartition at streamingProcessNew.scala:50, took 0.037442 s
2017-08-18 12:22:30,098 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030150000 ms.0 from job set of time 1503030150000 ms
2017-08-18 12:22:30,099 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.098 s for time 1503030150000 ms (execution: 0.076 s)
2017-08-18 12:22:30,099 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 777 from persistence list
2017-08-18 12:22:30,099 [block-manager-slave-async-thread-pool-34] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 777
2017-08-18 12:22:30,099 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 776 from persistence list
2017-08-18 12:22:30,100 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 776
2017-08-18 12:22:30,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:22:30,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030140000 ms
2017-08-18 12:22:35,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030155000 ms
2017-08-18 12:22:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030155000 ms.0 from job set of time 1503030155000 ms
2017-08-18 12:22:35,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:22:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 390 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:22:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 390 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:22:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:22:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:22:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 390 (MapPartitionsRDD[781] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:22:35,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_390 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:22:35,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_390_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:22:35,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_390_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:22:35,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 390 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:22:35,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 390 (MapPartitionsRDD[781] at map at streamingProcessNew.scala:49)
2017-08-18 12:22:35,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 390.0 with 2 tasks
2017-08-18 12:22:35,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 390.0 (TID 780, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:22:35,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 390.0 (TID 781, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:22:35,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 390.0 (TID 780)
2017-08-18 12:22:35,088 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 390.0 (TID 781)
2017-08-18 12:22:35,091 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:22:35,091 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:22:35,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_375_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:22:35,094 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_376_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:22:35,098 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 390.0 (TID 780). 787 bytes result sent to driver
2017-08-18 12:22:35,098 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 390.0 (TID 781). 714 bytes result sent to driver
2017-08-18 12:22:35,099 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_377_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:22:35,101 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_378_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:22:35,102 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 390.0 (TID 780) in 28 ms on localhost (1/2)
2017-08-18 12:22:35,102 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 390.0 (TID 781) in 26 ms on localhost (2/2)
2017-08-18 12:22:35,102 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 390.0, whose tasks have all completed, from pool 
2017-08-18 12:22:35,102 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 390 (foreachPartition at streamingProcessNew.scala:50) finished in 0.029 s
2017-08-18 12:22:35,103 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 390 finished: foreachPartition at streamingProcessNew.scala:50, took 0.043681 s
2017-08-18 12:22:35,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030155000 ms.0 from job set of time 1503030155000 ms
2017-08-18 12:22:35,105 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 779 from persistence list
2017-08-18 12:22:35,105 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.104 s for time 1503030155000 ms (execution: 0.083 s)
2017-08-18 12:22:35,105 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_379_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:22:35,106 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 779
2017-08-18 12:22:35,106 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 778 from persistence list
2017-08-18 12:22:35,106 [block-manager-slave-async-thread-pool-34] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 778
2017-08-18 12:22:35,106 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:22:35,107 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030145000 ms
2017-08-18 12:22:35,109 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_380_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:22:35,111 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_381_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:22:35,114 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_382_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:22:35,118 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_383_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:22:35,120 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_384_piece0 on 192.168.31.111:60809 in memory (size: 25.2 KB, free: 413.8 MB)
2017-08-18 12:22:35,123 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_385_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:22:35,126 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_386_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:22:35,128 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_387_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:22:35,129 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_388_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:22:35,130 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_389_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:22:40,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030160000 ms
2017-08-18 12:22:40,025 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030160000 ms.0 from job set of time 1503030160000 ms
2017-08-18 12:22:40,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:22:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 391 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:22:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 391 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:22:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:22:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:22:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 391 (MapPartitionsRDD[783] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:22:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_391 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:22:40,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_391_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:22:40,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_391_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:22:40,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 391 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:22:40,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 391 (MapPartitionsRDD[783] at map at streamingProcessNew.scala:49)
2017-08-18 12:22:40,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 391.0 with 2 tasks
2017-08-18 12:22:40,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 391.0 (TID 782, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:22:40,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 391.0 (TID 783, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:22:40,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 391.0 (TID 783)
2017-08-18 12:22:40,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 391.0 (TID 782)
2017-08-18 12:22:40,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:22:40,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:22:40,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 391.0 (TID 782). 714 bytes result sent to driver
2017-08-18 12:22:40,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 391.0 (TID 783). 714 bytes result sent to driver
2017-08-18 12:22:40,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 391.0 (TID 782) in 12 ms on localhost (1/2)
2017-08-18 12:22:40,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 391.0 (TID 783) in 9 ms on localhost (2/2)
2017-08-18 12:22:40,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 391.0, whose tasks have all completed, from pool 
2017-08-18 12:22:40,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 391 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:22:40,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 391 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025134 s
2017-08-18 12:22:40,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030160000 ms.0 from job set of time 1503030160000 ms
2017-08-18 12:22:40,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1503030160000 ms (execution: 0.052 s)
2017-08-18 12:22:40,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 781 from persistence list
2017-08-18 12:22:40,077 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 781
2017-08-18 12:22:40,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 780 from persistence list
2017-08-18 12:22:40,077 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 780
2017-08-18 12:22:40,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:22:40,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030150000 ms
2017-08-18 12:22:45,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030165000 ms
2017-08-18 12:22:45,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030165000 ms.0 from job set of time 1503030165000 ms
2017-08-18 12:22:45,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:22:45,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 392 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:22:45,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 392 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:22:45,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:22:45,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:22:45,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 392 (MapPartitionsRDD[785] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:22:45,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_392 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:22:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_392_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:22:45,053 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_392_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:22:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 392 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:22:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 392 (MapPartitionsRDD[785] at map at streamingProcessNew.scala:49)
2017-08-18 12:22:45,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 392.0 with 2 tasks
2017-08-18 12:22:45,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 392.0 (TID 784, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:22:45,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 392.0 (TID 785, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:22:45,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 392.0 (TID 784)
2017-08-18 12:22:45,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 392.0 (TID 785)
2017-08-18 12:22:45,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:22:45,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:22:45,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 392.0 (TID 784). 801 bytes result sent to driver
2017-08-18 12:22:45,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 392.0 (TID 785). 801 bytes result sent to driver
2017-08-18 12:22:45,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 392.0 (TID 785) in 14 ms on localhost (1/2)
2017-08-18 12:22:45,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 392.0 (TID 784) in 20 ms on localhost (2/2)
2017-08-18 12:22:45,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 392.0, whose tasks have all completed, from pool 
2017-08-18 12:22:45,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 392 (foreachPartition at streamingProcessNew.scala:50) finished in 0.021 s
2017-08-18 12:22:45,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 392 finished: foreachPartition at streamingProcessNew.scala:50, took 0.035516 s
2017-08-18 12:22:45,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030165000 ms.0 from job set of time 1503030165000 ms
2017-08-18 12:22:45,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1503030165000 ms (execution: 0.058 s)
2017-08-18 12:22:45,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 783 from persistence list
2017-08-18 12:22:45,077 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 783
2017-08-18 12:22:45,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 782 from persistence list
2017-08-18 12:22:45,078 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 782
2017-08-18 12:22:45,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:22:45,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030155000 ms
2017-08-18 12:22:50,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030170000 ms
2017-08-18 12:22:50,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030170000 ms.0 from job set of time 1503030170000 ms
2017-08-18 12:22:50,056 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:22:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 393 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:22:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 393 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:22:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:22:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:22:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 393 (MapPartitionsRDD[787] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:22:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_393 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:22:50,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_393_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:22:50,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_393_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:22:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 393 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:22:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 393 (MapPartitionsRDD[787] at map at streamingProcessNew.scala:49)
2017-08-18 12:22:50,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 393.0 with 2 tasks
2017-08-18 12:22:50,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 393.0 (TID 786, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:22:50,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 393.0 (TID 787, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:22:50,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 393.0 (TID 787)
2017-08-18 12:22:50,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 393.0 (TID 786)
2017-08-18 12:22:50,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:22:50,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:22:50,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 393.0 (TID 786). 714 bytes result sent to driver
2017-08-18 12:22:50,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 393.0 (TID 787). 714 bytes result sent to driver
2017-08-18 12:22:50,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 393.0 (TID 786) in 12 ms on localhost (1/2)
2017-08-18 12:22:50,081 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 393.0 (TID 787) in 9 ms on localhost (2/2)
2017-08-18 12:22:50,081 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 393.0, whose tasks have all completed, from pool 
2017-08-18 12:22:50,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 393 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:22:50,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 393 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025318 s
2017-08-18 12:22:50,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030170000 ms.0 from job set of time 1503030170000 ms
2017-08-18 12:22:50,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1503030170000 ms (execution: 0.062 s)
2017-08-18 12:22:50,082 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 785 from persistence list
2017-08-18 12:22:50,082 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 785
2017-08-18 12:22:50,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 784 from persistence list
2017-08-18 12:22:50,083 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 784
2017-08-18 12:22:50,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:22:50,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030160000 ms
2017-08-18 12:22:55,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030175000 ms
2017-08-18 12:22:55,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030175000 ms.0 from job set of time 1503030175000 ms
2017-08-18 12:22:55,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:22:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 394 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:22:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 394 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:22:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:22:55,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:22:55,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 394 (MapPartitionsRDD[789] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:22:55,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_394 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:22:55,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_394_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:22:55,081 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_394_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:22:55,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 394 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:22:55,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 394 (MapPartitionsRDD[789] at map at streamingProcessNew.scala:49)
2017-08-18 12:22:55,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 394.0 with 2 tasks
2017-08-18 12:22:55,088 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 394.0 (TID 788, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:22:55,089 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 394.0 (TID 789, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:22:55,090 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 394.0 (TID 789)
2017-08-18 12:22:55,090 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 394.0 (TID 788)
2017-08-18 12:22:55,095 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:22:55,096 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:22:55,100 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 394.0 (TID 789). 714 bytes result sent to driver
2017-08-18 12:22:55,101 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 394.0 (TID 788). 714 bytes result sent to driver
2017-08-18 12:22:55,103 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 394.0 (TID 789) in 15 ms on localhost (1/2)
2017-08-18 12:22:55,104 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 394.0 (TID 788) in 20 ms on localhost (2/2)
2017-08-18 12:22:55,104 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 394.0, whose tasks have all completed, from pool 
2017-08-18 12:22:55,104 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 394 (foreachPartition at streamingProcessNew.scala:50) finished in 0.021 s
2017-08-18 12:22:55,105 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 394 finished: foreachPartition at streamingProcessNew.scala:50, took 0.045490 s
2017-08-18 12:22:55,105 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030175000 ms.0 from job set of time 1503030175000 ms
2017-08-18 12:22:55,106 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.105 s for time 1503030175000 ms (execution: 0.082 s)
2017-08-18 12:22:55,106 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 787 from persistence list
2017-08-18 12:22:55,106 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 787
2017-08-18 12:22:55,106 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 786 from persistence list
2017-08-18 12:22:55,107 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 786
2017-08-18 12:22:55,107 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:22:55,107 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030165000 ms
2017-08-18 12:23:00,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030180000 ms
2017-08-18 12:23:00,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030180000 ms.0 from job set of time 1503030180000 ms
2017-08-18 12:23:00,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:23:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 395 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:23:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 395 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:23:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:23:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:23:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 395 (MapPartitionsRDD[791] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:23:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_395 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:23:00,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_395_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:23:00,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_395_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:23:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 395 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:23:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 395 (MapPartitionsRDD[791] at map at streamingProcessNew.scala:49)
2017-08-18 12:23:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 395.0 with 2 tasks
2017-08-18 12:23:00,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 395.0 (TID 790, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:23:00,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 395.0 (TID 791, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:23:00,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 395.0 (TID 790)
2017-08-18 12:23:00,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 395.0 (TID 791)
2017-08-18 12:23:00,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:23:00,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:23:00,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 395.0 (TID 790). 714 bytes result sent to driver
2017-08-18 12:23:00,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 395.0 (TID 791). 714 bytes result sent to driver
2017-08-18 12:23:00,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 395.0 (TID 791) in 10 ms on localhost (1/2)
2017-08-18 12:23:00,076 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 395.0 (TID 790) in 13 ms on localhost (2/2)
2017-08-18 12:23:00,076 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 395.0, whose tasks have all completed, from pool 
2017-08-18 12:23:00,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 395 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:23:00,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 395 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024729 s
2017-08-18 12:23:00,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030180000 ms.0 from job set of time 1503030180000 ms
2017-08-18 12:23:00,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1503030180000 ms (execution: 0.056 s)
2017-08-18 12:23:00,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 789 from persistence list
2017-08-18 12:23:00,077 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 789
2017-08-18 12:23:00,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 788 from persistence list
2017-08-18 12:23:00,078 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 788
2017-08-18 12:23:00,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:23:00,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030170000 ms
2017-08-18 12:23:05,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030185000 ms
2017-08-18 12:23:05,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030185000 ms.0 from job set of time 1503030185000 ms
2017-08-18 12:23:05,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:23:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 396 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:23:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 396 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:23:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:23:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:23:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 396 (MapPartitionsRDD[793] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:23:05,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_396 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:23:05,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_396_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:23:05,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_396_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:23:05,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 396 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:23:05,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 396 (MapPartitionsRDD[793] at map at streamingProcessNew.scala:49)
2017-08-18 12:23:05,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 396.0 with 2 tasks
2017-08-18 12:23:05,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 396.0 (TID 792, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:23:05,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 396.0 (TID 793, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:23:05,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 396.0 (TID 793)
2017-08-18 12:23:05,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 396.0 (TID 792)
2017-08-18 12:23:05,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:23:05,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:23:05,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 396.0 (TID 793). 714 bytes result sent to driver
2017-08-18 12:23:05,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 396.0 (TID 792). 714 bytes result sent to driver
2017-08-18 12:23:05,078 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 396.0 (TID 793) in 8 ms on localhost (1/2)
2017-08-18 12:23:05,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 396.0 (TID 792) in 11 ms on localhost (2/2)
2017-08-18 12:23:05,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 396.0, whose tasks have all completed, from pool 
2017-08-18 12:23:05,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 396 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:23:05,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 396 finished: foreachPartition at streamingProcessNew.scala:50, took 0.020223 s
2017-08-18 12:23:05,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030185000 ms.0 from job set of time 1503030185000 ms
2017-08-18 12:23:05,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1503030185000 ms (execution: 0.056 s)
2017-08-18 12:23:05,080 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 791 from persistence list
2017-08-18 12:23:05,080 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 791
2017-08-18 12:23:05,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 790 from persistence list
2017-08-18 12:23:05,080 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 790
2017-08-18 12:23:05,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:23:05,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030175000 ms
2017-08-18 12:23:10,026 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030190000 ms
2017-08-18 12:23:10,028 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030190000 ms.0 from job set of time 1503030190000 ms
2017-08-18 12:23:10,065 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:23:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 397 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:23:10,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 397 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:23:10,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:23:10,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:23:10,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 397 (MapPartitionsRDD[795] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:23:10,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_397 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:23:10,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_397_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:23:10,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_397_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:23:10,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 397 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:23:10,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 397 (MapPartitionsRDD[795] at map at streamingProcessNew.scala:49)
2017-08-18 12:23:10,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 397.0 with 2 tasks
2017-08-18 12:23:10,083 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 397.0 (TID 794, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:23:10,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 397.0 (TID 795, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:23:10,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 397.0 (TID 795)
2017-08-18 12:23:10,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 397.0 (TID 794)
2017-08-18 12:23:10,088 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:23:10,088 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:23:10,092 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 397.0 (TID 794). 714 bytes result sent to driver
2017-08-18 12:23:10,092 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 397.0 (TID 795). 714 bytes result sent to driver
2017-08-18 12:23:10,095 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 397.0 (TID 795) in 12 ms on localhost (1/2)
2017-08-18 12:23:10,095 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 397.0 (TID 794) in 15 ms on localhost (2/2)
2017-08-18 12:23:10,095 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 397.0, whose tasks have all completed, from pool 
2017-08-18 12:23:10,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 397 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:23:10,096 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 397 finished: foreachPartition at streamingProcessNew.scala:50, took 0.030161 s
2017-08-18 12:23:10,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030190000 ms.0 from job set of time 1503030190000 ms
2017-08-18 12:23:10,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.097 s for time 1503030190000 ms (execution: 0.069 s)
2017-08-18 12:23:10,097 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 793 from persistence list
2017-08-18 12:23:10,097 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 793
2017-08-18 12:23:10,097 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 792 from persistence list
2017-08-18 12:23:10,098 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 792
2017-08-18 12:23:10,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:23:10,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030180000 ms
2017-08-18 12:23:15,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030195000 ms
2017-08-18 12:23:15,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030195000 ms.0 from job set of time 1503030195000 ms
2017-08-18 12:23:15,063 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:23:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 398 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:23:15,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 398 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:23:15,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:23:15,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:23:15,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 398 (MapPartitionsRDD[797] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:23:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_398 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:23:15,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_398_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:23:15,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_398_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:23:15,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 398 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:23:15,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 398 (MapPartitionsRDD[797] at map at streamingProcessNew.scala:49)
2017-08-18 12:23:15,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 398.0 with 2 tasks
2017-08-18 12:23:15,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 398.0 (TID 796, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:23:15,078 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 398.0 (TID 797, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:23:15,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 398.0 (TID 797)
2017-08-18 12:23:15,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 398.0 (TID 796)
2017-08-18 12:23:15,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:23:15,080 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:23:15,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 398.0 (TID 796). 714 bytes result sent to driver
2017-08-18 12:23:15,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 398.0 (TID 797). 714 bytes result sent to driver
2017-08-18 12:23:15,085 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 398.0 (TID 797) in 8 ms on localhost (1/2)
2017-08-18 12:23:15,085 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 398.0 (TID 796) in 10 ms on localhost (2/2)
2017-08-18 12:23:15,085 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 398.0, whose tasks have all completed, from pool 
2017-08-18 12:23:15,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 398 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:23:15,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 398 finished: foreachPartition at streamingProcessNew.scala:50, took 0.022633 s
2017-08-18 12:23:15,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030195000 ms.0 from job set of time 1503030195000 ms
2017-08-18 12:23:15,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.086 s for time 1503030195000 ms (execution: 0.064 s)
2017-08-18 12:23:15,086 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 795 from persistence list
2017-08-18 12:23:15,087 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 795
2017-08-18 12:23:15,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 794 from persistence list
2017-08-18 12:23:15,087 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 794
2017-08-18 12:23:15,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:23:15,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030185000 ms
2017-08-18 12:23:20,024 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030200000 ms
2017-08-18 12:23:20,025 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030200000 ms.0 from job set of time 1503030200000 ms
2017-08-18 12:23:20,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:23:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 399 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:23:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 399 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:23:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:23:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:23:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 399 (MapPartitionsRDD[799] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:23:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_399 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:23:20,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_399_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:23:20,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_399_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:23:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 399 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:23:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 399 (MapPartitionsRDD[799] at map at streamingProcessNew.scala:49)
2017-08-18 12:23:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 399.0 with 2 tasks
2017-08-18 12:23:20,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 399.0 (TID 798, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:23:20,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 399.0 (TID 799, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:23:20,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 399.0 (TID 799)
2017-08-18 12:23:20,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 399.0 (TID 798)
2017-08-18 12:23:20,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:23:20,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:23:20,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 399.0 (TID 799). 714 bytes result sent to driver
2017-08-18 12:23:20,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 399.0 (TID 798). 714 bytes result sent to driver
2017-08-18 12:23:20,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 399.0 (TID 799) in 8 ms on localhost (1/2)
2017-08-18 12:23:20,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 399.0 (TID 798) in 10 ms on localhost (2/2)
2017-08-18 12:23:20,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 399 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:23:20,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 399.0, whose tasks have all completed, from pool 
2017-08-18 12:23:20,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 399 finished: foreachPartition at streamingProcessNew.scala:50, took 0.022192 s
2017-08-18 12:23:20,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030200000 ms.0 from job set of time 1503030200000 ms
2017-08-18 12:23:20,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1503030200000 ms (execution: 0.046 s)
2017-08-18 12:23:20,071 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 797 from persistence list
2017-08-18 12:23:20,071 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 797
2017-08-18 12:23:20,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 796 from persistence list
2017-08-18 12:23:20,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:23:20,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030190000 ms
2017-08-18 12:23:20,071 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 796
2017-08-18 12:23:25,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030205000 ms
2017-08-18 12:23:25,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030205000 ms.0 from job set of time 1503030205000 ms
2017-08-18 12:23:25,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:23:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 400 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:23:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 400 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:23:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:23:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:23:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 400 (MapPartitionsRDD[801] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:23:25,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_400 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:23:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_400_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:23:25,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_400_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:23:25,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 400 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:23:25,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 400 (MapPartitionsRDD[801] at map at streamingProcessNew.scala:49)
2017-08-18 12:23:25,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 400.0 with 2 tasks
2017-08-18 12:23:25,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 400.0 (TID 800, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:23:25,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 400.0 (TID 801, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:23:25,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 400.0 (TID 801)
2017-08-18 12:23:25,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 400.0 (TID 800)
2017-08-18 12:23:25,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:23:25,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:23:25,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 400.0 (TID 800). 714 bytes result sent to driver
2017-08-18 12:23:25,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 400.0 (TID 801). 714 bytes result sent to driver
2017-08-18 12:23:25,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 400.0 (TID 800) in 19 ms on localhost (1/2)
2017-08-18 12:23:25,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 400.0 (TID 801) in 16 ms on localhost (2/2)
2017-08-18 12:23:25,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 400.0, whose tasks have all completed, from pool 
2017-08-18 12:23:25,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 400 (foreachPartition at streamingProcessNew.scala:50) finished in 0.020 s
2017-08-18 12:23:25,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 400 finished: foreachPartition at streamingProcessNew.scala:50, took 0.040184 s
2017-08-18 12:23:25,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030205000 ms.0 from job set of time 1503030205000 ms
2017-08-18 12:23:25,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1503030205000 ms (execution: 0.070 s)
2017-08-18 12:23:25,090 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 799 from persistence list
2017-08-18 12:23:25,091 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 799
2017-08-18 12:23:25,091 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 798 from persistence list
2017-08-18 12:23:25,091 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 798
2017-08-18 12:23:25,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:23:25,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030195000 ms
2017-08-18 12:23:30,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030210000 ms
2017-08-18 12:23:30,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030210000 ms.0 from job set of time 1503030210000 ms
2017-08-18 12:23:30,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:23:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 401 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:23:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 401 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:23:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:23:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:23:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 401 (MapPartitionsRDD[803] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:23:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_401 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:23:30,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_401_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:23:30,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_401_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:23:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 401 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:23:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 401 (MapPartitionsRDD[803] at map at streamingProcessNew.scala:49)
2017-08-18 12:23:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 401.0 with 2 tasks
2017-08-18 12:23:30,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 401.0 (TID 802, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:23:30,073 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 401.0 (TID 803, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:23:30,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 401.0 (TID 802)
2017-08-18 12:23:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 401.0 (TID 803)
2017-08-18 12:23:30,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:23:30,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:23:30,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 401.0 (TID 802). 714 bytes result sent to driver
2017-08-18 12:23:30,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 401.0 (TID 803). 714 bytes result sent to driver
2017-08-18 12:23:30,081 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 401.0 (TID 802) in 12 ms on localhost (1/2)
2017-08-18 12:23:30,081 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 401.0 (TID 803) in 9 ms on localhost (2/2)
2017-08-18 12:23:30,081 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 401.0, whose tasks have all completed, from pool 
2017-08-18 12:23:30,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 401 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:23:30,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 401 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025995 s
2017-08-18 12:23:30,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030210000 ms.0 from job set of time 1503030210000 ms
2017-08-18 12:23:30,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1503030210000 ms (execution: 0.059 s)
2017-08-18 12:23:30,082 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 801 from persistence list
2017-08-18 12:23:30,083 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 801
2017-08-18 12:23:30,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 800 from persistence list
2017-08-18 12:23:30,083 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 800
2017-08-18 12:23:30,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:23:30,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030200000 ms
2017-08-18 12:23:35,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030215000 ms
2017-08-18 12:23:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030215000 ms.0 from job set of time 1503030215000 ms
2017-08-18 12:23:35,056 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:23:35,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 402 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:23:35,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 402 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:23:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:23:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:23:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 402 (MapPartitionsRDD[805] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:23:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_402 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:23:35,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_402_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:23:35,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_402_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:23:35,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 402 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:23:35,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 402 (MapPartitionsRDD[805] at map at streamingProcessNew.scala:49)
2017-08-18 12:23:35,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 402.0 with 2 tasks
2017-08-18 12:23:35,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 402.0 (TID 804, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:23:35,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 402.0 (TID 805, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:23:35,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 402.0 (TID 805)
2017-08-18 12:23:35,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 402.0 (TID 804)
2017-08-18 12:23:35,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:23:35,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:23:35,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 402.0 (TID 805). 714 bytes result sent to driver
2017-08-18 12:23:35,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 402.0 (TID 804). 714 bytes result sent to driver
2017-08-18 12:23:35,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 402.0 (TID 804) in 13 ms on localhost (1/2)
2017-08-18 12:23:35,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 402.0 (TID 805) in 12 ms on localhost (2/2)
2017-08-18 12:23:35,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 402.0, whose tasks have all completed, from pool 
2017-08-18 12:23:35,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 402 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:23:35,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 402 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024537 s
2017-08-18 12:23:35,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030215000 ms.0 from job set of time 1503030215000 ms
2017-08-18 12:23:35,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1503030215000 ms (execution: 0.060 s)
2017-08-18 12:23:35,081 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 803 from persistence list
2017-08-18 12:23:35,081 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 803
2017-08-18 12:23:35,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 802 from persistence list
2017-08-18 12:23:35,082 [block-manager-slave-async-thread-pool-36] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 802
2017-08-18 12:23:35,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:23:35,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030205000 ms
2017-08-18 12:23:40,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030220000 ms
2017-08-18 12:23:40,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030220000 ms.0 from job set of time 1503030220000 ms
2017-08-18 12:23:40,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:23:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 403 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:23:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 403 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:23:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:23:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:23:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 403 (MapPartitionsRDD[807] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:23:40,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_403 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:23:40,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_403_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:23:40,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_403_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:23:40,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 403 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:23:40,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 403 (MapPartitionsRDD[807] at map at streamingProcessNew.scala:49)
2017-08-18 12:23:40,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 403.0 with 2 tasks
2017-08-18 12:23:40,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 403.0 (TID 806, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:23:40,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 403.0 (TID 807, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:23:40,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 403.0 (TID 807)
2017-08-18 12:23:40,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 403.0 (TID 806)
2017-08-18 12:23:40,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:23:40,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:23:40,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 403.0 (TID 806). 714 bytes result sent to driver
2017-08-18 12:23:40,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 403.0 (TID 807). 714 bytes result sent to driver
2017-08-18 12:23:40,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 403.0 (TID 806) in 10 ms on localhost (1/2)
2017-08-18 12:23:40,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 403.0 (TID 807) in 8 ms on localhost (2/2)
2017-08-18 12:23:40,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 403.0, whose tasks have all completed, from pool 
2017-08-18 12:23:40,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 403 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:23:40,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 403 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024469 s
2017-08-18 12:23:40,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030220000 ms.0 from job set of time 1503030220000 ms
2017-08-18 12:23:40,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1503030220000 ms (execution: 0.054 s)
2017-08-18 12:23:40,076 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 805 from persistence list
2017-08-18 12:23:40,076 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 805
2017-08-18 12:23:40,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 804 from persistence list
2017-08-18 12:23:40,077 [block-manager-slave-async-thread-pool-36] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 804
2017-08-18 12:23:40,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:23:40,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030210000 ms
2017-08-18 12:23:45,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030225000 ms
2017-08-18 12:23:45,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030225000 ms.0 from job set of time 1503030225000 ms
2017-08-18 12:23:45,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:23:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 404 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:23:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 404 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:23:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:23:45,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:23:45,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 404 (MapPartitionsRDD[809] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:23:45,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_404 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:23:45,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_404_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:23:45,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_404_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:23:45,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 404 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:23:45,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 404 (MapPartitionsRDD[809] at map at streamingProcessNew.scala:49)
2017-08-18 12:23:45,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 404.0 with 2 tasks
2017-08-18 12:23:45,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 404.0 (TID 808, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:23:45,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 404.0 (TID 809, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:23:45,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 404.0 (TID 809)
2017-08-18 12:23:45,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 404.0 (TID 808)
2017-08-18 12:23:45,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:23:45,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:23:45,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 404.0 (TID 809). 714 bytes result sent to driver
2017-08-18 12:23:45,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 404.0 (TID 808). 714 bytes result sent to driver
2017-08-18 12:23:45,082 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 404.0 (TID 809) in 12 ms on localhost (1/2)
2017-08-18 12:23:45,082 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 404.0 (TID 808) in 16 ms on localhost (2/2)
2017-08-18 12:23:45,082 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 404.0, whose tasks have all completed, from pool 
2017-08-18 12:23:45,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 404 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:23:45,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 404 finished: foreachPartition at streamingProcessNew.scala:50, took 0.034928 s
2017-08-18 12:23:45,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030225000 ms.0 from job set of time 1503030225000 ms
2017-08-18 12:23:45,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1503030225000 ms (execution: 0.063 s)
2017-08-18 12:23:45,084 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 807 from persistence list
2017-08-18 12:23:45,084 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 807
2017-08-18 12:23:45,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 806 from persistence list
2017-08-18 12:23:45,084 [block-manager-slave-async-thread-pool-36] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 806
2017-08-18 12:23:45,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:23:45,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030215000 ms
2017-08-18 12:23:50,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030230000 ms
2017-08-18 12:23:50,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030230000 ms.0 from job set of time 1503030230000 ms
2017-08-18 12:23:50,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:23:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 405 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:23:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 405 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:23:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:23:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:23:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 405 (MapPartitionsRDD[811] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:23:50,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_405 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:23:50,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_405_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:23:50,089 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_405_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:23:50,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 405 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:23:50,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 405 (MapPartitionsRDD[811] at map at streamingProcessNew.scala:49)
2017-08-18 12:23:50,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 405.0 with 2 tasks
2017-08-18 12:23:50,097 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 405.0 (TID 810, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:23:50,098 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 405.0 (TID 811, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:23:50,099 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 405.0 (TID 810)
2017-08-18 12:23:50,099 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 405.0 (TID 811)
2017-08-18 12:23:50,114 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:23:50,114 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:23:50,119 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 405.0 (TID 811). 714 bytes result sent to driver
2017-08-18 12:23:50,119 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 405.0 (TID 810). 714 bytes result sent to driver
2017-08-18 12:23:50,122 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 405.0 (TID 811) in 25 ms on localhost (1/2)
2017-08-18 12:23:50,122 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 405.0 (TID 810) in 30 ms on localhost (2/2)
2017-08-18 12:23:50,122 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 405.0, whose tasks have all completed, from pool 
2017-08-18 12:23:50,122 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 405 (foreachPartition at streamingProcessNew.scala:50) finished in 0.030 s
2017-08-18 12:23:50,123 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 405 finished: foreachPartition at streamingProcessNew.scala:50, took 0.062774 s
2017-08-18 12:23:50,123 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030230000 ms.0 from job set of time 1503030230000 ms
2017-08-18 12:23:50,124 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 809 from persistence list
2017-08-18 12:23:50,124 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.123 s for time 1503030230000 ms (execution: 0.101 s)
2017-08-18 12:23:50,124 [block-manager-slave-async-thread-pool-29] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 809
2017-08-18 12:23:50,124 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 808 from persistence list
2017-08-18 12:23:50,125 [block-manager-slave-async-thread-pool-36] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 808
2017-08-18 12:23:50,125 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:23:50,125 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030220000 ms
2017-08-18 12:23:55,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030235000 ms
2017-08-18 12:23:55,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030235000 ms.0 from job set of time 1503030235000 ms
2017-08-18 12:23:55,062 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:23:55,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 406 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:23:55,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 406 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:23:55,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:23:55,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:23:55,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 406 (MapPartitionsRDD[813] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:23:55,086 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_405_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:23:55,088 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_390_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:23:55,092 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_391_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:23:55,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_406 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:23:55,099 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_392_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:23:55,102 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_393_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:23:55,104 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_406_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:23:55,104 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_406_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:23:55,105 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_394_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:23:55,106 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 406 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:23:55,106 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 406 (MapPartitionsRDD[813] at map at streamingProcessNew.scala:49)
2017-08-18 12:23:55,107 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 406.0 with 2 tasks
2017-08-18 12:23:55,113 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 406.0 (TID 812, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:23:55,113 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_395_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:23:55,114 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 406.0 (TID 813, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:23:55,115 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 406.0 (TID 812)
2017-08-18 12:23:55,115 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 406.0 (TID 813)
2017-08-18 12:23:55,118 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_396_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:23:55,122 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:23:55,122 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:23:55,122 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_397_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:23:55,126 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_398_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:23:55,128 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 406.0 (TID 813). 714 bytes result sent to driver
2017-08-18 12:23:55,128 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 406.0 (TID 812). 714 bytes result sent to driver
2017-08-18 12:23:55,130 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_399_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:23:55,134 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 406.0 (TID 813) in 21 ms on localhost (1/2)
2017-08-18 12:23:55,135 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 406.0 (TID 812) in 27 ms on localhost (2/2)
2017-08-18 12:23:55,135 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_400_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:23:55,135 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 406.0, whose tasks have all completed, from pool 
2017-08-18 12:23:55,135 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 406 (foreachPartition at streamingProcessNew.scala:50) finished in 0.028 s
2017-08-18 12:23:55,135 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 406 finished: foreachPartition at streamingProcessNew.scala:50, took 0.055977 s
2017-08-18 12:23:55,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030235000 ms.0 from job set of time 1503030235000 ms
2017-08-18 12:23:55,136 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 811 from persistence list
2017-08-18 12:23:55,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.136 s for time 1503030235000 ms (execution: 0.113 s)
2017-08-18 12:23:55,136 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_401_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:23:55,136 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 811
2017-08-18 12:23:55,137 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 810 from persistence list
2017-08-18 12:23:55,137 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:23:55,137 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030225000 ms
2017-08-18 12:23:55,137 [block-manager-slave-async-thread-pool-39] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 810
2017-08-18 12:23:55,138 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_402_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:23:55,139 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_403_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:23:55,142 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_404_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:24:00,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030240000 ms
2017-08-18 12:24:00,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030240000 ms.0 from job set of time 1503030240000 ms
2017-08-18 12:24:00,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:24:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 407 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:24:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 407 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:24:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:24:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:24:00,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 407 (MapPartitionsRDD[815] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:24:00,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_407 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:24:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_407_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:24:00,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_407_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:24:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 407 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:24:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 407 (MapPartitionsRDD[815] at map at streamingProcessNew.scala:49)
2017-08-18 12:24:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 407.0 with 2 tasks
2017-08-18 12:24:00,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 407.0 (TID 814, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:24:00,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 407.0 (TID 815, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:24:00,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 407.0 (TID 815)
2017-08-18 12:24:00,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 407.0 (TID 814)
2017-08-18 12:24:00,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:24:00,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:24:00,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 407.0 (TID 814). 714 bytes result sent to driver
2017-08-18 12:24:00,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 407.0 (TID 815). 714 bytes result sent to driver
2017-08-18 12:24:00,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 407.0 (TID 814) in 10 ms on localhost (1/2)
2017-08-18 12:24:00,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 407.0 (TID 815) in 8 ms on localhost (2/2)
2017-08-18 12:24:00,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 407.0, whose tasks have all completed, from pool 
2017-08-18 12:24:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 407 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:24:00,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 407 finished: foreachPartition at streamingProcessNew.scala:50, took 0.021861 s
2017-08-18 12:24:00,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030240000 ms.0 from job set of time 1503030240000 ms
2017-08-18 12:24:00,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1503030240000 ms (execution: 0.045 s)
2017-08-18 12:24:00,064 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 813 from persistence list
2017-08-18 12:24:00,064 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 813
2017-08-18 12:24:00,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 812 from persistence list
2017-08-18 12:24:00,065 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 812
2017-08-18 12:24:00,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:24:00,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030230000 ms
2017-08-18 12:24:05,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030245000 ms
2017-08-18 12:24:05,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030245000 ms.0 from job set of time 1503030245000 ms
2017-08-18 12:24:05,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:24:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 408 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:24:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 408 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:24:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:24:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:24:05,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 408 (MapPartitionsRDD[817] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:24:05,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_408 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:24:05,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_408_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:24:05,032 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_408_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:24:05,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 408 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:24:05,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 408 (MapPartitionsRDD[817] at map at streamingProcessNew.scala:49)
2017-08-18 12:24:05,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 408.0 with 2 tasks
2017-08-18 12:24:05,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 408.0 (TID 816, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:24:05,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 408.0 (TID 817, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:24:05,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 408.0 (TID 817)
2017-08-18 12:24:05,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 408.0 (TID 816)
2017-08-18 12:24:05,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:24:05,037 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:24:05,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 408.0 (TID 816). 714 bytes result sent to driver
2017-08-18 12:24:05,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 408.0 (TID 817). 714 bytes result sent to driver
2017-08-18 12:24:05,043 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 408.0 (TID 817) in 7 ms on localhost (1/2)
2017-08-18 12:24:05,043 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 408.0 (TID 816) in 10 ms on localhost (2/2)
2017-08-18 12:24:05,043 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 408.0, whose tasks have all completed, from pool 
2017-08-18 12:24:05,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 408 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:24:05,043 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 408 finished: foreachPartition at streamingProcessNew.scala:50, took 0.017418 s
2017-08-18 12:24:05,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030245000 ms.0 from job set of time 1503030245000 ms
2017-08-18 12:24:05,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.044 s for time 1503030245000 ms (execution: 0.030 s)
2017-08-18 12:24:05,044 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 815 from persistence list
2017-08-18 12:24:05,044 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 815
2017-08-18 12:24:05,044 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 814 from persistence list
2017-08-18 12:24:05,045 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 814
2017-08-18 12:24:05,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:24:05,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030235000 ms
2017-08-18 12:24:10,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030250000 ms
2017-08-18 12:24:10,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030250000 ms.0 from job set of time 1503030250000 ms
2017-08-18 12:24:10,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:24:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 409 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:24:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 409 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:24:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:24:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:24:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 409 (MapPartitionsRDD[819] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:24:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_409 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:24:10,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_409_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:24:10,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_409_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:24:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 409 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:24:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 409 (MapPartitionsRDD[819] at map at streamingProcessNew.scala:49)
2017-08-18 12:24:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 409.0 with 2 tasks
2017-08-18 12:24:10,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 409.0 (TID 818, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:24:10,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 409.0 (TID 819, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:24:10,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 409.0 (TID 818)
2017-08-18 12:24:10,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 409.0 (TID 819)
2017-08-18 12:24:10,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:24:10,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:24:10,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 409.0 (TID 819). 714 bytes result sent to driver
2017-08-18 12:24:10,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 409.0 (TID 818). 714 bytes result sent to driver
2017-08-18 12:24:10,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 409.0 (TID 819) in 8 ms on localhost (1/2)
2017-08-18 12:24:10,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 409.0 (TID 818) in 12 ms on localhost (2/2)
2017-08-18 12:24:10,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 409.0, whose tasks have all completed, from pool 
2017-08-18 12:24:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 409 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:24:10,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 409 finished: foreachPartition at streamingProcessNew.scala:50, took 0.023887 s
2017-08-18 12:24:10,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030250000 ms.0 from job set of time 1503030250000 ms
2017-08-18 12:24:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1503030250000 ms (execution: 0.050 s)
2017-08-18 12:24:10,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 817 from persistence list
2017-08-18 12:24:10,075 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 817
2017-08-18 12:24:10,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 816 from persistence list
2017-08-18 12:24:10,075 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 816
2017-08-18 12:24:10,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:24:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030240000 ms
2017-08-18 12:24:15,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030255000 ms
2017-08-18 12:24:15,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030255000 ms.0 from job set of time 1503030255000 ms
2017-08-18 12:24:15,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:24:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 410 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:24:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 410 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:24:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:24:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:24:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 410 (MapPartitionsRDD[821] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:24:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_410 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:24:15,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_410_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:24:15,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_410_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:24:15,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 410 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:24:15,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 410 (MapPartitionsRDD[821] at map at streamingProcessNew.scala:49)
2017-08-18 12:24:15,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 410.0 with 2 tasks
2017-08-18 12:24:15,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 410.0 (TID 820, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:24:15,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 410.0 (TID 821, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:24:15,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 410.0 (TID 821)
2017-08-18 12:24:15,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 410.0 (TID 820)
2017-08-18 12:24:15,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:24:15,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:24:15,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 410.0 (TID 821). 714 bytes result sent to driver
2017-08-18 12:24:15,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 410.0 (TID 820). 714 bytes result sent to driver
2017-08-18 12:24:15,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 410.0 (TID 821) in 9 ms on localhost (1/2)
2017-08-18 12:24:15,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 410.0 (TID 820) in 13 ms on localhost (2/2)
2017-08-18 12:24:15,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 410.0, whose tasks have all completed, from pool 
2017-08-18 12:24:15,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 410 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:24:15,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 410 finished: foreachPartition at streamingProcessNew.scala:50, took 0.027529 s
2017-08-18 12:24:15,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030255000 ms.0 from job set of time 1503030255000 ms
2017-08-18 12:24:15,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1503030255000 ms (execution: 0.060 s)
2017-08-18 12:24:15,082 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 819 from persistence list
2017-08-18 12:24:15,082 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 819
2017-08-18 12:24:15,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 818 from persistence list
2017-08-18 12:24:15,082 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 818
2017-08-18 12:24:15,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:24:15,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030245000 ms
2017-08-18 12:24:20,322 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030260000 ms
2017-08-18 12:24:20,323 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030260000 ms.0 from job set of time 1503030260000 ms
2017-08-18 12:24:20,333 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:24:20,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 411 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:24:20,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 411 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:24:20,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:24:20,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:24:20,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 411 (MapPartitionsRDD[823] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:24:20,338 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_411 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:24:20,340 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_411_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:24:20,341 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_411_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:24:20,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 411 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:24:20,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 411 (MapPartitionsRDD[823] at map at streamingProcessNew.scala:49)
2017-08-18 12:24:20,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 411.0 with 2 tasks
2017-08-18 12:24:20,344 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 411.0 (TID 822, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:24:20,345 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 411.0 (TID 823, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:24:20,345 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 411.0 (TID 823)
2017-08-18 12:24:20,345 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 411.0 (TID 822)
2017-08-18 12:24:20,347 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:24:20,347 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:24:20,350 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 411.0 (TID 822). 714 bytes result sent to driver
2017-08-18 12:24:20,350 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 411.0 (TID 823). 714 bytes result sent to driver
2017-08-18 12:24:20,352 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 411.0 (TID 823) in 8 ms on localhost (1/2)
2017-08-18 12:24:20,352 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 411.0 (TID 822) in 10 ms on localhost (2/2)
2017-08-18 12:24:20,352 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 411.0, whose tasks have all completed, from pool 
2017-08-18 12:24:20,352 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 411 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:24:20,352 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 411 finished: foreachPartition at streamingProcessNew.scala:50, took 0.018595 s
2017-08-18 12:24:20,353 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030260000 ms.0 from job set of time 1503030260000 ms
2017-08-18 12:24:20,353 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.353 s for time 1503030260000 ms (execution: 0.030 s)
2017-08-18 12:24:20,353 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 821 from persistence list
2017-08-18 12:24:20,354 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 820 from persistence list
2017-08-18 12:24:20,354 [block-manager-slave-async-thread-pool-37] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 821
2017-08-18 12:24:20,354 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 820
2017-08-18 12:24:20,354 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:24:20,354 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030250000 ms
2017-08-18 12:24:25,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030265000 ms
2017-08-18 12:24:25,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030265000 ms.0 from job set of time 1503030265000 ms
2017-08-18 12:24:25,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:24:25,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 412 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:24:25,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 412 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:24:25,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:24:25,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:24:25,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 412 (MapPartitionsRDD[825] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:24:25,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_412 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:24:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_412_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:24:25,048 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_412_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:24:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 412 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:24:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 412 (MapPartitionsRDD[825] at map at streamingProcessNew.scala:49)
2017-08-18 12:24:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 412.0 with 2 tasks
2017-08-18 12:24:25,052 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 412.0 (TID 824, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:24:25,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 412.0 (TID 825, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:24:25,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 412.0 (TID 825)
2017-08-18 12:24:25,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 412.0 (TID 824)
2017-08-18 12:24:25,055 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:24:25,055 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:24:25,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 412.0 (TID 825). 714 bytes result sent to driver
2017-08-18 12:24:25,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 412.0 (TID 824). 714 bytes result sent to driver
2017-08-18 12:24:25,061 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 412.0 (TID 825) in 8 ms on localhost (1/2)
2017-08-18 12:24:25,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 412.0 (TID 824) in 11 ms on localhost (2/2)
2017-08-18 12:24:25,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 412.0, whose tasks have all completed, from pool 
2017-08-18 12:24:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 412 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:24:25,061 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 412 finished: foreachPartition at streamingProcessNew.scala:50, took 0.020824 s
2017-08-18 12:24:25,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030265000 ms.0 from job set of time 1503030265000 ms
2017-08-18 12:24:25,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.062 s for time 1503030265000 ms (execution: 0.044 s)
2017-08-18 12:24:25,062 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 823 from persistence list
2017-08-18 12:24:25,062 [block-manager-slave-async-thread-pool-37] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 823
2017-08-18 12:24:25,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 822 from persistence list
2017-08-18 12:24:25,063 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 822
2017-08-18 12:24:25,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:24:25,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030255000 ms
2017-08-18 12:24:30,024 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030270000 ms
2017-08-18 12:24:30,025 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030270000 ms.0 from job set of time 1503030270000 ms
2017-08-18 12:24:30,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:24:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 413 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:24:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 413 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:24:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:24:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:24:30,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 413 (MapPartitionsRDD[827] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:24:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_413 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:24:30,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_413_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:24:30,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_413_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:24:30,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 413 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:24:30,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 413 (MapPartitionsRDD[827] at map at streamingProcessNew.scala:49)
2017-08-18 12:24:30,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 413.0 with 2 tasks
2017-08-18 12:24:30,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 413.0 (TID 826, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:24:30,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 413.0 (TID 827, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:24:30,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 413.0 (TID 827)
2017-08-18 12:24:30,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 413.0 (TID 826)
2017-08-18 12:24:30,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:24:30,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:24:30,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 413.0 (TID 826). 714 bytes result sent to driver
2017-08-18 12:24:30,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 413.0 (TID 827). 714 bytes result sent to driver
2017-08-18 12:24:30,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 413.0 (TID 827) in 9 ms on localhost (1/2)
2017-08-18 12:24:30,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 413.0 (TID 826) in 12 ms on localhost (2/2)
2017-08-18 12:24:30,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 413.0, whose tasks have all completed, from pool 
2017-08-18 12:24:30,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 413 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:24:30,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 413 finished: foreachPartition at streamingProcessNew.scala:50, took 0.027750 s
2017-08-18 12:24:30,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030270000 ms.0 from job set of time 1503030270000 ms
2017-08-18 12:24:30,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1503030270000 ms (execution: 0.059 s)
2017-08-18 12:24:30,083 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 825 from persistence list
2017-08-18 12:24:30,084 [block-manager-slave-async-thread-pool-37] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 825
2017-08-18 12:24:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 824 from persistence list
2017-08-18 12:24:30,084 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 824
2017-08-18 12:24:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:24:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030260000 ms
2017-08-18 12:24:35,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030275000 ms
2017-08-18 12:24:35,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030275000 ms.0 from job set of time 1503030275000 ms
2017-08-18 12:24:35,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:24:35,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 414 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:24:35,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 414 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:24:35,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:24:35,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:24:35,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 414 (MapPartitionsRDD[829] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:24:35,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_414 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:24:35,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_414_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:24:35,085 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_414_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:24:35,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 414 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:24:35,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 414 (MapPartitionsRDD[829] at map at streamingProcessNew.scala:49)
2017-08-18 12:24:35,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 414.0 with 2 tasks
2017-08-18 12:24:35,093 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 414.0 (TID 828, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:24:35,095 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 414.0 (TID 829, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:24:35,095 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 414.0 (TID 829)
2017-08-18 12:24:35,095 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 414.0 (TID 828)
2017-08-18 12:24:35,101 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:24:35,102 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:24:35,109 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 414.0 (TID 828). 714 bytes result sent to driver
2017-08-18 12:24:35,109 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 414.0 (TID 829). 714 bytes result sent to driver
2017-08-18 12:24:35,114 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 414.0 (TID 828) in 26 ms on localhost (1/2)
2017-08-18 12:24:35,115 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 414.0 (TID 829) in 21 ms on localhost (2/2)
2017-08-18 12:24:35,115 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 414.0, whose tasks have all completed, from pool 
2017-08-18 12:24:35,115 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 414 (foreachPartition at streamingProcessNew.scala:50) finished in 0.028 s
2017-08-18 12:24:35,116 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 414 finished: foreachPartition at streamingProcessNew.scala:50, took 0.054973 s
2017-08-18 12:24:35,116 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030275000 ms.0 from job set of time 1503030275000 ms
2017-08-18 12:24:35,117 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.116 s for time 1503030275000 ms (execution: 0.094 s)
2017-08-18 12:24:35,117 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 827 from persistence list
2017-08-18 12:24:35,117 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 827
2017-08-18 12:24:35,117 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 826 from persistence list
2017-08-18 12:24:35,117 [block-manager-slave-async-thread-pool-37] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 826
2017-08-18 12:24:35,118 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:24:35,118 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030265000 ms
2017-08-18 12:24:40,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030280000 ms
2017-08-18 12:24:40,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030280000 ms.0 from job set of time 1503030280000 ms
2017-08-18 12:24:40,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:24:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 415 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:24:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 415 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:24:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:24:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:24:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 415 (MapPartitionsRDD[831] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:24:40,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_415 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:24:40,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_415_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:24:40,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_415_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:24:40,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 415 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:24:40,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 415 (MapPartitionsRDD[831] at map at streamingProcessNew.scala:49)
2017-08-18 12:24:40,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 415.0 with 2 tasks
2017-08-18 12:24:40,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 415.0 (TID 830, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:24:40,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 415.0 (TID 831, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:24:40,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 415.0 (TID 831)
2017-08-18 12:24:40,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 415.0 (TID 830)
2017-08-18 12:24:40,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:24:40,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:24:40,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 415.0 (TID 831). 714 bytes result sent to driver
2017-08-18 12:24:40,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 415.0 (TID 830). 714 bytes result sent to driver
2017-08-18 12:24:40,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 415.0 (TID 831) in 10 ms on localhost (1/2)
2017-08-18 12:24:40,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 415.0 (TID 830) in 15 ms on localhost (2/2)
2017-08-18 12:24:40,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 415.0, whose tasks have all completed, from pool 
2017-08-18 12:24:40,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 415 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:24:40,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 415 finished: foreachPartition at streamingProcessNew.scala:50, took 0.031658 s
2017-08-18 12:24:40,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030280000 ms.0 from job set of time 1503030280000 ms
2017-08-18 12:24:40,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1503030280000 ms (execution: 0.062 s)
2017-08-18 12:24:40,083 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 829 from persistence list
2017-08-18 12:24:40,084 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 829
2017-08-18 12:24:40,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 828 from persistence list
2017-08-18 12:24:40,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:24:40,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030270000 ms
2017-08-18 12:24:40,085 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 828
2017-08-18 12:24:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030285000 ms
2017-08-18 12:24:45,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030285000 ms.0 from job set of time 1503030285000 ms
2017-08-18 12:24:45,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:24:45,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 416 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:24:45,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 416 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:24:45,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:24:45,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:24:45,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 416 (MapPartitionsRDD[833] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:24:45,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_416 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:24:45,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_416_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:24:45,032 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_416_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:24:45,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 416 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:24:45,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 416 (MapPartitionsRDD[833] at map at streamingProcessNew.scala:49)
2017-08-18 12:24:45,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 416.0 with 2 tasks
2017-08-18 12:24:45,035 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 416.0 (TID 832, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:24:45,035 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 416.0 (TID 833, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:24:45,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 416.0 (TID 833)
2017-08-18 12:24:45,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 416.0 (TID 832)
2017-08-18 12:24:45,037 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:24:45,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:24:45,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 416.0 (TID 833). 714 bytes result sent to driver
2017-08-18 12:24:45,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 416.0 (TID 832). 714 bytes result sent to driver
2017-08-18 12:24:45,042 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 416.0 (TID 833) in 7 ms on localhost (1/2)
2017-08-18 12:24:45,043 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 416.0 (TID 832) in 9 ms on localhost (2/2)
2017-08-18 12:24:45,043 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 416.0, whose tasks have all completed, from pool 
2017-08-18 12:24:45,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 416 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:24:45,043 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 416 finished: foreachPartition at streamingProcessNew.scala:50, took 0.018140 s
2017-08-18 12:24:45,043 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030285000 ms.0 from job set of time 1503030285000 ms
2017-08-18 12:24:45,043 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.043 s for time 1503030285000 ms (execution: 0.029 s)
2017-08-18 12:24:45,043 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 831 from persistence list
2017-08-18 12:24:45,044 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 831
2017-08-18 12:24:45,044 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 830 from persistence list
2017-08-18 12:24:45,044 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 830
2017-08-18 12:24:45,044 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:24:45,044 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030275000 ms
2017-08-18 12:24:50,024 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030290000 ms
2017-08-18 12:24:50,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030290000 ms.0 from job set of time 1503030290000 ms
2017-08-18 12:24:50,062 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:24:50,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 417 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:24:50,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 417 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:24:50,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:24:50,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:24:50,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 417 (MapPartitionsRDD[835] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:24:50,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_417 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:24:50,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_417_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:24:50,076 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_417_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:24:50,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 417 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:24:50,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 417 (MapPartitionsRDD[835] at map at streamingProcessNew.scala:49)
2017-08-18 12:24:50,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 417.0 with 2 tasks
2017-08-18 12:24:50,082 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 417.0 (TID 834, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:24:50,083 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 417.0 (TID 835, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:24:50,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 417.0 (TID 835)
2017-08-18 12:24:50,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 417.0 (TID 834)
2017-08-18 12:24:50,087 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:24:50,087 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:24:50,093 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 417.0 (TID 834). 714 bytes result sent to driver
2017-08-18 12:24:50,093 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 417.0 (TID 835). 714 bytes result sent to driver
2017-08-18 12:24:50,096 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 417.0 (TID 834) in 18 ms on localhost (1/2)
2017-08-18 12:24:50,096 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 417.0 (TID 835) in 14 ms on localhost (2/2)
2017-08-18 12:24:50,097 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 417.0, whose tasks have all completed, from pool 
2017-08-18 12:24:50,097 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 417 (foreachPartition at streamingProcessNew.scala:50) finished in 0.019 s
2017-08-18 12:24:50,097 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 417 finished: foreachPartition at streamingProcessNew.scala:50, took 0.034711 s
2017-08-18 12:24:50,098 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030290000 ms.0 from job set of time 1503030290000 ms
2017-08-18 12:24:50,098 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.098 s for time 1503030290000 ms (execution: 0.074 s)
2017-08-18 12:24:50,098 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 833 from persistence list
2017-08-18 12:24:50,099 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 833
2017-08-18 12:24:50,099 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 832 from persistence list
2017-08-18 12:24:50,100 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 832
2017-08-18 12:24:50,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:24:50,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030280000 ms
2017-08-18 12:24:55,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030295000 ms
2017-08-18 12:24:55,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030295000 ms.0 from job set of time 1503030295000 ms
2017-08-18 12:24:55,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:24:55,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 418 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:24:55,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 418 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:24:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:24:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:24:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 418 (MapPartitionsRDD[837] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:24:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_418 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:24:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_418_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:24:55,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_418_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:24:55,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 418 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:24:55,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 418 (MapPartitionsRDD[837] at map at streamingProcessNew.scala:49)
2017-08-18 12:24:55,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 418.0 with 2 tasks
2017-08-18 12:24:55,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 418.0 (TID 836, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:24:55,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 418.0 (TID 837, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:24:55,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 418.0 (TID 837)
2017-08-18 12:24:55,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 418.0 (TID 836)
2017-08-18 12:24:55,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:24:55,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:24:55,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 418.0 (TID 836). 714 bytes result sent to driver
2017-08-18 12:24:55,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 418.0 (TID 837). 714 bytes result sent to driver
2017-08-18 12:24:55,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 418.0 (TID 836) in 10 ms on localhost (1/2)
2017-08-18 12:24:55,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 418.0 (TID 837) in 8 ms on localhost (2/2)
2017-08-18 12:24:55,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 418.0, whose tasks have all completed, from pool 
2017-08-18 12:24:55,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 418 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:24:55,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 418 finished: foreachPartition at streamingProcessNew.scala:50, took 0.026693 s
2017-08-18 12:24:55,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030295000 ms.0 from job set of time 1503030295000 ms
2017-08-18 12:24:55,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1503030295000 ms (execution: 0.058 s)
2017-08-18 12:24:55,078 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 835 from persistence list
2017-08-18 12:24:55,078 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 835
2017-08-18 12:24:55,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 834 from persistence list
2017-08-18 12:24:55,078 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 834
2017-08-18 12:24:55,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:24:55,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030285000 ms
2017-08-18 12:25:00,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030300000 ms
2017-08-18 12:25:00,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030300000 ms.0 from job set of time 1503030300000 ms
2017-08-18 12:25:00,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:25:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 419 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:25:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 419 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:25:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:25:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:25:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 419 (MapPartitionsRDD[839] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:25:00,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_419 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:25:00,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_419_piece0 stored as bytes in memory (estimated size 25.2 KB, free 413.1 MB)
2017-08-18 12:25:00,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_419_piece0 in memory on 192.168.31.111:60809 (size: 25.2 KB, free: 413.6 MB)
2017-08-18 12:25:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 419 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:25:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 419 (MapPartitionsRDD[839] at map at streamingProcessNew.scala:49)
2017-08-18 12:25:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 419.0 with 2 tasks
2017-08-18 12:25:00,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 419.0 (TID 838, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:25:00,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 419.0 (TID 839, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:25:00,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 419.0 (TID 839)
2017-08-18 12:25:00,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 419.0 (TID 838)
2017-08-18 12:25:00,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:25:00,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:25:00,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 419.0 (TID 838). 714 bytes result sent to driver
2017-08-18 12:25:00,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 419.0 (TID 839). 714 bytes result sent to driver
2017-08-18 12:25:00,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 419.0 (TID 839) in 10 ms on localhost (1/2)
2017-08-18 12:25:00,080 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 419.0 (TID 838) in 14 ms on localhost (2/2)
2017-08-18 12:25:00,080 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 419.0, whose tasks have all completed, from pool 
2017-08-18 12:25:00,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 419 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:25:00,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 419 finished: foreachPartition at streamingProcessNew.scala:50, took 0.027887 s
2017-08-18 12:25:00,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030300000 ms.0 from job set of time 1503030300000 ms
2017-08-18 12:25:00,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1503030300000 ms (execution: 0.060 s)
2017-08-18 12:25:00,081 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 837 from persistence list
2017-08-18 12:25:00,082 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 837
2017-08-18 12:25:00,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 836 from persistence list
2017-08-18 12:25:00,082 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 836
2017-08-18 12:25:00,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:25:00,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030290000 ms
2017-08-18 12:25:05,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030305000 ms
2017-08-18 12:25:05,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030305000 ms.0 from job set of time 1503030305000 ms
2017-08-18 12:25:05,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:25:05,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 420 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:25:05,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 420 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:25:05,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:25:05,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:25:05,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 420 (MapPartitionsRDD[841] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:25:05,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_420 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:25:05,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_420_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:25:05,040 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_420_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:25:05,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 420 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:25:05,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 420 (MapPartitionsRDD[841] at map at streamingProcessNew.scala:49)
2017-08-18 12:25:05,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 420.0 with 2 tasks
2017-08-18 12:25:05,046 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 420.0 (TID 840, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:25:05,047 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 420.0 (TID 841, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:25:05,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 420.0 (TID 841)
2017-08-18 12:25:05,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 420.0 (TID 840)
2017-08-18 12:25:05,050 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:25:05,050 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:25:05,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 420.0 (TID 840). 714 bytes result sent to driver
2017-08-18 12:25:05,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 420.0 (TID 841). 714 bytes result sent to driver
2017-08-18 12:25:05,056 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 420.0 (TID 840) in 15 ms on localhost (1/2)
2017-08-18 12:25:05,057 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 420.0 (TID 841) in 10 ms on localhost (2/2)
2017-08-18 12:25:05,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 420 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:25:05,057 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 420.0, whose tasks have all completed, from pool 
2017-08-18 12:25:05,057 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 420 finished: foreachPartition at streamingProcessNew.scala:50, took 0.026771 s
2017-08-18 12:25:05,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030305000 ms.0 from job set of time 1503030305000 ms
2017-08-18 12:25:05,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.058 s for time 1503030305000 ms (execution: 0.044 s)
2017-08-18 12:25:05,058 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 839 from persistence list
2017-08-18 12:25:05,058 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 839
2017-08-18 12:25:05,058 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 838 from persistence list
2017-08-18 12:25:05,059 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 838
2017-08-18 12:25:05,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:25:05,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030295000 ms
2017-08-18 12:25:10,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030310000 ms
2017-08-18 12:25:10,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030310000 ms.0 from job set of time 1503030310000 ms
2017-08-18 12:25:10,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:25:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 421 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:25:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 421 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:25:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:25:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:25:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 421 (MapPartitionsRDD[843] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:25:10,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_421 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:25:10,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_421_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:25:10,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_421_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:25:10,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_406_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:25:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 421 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:25:10,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 421 (MapPartitionsRDD[843] at map at streamingProcessNew.scala:49)
2017-08-18 12:25:10,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 421.0 with 2 tasks
2017-08-18 12:25:10,080 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 421.0 (TID 842, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:25:10,082 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_407_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:25:10,082 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 421.0 (TID 843, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:25:10,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 421.0 (TID 843)
2017-08-18 12:25:10,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 421.0 (TID 842)
2017-08-18 12:25:10,086 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_408_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:25:10,088 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:25:10,088 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:25:10,090 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_409_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:25:10,095 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_410_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:25:10,097 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 421.0 (TID 843). 714 bytes result sent to driver
2017-08-18 12:25:10,097 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 421.0 (TID 842). 801 bytes result sent to driver
2017-08-18 12:25:10,099 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_411_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:25:10,100 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 421.0 (TID 843) in 20 ms on localhost (1/2)
2017-08-18 12:25:10,100 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 421.0 (TID 842) in 25 ms on localhost (2/2)
2017-08-18 12:25:10,101 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 421.0, whose tasks have all completed, from pool 
2017-08-18 12:25:10,101 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 421 (foreachPartition at streamingProcessNew.scala:50) finished in 0.026 s
2017-08-18 12:25:10,102 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_412_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:25:10,102 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 421 finished: foreachPartition at streamingProcessNew.scala:50, took 0.051844 s
2017-08-18 12:25:10,103 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030310000 ms.0 from job set of time 1503030310000 ms
2017-08-18 12:25:10,103 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.103 s for time 1503030310000 ms (execution: 0.081 s)
2017-08-18 12:25:10,103 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 841 from persistence list
2017-08-18 12:25:10,104 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 841
2017-08-18 12:25:10,105 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 840 from persistence list
2017-08-18 12:25:10,105 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_413_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:25:10,106 [block-manager-slave-async-thread-pool-40] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 840
2017-08-18 12:25:10,106 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:25:10,107 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030300000 ms
2017-08-18 12:25:10,110 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_414_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:25:10,112 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_415_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:25:10,118 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_416_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:25:10,119 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_417_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:25:10,120 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_418_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:25:10,121 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_419_piece0 on 192.168.31.111:60809 in memory (size: 25.2 KB, free: 413.9 MB)
2017-08-18 12:25:10,123 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_420_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:25:15,024 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030315000 ms
2017-08-18 12:25:15,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030315000 ms.0 from job set of time 1503030315000 ms
2017-08-18 12:25:15,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:25:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 422 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:25:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 422 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:25:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:25:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:25:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 422 (MapPartitionsRDD[845] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:25:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_422 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:25:15,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_422_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:25:15,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_422_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:25:15,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 422 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:25:15,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 422 (MapPartitionsRDD[845] at map at streamingProcessNew.scala:49)
2017-08-18 12:25:15,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 422.0 with 2 tasks
2017-08-18 12:25:15,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 422.0 (TID 844, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:25:15,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 422.0 (TID 845, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:25:15,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 422.0 (TID 845)
2017-08-18 12:25:15,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 422.0 (TID 844)
2017-08-18 12:25:15,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:25:15,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:25:15,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 422.0 (TID 845). 714 bytes result sent to driver
2017-08-18 12:25:15,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 422.0 (TID 844). 714 bytes result sent to driver
2017-08-18 12:25:15,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 422.0 (TID 844) in 12 ms on localhost (1/2)
2017-08-18 12:25:15,081 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 422.0 (TID 845) in 10 ms on localhost (2/2)
2017-08-18 12:25:15,081 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 422.0, whose tasks have all completed, from pool 
2017-08-18 12:25:15,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 422 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:25:15,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 422 finished: foreachPartition at streamingProcessNew.scala:50, took 0.026357 s
2017-08-18 12:25:15,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030315000 ms.0 from job set of time 1503030315000 ms
2017-08-18 12:25:15,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1503030315000 ms (execution: 0.057 s)
2017-08-18 12:25:15,082 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 843 from persistence list
2017-08-18 12:25:15,082 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 843
2017-08-18 12:25:15,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 842 from persistence list
2017-08-18 12:25:15,083 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 842
2017-08-18 12:25:15,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:25:15,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030305000 ms
2017-08-18 12:25:20,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030320000 ms
2017-08-18 12:25:20,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030320000 ms.0 from job set of time 1503030320000 ms
2017-08-18 12:25:20,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:25:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 423 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:25:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 423 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:25:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:25:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:25:20,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 423 (MapPartitionsRDD[847] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:25:20,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_423 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:25:20,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_423_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:25:20,087 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_423_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:25:20,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 423 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:25:20,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 423 (MapPartitionsRDD[847] at map at streamingProcessNew.scala:49)
2017-08-18 12:25:20,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 423.0 with 2 tasks
2017-08-18 12:25:20,091 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 423.0 (TID 846, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:25:20,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 423.0 (TID 847, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:25:20,092 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 423.0 (TID 846)
2017-08-18 12:25:20,092 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 423.0 (TID 847)
2017-08-18 12:25:20,096 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:25:20,096 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:25:20,100 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 423.0 (TID 846). 714 bytes result sent to driver
2017-08-18 12:25:20,100 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 423.0 (TID 847). 714 bytes result sent to driver
2017-08-18 12:25:20,103 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 423.0 (TID 846) in 14 ms on localhost (1/2)
2017-08-18 12:25:20,103 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 423.0 (TID 847) in 12 ms on localhost (2/2)
2017-08-18 12:25:20,103 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 423.0, whose tasks have all completed, from pool 
2017-08-18 12:25:20,103 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 423 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:25:20,104 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 423 finished: foreachPartition at streamingProcessNew.scala:50, took 0.042130 s
2017-08-18 12:25:20,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030320000 ms.0 from job set of time 1503030320000 ms
2017-08-18 12:25:20,105 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.104 s for time 1503030320000 ms (execution: 0.082 s)
2017-08-18 12:25:20,105 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 845 from persistence list
2017-08-18 12:25:20,105 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 845
2017-08-18 12:25:20,105 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 844 from persistence list
2017-08-18 12:25:20,106 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 844
2017-08-18 12:25:20,106 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:25:20,106 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030310000 ms
2017-08-18 12:25:25,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030325000 ms
2017-08-18 12:25:25,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030325000 ms.0 from job set of time 1503030325000 ms
2017-08-18 12:25:25,028 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:25:25,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 424 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:25:25,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 424 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:25:25,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:25:25,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:25:25,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 424 (MapPartitionsRDD[849] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:25:25,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_424 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:25:25,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_424_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:25:25,037 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_424_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:25:25,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 424 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:25:25,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 424 (MapPartitionsRDD[849] at map at streamingProcessNew.scala:49)
2017-08-18 12:25:25,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 424.0 with 2 tasks
2017-08-18 12:25:25,041 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 424.0 (TID 848, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:25:25,041 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 424.0 (TID 849, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:25:25,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 424.0 (TID 848)
2017-08-18 12:25:25,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 424.0 (TID 849)
2017-08-18 12:25:25,044 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:25:25,044 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:25:25,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 424.0 (TID 849). 714 bytes result sent to driver
2017-08-18 12:25:25,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 424.0 (TID 848). 714 bytes result sent to driver
2017-08-18 12:25:25,050 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 424.0 (TID 849) in 9 ms on localhost (1/2)
2017-08-18 12:25:25,050 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 424.0 (TID 848) in 12 ms on localhost (2/2)
2017-08-18 12:25:25,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 424.0, whose tasks have all completed, from pool 
2017-08-18 12:25:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 424 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:25:25,051 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 424 finished: foreachPartition at streamingProcessNew.scala:50, took 0.022472 s
2017-08-18 12:25:25,051 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030325000 ms.0 from job set of time 1503030325000 ms
2017-08-18 12:25:25,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.051 s for time 1503030325000 ms (execution: 0.037 s)
2017-08-18 12:25:25,052 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 847 from persistence list
2017-08-18 12:25:25,052 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 847
2017-08-18 12:25:25,052 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 846 from persistence list
2017-08-18 12:25:25,052 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 846
2017-08-18 12:25:25,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:25:25,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030315000 ms
2017-08-18 12:25:30,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030330000 ms
2017-08-18 12:25:30,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030330000 ms.0 from job set of time 1503030330000 ms
2017-08-18 12:25:30,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:25:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 425 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:25:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 425 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:25:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:25:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:25:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 425 (MapPartitionsRDD[851] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:25:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_425 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:25:30,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_425_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:25:30,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_425_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:25:30,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 425 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:25:30,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 425 (MapPartitionsRDD[851] at map at streamingProcessNew.scala:49)
2017-08-18 12:25:30,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 425.0 with 2 tasks
2017-08-18 12:25:30,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 425.0 (TID 850, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:25:30,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 425.0 (TID 851, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:25:30,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 425.0 (TID 851)
2017-08-18 12:25:30,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 425.0 (TID 850)
2017-08-18 12:25:30,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:25:30,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:25:30,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 425.0 (TID 850). 714 bytes result sent to driver
2017-08-18 12:25:30,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 425.0 (TID 851). 714 bytes result sent to driver
2017-08-18 12:25:30,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 425.0 (TID 851) in 9 ms on localhost (1/2)
2017-08-18 12:25:30,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 425.0 (TID 850) in 13 ms on localhost (2/2)
2017-08-18 12:25:30,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 425.0, whose tasks have all completed, from pool 
2017-08-18 12:25:30,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 425 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:25:30,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 425 finished: foreachPartition at streamingProcessNew.scala:50, took 0.028141 s
2017-08-18 12:25:30,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030330000 ms.0 from job set of time 1503030330000 ms
2017-08-18 12:25:30,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1503030330000 ms (execution: 0.062 s)
2017-08-18 12:25:30,084 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 849 from persistence list
2017-08-18 12:25:30,085 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 849
2017-08-18 12:25:30,085 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 848 from persistence list
2017-08-18 12:25:30,085 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 848
2017-08-18 12:25:30,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:25:30,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030320000 ms
2017-08-18 12:25:35,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030335000 ms
2017-08-18 12:25:35,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030335000 ms.0 from job set of time 1503030335000 ms
2017-08-18 12:25:35,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:25:35,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 426 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:25:35,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 426 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:25:35,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:25:35,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:25:35,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 426 (MapPartitionsRDD[853] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:25:35,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_426 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:25:35,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_426_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:25:35,086 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_426_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:25:35,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 426 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:25:35,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 426 (MapPartitionsRDD[853] at map at streamingProcessNew.scala:49)
2017-08-18 12:25:35,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 426.0 with 2 tasks
2017-08-18 12:25:35,090 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 426.0 (TID 852, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:25:35,091 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 426.0 (TID 853, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:25:35,092 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 426.0 (TID 852)
2017-08-18 12:25:35,092 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 426.0 (TID 853)
2017-08-18 12:25:35,095 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:25:35,095 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:25:35,099 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 426.0 (TID 852). 714 bytes result sent to driver
2017-08-18 12:25:35,099 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 426.0 (TID 853). 714 bytes result sent to driver
2017-08-18 12:25:35,102 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 426.0 (TID 853) in 11 ms on localhost (1/2)
2017-08-18 12:25:35,102 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 426.0 (TID 852) in 15 ms on localhost (2/2)
2017-08-18 12:25:35,103 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 426.0, whose tasks have all completed, from pool 
2017-08-18 12:25:35,103 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 426 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:25:35,103 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 426 finished: foreachPartition at streamingProcessNew.scala:50, took 0.042123 s
2017-08-18 12:25:35,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030335000 ms.0 from job set of time 1503030335000 ms
2017-08-18 12:25:35,104 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.104 s for time 1503030335000 ms (execution: 0.082 s)
2017-08-18 12:25:35,104 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 851 from persistence list
2017-08-18 12:25:35,105 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 851
2017-08-18 12:25:35,105 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 850 from persistence list
2017-08-18 12:25:35,105 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 850
2017-08-18 12:25:35,105 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:25:35,105 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030325000 ms
2017-08-18 12:25:40,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030340000 ms
2017-08-18 12:25:40,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030340000 ms.0 from job set of time 1503030340000 ms
2017-08-18 12:25:40,063 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:25:40,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 427 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:25:40,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 427 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:25:40,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:25:40,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:25:40,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 427 (MapPartitionsRDD[855] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:25:40,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_427 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:25:40,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_427_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:25:40,087 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_427_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:25:40,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 427 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:25:40,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 427 (MapPartitionsRDD[855] at map at streamingProcessNew.scala:49)
2017-08-18 12:25:40,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 427.0 with 2 tasks
2017-08-18 12:25:40,096 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 427.0 (TID 854, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:25:40,096 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 427.0 (TID 855, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:25:40,097 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 427.0 (TID 855)
2017-08-18 12:25:40,097 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 427.0 (TID 854)
2017-08-18 12:25:40,100 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:25:40,100 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:25:40,104 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 427.0 (TID 855). 714 bytes result sent to driver
2017-08-18 12:25:40,104 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 427.0 (TID 854). 714 bytes result sent to driver
2017-08-18 12:25:40,107 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 427.0 (TID 855) in 11 ms on localhost (1/2)
2017-08-18 12:25:40,108 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 427.0 (TID 854) in 18 ms on localhost (2/2)
2017-08-18 12:25:40,108 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 427 (foreachPartition at streamingProcessNew.scala:50) finished in 0.018 s
2017-08-18 12:25:40,108 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 427.0, whose tasks have all completed, from pool 
2017-08-18 12:25:40,109 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 427 finished: foreachPartition at streamingProcessNew.scala:50, took 0.045949 s
2017-08-18 12:25:40,110 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030340000 ms.0 from job set of time 1503030340000 ms
2017-08-18 12:25:40,110 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.110 s for time 1503030340000 ms (execution: 0.089 s)
2017-08-18 12:25:40,110 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 853 from persistence list
2017-08-18 12:25:40,111 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 853
2017-08-18 12:25:40,111 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 852 from persistence list
2017-08-18 12:25:40,111 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 852
2017-08-18 12:25:40,111 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:25:40,112 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030330000 ms
2017-08-18 12:25:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030345000 ms
2017-08-18 12:25:45,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030345000 ms.0 from job set of time 1503030345000 ms
2017-08-18 12:25:45,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:25:45,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 428 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:25:45,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 428 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:25:45,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:25:45,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:25:45,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 428 (MapPartitionsRDD[857] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:25:45,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_428 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:25:45,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_428_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:25:45,036 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_428_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:25:45,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 428 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:25:45,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 428 (MapPartitionsRDD[857] at map at streamingProcessNew.scala:49)
2017-08-18 12:25:45,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 428.0 with 2 tasks
2017-08-18 12:25:45,040 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 428.0 (TID 856, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:25:45,040 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 428.0 (TID 857, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:25:45,040 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 428.0 (TID 857)
2017-08-18 12:25:45,040 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 428.0 (TID 856)
2017-08-18 12:25:45,043 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:25:45,043 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:25:45,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 428.0 (TID 856). 714 bytes result sent to driver
2017-08-18 12:25:45,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 428.0 (TID 857). 714 bytes result sent to driver
2017-08-18 12:25:45,048 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 428.0 (TID 856) in 11 ms on localhost (1/2)
2017-08-18 12:25:45,048 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 428.0 (TID 857) in 8 ms on localhost (2/2)
2017-08-18 12:25:45,049 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 428.0, whose tasks have all completed, from pool 
2017-08-18 12:25:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 428 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:25:45,049 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 428 finished: foreachPartition at streamingProcessNew.scala:50, took 0.021145 s
2017-08-18 12:25:45,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030345000 ms.0 from job set of time 1503030345000 ms
2017-08-18 12:25:45,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.049 s for time 1503030345000 ms (execution: 0.035 s)
2017-08-18 12:25:45,049 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 855 from persistence list
2017-08-18 12:25:45,050 [block-manager-slave-async-thread-pool-37] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 855
2017-08-18 12:25:45,050 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 854 from persistence list
2017-08-18 12:25:45,050 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 854
2017-08-18 12:25:45,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:25:45,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030335000 ms
2017-08-18 12:25:50,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030350000 ms
2017-08-18 12:25:50,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030350000 ms.0 from job set of time 1503030350000 ms
2017-08-18 12:25:50,064 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:25:50,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 429 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:25:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 429 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:25:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:25:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:25:50,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 429 (MapPartitionsRDD[859] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:25:50,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_429 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:25:50,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_429_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:25:50,087 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_429_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:25:50,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 429 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:25:50,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 429 (MapPartitionsRDD[859] at map at streamingProcessNew.scala:49)
2017-08-18 12:25:50,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 429.0 with 2 tasks
2017-08-18 12:25:50,093 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 429.0 (TID 858, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:25:50,094 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 429.0 (TID 859, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:25:50,094 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 429.0 (TID 858)
2017-08-18 12:25:50,094 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 429.0 (TID 859)
2017-08-18 12:25:50,098 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:25:50,098 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:25:50,104 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 429.0 (TID 859). 714 bytes result sent to driver
2017-08-18 12:25:50,104 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 429.0 (TID 858). 714 bytes result sent to driver
2017-08-18 12:25:50,107 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 429.0 (TID 858) in 18 ms on localhost (1/2)
2017-08-18 12:25:50,107 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 429.0 (TID 859) in 14 ms on localhost (2/2)
2017-08-18 12:25:50,107 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 429.0, whose tasks have all completed, from pool 
2017-08-18 12:25:50,108 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 429 (foreachPartition at streamingProcessNew.scala:50) finished in 0.019 s
2017-08-18 12:25:50,109 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 429 finished: foreachPartition at streamingProcessNew.scala:50, took 0.043668 s
2017-08-18 12:25:50,109 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030350000 ms.0 from job set of time 1503030350000 ms
2017-08-18 12:25:50,110 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.109 s for time 1503030350000 ms (execution: 0.086 s)
2017-08-18 12:25:50,110 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 857 from persistence list
2017-08-18 12:25:50,110 [block-manager-slave-async-thread-pool-37] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 857
2017-08-18 12:25:50,110 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 856 from persistence list
2017-08-18 12:25:50,111 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 856
2017-08-18 12:25:50,111 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:25:50,111 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030340000 ms
2017-08-18 12:25:55,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030355000 ms
2017-08-18 12:25:55,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030355000 ms.0 from job set of time 1503030355000 ms
2017-08-18 12:25:55,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:25:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 430 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:25:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 430 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:25:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:25:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:25:55,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 430 (MapPartitionsRDD[861] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:25:55,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_430 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:25:55,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_430_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:25:55,077 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_430_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:25:55,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 430 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:25:55,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 430 (MapPartitionsRDD[861] at map at streamingProcessNew.scala:49)
2017-08-18 12:25:55,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 430.0 with 2 tasks
2017-08-18 12:25:55,083 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 430.0 (TID 860, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:25:55,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 430.0 (TID 861, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:25:55,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 430.0 (TID 861)
2017-08-18 12:25:55,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 430.0 (TID 860)
2017-08-18 12:25:55,088 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:25:55,089 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:25:55,094 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 430.0 (TID 861). 714 bytes result sent to driver
2017-08-18 12:25:55,094 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 430.0 (TID 860). 714 bytes result sent to driver
2017-08-18 12:25:55,097 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 430.0 (TID 861) in 13 ms on localhost (1/2)
2017-08-18 12:25:55,097 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 430.0 (TID 860) in 18 ms on localhost (2/2)
2017-08-18 12:25:55,097 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 430.0, whose tasks have all completed, from pool 
2017-08-18 12:25:55,097 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 430 (foreachPartition at streamingProcessNew.scala:50) finished in 0.018 s
2017-08-18 12:25:55,098 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 430 finished: foreachPartition at streamingProcessNew.scala:50, took 0.035966 s
2017-08-18 12:25:55,098 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030355000 ms.0 from job set of time 1503030355000 ms
2017-08-18 12:25:55,099 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 859 from persistence list
2017-08-18 12:25:55,099 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.098 s for time 1503030355000 ms (execution: 0.077 s)
2017-08-18 12:25:55,099 [block-manager-slave-async-thread-pool-37] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 859
2017-08-18 12:25:55,099 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 858 from persistence list
2017-08-18 12:25:55,099 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 858
2017-08-18 12:25:55,099 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:25:55,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030345000 ms
2017-08-18 12:26:00,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030360000 ms
2017-08-18 12:26:00,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030360000 ms.0 from job set of time 1503030360000 ms
2017-08-18 12:26:00,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:26:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 431 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:26:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 431 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:26:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:26:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:26:00,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 431 (MapPartitionsRDD[863] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:26:00,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_431 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:26:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_431_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:26:00,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_431_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:26:00,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 431 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:26:00,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 431 (MapPartitionsRDD[863] at map at streamingProcessNew.scala:49)
2017-08-18 12:26:00,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 431.0 with 2 tasks
2017-08-18 12:26:00,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 431.0 (TID 862, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:26:00,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 431.0 (TID 863, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:26:00,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 431.0 (TID 863)
2017-08-18 12:26:00,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 431.0 (TID 862)
2017-08-18 12:26:00,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:26:00,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:26:00,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 431.0 (TID 863). 714 bytes result sent to driver
2017-08-18 12:26:00,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 431.0 (TID 862). 714 bytes result sent to driver
2017-08-18 12:26:00,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 431.0 (TID 863) in 12 ms on localhost (1/2)
2017-08-18 12:26:00,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 431.0 (TID 862) in 17 ms on localhost (2/2)
2017-08-18 12:26:00,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 431.0, whose tasks have all completed, from pool 
2017-08-18 12:26:00,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 431 (foreachPartition at streamingProcessNew.scala:50) finished in 0.017 s
2017-08-18 12:26:00,084 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 431 finished: foreachPartition at streamingProcessNew.scala:50, took 0.031316 s
2017-08-18 12:26:00,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030360000 ms.0 from job set of time 1503030360000 ms
2017-08-18 12:26:00,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1503030360000 ms (execution: 0.064 s)
2017-08-18 12:26:00,085 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 861 from persistence list
2017-08-18 12:26:00,086 [block-manager-slave-async-thread-pool-37] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 861
2017-08-18 12:26:00,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 860 from persistence list
2017-08-18 12:26:00,086 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 860
2017-08-18 12:26:00,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:26:00,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030350000 ms
2017-08-18 12:26:05,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030365000 ms
2017-08-18 12:26:05,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030365000 ms.0 from job set of time 1503030365000 ms
2017-08-18 12:26:05,034 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:26:05,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 432 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:26:05,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 432 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:26:05,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:26:05,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:26:05,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 432 (MapPartitionsRDD[865] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:26:05,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_432 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:26:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_432_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:26:05,045 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_432_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:26:05,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 432 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:26:05,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 432 (MapPartitionsRDD[865] at map at streamingProcessNew.scala:49)
2017-08-18 12:26:05,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 432.0 with 2 tasks
2017-08-18 12:26:05,052 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 432.0 (TID 864, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:26:05,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 432.0 (TID 865, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:26:05,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 432.0 (TID 865)
2017-08-18 12:26:05,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 432.0 (TID 864)
2017-08-18 12:26:05,056 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:26:05,056 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:26:05,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 432.0 (TID 865). 714 bytes result sent to driver
2017-08-18 12:26:05,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 432.0 (TID 864). 714 bytes result sent to driver
2017-08-18 12:26:05,061 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 432.0 (TID 865) in 8 ms on localhost (1/2)
2017-08-18 12:26:05,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 432.0 (TID 864) in 14 ms on localhost (2/2)
2017-08-18 12:26:05,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 432.0, whose tasks have all completed, from pool 
2017-08-18 12:26:05,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 432 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:26:05,061 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 432 finished: foreachPartition at streamingProcessNew.scala:50, took 0.027142 s
2017-08-18 12:26:05,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030365000 ms.0 from job set of time 1503030365000 ms
2017-08-18 12:26:05,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.062 s for time 1503030365000 ms (execution: 0.047 s)
2017-08-18 12:26:05,062 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 863 from persistence list
2017-08-18 12:26:05,062 [block-manager-slave-async-thread-pool-37] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 863
2017-08-18 12:26:05,062 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 862 from persistence list
2017-08-18 12:26:05,063 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 862
2017-08-18 12:26:05,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:26:05,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030355000 ms
2017-08-18 12:26:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030370000 ms
2017-08-18 12:26:10,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030370000 ms.0 from job set of time 1503030370000 ms
2017-08-18 12:26:10,064 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:26:10,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 433 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:26:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 433 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:26:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:26:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:26:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 433 (MapPartitionsRDD[867] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:26:10,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_433 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:26:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_433_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:26:10,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_433_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:26:10,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 433 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:26:10,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 433 (MapPartitionsRDD[867] at map at streamingProcessNew.scala:49)
2017-08-18 12:26:10,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 433.0 with 2 tasks
2017-08-18 12:26:10,082 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 433.0 (TID 866, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:26:10,083 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 433.0 (TID 867, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:26:10,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 433.0 (TID 866)
2017-08-18 12:26:10,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 433.0 (TID 867)
2017-08-18 12:26:10,086 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:26:10,086 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:26:10,091 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 433.0 (TID 867). 714 bytes result sent to driver
2017-08-18 12:26:10,091 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 433.0 (TID 866). 714 bytes result sent to driver
2017-08-18 12:26:10,094 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 433.0 (TID 867) in 12 ms on localhost (1/2)
2017-08-18 12:26:10,095 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 433.0 (TID 866) in 15 ms on localhost (2/2)
2017-08-18 12:26:10,095 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 433.0, whose tasks have all completed, from pool 
2017-08-18 12:26:10,095 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 433 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:26:10,095 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 433 finished: foreachPartition at streamingProcessNew.scala:50, took 0.031204 s
2017-08-18 12:26:10,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030370000 ms.0 from job set of time 1503030370000 ms
2017-08-18 12:26:10,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.096 s for time 1503030370000 ms (execution: 0.075 s)
2017-08-18 12:26:10,096 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 865 from persistence list
2017-08-18 12:26:10,096 [block-manager-slave-async-thread-pool-37] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 865
2017-08-18 12:26:10,096 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 864 from persistence list
2017-08-18 12:26:10,097 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 864
2017-08-18 12:26:10,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:26:10,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030360000 ms
2017-08-18 12:26:15,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030375000 ms
2017-08-18 12:26:15,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030375000 ms.0 from job set of time 1503030375000 ms
2017-08-18 12:26:15,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:26:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 434 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:26:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 434 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:26:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:26:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:26:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 434 (MapPartitionsRDD[869] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:26:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_434 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:26:15,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_434_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:26:15,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_434_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:26:15,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 434 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:26:15,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 434 (MapPartitionsRDD[869] at map at streamingProcessNew.scala:49)
2017-08-18 12:26:15,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 434.0 with 2 tasks
2017-08-18 12:26:15,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 434.0 (TID 868, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:26:15,076 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 434.0 (TID 869, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:26:15,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 434.0 (TID 868)
2017-08-18 12:26:15,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 434.0 (TID 869)
2017-08-18 12:26:15,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:26:15,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:26:15,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 434.0 (TID 869). 714 bytes result sent to driver
2017-08-18 12:26:15,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 434.0 (TID 868). 714 bytes result sent to driver
2017-08-18 12:26:15,083 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 434.0 (TID 869) in 8 ms on localhost (1/2)
2017-08-18 12:26:15,083 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 434.0 (TID 868) in 14 ms on localhost (2/2)
2017-08-18 12:26:15,083 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 434.0, whose tasks have all completed, from pool 
2017-08-18 12:26:15,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 434 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:26:15,084 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 434 finished: foreachPartition at streamingProcessNew.scala:50, took 0.029067 s
2017-08-18 12:26:15,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030375000 ms.0 from job set of time 1503030375000 ms
2017-08-18 12:26:15,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1503030375000 ms (execution: 0.061 s)
2017-08-18 12:26:15,084 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 867 from persistence list
2017-08-18 12:26:15,085 [block-manager-slave-async-thread-pool-37] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 867
2017-08-18 12:26:15,085 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 866 from persistence list
2017-08-18 12:26:15,085 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 866
2017-08-18 12:26:15,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:26:15,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030365000 ms
2017-08-18 12:26:20,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030380000 ms
2017-08-18 12:26:20,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030380000 ms.0 from job set of time 1503030380000 ms
2017-08-18 12:26:20,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:26:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 435 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:26:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 435 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:26:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:26:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:26:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 435 (MapPartitionsRDD[871] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:26:20,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_435 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:26:20,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_435_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:26:20,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_435_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:26:20,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 435 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:26:20,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 435 (MapPartitionsRDD[871] at map at streamingProcessNew.scala:49)
2017-08-18 12:26:20,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 435.0 with 2 tasks
2017-08-18 12:26:20,076 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 435.0 (TID 870, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:26:20,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 435.0 (TID 871, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:26:20,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 435.0 (TID 871)
2017-08-18 12:26:20,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 435.0 (TID 870)
2017-08-18 12:26:20,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:26:20,080 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:26:20,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 435.0 (TID 870). 801 bytes result sent to driver
2017-08-18 12:26:20,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 435.0 (TID 871). 801 bytes result sent to driver
2017-08-18 12:26:20,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 435.0 (TID 871) in 11 ms on localhost (1/2)
2017-08-18 12:26:20,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 435.0 (TID 870) in 15 ms on localhost (2/2)
2017-08-18 12:26:20,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 435.0, whose tasks have all completed, from pool 
2017-08-18 12:26:20,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 435 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:26:20,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 435 finished: foreachPartition at streamingProcessNew.scala:50, took 0.029824 s
2017-08-18 12:26:20,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030380000 ms.0 from job set of time 1503030380000 ms
2017-08-18 12:26:20,090 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 869 from persistence list
2017-08-18 12:26:20,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1503030380000 ms (execution: 0.068 s)
2017-08-18 12:26:20,090 [block-manager-slave-async-thread-pool-37] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 869
2017-08-18 12:26:20,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 868 from persistence list
2017-08-18 12:26:20,091 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 868
2017-08-18 12:26:20,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:26:20,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030370000 ms
2017-08-18 12:26:25,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030385000 ms
2017-08-18 12:26:25,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030385000 ms.0 from job set of time 1503030385000 ms
2017-08-18 12:26:25,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:26:25,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 436 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:26:25,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 436 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:26:25,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:26:25,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:26:25,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 436 (MapPartitionsRDD[873] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:26:25,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_436 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:26:25,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_436_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:26:25,036 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_436_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:26:25,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 436 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:26:25,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 436 (MapPartitionsRDD[873] at map at streamingProcessNew.scala:49)
2017-08-18 12:26:25,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 436.0 with 2 tasks
2017-08-18 12:26:25,040 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 436.0 (TID 872, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:26:25,040 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 436.0 (TID 873, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:26:25,040 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 436.0 (TID 873)
2017-08-18 12:26:25,040 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 436.0 (TID 872)
2017-08-18 12:26:25,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_421_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:26:25,051 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:26:25,051 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:26:25,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_422_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:26:25,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 436.0 (TID 872). 787 bytes result sent to driver
2017-08-18 12:26:25,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_423_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:26:25,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 436.0 (TID 873). 787 bytes result sent to driver
2017-08-18 12:26:25,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_424_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:26:25,062 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 436.0 (TID 872) in 24 ms on localhost (1/2)
2017-08-18 12:26:25,062 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 436.0 (TID 873) in 22 ms on localhost (2/2)
2017-08-18 12:26:25,063 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 436.0, whose tasks have all completed, from pool 
2017-08-18 12:26:25,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 436 (foreachPartition at streamingProcessNew.scala:50) finished in 0.025 s
2017-08-18 12:26:25,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 436 finished: foreachPartition at streamingProcessNew.scala:50, took 0.034578 s
2017-08-18 12:26:25,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_425_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:26:25,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030385000 ms.0 from job set of time 1503030385000 ms
2017-08-18 12:26:25,065 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 871 from persistence list
2017-08-18 12:26:25,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1503030385000 ms (execution: 0.048 s)
2017-08-18 12:26:25,065 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 871
2017-08-18 12:26:25,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 870 from persistence list
2017-08-18 12:26:25,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:26:25,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030375000 ms
2017-08-18 12:26:25,068 [block-manager-slave-async-thread-pool-42] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 870
2017-08-18 12:26:25,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_426_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:26:25,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_427_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:26:25,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_428_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:26:25,077 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_429_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:26:25,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_430_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:26:25,081 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_431_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:26:25,085 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_432_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:26:25,086 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_433_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:26:25,087 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_434_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:26:25,088 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_435_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:26:30,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030390000 ms
2017-08-18 12:26:30,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030390000 ms.0 from job set of time 1503030390000 ms
2017-08-18 12:26:30,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:26:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 437 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:26:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 437 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:26:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:26:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:26:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 437 (MapPartitionsRDD[875] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:26:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_437 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:26:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_437_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:26:30,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_437_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:26:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 437 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:26:30,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 437 (MapPartitionsRDD[875] at map at streamingProcessNew.scala:49)
2017-08-18 12:26:30,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 437.0 with 2 tasks
2017-08-18 12:26:30,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 437.0 (TID 874, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:26:30,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 437.0 (TID 875, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:26:30,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 437.0 (TID 875)
2017-08-18 12:26:30,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 437.0 (TID 874)
2017-08-18 12:26:30,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:26:30,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:26:30,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 437.0 (TID 874). 714 bytes result sent to driver
2017-08-18 12:26:30,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 437.0 (TID 875). 714 bytes result sent to driver
2017-08-18 12:26:30,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 437.0 (TID 875) in 10 ms on localhost (1/2)
2017-08-18 12:26:30,080 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 437.0 (TID 874) in 14 ms on localhost (2/2)
2017-08-18 12:26:30,080 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 437.0, whose tasks have all completed, from pool 
2017-08-18 12:26:30,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 437 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:26:30,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 437 finished: foreachPartition at streamingProcessNew.scala:50, took 0.027462 s
2017-08-18 12:26:30,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030390000 ms.0 from job set of time 1503030390000 ms
2017-08-18 12:26:30,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1503030390000 ms (execution: 0.060 s)
2017-08-18 12:26:30,081 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 873 from persistence list
2017-08-18 12:26:30,082 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 873
2017-08-18 12:26:30,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 872 from persistence list
2017-08-18 12:26:30,082 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 872
2017-08-18 12:26:30,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:26:30,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030380000 ms
2017-08-18 12:26:35,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030395000 ms
2017-08-18 12:26:35,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030395000 ms.0 from job set of time 1503030395000 ms
2017-08-18 12:26:35,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:26:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 438 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:26:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 438 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:26:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:26:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:26:35,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 438 (MapPartitionsRDD[877] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:26:35,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_438 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:26:35,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_438_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:26:35,076 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_438_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:26:35,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 438 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:26:35,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 438 (MapPartitionsRDD[877] at map at streamingProcessNew.scala:49)
2017-08-18 12:26:35,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 438.0 with 2 tasks
2017-08-18 12:26:35,081 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 438.0 (TID 876, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:26:35,082 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 438.0 (TID 877, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:26:35,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 438.0 (TID 877)
2017-08-18 12:26:35,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 438.0 (TID 876)
2017-08-18 12:26:35,086 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:26:35,086 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:26:35,092 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 438.0 (TID 877). 714 bytes result sent to driver
2017-08-18 12:26:35,092 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 438.0 (TID 876). 714 bytes result sent to driver
2017-08-18 12:26:35,096 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 438.0 (TID 877) in 13 ms on localhost (1/2)
2017-08-18 12:26:35,096 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 438.0 (TID 876) in 18 ms on localhost (2/2)
2017-08-18 12:26:35,096 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 438.0, whose tasks have all completed, from pool 
2017-08-18 12:26:35,097 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 438 (foreachPartition at streamingProcessNew.scala:50) finished in 0.018 s
2017-08-18 12:26:35,097 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 438 finished: foreachPartition at streamingProcessNew.scala:50, took 0.036755 s
2017-08-18 12:26:35,098 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030395000 ms.0 from job set of time 1503030395000 ms
2017-08-18 12:26:35,098 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.098 s for time 1503030395000 ms (execution: 0.076 s)
2017-08-18 12:26:35,098 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 875 from persistence list
2017-08-18 12:26:35,099 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 875
2017-08-18 12:26:35,099 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 874 from persistence list
2017-08-18 12:26:35,099 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 874
2017-08-18 12:26:35,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:26:35,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030385000 ms
2017-08-18 12:26:40,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030400000 ms
2017-08-18 12:26:40,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030400000 ms.0 from job set of time 1503030400000 ms
2017-08-18 12:26:40,065 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:26:40,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 439 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:26:40,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 439 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:26:40,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:26:40,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:26:40,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 439 (MapPartitionsRDD[879] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:26:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_439 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:26:40,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_439_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:26:40,079 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_439_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:26:40,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 439 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:26:40,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 439 (MapPartitionsRDD[879] at map at streamingProcessNew.scala:49)
2017-08-18 12:26:40,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 439.0 with 2 tasks
2017-08-18 12:26:40,086 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 439.0 (TID 878, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:26:40,087 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 439.0 (TID 879, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:26:40,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 439.0 (TID 879)
2017-08-18 12:26:40,087 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 439.0 (TID 878)
2017-08-18 12:26:40,091 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:26:40,091 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:26:40,096 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 439.0 (TID 879). 714 bytes result sent to driver
2017-08-18 12:26:40,096 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 439.0 (TID 878). 714 bytes result sent to driver
2017-08-18 12:26:40,098 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 439.0 (TID 878) in 16 ms on localhost (1/2)
2017-08-18 12:26:40,098 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 439.0 (TID 879) in 12 ms on localhost (2/2)
2017-08-18 12:26:40,098 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 439.0, whose tasks have all completed, from pool 
2017-08-18 12:26:40,098 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 439 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:26:40,099 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 439 finished: foreachPartition at streamingProcessNew.scala:50, took 0.032962 s
2017-08-18 12:26:40,099 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030400000 ms.0 from job set of time 1503030400000 ms
2017-08-18 12:26:40,099 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.099 s for time 1503030400000 ms (execution: 0.075 s)
2017-08-18 12:26:40,099 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 877 from persistence list
2017-08-18 12:26:40,100 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 877
2017-08-18 12:26:40,100 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 876 from persistence list
2017-08-18 12:26:40,100 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 876
2017-08-18 12:26:40,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:26:40,100 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030390000 ms
2017-08-18 12:26:45,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030405000 ms
2017-08-18 12:26:45,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030405000 ms.0 from job set of time 1503030405000 ms
2017-08-18 12:26:45,035 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:26:45,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 440 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:26:45,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 440 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:26:45,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:26:45,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:26:45,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 440 (MapPartitionsRDD[881] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:26:45,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_440 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:26:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_440_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:26:45,047 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_440_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:26:45,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 440 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:26:45,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 440 (MapPartitionsRDD[881] at map at streamingProcessNew.scala:49)
2017-08-18 12:26:45,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 440.0 with 2 tasks
2017-08-18 12:26:45,050 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 440.0 (TID 880, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:26:45,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 440.0 (TID 881, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:26:45,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 440.0 (TID 881)
2017-08-18 12:26:45,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 440.0 (TID 880)
2017-08-18 12:26:45,053 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:26:45,053 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:26:45,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 440.0 (TID 881). 714 bytes result sent to driver
2017-08-18 12:26:45,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 440.0 (TID 880). 714 bytes result sent to driver
2017-08-18 12:26:45,059 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 440.0 (TID 880) in 11 ms on localhost (1/2)
2017-08-18 12:26:45,059 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 440.0 (TID 881) in 9 ms on localhost (2/2)
2017-08-18 12:26:45,059 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 440.0, whose tasks have all completed, from pool 
2017-08-18 12:26:45,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 440 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:26:45,060 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 440 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024263 s
2017-08-18 12:26:45,060 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030405000 ms.0 from job set of time 1503030405000 ms
2017-08-18 12:26:45,060 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.060 s for time 1503030405000 ms (execution: 0.045 s)
2017-08-18 12:26:45,060 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 879 from persistence list
2017-08-18 12:26:45,061 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 879
2017-08-18 12:26:45,061 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 878 from persistence list
2017-08-18 12:26:45,061 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 878
2017-08-18 12:26:45,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:26:45,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030395000 ms
2017-08-18 12:26:50,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030410000 ms
2017-08-18 12:26:50,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030410000 ms.0 from job set of time 1503030410000 ms
2017-08-18 12:26:50,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:26:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 441 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:26:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 441 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:26:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:26:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:26:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 441 (MapPartitionsRDD[883] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:26:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_441 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:26:50,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_441_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:26:50,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_441_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:26:50,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 441 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:26:50,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 441 (MapPartitionsRDD[883] at map at streamingProcessNew.scala:49)
2017-08-18 12:26:50,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 441.0 with 2 tasks
2017-08-18 12:26:50,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 441.0 (TID 882, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:26:50,080 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 441.0 (TID 883, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:26:50,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 441.0 (TID 883)
2017-08-18 12:26:50,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 441.0 (TID 882)
2017-08-18 12:26:50,084 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:26:50,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:26:50,087 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 441.0 (TID 882). 714 bytes result sent to driver
2017-08-18 12:26:50,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 441.0 (TID 883). 714 bytes result sent to driver
2017-08-18 12:26:50,089 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 441.0 (TID 882) in 13 ms on localhost (1/2)
2017-08-18 12:26:50,089 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 441.0 (TID 883) in 9 ms on localhost (2/2)
2017-08-18 12:26:50,089 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 441.0, whose tasks have all completed, from pool 
2017-08-18 12:26:50,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 441 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:26:50,090 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 441 finished: foreachPartition at streamingProcessNew.scala:50, took 0.029020 s
2017-08-18 12:26:50,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030410000 ms.0 from job set of time 1503030410000 ms
2017-08-18 12:26:50,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1503030410000 ms (execution: 0.068 s)
2017-08-18 12:26:50,090 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 881 from persistence list
2017-08-18 12:26:50,091 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 881
2017-08-18 12:26:50,091 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 880 from persistence list
2017-08-18 12:26:50,091 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 880
2017-08-18 12:26:50,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:26:50,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030400000 ms
2017-08-18 12:26:55,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030415000 ms
2017-08-18 12:26:55,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030415000 ms.0 from job set of time 1503030415000 ms
2017-08-18 12:26:55,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:26:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 442 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:26:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 442 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:26:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:26:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:26:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 442 (MapPartitionsRDD[885] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:26:55,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_442 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:26:55,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_442_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:26:55,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_442_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:26:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 442 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:26:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 442 (MapPartitionsRDD[885] at map at streamingProcessNew.scala:49)
2017-08-18 12:26:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 442.0 with 2 tasks
2017-08-18 12:26:55,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 442.0 (TID 884, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:26:55,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 442.0 (TID 885, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:26:55,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 442.0 (TID 884)
2017-08-18 12:26:55,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 442.0 (TID 885)
2017-08-18 12:26:55,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:26:55,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:26:55,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 442.0 (TID 885). 714 bytes result sent to driver
2017-08-18 12:26:55,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 442.0 (TID 884). 714 bytes result sent to driver
2017-08-18 12:26:55,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 442.0 (TID 885) in 8 ms on localhost (1/2)
2017-08-18 12:26:55,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 442.0 (TID 884) in 10 ms on localhost (2/2)
2017-08-18 12:26:55,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 442.0, whose tasks have all completed, from pool 
2017-08-18 12:26:55,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 442 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:26:55,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 442 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024507 s
2017-08-18 12:26:55,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030415000 ms.0 from job set of time 1503030415000 ms
2017-08-18 12:26:55,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1503030415000 ms (execution: 0.055 s)
2017-08-18 12:26:55,076 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 883 from persistence list
2017-08-18 12:26:55,077 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 883
2017-08-18 12:26:55,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 882 from persistence list
2017-08-18 12:26:55,077 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 882
2017-08-18 12:26:55,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:26:55,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030405000 ms
2017-08-18 12:27:00,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030420000 ms
2017-08-18 12:27:00,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030420000 ms.0 from job set of time 1503030420000 ms
2017-08-18 12:27:00,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:27:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 443 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:27:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 443 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:27:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:27:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:27:00,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 443 (MapPartitionsRDD[887] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:27:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_443 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:27:00,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_443_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:27:00,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_443_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:27:00,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 443 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:27:00,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 443 (MapPartitionsRDD[887] at map at streamingProcessNew.scala:49)
2017-08-18 12:27:00,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 443.0 with 2 tasks
2017-08-18 12:27:00,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 443.0 (TID 886, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:27:00,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 443.0 (TID 887, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:27:00,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 443.0 (TID 886)
2017-08-18 12:27:00,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 443.0 (TID 887)
2017-08-18 12:27:00,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:27:00,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:27:00,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 443.0 (TID 887). 714 bytes result sent to driver
2017-08-18 12:27:00,088 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 443.0 (TID 886). 714 bytes result sent to driver
2017-08-18 12:27:00,090 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 443.0 (TID 887) in 13 ms on localhost (1/2)
2017-08-18 12:27:00,091 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 443.0 (TID 886) in 17 ms on localhost (2/2)
2017-08-18 12:27:00,091 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 443.0, whose tasks have all completed, from pool 
2017-08-18 12:27:00,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 443 (foreachPartition at streamingProcessNew.scala:50) finished in 0.018 s
2017-08-18 12:27:00,092 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 443 finished: foreachPartition at streamingProcessNew.scala:50, took 0.035839 s
2017-08-18 12:27:00,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030420000 ms.0 from job set of time 1503030420000 ms
2017-08-18 12:27:00,093 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 885 from persistence list
2017-08-18 12:27:00,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1503030420000 ms (execution: 0.071 s)
2017-08-18 12:27:00,093 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 885
2017-08-18 12:27:00,093 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 884 from persistence list
2017-08-18 12:27:00,094 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 884
2017-08-18 12:27:00,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:27:00,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030410000 ms
2017-08-18 12:27:05,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030425000 ms
2017-08-18 12:27:05,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030425000 ms.0 from job set of time 1503030425000 ms
2017-08-18 12:27:05,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:27:05,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 444 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:27:05,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 444 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:27:05,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:27:05,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:27:05,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 444 (MapPartitionsRDD[889] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:27:05,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_444 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:27:05,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_444_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:27:05,040 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_444_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:27:05,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 444 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:27:05,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 444 (MapPartitionsRDD[889] at map at streamingProcessNew.scala:49)
2017-08-18 12:27:05,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 444.0 with 2 tasks
2017-08-18 12:27:05,043 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 444.0 (TID 888, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:27:05,044 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 444.0 (TID 889, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:27:05,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 444.0 (TID 889)
2017-08-18 12:27:05,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 444.0 (TID 888)
2017-08-18 12:27:05,046 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:27:05,046 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:27:05,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 444.0 (TID 888). 714 bytes result sent to driver
2017-08-18 12:27:05,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 444.0 (TID 889). 714 bytes result sent to driver
2017-08-18 12:27:05,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 444.0 (TID 889) in 8 ms on localhost (1/2)
2017-08-18 12:27:05,051 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 444.0 (TID 888) in 10 ms on localhost (2/2)
2017-08-18 12:27:05,051 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 444.0, whose tasks have all completed, from pool 
2017-08-18 12:27:05,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 444 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:27:05,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 444 finished: foreachPartition at streamingProcessNew.scala:50, took 0.018704 s
2017-08-18 12:27:05,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030425000 ms.0 from job set of time 1503030425000 ms
2017-08-18 12:27:05,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.052 s for time 1503030425000 ms (execution: 0.035 s)
2017-08-18 12:27:05,052 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 887 from persistence list
2017-08-18 12:27:05,053 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 887
2017-08-18 12:27:05,053 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 886 from persistence list
2017-08-18 12:27:05,053 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 886
2017-08-18 12:27:05,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:27:05,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030415000 ms
2017-08-18 12:27:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030430000 ms
2017-08-18 12:27:10,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030430000 ms.0 from job set of time 1503030430000 ms
2017-08-18 12:27:10,063 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:27:10,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 445 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:27:10,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 445 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:27:10,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:27:10,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:27:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 445 (MapPartitionsRDD[891] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:27:10,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_445 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:27:10,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_445_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:27:10,082 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_445_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:27:10,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 445 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:27:10,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 445 (MapPartitionsRDD[891] at map at streamingProcessNew.scala:49)
2017-08-18 12:27:10,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 445.0 with 2 tasks
2017-08-18 12:27:10,089 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 445.0 (TID 890, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:27:10,090 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 445.0 (TID 891, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:27:10,090 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 445.0 (TID 890)
2017-08-18 12:27:10,091 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 445.0 (TID 891)
2017-08-18 12:27:10,095 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:27:10,095 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:27:10,099 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 445.0 (TID 891). 714 bytes result sent to driver
2017-08-18 12:27:10,099 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 445.0 (TID 890). 714 bytes result sent to driver
2017-08-18 12:27:10,101 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 445.0 (TID 891) in 12 ms on localhost (1/2)
2017-08-18 12:27:10,101 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 445.0 (TID 890) in 16 ms on localhost (2/2)
2017-08-18 12:27:10,101 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 445.0, whose tasks have all completed, from pool 
2017-08-18 12:27:10,101 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 445 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:27:10,102 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 445 finished: foreachPartition at streamingProcessNew.scala:50, took 0.037788 s
2017-08-18 12:27:10,102 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030430000 ms.0 from job set of time 1503030430000 ms
2017-08-18 12:27:10,102 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.102 s for time 1503030430000 ms (execution: 0.081 s)
2017-08-18 12:27:10,102 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 889 from persistence list
2017-08-18 12:27:10,103 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 889
2017-08-18 12:27:10,103 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 888 from persistence list
2017-08-18 12:27:10,103 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 888
2017-08-18 12:27:10,103 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:27:10,103 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030420000 ms
2017-08-18 12:27:15,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030435000 ms
2017-08-18 12:27:15,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030435000 ms.0 from job set of time 1503030435000 ms
2017-08-18 12:27:15,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:27:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 446 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:27:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 446 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:27:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:27:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:27:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 446 (MapPartitionsRDD[893] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:27:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_446 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:27:15,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_446_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:27:15,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_446_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:27:15,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 446 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:27:15,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 446 (MapPartitionsRDD[893] at map at streamingProcessNew.scala:49)
2017-08-18 12:27:15,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 446.0 with 2 tasks
2017-08-18 12:27:15,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 446.0 (TID 892, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:27:15,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 446.0 (TID 893, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:27:15,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 446.0 (TID 893)
2017-08-18 12:27:15,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 446.0 (TID 892)
2017-08-18 12:27:15,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:27:15,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:27:15,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 446.0 (TID 892). 714 bytes result sent to driver
2017-08-18 12:27:15,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 446.0 (TID 893). 714 bytes result sent to driver
2017-08-18 12:27:15,078 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 446.0 (TID 893) in 10 ms on localhost (1/2)
2017-08-18 12:27:15,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 446.0 (TID 892) in 13 ms on localhost (2/2)
2017-08-18 12:27:15,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 446 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:27:15,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 446.0, whose tasks have all completed, from pool 
2017-08-18 12:27:15,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 446 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024403 s
2017-08-18 12:27:15,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030435000 ms.0 from job set of time 1503030435000 ms
2017-08-18 12:27:15,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 891 from persistence list
2017-08-18 12:27:15,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1503030435000 ms (execution: 0.055 s)
2017-08-18 12:27:15,080 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 891
2017-08-18 12:27:15,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 890 from persistence list
2017-08-18 12:27:15,080 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 890
2017-08-18 12:27:15,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:27:15,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030425000 ms
2017-08-18 12:27:20,024 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030440000 ms
2017-08-18 12:27:20,025 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030440000 ms.0 from job set of time 1503030440000 ms
2017-08-18 12:27:20,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:27:20,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 447 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:27:20,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 447 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:27:20,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:27:20,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:27:20,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 447 (MapPartitionsRDD[895] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:27:20,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_447 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:27:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_447_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:27:20,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_447_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:27:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 447 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:27:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 447 (MapPartitionsRDD[895] at map at streamingProcessNew.scala:49)
2017-08-18 12:27:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 447.0 with 2 tasks
2017-08-18 12:27:20,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 447.0 (TID 894, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:27:20,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 447.0 (TID 895, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:27:20,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 447.0 (TID 894)
2017-08-18 12:27:20,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 447.0 (TID 895)
2017-08-18 12:27:20,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:27:20,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:27:20,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 447.0 (TID 894). 714 bytes result sent to driver
2017-08-18 12:27:20,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 447.0 (TID 895). 714 bytes result sent to driver
2017-08-18 12:27:20,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 447.0 (TID 894) in 11 ms on localhost (1/2)
2017-08-18 12:27:20,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 447.0 (TID 895) in 9 ms on localhost (2/2)
2017-08-18 12:27:20,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 447.0, whose tasks have all completed, from pool 
2017-08-18 12:27:20,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 447 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:27:20,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 447 finished: foreachPartition at streamingProcessNew.scala:50, took 0.023547 s
2017-08-18 12:27:20,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030440000 ms.0 from job set of time 1503030440000 ms
2017-08-18 12:27:20,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1503030440000 ms (execution: 0.049 s)
2017-08-18 12:27:20,074 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 893 from persistence list
2017-08-18 12:27:20,074 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 893
2017-08-18 12:27:20,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 892 from persistence list
2017-08-18 12:27:20,074 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 892
2017-08-18 12:27:20,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:27:20,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030430000 ms
2017-08-18 12:27:25,025 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030445000 ms
2017-08-18 12:27:25,026 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030445000 ms.0 from job set of time 1503030445000 ms
2017-08-18 12:27:25,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:27:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 448 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:27:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 448 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:27:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:27:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:27:25,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 448 (MapPartitionsRDD[897] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:27:25,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_448 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:27:25,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_448_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:27:25,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_448_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:27:25,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 448 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:27:25,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 448 (MapPartitionsRDD[897] at map at streamingProcessNew.scala:49)
2017-08-18 12:27:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 448.0 with 2 tasks
2017-08-18 12:27:25,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 448.0 (TID 896, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:27:25,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 448.0 (TID 897, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:27:25,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 448.0 (TID 896)
2017-08-18 12:27:25,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 448.0 (TID 897)
2017-08-18 12:27:25,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:27:25,066 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:27:25,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 448.0 (TID 897). 714 bytes result sent to driver
2017-08-18 12:27:25,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 448.0 (TID 896). 714 bytes result sent to driver
2017-08-18 12:27:25,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 448.0 (TID 896) in 13 ms on localhost (1/2)
2017-08-18 12:27:25,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 448.0 (TID 897) in 11 ms on localhost (2/2)
2017-08-18 12:27:25,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 448.0, whose tasks have all completed, from pool 
2017-08-18 12:27:25,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 448 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:27:25,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 448 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025341 s
2017-08-18 12:27:25,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030445000 ms.0 from job set of time 1503030445000 ms
2017-08-18 12:27:25,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1503030445000 ms (execution: 0.049 s)
2017-08-18 12:27:25,075 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 895 from persistence list
2017-08-18 12:27:25,076 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 895
2017-08-18 12:27:25,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 894 from persistence list
2017-08-18 12:27:25,076 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 894
2017-08-18 12:27:25,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:27:25,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030435000 ms
2017-08-18 12:27:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030450000 ms
2017-08-18 12:27:30,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030450000 ms.0 from job set of time 1503030450000 ms
2017-08-18 12:27:30,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:27:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 449 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:27:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 449 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:27:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:27:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:27:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 449 (MapPartitionsRDD[899] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:27:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_449 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:27:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_449_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:27:30,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_449_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:27:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 449 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:27:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 449 (MapPartitionsRDD[899] at map at streamingProcessNew.scala:49)
2017-08-18 12:27:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 449.0 with 2 tasks
2017-08-18 12:27:30,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 449.0 (TID 898, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:27:30,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 449.0 (TID 899, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:27:30,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 449.0 (TID 899)
2017-08-18 12:27:30,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 449.0 (TID 898)
2017-08-18 12:27:30,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:27:30,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:27:30,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 449.0 (TID 898). 714 bytes result sent to driver
2017-08-18 12:27:30,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 449.0 (TID 899). 714 bytes result sent to driver
2017-08-18 12:27:30,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 449.0 (TID 898) in 12 ms on localhost (1/2)
2017-08-18 12:27:30,076 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 449.0 (TID 899) in 9 ms on localhost (2/2)
2017-08-18 12:27:30,076 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 449.0, whose tasks have all completed, from pool 
2017-08-18 12:27:30,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 449 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:27:30,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 449 finished: foreachPartition at streamingProcessNew.scala:50, took 0.026693 s
2017-08-18 12:27:30,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030450000 ms.0 from job set of time 1503030450000 ms
2017-08-18 12:27:30,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 897 from persistence list
2017-08-18 12:27:30,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1503030450000 ms (execution: 0.056 s)
2017-08-18 12:27:30,077 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 897
2017-08-18 12:27:30,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 896 from persistence list
2017-08-18 12:27:30,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:27:30,078 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 896
2017-08-18 12:27:30,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030440000 ms
2017-08-18 12:27:35,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030455000 ms
2017-08-18 12:27:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030455000 ms.0 from job set of time 1503030455000 ms
2017-08-18 12:27:35,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:27:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 450 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:27:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 450 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:27:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:27:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:27:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 450 (MapPartitionsRDD[901] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:27:35,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_450 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:27:35,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_450_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:27:35,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_450_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:27:35,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 450 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:27:35,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 450 (MapPartitionsRDD[901] at map at streamingProcessNew.scala:49)
2017-08-18 12:27:35,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 450.0 with 2 tasks
2017-08-18 12:27:35,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 450.0 (TID 900, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:27:35,079 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 450.0 (TID 901, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:27:35,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 450.0 (TID 900)
2017-08-18 12:27:35,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 450.0 (TID 901)
2017-08-18 12:27:35,083 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:27:35,083 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:27:35,087 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 450.0 (TID 901). 714 bytes result sent to driver
2017-08-18 12:27:35,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 450.0 (TID 900). 714 bytes result sent to driver
2017-08-18 12:27:35,089 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 450.0 (TID 901) in 11 ms on localhost (1/2)
2017-08-18 12:27:35,089 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 450.0 (TID 900) in 15 ms on localhost (2/2)
2017-08-18 12:27:35,089 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 450.0, whose tasks have all completed, from pool 
2017-08-18 12:27:35,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 450 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:27:35,090 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 450 finished: foreachPartition at streamingProcessNew.scala:50, took 0.032432 s
2017-08-18 12:27:35,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030455000 ms.0 from job set of time 1503030455000 ms
2017-08-18 12:27:35,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1503030455000 ms (execution: 0.070 s)
2017-08-18 12:27:35,091 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 899 from persistence list
2017-08-18 12:27:35,091 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 899
2017-08-18 12:27:35,091 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 898 from persistence list
2017-08-18 12:27:35,091 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 898
2017-08-18 12:27:35,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:27:35,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030445000 ms
2017-08-18 12:27:40,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030460000 ms
2017-08-18 12:27:40,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030460000 ms.0 from job set of time 1503030460000 ms
2017-08-18 12:27:40,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:27:40,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 451 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:27:40,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 451 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:27:40,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:27:40,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:27:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 451 (MapPartitionsRDD[903] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:27:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_451 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:27:40,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_451_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:27:40,045 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_451_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:27:40,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 451 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:27:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 451 (MapPartitionsRDD[903] at map at streamingProcessNew.scala:49)
2017-08-18 12:27:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 451.0 with 2 tasks
2017-08-18 12:27:40,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 451.0 (TID 902, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:27:40,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 451.0 (TID 903, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:27:40,049 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 451.0 (TID 903)
2017-08-18 12:27:40,049 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 451.0 (TID 902)
2017-08-18 12:27:40,051 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:27:40,051 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:27:40,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 451.0 (TID 902). 714 bytes result sent to driver
2017-08-18 12:27:40,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 451.0 (TID 903). 714 bytes result sent to driver
2017-08-18 12:27:40,055 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 451.0 (TID 902) in 9 ms on localhost (1/2)
2017-08-18 12:27:40,056 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 451.0 (TID 903) in 7 ms on localhost (2/2)
2017-08-18 12:27:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 451 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:27:40,056 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 451.0, whose tasks have all completed, from pool 
2017-08-18 12:27:40,056 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 451 finished: foreachPartition at streamingProcessNew.scala:50, took 0.018456 s
2017-08-18 12:27:40,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030460000 ms.0 from job set of time 1503030460000 ms
2017-08-18 12:27:40,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.057 s for time 1503030460000 ms (execution: 0.039 s)
2017-08-18 12:27:40,057 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 901 from persistence list
2017-08-18 12:27:40,057 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 901
2017-08-18 12:27:40,057 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 900 from persistence list
2017-08-18 12:27:40,058 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 900
2017-08-18 12:27:40,058 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:27:40,058 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030450000 ms
2017-08-18 12:27:45,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030465000 ms
2017-08-18 12:27:45,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030465000 ms.0 from job set of time 1503030465000 ms
2017-08-18 12:27:45,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:27:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 452 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:27:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 452 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:27:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:27:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:27:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 452 (MapPartitionsRDD[905] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:27:45,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_451_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:27:45,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_452 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:27:45,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_436_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:27:45,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_437_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:27:45,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_438_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:27:45,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_452_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:27:45,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_439_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:27:45,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_452_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:27:45,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 452 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:27:45,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 452 (MapPartitionsRDD[905] at map at streamingProcessNew.scala:49)
2017-08-18 12:27:45,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 452.0 with 2 tasks
2017-08-18 12:27:45,077 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_440_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:27:45,078 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 452.0 (TID 904, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:27:45,078 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 452.0 (TID 905, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:27:45,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 452.0 (TID 905)
2017-08-18 12:27:45,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 452.0 (TID 904)
2017-08-18 12:27:45,081 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_441_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:27:45,083 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_442_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:27:45,086 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_443_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:27:45,089 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_444_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:27:45,095 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:27:45,095 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_445_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:27:45,095 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:27:45,097 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_446_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:27:45,098 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_447_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:27:45,099 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 452.0 (TID 905). 714 bytes result sent to driver
2017-08-18 12:27:45,099 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 452.0 (TID 904). 714 bytes result sent to driver
2017-08-18 12:27:45,099 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_448_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:27:45,100 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 452.0 (TID 905) in 22 ms on localhost (1/2)
2017-08-18 12:27:45,101 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 452.0 (TID 904) in 25 ms on localhost (2/2)
2017-08-18 12:27:45,101 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_449_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:27:45,101 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 452.0, whose tasks have all completed, from pool 
2017-08-18 12:27:45,101 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 452 (foreachPartition at streamingProcessNew.scala:50) finished in 0.027 s
2017-08-18 12:27:45,101 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 452 finished: foreachPartition at streamingProcessNew.scala:50, took 0.056884 s
2017-08-18 12:27:45,102 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030465000 ms.0 from job set of time 1503030465000 ms
2017-08-18 12:27:45,102 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_450_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:27:45,102 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 903 from persistence list
2017-08-18 12:27:45,102 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.101 s for time 1503030465000 ms (execution: 0.080 s)
2017-08-18 12:27:45,102 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 903
2017-08-18 12:27:45,102 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 902 from persistence list
2017-08-18 12:27:45,102 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 902
2017-08-18 12:27:45,102 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:27:45,103 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030455000 ms
2017-08-18 12:27:50,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030470000 ms
2017-08-18 12:27:50,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030470000 ms.0 from job set of time 1503030470000 ms
2017-08-18 12:27:50,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:27:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 453 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:27:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 453 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:27:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:27:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:27:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 453 (MapPartitionsRDD[907] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:27:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_453 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:27:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_453_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:27:50,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_453_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:27:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 453 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:27:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 453 (MapPartitionsRDD[907] at map at streamingProcessNew.scala:49)
2017-08-18 12:27:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 453.0 with 2 tasks
2017-08-18 12:27:50,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 453.0 (TID 906, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:27:50,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 453.0 (TID 907, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:27:50,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 453.0 (TID 907)
2017-08-18 12:27:50,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 453.0 (TID 906)
2017-08-18 12:27:50,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:27:50,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:27:50,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 453.0 (TID 907). 714 bytes result sent to driver
2017-08-18 12:27:50,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 453.0 (TID 906). 714 bytes result sent to driver
2017-08-18 12:27:50,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 453.0 (TID 907) in 8 ms on localhost (1/2)
2017-08-18 12:27:50,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 453.0 (TID 906) in 11 ms on localhost (2/2)
2017-08-18 12:27:50,070 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 453.0, whose tasks have all completed, from pool 
2017-08-18 12:27:50,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 453 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:27:50,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 453 finished: foreachPartition at streamingProcessNew.scala:50, took 0.023660 s
2017-08-18 12:27:50,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030470000 ms.0 from job set of time 1503030470000 ms
2017-08-18 12:27:50,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1503030470000 ms (execution: 0.046 s)
2017-08-18 12:27:50,070 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 905 from persistence list
2017-08-18 12:27:50,071 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 905
2017-08-18 12:27:50,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 904 from persistence list
2017-08-18 12:27:50,071 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 904
2017-08-18 12:27:50,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:27:50,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030460000 ms
2017-08-18 12:27:55,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030475000 ms
2017-08-18 12:27:55,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030475000 ms.0 from job set of time 1503030475000 ms
2017-08-18 12:27:55,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:27:55,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 454 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:27:55,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 454 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:27:55,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:27:55,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:27:55,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 454 (MapPartitionsRDD[909] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:27:55,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_454 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:27:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_454_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:27:55,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_454_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:27:55,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 454 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:27:55,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 454 (MapPartitionsRDD[909] at map at streamingProcessNew.scala:49)
2017-08-18 12:27:55,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 454.0 with 2 tasks
2017-08-18 12:27:55,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 454.0 (TID 908, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:27:55,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 454.0 (TID 909, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:27:55,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 454.0 (TID 909)
2017-08-18 12:27:55,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 454.0 (TID 908)
2017-08-18 12:27:55,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:27:55,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:27:55,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 454.0 (TID 909). 714 bytes result sent to driver
2017-08-18 12:27:55,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 454.0 (TID 908). 714 bytes result sent to driver
2017-08-18 12:27:55,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 454.0 (TID 909) in 8 ms on localhost (1/2)
2017-08-18 12:27:55,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 454.0 (TID 908) in 10 ms on localhost (2/2)
2017-08-18 12:27:55,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 454.0, whose tasks have all completed, from pool 
2017-08-18 12:27:55,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 454 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:27:55,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 454 finished: foreachPartition at streamingProcessNew.scala:50, took 0.026139 s
2017-08-18 12:27:55,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030475000 ms.0 from job set of time 1503030475000 ms
2017-08-18 12:27:55,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1503030475000 ms (execution: 0.054 s)
2017-08-18 12:27:55,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 907 from persistence list
2017-08-18 12:27:55,077 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 907
2017-08-18 12:27:55,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 906 from persistence list
2017-08-18 12:27:55,078 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 906
2017-08-18 12:27:55,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:27:55,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030465000 ms
2017-08-18 12:28:00,025 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030480000 ms
2017-08-18 12:28:00,026 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030480000 ms.0 from job set of time 1503030480000 ms
2017-08-18 12:28:00,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:28:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 455 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:28:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 455 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:28:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:28:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:28:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 455 (MapPartitionsRDD[911] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:28:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_455 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:28:00,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_455_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:28:00,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_455_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:28:00,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 455 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:28:00,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 455 (MapPartitionsRDD[911] at map at streamingProcessNew.scala:49)
2017-08-18 12:28:00,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 455.0 with 2 tasks
2017-08-18 12:28:00,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 455.0 (TID 910, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:28:00,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 455.0 (TID 911, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:28:00,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 455.0 (TID 911)
2017-08-18 12:28:00,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 455.0 (TID 910)
2017-08-18 12:28:00,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:28:00,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:28:00,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 455.0 (TID 910). 714 bytes result sent to driver
2017-08-18 12:28:00,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 455.0 (TID 911). 714 bytes result sent to driver
2017-08-18 12:28:00,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 455.0 (TID 911) in 9 ms on localhost (1/2)
2017-08-18 12:28:00,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 455.0 (TID 910) in 13 ms on localhost (2/2)
2017-08-18 12:28:00,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 455.0, whose tasks have all completed, from pool 
2017-08-18 12:28:00,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 455 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:28:00,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 455 finished: foreachPartition at streamingProcessNew.scala:50, took 0.026913 s
2017-08-18 12:28:00,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030480000 ms.0 from job set of time 1503030480000 ms
2017-08-18 12:28:00,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1503030480000 ms (execution: 0.057 s)
2017-08-18 12:28:00,083 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 909 from persistence list
2017-08-18 12:28:00,084 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 909
2017-08-18 12:28:00,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 908 from persistence list
2017-08-18 12:28:00,084 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 908
2017-08-18 12:28:00,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:28:00,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030470000 ms
2017-08-18 12:28:05,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030485000 ms
2017-08-18 12:28:05,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030485000 ms.0 from job set of time 1503030485000 ms
2017-08-18 12:28:05,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:28:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 456 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:28:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 456 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:28:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:28:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:28:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 456 (MapPartitionsRDD[913] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:28:05,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_456 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:28:05,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_456_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:28:05,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_456_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:28:05,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 456 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:28:05,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 456 (MapPartitionsRDD[913] at map at streamingProcessNew.scala:49)
2017-08-18 12:28:05,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 456.0 with 2 tasks
2017-08-18 12:28:05,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 456.0 (TID 912, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:28:05,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 456.0 (TID 913, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:28:05,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 456.0 (TID 913)
2017-08-18 12:28:05,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 456.0 (TID 912)
2017-08-18 12:28:05,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:28:05,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:28:05,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 456.0 (TID 913). 714 bytes result sent to driver
2017-08-18 12:28:05,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 456.0 (TID 912). 714 bytes result sent to driver
2017-08-18 12:28:05,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 456.0 (TID 913) in 8 ms on localhost (1/2)
2017-08-18 12:28:05,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 456.0 (TID 912) in 12 ms on localhost (2/2)
2017-08-18 12:28:05,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 456.0, whose tasks have all completed, from pool 
2017-08-18 12:28:05,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 456 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:28:05,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 456 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024306 s
2017-08-18 12:28:05,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030485000 ms.0 from job set of time 1503030485000 ms
2017-08-18 12:28:05,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1503030485000 ms (execution: 0.044 s)
2017-08-18 12:28:05,068 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 911 from persistence list
2017-08-18 12:28:05,068 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 911
2017-08-18 12:28:05,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 910 from persistence list
2017-08-18 12:28:05,069 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 910
2017-08-18 12:28:05,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:28:05,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030475000 ms
2017-08-18 12:28:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030490000 ms
2017-08-18 12:28:10,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030490000 ms.0 from job set of time 1503030490000 ms
2017-08-18 12:28:10,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:28:10,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 457 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:28:10,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 457 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:28:10,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:28:10,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:28:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 457 (MapPartitionsRDD[915] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:28:10,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_457 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:28:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_457_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:28:10,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_457_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:28:10,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 457 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:28:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 457 (MapPartitionsRDD[915] at map at streamingProcessNew.scala:49)
2017-08-18 12:28:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 457.0 with 2 tasks
2017-08-18 12:28:10,079 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 457.0 (TID 914, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:28:10,080 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 457.0 (TID 915, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:28:10,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 457.0 (TID 914)
2017-08-18 12:28:10,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 457.0 (TID 915)
2017-08-18 12:28:10,084 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:28:10,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:28:10,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 457.0 (TID 915). 714 bytes result sent to driver
2017-08-18 12:28:10,088 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 457.0 (TID 914). 714 bytes result sent to driver
2017-08-18 12:28:10,091 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 457.0 (TID 915) in 11 ms on localhost (1/2)
2017-08-18 12:28:10,091 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 457.0 (TID 914) in 15 ms on localhost (2/2)
2017-08-18 12:28:10,091 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 457.0, whose tasks have all completed, from pool 
2017-08-18 12:28:10,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 457 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:28:10,092 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 457 finished: foreachPartition at streamingProcessNew.scala:50, took 0.032981 s
2017-08-18 12:28:10,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030490000 ms.0 from job set of time 1503030490000 ms
2017-08-18 12:28:10,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1503030490000 ms (execution: 0.071 s)
2017-08-18 12:28:10,093 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 913 from persistence list
2017-08-18 12:28:10,093 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 913
2017-08-18 12:28:10,093 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 912 from persistence list
2017-08-18 12:28:10,094 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 912
2017-08-18 12:28:10,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:28:10,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030480000 ms
2017-08-18 12:28:15,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030495000 ms
2017-08-18 12:28:15,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030495000 ms.0 from job set of time 1503030495000 ms
2017-08-18 12:28:15,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:28:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 458 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:28:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 458 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:28:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:28:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:28:15,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 458 (MapPartitionsRDD[917] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:28:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_458 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:28:15,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_458_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:28:15,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_458_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:28:15,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 458 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:28:15,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 458 (MapPartitionsRDD[917] at map at streamingProcessNew.scala:49)
2017-08-18 12:28:15,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 458.0 with 2 tasks
2017-08-18 12:28:15,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 458.0 (TID 916, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:28:15,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 458.0 (TID 917, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:28:15,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 458.0 (TID 917)
2017-08-18 12:28:15,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 458.0 (TID 916)
2017-08-18 12:28:15,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:28:15,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:28:15,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 458.0 (TID 916). 714 bytes result sent to driver
2017-08-18 12:28:15,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 458.0 (TID 917). 714 bytes result sent to driver
2017-08-18 12:28:15,081 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 458.0 (TID 916) in 13 ms on localhost (1/2)
2017-08-18 12:28:15,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 458.0 (TID 917) in 9 ms on localhost (2/2)
2017-08-18 12:28:15,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 458.0, whose tasks have all completed, from pool 
2017-08-18 12:28:15,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 458 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:28:15,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 458 finished: foreachPartition at streamingProcessNew.scala:50, took 0.026681 s
2017-08-18 12:28:15,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030495000 ms.0 from job set of time 1503030495000 ms
2017-08-18 12:28:15,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1503030495000 ms (execution: 0.061 s)
2017-08-18 12:28:15,082 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 915 from persistence list
2017-08-18 12:28:15,083 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 915
2017-08-18 12:28:15,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 914 from persistence list
2017-08-18 12:28:15,083 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 914
2017-08-18 12:28:15,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:28:15,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030485000 ms
2017-08-18 12:28:20,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030500000 ms
2017-08-18 12:28:20,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030500000 ms.0 from job set of time 1503030500000 ms
2017-08-18 12:28:20,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:28:20,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 459 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:28:20,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 459 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:28:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:28:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:28:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 459 (MapPartitionsRDD[919] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:28:20,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_459 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:28:20,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_459_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:28:20,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_459_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:28:20,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 459 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:28:20,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 459 (MapPartitionsRDD[919] at map at streamingProcessNew.scala:49)
2017-08-18 12:28:20,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 459.0 with 2 tasks
2017-08-18 12:28:20,078 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 459.0 (TID 918, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:28:20,080 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 459.0 (TID 919, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:28:20,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 459.0 (TID 918)
2017-08-18 12:28:20,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 459.0 (TID 919)
2017-08-18 12:28:20,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:28:20,084 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:28:20,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 459.0 (TID 919). 714 bytes result sent to driver
2017-08-18 12:28:20,089 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 459.0 (TID 918). 714 bytes result sent to driver
2017-08-18 12:28:20,092 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 459.0 (TID 918) in 18 ms on localhost (1/2)
2017-08-18 12:28:20,092 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 459.0 (TID 919) in 13 ms on localhost (2/2)
2017-08-18 12:28:20,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 459 (foreachPartition at streamingProcessNew.scala:50) finished in 0.019 s
2017-08-18 12:28:20,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 459.0, whose tasks have all completed, from pool 
2017-08-18 12:28:20,094 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 459 finished: foreachPartition at streamingProcessNew.scala:50, took 0.036186 s
2017-08-18 12:28:20,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030500000 ms.0 from job set of time 1503030500000 ms
2017-08-18 12:28:20,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.094 s for time 1503030500000 ms (execution: 0.074 s)
2017-08-18 12:28:20,095 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 917 from persistence list
2017-08-18 12:28:20,095 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 917
2017-08-18 12:28:20,095 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 916 from persistence list
2017-08-18 12:28:20,096 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 916
2017-08-18 12:28:20,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:28:20,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030490000 ms
2017-08-18 12:28:25,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030505000 ms
2017-08-18 12:28:25,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030505000 ms.0 from job set of time 1503030505000 ms
2017-08-18 12:28:25,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:28:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 460 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:28:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 460 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:28:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:28:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:28:25,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 460 (MapPartitionsRDD[921] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:28:25,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_460 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:28:25,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_460_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:28:25,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_460_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:28:25,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 460 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:28:25,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 460 (MapPartitionsRDD[921] at map at streamingProcessNew.scala:49)
2017-08-18 12:28:25,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 460.0 with 2 tasks
2017-08-18 12:28:25,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 460.0 (TID 920, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:28:25,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 460.0 (TID 921, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:28:25,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 460.0 (TID 920)
2017-08-18 12:28:25,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 460.0 (TID 921)
2017-08-18 12:28:25,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:28:25,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:28:25,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 460.0 (TID 920). 714 bytes result sent to driver
2017-08-18 12:28:25,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 460.0 (TID 921). 714 bytes result sent to driver
2017-08-18 12:28:25,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 460.0 (TID 920) in 11 ms on localhost (1/2)
2017-08-18 12:28:25,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 460.0 (TID 921) in 8 ms on localhost (2/2)
2017-08-18 12:28:25,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 460.0, whose tasks have all completed, from pool 
2017-08-18 12:28:25,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 460 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:28:25,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 460 finished: foreachPartition at streamingProcessNew.scala:50, took 0.022946 s
2017-08-18 12:28:25,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030505000 ms.0 from job set of time 1503030505000 ms
2017-08-18 12:28:25,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1503030505000 ms (execution: 0.048 s)
2017-08-18 12:28:25,071 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 919 from persistence list
2017-08-18 12:28:25,071 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 919
2017-08-18 12:28:25,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 918 from persistence list
2017-08-18 12:28:25,071 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 918
2017-08-18 12:28:25,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:28:25,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030495000 ms
2017-08-18 12:28:30,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030510000 ms
2017-08-18 12:28:30,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030510000 ms.0 from job set of time 1503030510000 ms
2017-08-18 12:28:30,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:28:30,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 461 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:28:30,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 461 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:28:30,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:28:30,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:28:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 461 (MapPartitionsRDD[923] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:28:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_461 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:28:30,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_461_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:28:30,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_461_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:28:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 461 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:28:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 461 (MapPartitionsRDD[923] at map at streamingProcessNew.scala:49)
2017-08-18 12:28:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 461.0 with 2 tasks
2017-08-18 12:28:30,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 461.0 (TID 922, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:28:30,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 461.0 (TID 923, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:28:30,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 461.0 (TID 923)
2017-08-18 12:28:30,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 461.0 (TID 922)
2017-08-18 12:28:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:28:30,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:28:30,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 461.0 (TID 922). 714 bytes result sent to driver
2017-08-18 12:28:30,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 461.0 (TID 923). 714 bytes result sent to driver
2017-08-18 12:28:30,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 461.0 (TID 922) in 18 ms on localhost (1/2)
2017-08-18 12:28:30,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 461.0 (TID 923) in 14 ms on localhost (2/2)
2017-08-18 12:28:30,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 461.0, whose tasks have all completed, from pool 
2017-08-18 12:28:30,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 461 (foreachPartition at streamingProcessNew.scala:50) finished in 0.019 s
2017-08-18 12:28:30,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 461 finished: foreachPartition at streamingProcessNew.scala:50, took 0.038889 s
2017-08-18 12:28:30,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030510000 ms.0 from job set of time 1503030510000 ms
2017-08-18 12:28:30,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1503030510000 ms (execution: 0.065 s)
2017-08-18 12:28:30,085 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 921 from persistence list
2017-08-18 12:28:30,085 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 921
2017-08-18 12:28:30,085 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 920 from persistence list
2017-08-18 12:28:30,086 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 920
2017-08-18 12:28:30,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:28:30,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030500000 ms
2017-08-18 12:28:35,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030515000 ms
2017-08-18 12:28:35,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030515000 ms.0 from job set of time 1503030515000 ms
2017-08-18 12:28:35,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:28:35,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 462 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:28:35,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 462 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:28:35,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:28:35,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:28:35,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 462 (MapPartitionsRDD[925] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:28:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_462 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:28:35,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_462_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:28:35,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_462_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:28:35,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 462 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:28:35,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 462 (MapPartitionsRDD[925] at map at streamingProcessNew.scala:49)
2017-08-18 12:28:35,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 462.0 with 2 tasks
2017-08-18 12:28:35,075 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 462.0 (TID 924, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:28:35,076 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 462.0 (TID 925, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:28:35,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 462.0 (TID 925)
2017-08-18 12:28:35,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 462.0 (TID 924)
2017-08-18 12:28:35,080 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:28:35,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:28:35,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 462.0 (TID 925). 714 bytes result sent to driver
2017-08-18 12:28:35,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 462.0 (TID 924). 714 bytes result sent to driver
2017-08-18 12:28:35,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 462.0 (TID 925) in 12 ms on localhost (1/2)
2017-08-18 12:28:35,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 462.0 (TID 924) in 17 ms on localhost (2/2)
2017-08-18 12:28:35,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 462 (foreachPartition at streamingProcessNew.scala:50) finished in 0.020 s
2017-08-18 12:28:35,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 462.0, whose tasks have all completed, from pool 
2017-08-18 12:28:35,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 462 finished: foreachPartition at streamingProcessNew.scala:50, took 0.036993 s
2017-08-18 12:28:35,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030515000 ms.0 from job set of time 1503030515000 ms
2017-08-18 12:28:35,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1503030515000 ms (execution: 0.066 s)
2017-08-18 12:28:35,089 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 923 from persistence list
2017-08-18 12:28:35,089 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 923
2017-08-18 12:28:35,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 922 from persistence list
2017-08-18 12:28:35,090 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 922
2017-08-18 12:28:35,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:28:35,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030505000 ms
2017-08-18 12:28:40,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030520000 ms
2017-08-18 12:28:40,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030520000 ms.0 from job set of time 1503030520000 ms
2017-08-18 12:28:40,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:28:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 463 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:28:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 463 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:28:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:28:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:28:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 463 (MapPartitionsRDD[927] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:28:40,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_463 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:28:40,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_463_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:28:40,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_463_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:28:40,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 463 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:28:40,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 463 (MapPartitionsRDD[927] at map at streamingProcessNew.scala:49)
2017-08-18 12:28:40,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 463.0 with 2 tasks
2017-08-18 12:28:40,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 463.0 (TID 926, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:28:40,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 463.0 (TID 927, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:28:40,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 463.0 (TID 927)
2017-08-18 12:28:40,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 463.0 (TID 926)
2017-08-18 12:28:40,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:28:40,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:28:40,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 463.0 (TID 927). 714 bytes result sent to driver
2017-08-18 12:28:40,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 463.0 (TID 926). 714 bytes result sent to driver
2017-08-18 12:28:40,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 463.0 (TID 927) in 13 ms on localhost (1/2)
2017-08-18 12:28:40,084 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 463.0 (TID 926) in 17 ms on localhost (2/2)
2017-08-18 12:28:40,084 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 463.0, whose tasks have all completed, from pool 
2017-08-18 12:28:40,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 463 (foreachPartition at streamingProcessNew.scala:50) finished in 0.018 s
2017-08-18 12:28:40,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 463 finished: foreachPartition at streamingProcessNew.scala:50, took 0.034305 s
2017-08-18 12:28:40,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030520000 ms.0 from job set of time 1503030520000 ms
2017-08-18 12:28:40,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.086 s for time 1503030520000 ms (execution: 0.064 s)
2017-08-18 12:28:40,086 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 925 from persistence list
2017-08-18 12:28:40,087 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 925
2017-08-18 12:28:40,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 924 from persistence list
2017-08-18 12:28:40,087 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 924
2017-08-18 12:28:40,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:28:40,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030510000 ms
2017-08-18 12:28:45,026 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030525000 ms
2017-08-18 12:28:45,026 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030525000 ms.0 from job set of time 1503030525000 ms
2017-08-18 12:28:45,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:28:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 464 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:28:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 464 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:28:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:28:45,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:28:45,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 464 (MapPartitionsRDD[929] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:28:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_464 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:28:45,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_464_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:28:45,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_464_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:28:45,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 464 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:28:45,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 464 (MapPartitionsRDD[929] at map at streamingProcessNew.scala:49)
2017-08-18 12:28:45,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 464.0 with 2 tasks
2017-08-18 12:28:45,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 464.0 (TID 928, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:28:45,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 464.0 (TID 929, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:28:45,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 464.0 (TID 929)
2017-08-18 12:28:45,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 464.0 (TID 928)
2017-08-18 12:28:45,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:28:45,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:28:45,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 464.0 (TID 929). 714 bytes result sent to driver
2017-08-18 12:28:45,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 464.0 (TID 928). 714 bytes result sent to driver
2017-08-18 12:28:45,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 464.0 (TID 929) in 8 ms on localhost (1/2)
2017-08-18 12:28:45,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 464.0 (TID 928) in 11 ms on localhost (2/2)
2017-08-18 12:28:45,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 464.0, whose tasks have all completed, from pool 
2017-08-18 12:28:45,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 464 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:28:45,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 464 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024526 s
2017-08-18 12:28:45,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030525000 ms.0 from job set of time 1503030525000 ms
2017-08-18 12:28:45,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1503030525000 ms (execution: 0.051 s)
2017-08-18 12:28:45,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 927 from persistence list
2017-08-18 12:28:45,078 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 927
2017-08-18 12:28:45,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 926 from persistence list
2017-08-18 12:28:45,078 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 926
2017-08-18 12:28:45,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:28:45,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030515000 ms
2017-08-18 12:28:50,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030530000 ms
2017-08-18 12:28:50,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030530000 ms.0 from job set of time 1503030530000 ms
2017-08-18 12:28:50,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:28:50,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 465 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:28:50,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 465 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:28:50,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:28:50,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:28:50,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 465 (MapPartitionsRDD[931] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:28:50,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_465 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:28:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_465_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:28:50,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_465_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:28:50,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 465 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:28:50,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 465 (MapPartitionsRDD[931] at map at streamingProcessNew.scala:49)
2017-08-18 12:28:50,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 465.0 with 2 tasks
2017-08-18 12:28:50,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 465.0 (TID 930, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:28:50,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 465.0 (TID 931, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:28:50,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 465.0 (TID 931)
2017-08-18 12:28:50,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 465.0 (TID 930)
2017-08-18 12:28:50,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:28:50,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:28:50,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 465.0 (TID 930). 714 bytes result sent to driver
2017-08-18 12:28:50,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 465.0 (TID 931). 714 bytes result sent to driver
2017-08-18 12:28:50,085 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 465.0 (TID 930) in 13 ms on localhost (1/2)
2017-08-18 12:28:50,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 465.0 (TID 931) in 10 ms on localhost (2/2)
2017-08-18 12:28:50,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 465.0, whose tasks have all completed, from pool 
2017-08-18 12:28:50,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 465 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:28:50,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 465 finished: foreachPartition at streamingProcessNew.scala:50, took 0.027531 s
2017-08-18 12:28:50,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030530000 ms.0 from job set of time 1503030530000 ms
2017-08-18 12:28:50,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.086 s for time 1503030530000 ms (execution: 0.063 s)
2017-08-18 12:28:50,087 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 929 from persistence list
2017-08-18 12:28:50,087 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 929
2017-08-18 12:28:50,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 928 from persistence list
2017-08-18 12:28:50,088 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 928
2017-08-18 12:28:50,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:28:50,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030520000 ms
2017-08-18 12:28:55,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030535000 ms
2017-08-18 12:28:55,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030535000 ms.0 from job set of time 1503030535000 ms
2017-08-18 12:28:55,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:28:55,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 466 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:28:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 466 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:28:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:28:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:28:55,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 466 (MapPartitionsRDD[933] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:28:55,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_466 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:28:55,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_466_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:28:55,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_466_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:28:55,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 466 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:28:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 466 (MapPartitionsRDD[933] at map at streamingProcessNew.scala:49)
2017-08-18 12:28:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 466.0 with 2 tasks
2017-08-18 12:28:55,079 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 466.0 (TID 932, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:28:55,079 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 466.0 (TID 933, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:28:55,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 466.0 (TID 932)
2017-08-18 12:28:55,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 466.0 (TID 933)
2017-08-18 12:28:55,083 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:28:55,083 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:28:55,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 466.0 (TID 932). 714 bytes result sent to driver
2017-08-18 12:28:55,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 466.0 (TID 933). 714 bytes result sent to driver
2017-08-18 12:28:55,090 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 466.0 (TID 932) in 14 ms on localhost (1/2)
2017-08-18 12:28:55,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 466.0 (TID 933) in 11 ms on localhost (2/2)
2017-08-18 12:28:55,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 466.0, whose tasks have all completed, from pool 
2017-08-18 12:28:55,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 466 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:28:55,091 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 466 finished: foreachPartition at streamingProcessNew.scala:50, took 0.032420 s
2017-08-18 12:28:55,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030535000 ms.0 from job set of time 1503030535000 ms
2017-08-18 12:28:55,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.091 s for time 1503030535000 ms (execution: 0.071 s)
2017-08-18 12:28:55,092 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 931 from persistence list
2017-08-18 12:28:55,092 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 931
2017-08-18 12:28:55,092 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 930 from persistence list
2017-08-18 12:28:55,093 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 930
2017-08-18 12:28:55,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:28:55,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030525000 ms
2017-08-18 12:29:00,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030540000 ms
2017-08-18 12:29:00,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030540000 ms.0 from job set of time 1503030540000 ms
2017-08-18 12:29:00,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:29:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 467 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:29:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 467 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:29:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:29:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:29:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 467 (MapPartitionsRDD[935] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:29:00,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_467 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:29:00,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_467_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:29:00,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_467_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:29:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 467 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:29:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 467 (MapPartitionsRDD[935] at map at streamingProcessNew.scala:49)
2017-08-18 12:29:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 467.0 with 2 tasks
2017-08-18 12:29:00,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 467.0 (TID 934, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:29:00,070 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 467.0 (TID 935, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:29:00,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 467.0 (TID 935)
2017-08-18 12:29:00,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 467.0 (TID 934)
2017-08-18 12:29:00,091 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:29:00,091 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:29:00,092 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_452_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:29:00,097 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_453_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:29:00,098 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 467.0 (TID 935). 787 bytes result sent to driver
2017-08-18 12:29:00,098 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 467.0 (TID 934). 787 bytes result sent to driver
2017-08-18 12:29:00,103 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_454_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:29:00,106 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 467.0 (TID 934) in 38 ms on localhost (1/2)
2017-08-18 12:29:00,106 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_455_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:29:00,106 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 467.0 (TID 935) in 37 ms on localhost (2/2)
2017-08-18 12:29:00,106 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 467.0, whose tasks have all completed, from pool 
2017-08-18 12:29:00,108 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 467 (foreachPartition at streamingProcessNew.scala:50) finished in 0.041 s
2017-08-18 12:29:00,108 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 467 finished: foreachPartition at streamingProcessNew.scala:50, took 0.055599 s
2017-08-18 12:29:00,108 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030540000 ms.0 from job set of time 1503030540000 ms
2017-08-18 12:29:00,110 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.108 s for time 1503030540000 ms (execution: 0.087 s)
2017-08-18 12:29:00,110 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 933 from persistence list
2017-08-18 12:29:00,111 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_456_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:29:00,111 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 933
2017-08-18 12:29:00,111 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 932 from persistence list
2017-08-18 12:29:00,111 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 932
2017-08-18 12:29:00,111 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:29:00,112 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030530000 ms
2017-08-18 12:29:00,114 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_457_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:29:00,117 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_458_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:29:00,119 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_459_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:29:00,121 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_460_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:29:00,125 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_461_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:29:00,130 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_462_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:29:00,131 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_463_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:29:00,132 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_464_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:29:00,134 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_465_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:29:00,135 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_466_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:29:05,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030545000 ms
2017-08-18 12:29:05,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030545000 ms.0 from job set of time 1503030545000 ms
2017-08-18 12:29:05,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:29:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 468 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:29:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 468 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:29:05,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:29:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:29:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 468 (MapPartitionsRDD[937] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:29:05,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_468 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:29:05,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_468_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:29:05,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_468_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:29:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 468 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:29:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 468 (MapPartitionsRDD[937] at map at streamingProcessNew.scala:49)
2017-08-18 12:29:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 468.0 with 2 tasks
2017-08-18 12:29:05,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 468.0 (TID 936, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:29:05,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 468.0 (TID 937, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:29:05,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 468.0 (TID 936)
2017-08-18 12:29:05,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 468.0 (TID 937)
2017-08-18 12:29:05,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:29:05,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:29:05,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 468.0 (TID 936). 714 bytes result sent to driver
2017-08-18 12:29:05,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 468.0 (TID 937). 714 bytes result sent to driver
2017-08-18 12:29:05,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 468.0 (TID 936) in 10 ms on localhost (1/2)
2017-08-18 12:29:05,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 468.0 (TID 937) in 7 ms on localhost (2/2)
2017-08-18 12:29:05,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 468.0, whose tasks have all completed, from pool 
2017-08-18 12:29:05,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 468 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:29:05,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 468 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024356 s
2017-08-18 12:29:05,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030545000 ms.0 from job set of time 1503030545000 ms
2017-08-18 12:29:05,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1503030545000 ms (execution: 0.046 s)
2017-08-18 12:29:05,069 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 935 from persistence list
2017-08-18 12:29:05,069 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 935
2017-08-18 12:29:05,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 934 from persistence list
2017-08-18 12:29:05,069 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 934
2017-08-18 12:29:05,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:29:05,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030535000 ms
2017-08-18 12:29:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030550000 ms
2017-08-18 12:29:10,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030550000 ms.0 from job set of time 1503030550000 ms
2017-08-18 12:29:10,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:29:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 469 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:29:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 469 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:29:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:29:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:29:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 469 (MapPartitionsRDD[939] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:29:10,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_469 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:29:10,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_469_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:29:10,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_469_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:29:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 469 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:29:10,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 469 (MapPartitionsRDD[939] at map at streamingProcessNew.scala:49)
2017-08-18 12:29:10,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 469.0 with 2 tasks
2017-08-18 12:29:10,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 469.0 (TID 938, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:29:10,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 469.0 (TID 939, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:29:10,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 469.0 (TID 939)
2017-08-18 12:29:10,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 469.0 (TID 938)
2017-08-18 12:29:10,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:29:10,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:29:10,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 469.0 (TID 939). 714 bytes result sent to driver
2017-08-18 12:29:10,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 469.0 (TID 938). 714 bytes result sent to driver
2017-08-18 12:29:10,077 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 469.0 (TID 938) in 10 ms on localhost (1/2)
2017-08-18 12:29:10,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 469.0 (TID 939) in 8 ms on localhost (2/2)
2017-08-18 12:29:10,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 469.0, whose tasks have all completed, from pool 
2017-08-18 12:29:10,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 469 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:29:10,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 469 finished: foreachPartition at streamingProcessNew.scala:50, took 0.027871 s
2017-08-18 12:29:10,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030550000 ms.0 from job set of time 1503030550000 ms
2017-08-18 12:29:10,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1503030550000 ms (execution: 0.057 s)
2017-08-18 12:29:10,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 937 from persistence list
2017-08-18 12:29:10,079 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 937
2017-08-18 12:29:10,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 936 from persistence list
2017-08-18 12:29:10,079 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 936
2017-08-18 12:29:10,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:29:10,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030540000 ms
2017-08-18 12:29:15,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030555000 ms
2017-08-18 12:29:15,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030555000 ms.0 from job set of time 1503030555000 ms
2017-08-18 12:29:15,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:29:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 470 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:29:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 470 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:29:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:29:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:29:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 470 (MapPartitionsRDD[941] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:29:15,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_470 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:29:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_470_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:29:15,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_470_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:29:15,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 470 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:29:15,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 470 (MapPartitionsRDD[941] at map at streamingProcessNew.scala:49)
2017-08-18 12:29:15,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 470.0 with 2 tasks
2017-08-18 12:29:15,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 470.0 (TID 940, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:29:15,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 470.0 (TID 941, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:29:15,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 470.0 (TID 941)
2017-08-18 12:29:15,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 470.0 (TID 940)
2017-08-18 12:29:15,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:29:15,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:29:15,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 470.0 (TID 941). 714 bytes result sent to driver
2017-08-18 12:29:15,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 470.0 (TID 940). 714 bytes result sent to driver
2017-08-18 12:29:15,078 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 470.0 (TID 940) in 13 ms on localhost (1/2)
2017-08-18 12:29:15,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 470.0 (TID 941) in 10 ms on localhost (2/2)
2017-08-18 12:29:15,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 470.0, whose tasks have all completed, from pool 
2017-08-18 12:29:15,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 470 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:29:15,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 470 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025213 s
2017-08-18 12:29:15,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030555000 ms.0 from job set of time 1503030555000 ms
2017-08-18 12:29:15,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1503030555000 ms (execution: 0.056 s)
2017-08-18 12:29:15,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 939 from persistence list
2017-08-18 12:29:15,079 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 939
2017-08-18 12:29:15,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 938 from persistence list
2017-08-18 12:29:15,080 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 938
2017-08-18 12:29:15,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:29:15,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030545000 ms
2017-08-18 12:29:20,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030560000 ms
2017-08-18 12:29:20,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030560000 ms.0 from job set of time 1503030560000 ms
2017-08-18 12:29:20,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:29:20,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 471 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:29:20,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 471 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:29:20,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:29:20,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:29:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 471 (MapPartitionsRDD[943] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:29:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_471 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:29:20,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_471_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:29:20,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_471_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:29:20,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 471 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:29:20,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 471 (MapPartitionsRDD[943] at map at streamingProcessNew.scala:49)
2017-08-18 12:29:20,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 471.0 with 2 tasks
2017-08-18 12:29:20,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 471.0 (TID 942, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:29:20,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 471.0 (TID 943, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:29:20,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 471.0 (TID 943)
2017-08-18 12:29:20,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 471.0 (TID 942)
2017-08-18 12:29:20,058 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:29:20,058 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:29:20,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 471.0 (TID 942). 714 bytes result sent to driver
2017-08-18 12:29:20,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 471.0 (TID 943). 714 bytes result sent to driver
2017-08-18 12:29:20,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 471.0 (TID 943) in 7 ms on localhost (1/2)
2017-08-18 12:29:20,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 471.0 (TID 942) in 10 ms on localhost (2/2)
2017-08-18 12:29:20,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 471.0, whose tasks have all completed, from pool 
2017-08-18 12:29:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 471 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:29:20,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 471 finished: foreachPartition at streamingProcessNew.scala:50, took 0.021816 s
2017-08-18 12:29:20,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030560000 ms.0 from job set of time 1503030560000 ms
2017-08-18 12:29:20,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1503030560000 ms (execution: 0.040 s)
2017-08-18 12:29:20,064 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 941 from persistence list
2017-08-18 12:29:20,064 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 941
2017-08-18 12:29:20,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 940 from persistence list
2017-08-18 12:29:20,065 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 940
2017-08-18 12:29:20,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:29:20,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030550000 ms
2017-08-18 12:29:25,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030565000 ms
2017-08-18 12:29:25,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030565000 ms.0 from job set of time 1503030565000 ms
2017-08-18 12:29:25,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:29:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 472 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:29:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 472 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:29:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:29:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:29:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 472 (MapPartitionsRDD[945] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:29:25,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_472 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:29:25,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_472_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:29:25,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_472_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:29:25,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 472 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:29:25,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 472 (MapPartitionsRDD[945] at map at streamingProcessNew.scala:49)
2017-08-18 12:29:25,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 472.0 with 2 tasks
2017-08-18 12:29:25,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 472.0 (TID 944, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:29:25,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 472.0 (TID 945, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:29:25,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 472.0 (TID 944)
2017-08-18 12:29:25,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 472.0 (TID 945)
2017-08-18 12:29:25,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:29:25,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:29:25,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 472.0 (TID 944). 714 bytes result sent to driver
2017-08-18 12:29:25,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 472.0 (TID 945). 714 bytes result sent to driver
2017-08-18 12:29:25,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 472.0 (TID 944) in 11 ms on localhost (1/2)
2017-08-18 12:29:25,084 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 472.0 (TID 945) in 8 ms on localhost (2/2)
2017-08-18 12:29:25,084 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 472.0, whose tasks have all completed, from pool 
2017-08-18 12:29:25,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 472 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:29:25,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 472 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025365 s
2017-08-18 12:29:25,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030565000 ms.0 from job set of time 1503030565000 ms
2017-08-18 12:29:25,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1503030565000 ms (execution: 0.063 s)
2017-08-18 12:29:25,086 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 943 from persistence list
2017-08-18 12:29:25,086 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 943
2017-08-18 12:29:25,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 942 from persistence list
2017-08-18 12:29:25,086 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 942
2017-08-18 12:29:25,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:29:25,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030555000 ms
2017-08-18 12:29:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030570000 ms
2017-08-18 12:29:30,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030570000 ms.0 from job set of time 1503030570000 ms
2017-08-18 12:29:30,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:29:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 473 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:29:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 473 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:29:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:29:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:29:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 473 (MapPartitionsRDD[947] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:29:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_473 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:29:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_473_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:29:30,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_473_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:29:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 473 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:29:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 473 (MapPartitionsRDD[947] at map at streamingProcessNew.scala:49)
2017-08-18 12:29:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 473.0 with 2 tasks
2017-08-18 12:29:30,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 473.0 (TID 946, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:29:30,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 473.0 (TID 947, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:29:30,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 473.0 (TID 947)
2017-08-18 12:29:30,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 473.0 (TID 946)
2017-08-18 12:29:30,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:29:30,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:29:30,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 473.0 (TID 947). 714 bytes result sent to driver
2017-08-18 12:29:30,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 473.0 (TID 946). 714 bytes result sent to driver
2017-08-18 12:29:30,081 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 473.0 (TID 947) in 12 ms on localhost (1/2)
2017-08-18 12:29:30,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 473.0 (TID 946) in 16 ms on localhost (2/2)
2017-08-18 12:29:30,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 473 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:29:30,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 473.0, whose tasks have all completed, from pool 
2017-08-18 12:29:30,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 473 finished: foreachPartition at streamingProcessNew.scala:50, took 0.029665 s
2017-08-18 12:29:30,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030570000 ms.0 from job set of time 1503030570000 ms
2017-08-18 12:29:30,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1503030570000 ms (execution: 0.062 s)
2017-08-18 12:29:30,083 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 945 from persistence list
2017-08-18 12:29:30,084 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 945
2017-08-18 12:29:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 944 from persistence list
2017-08-18 12:29:30,084 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 944
2017-08-18 12:29:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:29:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030560000 ms
2017-08-18 12:29:35,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030575000 ms
2017-08-18 12:29:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030575000 ms.0 from job set of time 1503030575000 ms
2017-08-18 12:29:35,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:29:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 474 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:29:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 474 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:29:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:29:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:29:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 474 (MapPartitionsRDD[949] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:29:35,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_474 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:29:35,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_474_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:29:35,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_474_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:29:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 474 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:29:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 474 (MapPartitionsRDD[949] at map at streamingProcessNew.scala:49)
2017-08-18 12:29:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 474.0 with 2 tasks
2017-08-18 12:29:35,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 474.0 (TID 948, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:29:35,076 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 474.0 (TID 949, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:29:35,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 474.0 (TID 949)
2017-08-18 12:29:35,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 474.0 (TID 948)
2017-08-18 12:29:35,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:29:35,080 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:29:35,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 474.0 (TID 949). 714 bytes result sent to driver
2017-08-18 12:29:35,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 474.0 (TID 948). 714 bytes result sent to driver
2017-08-18 12:29:35,087 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 474.0 (TID 949) in 11 ms on localhost (1/2)
2017-08-18 12:29:35,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 474.0 (TID 948) in 15 ms on localhost (2/2)
2017-08-18 12:29:35,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 474.0, whose tasks have all completed, from pool 
2017-08-18 12:29:35,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 474 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:29:35,087 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 474 finished: foreachPartition at streamingProcessNew.scala:50, took 0.030175 s
2017-08-18 12:29:35,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030575000 ms.0 from job set of time 1503030575000 ms
2017-08-18 12:29:35,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1503030575000 ms (execution: 0.067 s)
2017-08-18 12:29:35,088 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 947 from persistence list
2017-08-18 12:29:35,088 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 947
2017-08-18 12:29:35,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 946 from persistence list
2017-08-18 12:29:35,088 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 946
2017-08-18 12:29:35,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:29:35,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030565000 ms
2017-08-18 12:29:40,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030580000 ms
2017-08-18 12:29:40,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030580000 ms.0 from job set of time 1503030580000 ms
2017-08-18 12:29:40,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:29:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 475 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:29:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 475 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:29:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:29:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:29:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 475 (MapPartitionsRDD[951] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:29:40,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_475 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:29:40,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_475_piece0 stored as bytes in memory (estimated size 25.2 KB, free 413.4 MB)
2017-08-18 12:29:40,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_475_piece0 in memory on 192.168.31.111:60809 (size: 25.2 KB, free: 413.7 MB)
2017-08-18 12:29:40,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 475 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:29:40,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 475 (MapPartitionsRDD[951] at map at streamingProcessNew.scala:49)
2017-08-18 12:29:40,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 475.0 with 2 tasks
2017-08-18 12:29:40,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 475.0 (TID 950, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:29:40,078 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 475.0 (TID 951, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:29:40,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 475.0 (TID 951)
2017-08-18 12:29:40,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 475.0 (TID 950)
2017-08-18 12:29:40,081 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:29:40,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:29:40,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 475.0 (TID 951). 714 bytes result sent to driver
2017-08-18 12:29:40,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 475.0 (TID 950). 714 bytes result sent to driver
2017-08-18 12:29:40,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 475.0 (TID 951) in 11 ms on localhost (1/2)
2017-08-18 12:29:40,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 475.0 (TID 950) in 15 ms on localhost (2/2)
2017-08-18 12:29:40,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 475.0, whose tasks have all completed, from pool 
2017-08-18 12:29:40,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 475 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:29:40,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 475 finished: foreachPartition at streamingProcessNew.scala:50, took 0.033659 s
2017-08-18 12:29:40,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030580000 ms.0 from job set of time 1503030580000 ms
2017-08-18 12:29:40,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1503030580000 ms (execution: 0.067 s)
2017-08-18 12:29:40,090 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 949 from persistence list
2017-08-18 12:29:40,090 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 949
2017-08-18 12:29:40,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 948 from persistence list
2017-08-18 12:29:40,091 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 948
2017-08-18 12:29:40,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:29:40,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030570000 ms
2017-08-18 12:29:45,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030585000 ms
2017-08-18 12:29:45,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030585000 ms.0 from job set of time 1503030585000 ms
2017-08-18 12:29:45,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:29:45,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 476 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:29:45,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 476 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:29:45,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:29:45,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:29:45,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 476 (MapPartitionsRDD[953] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:29:45,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_476 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:29:45,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_476_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:29:45,077 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_476_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:29:45,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 476 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:29:45,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 476 (MapPartitionsRDD[953] at map at streamingProcessNew.scala:49)
2017-08-18 12:29:45,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 476.0 with 2 tasks
2017-08-18 12:29:45,082 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 476.0 (TID 952, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:29:45,082 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 476.0 (TID 953, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:29:45,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 476.0 (TID 953)
2017-08-18 12:29:45,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 476.0 (TID 952)
2017-08-18 12:29:45,086 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:29:45,086 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:29:45,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 476.0 (TID 952). 714 bytes result sent to driver
2017-08-18 12:29:45,089 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 476.0 (TID 953). 714 bytes result sent to driver
2017-08-18 12:29:45,091 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 476.0 (TID 952) in 13 ms on localhost (1/2)
2017-08-18 12:29:45,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 476.0 (TID 953) in 9 ms on localhost (2/2)
2017-08-18 12:29:45,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 476.0, whose tasks have all completed, from pool 
2017-08-18 12:29:45,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 476 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:29:45,092 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 476 finished: foreachPartition at streamingProcessNew.scala:50, took 0.031813 s
2017-08-18 12:29:45,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030585000 ms.0 from job set of time 1503030585000 ms
2017-08-18 12:29:45,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1503030585000 ms (execution: 0.071 s)
2017-08-18 12:29:45,092 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 951 from persistence list
2017-08-18 12:29:45,093 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 951
2017-08-18 12:29:45,093 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 950 from persistence list
2017-08-18 12:29:45,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:29:45,093 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 950
2017-08-18 12:29:45,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030575000 ms
2017-08-18 12:29:50,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030590000 ms
2017-08-18 12:29:50,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030590000 ms.0 from job set of time 1503030590000 ms
2017-08-18 12:29:50,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:29:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 477 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:29:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 477 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:29:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:29:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:29:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 477 (MapPartitionsRDD[955] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:29:50,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_477 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:29:50,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_477_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:29:50,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_477_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:29:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 477 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:29:50,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 477 (MapPartitionsRDD[955] at map at streamingProcessNew.scala:49)
2017-08-18 12:29:50,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 477.0 with 2 tasks
2017-08-18 12:29:50,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 477.0 (TID 954, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:29:50,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 477.0 (TID 955, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:29:50,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 477.0 (TID 955)
2017-08-18 12:29:50,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 477.0 (TID 954)
2017-08-18 12:29:50,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:29:50,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:29:50,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 477.0 (TID 954). 714 bytes result sent to driver
2017-08-18 12:29:50,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 477.0 (TID 955). 714 bytes result sent to driver
2017-08-18 12:29:50,081 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 477.0 (TID 954) in 13 ms on localhost (1/2)
2017-08-18 12:29:50,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 477.0 (TID 955) in 10 ms on localhost (2/2)
2017-08-18 12:29:50,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 477.0, whose tasks have all completed, from pool 
2017-08-18 12:29:50,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 477 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:29:50,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 477 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025107 s
2017-08-18 12:29:50,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030590000 ms.0 from job set of time 1503030590000 ms
2017-08-18 12:29:50,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1503030590000 ms (execution: 0.061 s)
2017-08-18 12:29:50,083 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 953 from persistence list
2017-08-18 12:29:50,084 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 953
2017-08-18 12:29:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 952 from persistence list
2017-08-18 12:29:50,084 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 952
2017-08-18 12:29:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:29:50,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030580000 ms
2017-08-18 12:29:55,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030595000 ms
2017-08-18 12:29:55,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030595000 ms.0 from job set of time 1503030595000 ms
2017-08-18 12:29:55,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:29:55,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 478 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:29:55,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 478 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:29:55,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:29:55,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:29:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 478 (MapPartitionsRDD[957] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:29:55,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_478 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:29:55,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_478_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:29:55,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_478_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:29:55,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 478 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:29:55,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 478 (MapPartitionsRDD[957] at map at streamingProcessNew.scala:49)
2017-08-18 12:29:55,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 478.0 with 2 tasks
2017-08-18 12:29:55,079 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 478.0 (TID 956, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:29:55,080 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 478.0 (TID 957, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:29:55,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 478.0 (TID 956)
2017-08-18 12:29:55,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 478.0 (TID 957)
2017-08-18 12:29:55,083 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:29:55,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:29:55,088 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 478.0 (TID 956). 714 bytes result sent to driver
2017-08-18 12:29:55,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 478.0 (TID 957). 714 bytes result sent to driver
2017-08-18 12:29:55,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 478.0 (TID 957) in 11 ms on localhost (1/2)
2017-08-18 12:29:55,091 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 478.0 (TID 956) in 15 ms on localhost (2/2)
2017-08-18 12:29:55,091 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 478.0, whose tasks have all completed, from pool 
2017-08-18 12:29:55,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 478 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:29:55,092 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 478 finished: foreachPartition at streamingProcessNew.scala:50, took 0.033709 s
2017-08-18 12:29:55,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030595000 ms.0 from job set of time 1503030595000 ms
2017-08-18 12:29:55,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1503030595000 ms (execution: 0.072 s)
2017-08-18 12:29:55,092 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 955 from persistence list
2017-08-18 12:29:55,093 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 955
2017-08-18 12:29:55,093 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 954 from persistence list
2017-08-18 12:29:55,093 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 954
2017-08-18 12:29:55,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:29:55,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030585000 ms
2017-08-18 12:30:00,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030600000 ms
2017-08-18 12:30:00,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030600000 ms.0 from job set of time 1503030600000 ms
2017-08-18 12:30:00,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:30:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 479 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:30:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 479 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:30:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:30:00,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:30:00,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 479 (MapPartitionsRDD[959] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:30:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_479 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:30:00,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_479_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:30:00,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_479_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:30:00,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 479 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:30:00,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 479 (MapPartitionsRDD[959] at map at streamingProcessNew.scala:49)
2017-08-18 12:30:00,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 479.0 with 2 tasks
2017-08-18 12:30:00,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 479.0 (TID 958, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:30:00,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 479.0 (TID 959, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:30:00,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 479.0 (TID 959)
2017-08-18 12:30:00,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 479.0 (TID 958)
2017-08-18 12:30:00,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:30:00,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:30:00,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 479.0 (TID 958). 714 bytes result sent to driver
2017-08-18 12:30:00,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 479.0 (TID 959). 714 bytes result sent to driver
2017-08-18 12:30:00,084 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 479.0 (TID 958) in 15 ms on localhost (1/2)
2017-08-18 12:30:00,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 479.0 (TID 959) in 11 ms on localhost (2/2)
2017-08-18 12:30:00,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 479.0, whose tasks have all completed, from pool 
2017-08-18 12:30:00,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 479 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:30:00,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 479 finished: foreachPartition at streamingProcessNew.scala:50, took 0.032786 s
2017-08-18 12:30:00,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030600000 ms.0 from job set of time 1503030600000 ms
2017-08-18 12:30:00,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1503030600000 ms (execution: 0.065 s)
2017-08-18 12:30:00,085 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 957 from persistence list
2017-08-18 12:30:00,086 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 957
2017-08-18 12:30:00,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 956 from persistence list
2017-08-18 12:30:00,086 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 956
2017-08-18 12:30:00,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:30:00,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030590000 ms
2017-08-18 12:30:05,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030605000 ms
2017-08-18 12:30:05,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030605000 ms.0 from job set of time 1503030605000 ms
2017-08-18 12:30:05,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:30:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 480 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:30:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 480 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:30:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:30:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:30:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 480 (MapPartitionsRDD[961] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:30:05,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_480 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:30:05,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_480_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:30:05,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_480_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:30:05,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 480 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:30:05,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 480 (MapPartitionsRDD[961] at map at streamingProcessNew.scala:49)
2017-08-18 12:30:05,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 480.0 with 2 tasks
2017-08-18 12:30:05,080 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 480.0 (TID 960, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:30:05,082 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 480.0 (TID 961, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:30:05,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 480.0 (TID 960)
2017-08-18 12:30:05,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 480.0 (TID 961)
2017-08-18 12:30:05,085 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:30:05,086 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:30:05,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 480.0 (TID 960). 714 bytes result sent to driver
2017-08-18 12:30:05,089 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 480.0 (TID 961). 714 bytes result sent to driver
2017-08-18 12:30:05,091 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 480.0 (TID 961) in 10 ms on localhost (1/2)
2017-08-18 12:30:05,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 480.0 (TID 960) in 18 ms on localhost (2/2)
2017-08-18 12:30:05,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 480.0, whose tasks have all completed, from pool 
2017-08-18 12:30:05,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 480 (foreachPartition at streamingProcessNew.scala:50) finished in 0.019 s
2017-08-18 12:30:05,092 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 480 finished: foreachPartition at streamingProcessNew.scala:50, took 0.033645 s
2017-08-18 12:30:05,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030605000 ms.0 from job set of time 1503030605000 ms
2017-08-18 12:30:05,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1503030605000 ms (execution: 0.070 s)
2017-08-18 12:30:05,092 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 959 from persistence list
2017-08-18 12:30:05,093 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 958 from persistence list
2017-08-18 12:30:05,093 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 959
2017-08-18 12:30:05,093 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 958
2017-08-18 12:30:05,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:30:05,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030595000 ms
2017-08-18 12:30:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030610000 ms
2017-08-18 12:30:10,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030610000 ms.0 from job set of time 1503030610000 ms
2017-08-18 12:30:10,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:30:10,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 481 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:30:10,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 481 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:30:10,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:30:10,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:30:10,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 481 (MapPartitionsRDD[963] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:30:10,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_481 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:30:10,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_481_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:30:10,073 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_481_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:30:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 481 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:30:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 481 (MapPartitionsRDD[963] at map at streamingProcessNew.scala:49)
2017-08-18 12:30:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 481.0 with 2 tasks
2017-08-18 12:30:10,079 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 481.0 (TID 962, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:30:10,080 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 481.0 (TID 963, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:30:10,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 481.0 (TID 962)
2017-08-18 12:30:10,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 481.0 (TID 963)
2017-08-18 12:30:10,084 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:30:10,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:30:10,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 481.0 (TID 963). 714 bytes result sent to driver
2017-08-18 12:30:10,089 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 481.0 (TID 962). 714 bytes result sent to driver
2017-08-18 12:30:10,092 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 481.0 (TID 962) in 17 ms on localhost (1/2)
2017-08-18 12:30:10,092 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 481.0 (TID 963) in 13 ms on localhost (2/2)
2017-08-18 12:30:10,092 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 481.0, whose tasks have all completed, from pool 
2017-08-18 12:30:10,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 481 (foreachPartition at streamingProcessNew.scala:50) finished in 0.017 s
2017-08-18 12:30:10,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 481 finished: foreachPartition at streamingProcessNew.scala:50, took 0.034465 s
2017-08-18 12:30:10,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030610000 ms.0 from job set of time 1503030610000 ms
2017-08-18 12:30:10,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.094 s for time 1503030610000 ms (execution: 0.073 s)
2017-08-18 12:30:10,094 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 961 from persistence list
2017-08-18 12:30:10,095 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 961
2017-08-18 12:30:10,095 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 960 from persistence list
2017-08-18 12:30:10,095 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 960
2017-08-18 12:30:10,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:30:10,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030600000 ms
2017-08-18 12:30:15,024 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030615000 ms
2017-08-18 12:30:15,025 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030615000 ms.0 from job set of time 1503030615000 ms
2017-08-18 12:30:15,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:30:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 482 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:30:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 482 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:30:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:30:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:30:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 482 (MapPartitionsRDD[965] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:30:15,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_482 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:30:15,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_482_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:30:15,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_482_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:30:15,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 482 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:30:15,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 482 (MapPartitionsRDD[965] at map at streamingProcessNew.scala:49)
2017-08-18 12:30:15,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 482.0 with 2 tasks
2017-08-18 12:30:15,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 482.0 (TID 964, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:30:15,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 482.0 (TID 965, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:30:15,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 482.0 (TID 965)
2017-08-18 12:30:15,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 482.0 (TID 964)
2017-08-18 12:30:15,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:30:15,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:30:15,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 482.0 (TID 965). 714 bytes result sent to driver
2017-08-18 12:30:15,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 482.0 (TID 964). 714 bytes result sent to driver
2017-08-18 12:30:15,082 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 482.0 (TID 965) in 11 ms on localhost (1/2)
2017-08-18 12:30:15,082 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 482.0 (TID 964) in 15 ms on localhost (2/2)
2017-08-18 12:30:15,082 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 482.0, whose tasks have all completed, from pool 
2017-08-18 12:30:15,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 482 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:30:15,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 482 finished: foreachPartition at streamingProcessNew.scala:50, took 0.028882 s
2017-08-18 12:30:15,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030615000 ms.0 from job set of time 1503030615000 ms
2017-08-18 12:30:15,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1503030615000 ms (execution: 0.059 s)
2017-08-18 12:30:15,084 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 963 from persistence list
2017-08-18 12:30:15,085 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 963
2017-08-18 12:30:15,085 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 962 from persistence list
2017-08-18 12:30:15,085 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 962
2017-08-18 12:30:15,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:30:15,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030605000 ms
2017-08-18 12:30:20,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030620000 ms
2017-08-18 12:30:20,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030620000 ms.0 from job set of time 1503030620000 ms
2017-08-18 12:30:20,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:30:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 483 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:30:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 483 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:30:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:30:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:30:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 483 (MapPartitionsRDD[967] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:30:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_483 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:30:20,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_483_piece0 stored as bytes in memory (estimated size 25.1 KB, free 412.9 MB)
2017-08-18 12:30:20,081 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_483_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:30:20,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 483 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:30:20,082 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_467_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:30:20,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 483 (MapPartitionsRDD[967] at map at streamingProcessNew.scala:49)
2017-08-18 12:30:20,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 483.0 with 2 tasks
2017-08-18 12:30:20,087 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 483.0 (TID 966, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:30:20,087 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_468_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:30:20,087 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 483.0 (TID 967, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:30:20,088 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 483.0 (TID 967)
2017-08-18 12:30:20,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 483.0 (TID 966)
2017-08-18 12:30:20,090 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_469_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:30:20,096 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:30:20,096 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_470_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:30:20,097 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:30:20,099 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_471_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:30:20,103 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_472_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:30:20,105 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 483.0 (TID 967). 714 bytes result sent to driver
2017-08-18 12:30:20,105 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 483.0 (TID 966). 714 bytes result sent to driver
2017-08-18 12:30:20,109 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_473_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:30:20,111 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 483.0 (TID 966) in 29 ms on localhost (1/2)
2017-08-18 12:30:20,111 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 483.0 (TID 967) in 24 ms on localhost (2/2)
2017-08-18 12:30:20,111 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 483.0, whose tasks have all completed, from pool 
2017-08-18 12:30:20,111 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 483 (foreachPartition at streamingProcessNew.scala:50) finished in 0.029 s
2017-08-18 12:30:20,113 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 483 finished: foreachPartition at streamingProcessNew.scala:50, took 0.054405 s
2017-08-18 12:30:20,114 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_474_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:30:20,114 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030620000 ms.0 from job set of time 1503030620000 ms
2017-08-18 12:30:20,115 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 965 from persistence list
2017-08-18 12:30:20,115 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.114 s for time 1503030620000 ms (execution: 0.091 s)
2017-08-18 12:30:20,115 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 965
2017-08-18 12:30:20,115 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 964 from persistence list
2017-08-18 12:30:20,116 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 964
2017-08-18 12:30:20,116 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:30:20,117 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030610000 ms
2017-08-18 12:30:20,118 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_475_piece0 on 192.168.31.111:60809 in memory (size: 25.2 KB, free: 413.7 MB)
2017-08-18 12:30:20,121 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_476_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:30:20,123 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_477_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:30:20,126 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_478_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:30:20,128 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_479_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:30:20,129 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_480_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:30:20,130 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_481_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:30:20,131 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_482_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:30:25,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030625000 ms
2017-08-18 12:30:25,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030625000 ms.0 from job set of time 1503030625000 ms
2017-08-18 12:30:25,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:30:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 484 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:30:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 484 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:30:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:30:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:30:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 484 (MapPartitionsRDD[969] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:30:25,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_484 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:30:25,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_484_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:30:25,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_484_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:30:25,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 484 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:30:25,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 484 (MapPartitionsRDD[969] at map at streamingProcessNew.scala:49)
2017-08-18 12:30:25,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 484.0 with 2 tasks
2017-08-18 12:30:25,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 484.0 (TID 968, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:30:25,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 484.0 (TID 969, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:30:25,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 484.0 (TID 968)
2017-08-18 12:30:25,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 484.0 (TID 969)
2017-08-18 12:30:25,081 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:30:25,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:30:25,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 484.0 (TID 968). 714 bytes result sent to driver
2017-08-18 12:30:25,086 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 484.0 (TID 969). 714 bytes result sent to driver
2017-08-18 12:30:25,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 484.0 (TID 968) in 14 ms on localhost (1/2)
2017-08-18 12:30:25,089 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 484.0 (TID 969) in 11 ms on localhost (2/2)
2017-08-18 12:30:25,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 484 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:30:25,089 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 484.0, whose tasks have all completed, from pool 
2017-08-18 12:30:25,090 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 484 finished: foreachPartition at streamingProcessNew.scala:50, took 0.031402 s
2017-08-18 12:30:25,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030625000 ms.0 from job set of time 1503030625000 ms
2017-08-18 12:30:25,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.091 s for time 1503030625000 ms (execution: 0.070 s)
2017-08-18 12:30:25,091 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 967 from persistence list
2017-08-18 12:30:25,092 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 967
2017-08-18 12:30:25,092 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 966 from persistence list
2017-08-18 12:30:25,092 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 966
2017-08-18 12:30:25,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:30:25,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030615000 ms
2017-08-18 12:30:30,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030630000 ms
2017-08-18 12:30:30,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030630000 ms.0 from job set of time 1503030630000 ms
2017-08-18 12:30:30,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:30:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 485 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:30:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 485 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:30:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:30:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:30:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 485 (MapPartitionsRDD[971] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:30:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_485 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:30:30,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_485_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:30:30,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_485_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:30:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 485 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:30:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 485 (MapPartitionsRDD[971] at map at streamingProcessNew.scala:49)
2017-08-18 12:30:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 485.0 with 2 tasks
2017-08-18 12:30:30,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 485.0 (TID 970, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:30:30,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 485.0 (TID 971, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:30:30,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 485.0 (TID 970)
2017-08-18 12:30:30,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 485.0 (TID 971)
2017-08-18 12:30:30,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:30:30,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:30:30,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 485.0 (TID 970). 714 bytes result sent to driver
2017-08-18 12:30:30,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 485.0 (TID 971). 714 bytes result sent to driver
2017-08-18 12:30:30,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 485.0 (TID 970) in 13 ms on localhost (1/2)
2017-08-18 12:30:30,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 485.0 (TID 971) in 9 ms on localhost (2/2)
2017-08-18 12:30:30,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 485.0, whose tasks have all completed, from pool 
2017-08-18 12:30:30,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 485 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:30:30,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 485 finished: foreachPartition at streamingProcessNew.scala:50, took 0.028543 s
2017-08-18 12:30:30,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030630000 ms.0 from job set of time 1503030630000 ms
2017-08-18 12:30:30,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1503030630000 ms (execution: 0.062 s)
2017-08-18 12:30:30,084 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 969 from persistence list
2017-08-18 12:30:30,084 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 969
2017-08-18 12:30:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 968 from persistence list
2017-08-18 12:30:30,084 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 968
2017-08-18 12:30:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:30:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030620000 ms
2017-08-18 12:30:35,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030635000 ms
2017-08-18 12:30:35,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030635000 ms.0 from job set of time 1503030635000 ms
2017-08-18 12:30:35,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:30:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 486 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:30:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 486 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:30:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:30:35,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:30:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 486 (MapPartitionsRDD[973] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:30:35,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_486 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:30:35,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_486_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:30:35,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_486_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:30:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 486 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:30:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 486 (MapPartitionsRDD[973] at map at streamingProcessNew.scala:49)
2017-08-18 12:30:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 486.0 with 2 tasks
2017-08-18 12:30:35,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 486.0 (TID 972, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:30:35,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 486.0 (TID 973, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:30:35,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 486.0 (TID 972)
2017-08-18 12:30:35,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 486.0 (TID 973)
2017-08-18 12:30:35,080 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:30:35,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:30:35,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 486.0 (TID 973). 714 bytes result sent to driver
2017-08-18 12:30:35,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 486.0 (TID 972). 714 bytes result sent to driver
2017-08-18 12:30:35,087 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 486.0 (TID 973) in 13 ms on localhost (1/2)
2017-08-18 12:30:35,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 486.0 (TID 972) in 17 ms on localhost (2/2)
2017-08-18 12:30:35,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 486.0, whose tasks have all completed, from pool 
2017-08-18 12:30:35,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 486 (foreachPartition at streamingProcessNew.scala:50) finished in 0.017 s
2017-08-18 12:30:35,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 486 finished: foreachPartition at streamingProcessNew.scala:50, took 0.030385 s
2017-08-18 12:30:35,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030635000 ms.0 from job set of time 1503030635000 ms
2017-08-18 12:30:35,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1503030635000 ms (execution: 0.066 s)
2017-08-18 12:30:35,088 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 971 from persistence list
2017-08-18 12:30:35,088 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 971
2017-08-18 12:30:35,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 970 from persistence list
2017-08-18 12:30:35,089 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 970
2017-08-18 12:30:35,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:30:35,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030625000 ms
2017-08-18 12:30:40,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030640000 ms
2017-08-18 12:30:40,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030640000 ms.0 from job set of time 1503030640000 ms
2017-08-18 12:30:40,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:30:40,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 487 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:30:40,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 487 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:30:40,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:30:40,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:30:40,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 487 (MapPartitionsRDD[975] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:30:40,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_487 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:30:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_487_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:30:40,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_487_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:30:40,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 487 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:30:40,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 487 (MapPartitionsRDD[975] at map at streamingProcessNew.scala:49)
2017-08-18 12:30:40,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 487.0 with 2 tasks
2017-08-18 12:30:40,080 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 487.0 (TID 974, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:30:40,081 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 487.0 (TID 975, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:30:40,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 487.0 (TID 974)
2017-08-18 12:30:40,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 487.0 (TID 975)
2017-08-18 12:30:40,084 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:30:40,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:30:40,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 487.0 (TID 975). 714 bytes result sent to driver
2017-08-18 12:30:40,089 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 487.0 (TID 974). 714 bytes result sent to driver
2017-08-18 12:30:40,092 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 487.0 (TID 974) in 15 ms on localhost (1/2)
2017-08-18 12:30:40,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 487.0 (TID 975) in 12 ms on localhost (2/2)
2017-08-18 12:30:40,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 487.0, whose tasks have all completed, from pool 
2017-08-18 12:30:40,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 487 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:30:40,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 487 finished: foreachPartition at streamingProcessNew.scala:50, took 0.031704 s
2017-08-18 12:30:40,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030640000 ms.0 from job set of time 1503030640000 ms
2017-08-18 12:30:40,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.094 s for time 1503030640000 ms (execution: 0.072 s)
2017-08-18 12:30:40,094 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 973 from persistence list
2017-08-18 12:30:40,094 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 973
2017-08-18 12:30:40,095 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 972 from persistence list
2017-08-18 12:30:40,095 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 972
2017-08-18 12:30:40,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:30:40,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030630000 ms
2017-08-18 12:30:45,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030645000 ms
2017-08-18 12:30:45,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030645000 ms.0 from job set of time 1503030645000 ms
2017-08-18 12:30:45,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:30:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 488 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:30:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 488 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:30:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:30:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:30:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 488 (MapPartitionsRDD[977] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:30:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_488 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:30:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_488_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:30:45,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_488_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:30:45,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 488 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:30:45,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 488 (MapPartitionsRDD[977] at map at streamingProcessNew.scala:49)
2017-08-18 12:30:45,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 488.0 with 2 tasks
2017-08-18 12:30:45,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 488.0 (TID 976, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:30:45,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 488.0 (TID 977, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:30:45,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 488.0 (TID 976)
2017-08-18 12:30:45,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 488.0 (TID 977)
2017-08-18 12:30:45,058 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:30:45,058 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:30:45,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 488.0 (TID 977). 714 bytes result sent to driver
2017-08-18 12:30:45,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 488.0 (TID 976). 714 bytes result sent to driver
2017-08-18 12:30:45,063 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 488.0 (TID 977) in 7 ms on localhost (1/2)
2017-08-18 12:30:45,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 488.0 (TID 976) in 10 ms on localhost (2/2)
2017-08-18 12:30:45,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 488.0, whose tasks have all completed, from pool 
2017-08-18 12:30:45,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 488 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:30:45,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 488 finished: foreachPartition at streamingProcessNew.scala:50, took 0.017890 s
2017-08-18 12:30:45,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030645000 ms.0 from job set of time 1503030645000 ms
2017-08-18 12:30:45,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1503030645000 ms (execution: 0.044 s)
2017-08-18 12:30:45,065 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 975 from persistence list
2017-08-18 12:30:45,065 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 975
2017-08-18 12:30:45,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 974 from persistence list
2017-08-18 12:30:45,065 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 974
2017-08-18 12:30:45,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:30:45,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030635000 ms
2017-08-18 12:30:50,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030650000 ms
2017-08-18 12:30:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030650000 ms.0 from job set of time 1503030650000 ms
2017-08-18 12:30:50,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:30:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 489 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:30:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 489 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:30:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:30:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:30:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 489 (MapPartitionsRDD[979] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:30:50,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_489 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:30:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_489_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:30:50,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_489_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:30:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 489 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:30:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 489 (MapPartitionsRDD[979] at map at streamingProcessNew.scala:49)
2017-08-18 12:30:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 489.0 with 2 tasks
2017-08-18 12:30:50,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 489.0 (TID 978, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:30:50,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 489.0 (TID 979, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:30:50,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 489.0 (TID 979)
2017-08-18 12:30:50,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 489.0 (TID 978)
2017-08-18 12:30:50,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:30:50,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:30:50,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 489.0 (TID 979). 714 bytes result sent to driver
2017-08-18 12:30:50,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 489.0 (TID 978). 714 bytes result sent to driver
2017-08-18 12:30:50,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 489.0 (TID 978) in 12 ms on localhost (1/2)
2017-08-18 12:30:50,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 489.0 (TID 979) in 9 ms on localhost (2/2)
2017-08-18 12:30:50,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 489.0, whose tasks have all completed, from pool 
2017-08-18 12:30:50,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 489 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:30:50,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 489 finished: foreachPartition at streamingProcessNew.scala:50, took 0.023615 s
2017-08-18 12:30:50,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030650000 ms.0 from job set of time 1503030650000 ms
2017-08-18 12:30:50,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1503030650000 ms (execution: 0.062 s)
2017-08-18 12:30:50,083 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 977 from persistence list
2017-08-18 12:30:50,084 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 977
2017-08-18 12:30:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 976 from persistence list
2017-08-18 12:30:50,084 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 976
2017-08-18 12:30:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:30:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030640000 ms
2017-08-18 12:30:55,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030655000 ms
2017-08-18 12:30:55,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030655000 ms.0 from job set of time 1503030655000 ms
2017-08-18 12:30:55,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:30:55,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 490 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:30:55,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 490 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:30:55,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:30:55,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:30:55,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 490 (MapPartitionsRDD[981] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:30:55,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_490 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:30:55,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_490_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:30:55,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_490_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:30:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 490 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:30:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 490 (MapPartitionsRDD[981] at map at streamingProcessNew.scala:49)
2017-08-18 12:30:55,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 490.0 with 2 tasks
2017-08-18 12:30:55,078 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 490.0 (TID 980, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:30:55,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 490.0 (TID 981, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:30:55,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 490.0 (TID 980)
2017-08-18 12:30:55,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 490.0 (TID 981)
2017-08-18 12:30:55,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:30:55,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:30:55,088 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 490.0 (TID 981). 714 bytes result sent to driver
2017-08-18 12:30:55,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 490.0 (TID 980). 714 bytes result sent to driver
2017-08-18 12:30:55,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 490.0 (TID 980) in 16 ms on localhost (1/2)
2017-08-18 12:30:55,091 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 490.0 (TID 981) in 13 ms on localhost (2/2)
2017-08-18 12:30:55,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 490.0, whose tasks have all completed, from pool 
2017-08-18 12:30:55,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 490 (foreachPartition at streamingProcessNew.scala:50) finished in 0.018 s
2017-08-18 12:30:55,092 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 490 finished: foreachPartition at streamingProcessNew.scala:50, took 0.032909 s
2017-08-18 12:30:55,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030655000 ms.0 from job set of time 1503030655000 ms
2017-08-18 12:30:55,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.093 s for time 1503030655000 ms (execution: 0.072 s)
2017-08-18 12:30:55,093 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 979 from persistence list
2017-08-18 12:30:55,094 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 979
2017-08-18 12:30:55,094 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 978 from persistence list
2017-08-18 12:30:55,094 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 978
2017-08-18 12:30:55,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:30:55,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030645000 ms
2017-08-18 12:31:00,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030660000 ms
2017-08-18 12:31:00,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030660000 ms.0 from job set of time 1503030660000 ms
2017-08-18 12:31:00,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:31:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 491 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:31:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 491 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:31:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:31:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:31:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 491 (MapPartitionsRDD[983] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:31:00,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_491 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:31:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_491_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:31:00,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_491_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:31:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 491 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:31:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 491 (MapPartitionsRDD[983] at map at streamingProcessNew.scala:49)
2017-08-18 12:31:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 491.0 with 2 tasks
2017-08-18 12:31:00,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 491.0 (TID 982, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:31:00,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 491.0 (TID 983, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:31:00,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 491.0 (TID 982)
2017-08-18 12:31:00,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 491.0 (TID 983)
2017-08-18 12:31:00,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:31:00,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:31:00,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 491.0 (TID 982). 714 bytes result sent to driver
2017-08-18 12:31:00,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 491.0 (TID 983). 714 bytes result sent to driver
2017-08-18 12:31:00,076 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 491.0 (TID 982) in 12 ms on localhost (1/2)
2017-08-18 12:31:00,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 491.0 (TID 983) in 9 ms on localhost (2/2)
2017-08-18 12:31:00,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 491.0, whose tasks have all completed, from pool 
2017-08-18 12:31:00,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 491 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:31:00,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 491 finished: foreachPartition at streamingProcessNew.scala:50, took 0.023731 s
2017-08-18 12:31:00,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030660000 ms.0 from job set of time 1503030660000 ms
2017-08-18 12:31:00,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1503030660000 ms (execution: 0.055 s)
2017-08-18 12:31:00,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 981 from persistence list
2017-08-18 12:31:00,078 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 981
2017-08-18 12:31:00,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 980 from persistence list
2017-08-18 12:31:00,078 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 980
2017-08-18 12:31:00,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:31:00,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030650000 ms
2017-08-18 12:31:05,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030665000 ms
2017-08-18 12:31:05,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030665000 ms.0 from job set of time 1503030665000 ms
2017-08-18 12:31:05,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:31:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 492 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:31:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 492 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:31:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:31:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:31:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 492 (MapPartitionsRDD[985] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:31:05,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_492 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:31:05,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_492_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:31:05,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_492_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:31:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 492 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:31:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 492 (MapPartitionsRDD[985] at map at streamingProcessNew.scala:49)
2017-08-18 12:31:05,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 492.0 with 2 tasks
2017-08-18 12:31:05,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 492.0 (TID 984, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:31:05,060 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 492.0 (TID 985, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:31:05,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 492.0 (TID 984)
2017-08-18 12:31:05,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 492.0 (TID 985)
2017-08-18 12:31:05,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:31:05,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:31:05,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 492.0 (TID 985). 714 bytes result sent to driver
2017-08-18 12:31:05,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 492.0 (TID 984). 714 bytes result sent to driver
2017-08-18 12:31:05,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 492.0 (TID 985) in 9 ms on localhost (1/2)
2017-08-18 12:31:05,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 492.0 (TID 984) in 12 ms on localhost (2/2)
2017-08-18 12:31:05,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 492.0, whose tasks have all completed, from pool 
2017-08-18 12:31:05,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 492 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:31:05,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 492 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024836 s
2017-08-18 12:31:05,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030665000 ms.0 from job set of time 1503030665000 ms
2017-08-18 12:31:05,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1503030665000 ms (execution: 0.046 s)
2017-08-18 12:31:05,069 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 983 from persistence list
2017-08-18 12:31:05,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 982 from persistence list
2017-08-18 12:31:05,070 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 983
2017-08-18 12:31:05,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:31:05,070 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 982
2017-08-18 12:31:05,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030655000 ms
2017-08-18 12:31:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030670000 ms
2017-08-18 12:31:10,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030670000 ms.0 from job set of time 1503030670000 ms
2017-08-18 12:31:10,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:31:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 493 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:31:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 493 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:31:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:31:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:31:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 493 (MapPartitionsRDD[987] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:31:10,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_493 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:31:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_493_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:31:10,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_493_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:31:10,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 493 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:31:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 493 (MapPartitionsRDD[987] at map at streamingProcessNew.scala:49)
2017-08-18 12:31:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 493.0 with 2 tasks
2017-08-18 12:31:10,082 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 493.0 (TID 986, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:31:10,083 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 493.0 (TID 987, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:31:10,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 493.0 (TID 987)
2017-08-18 12:31:10,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 493.0 (TID 986)
2017-08-18 12:31:10,086 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:31:10,086 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:31:10,090 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 493.0 (TID 987). 714 bytes result sent to driver
2017-08-18 12:31:10,090 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 493.0 (TID 986). 714 bytes result sent to driver
2017-08-18 12:31:10,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 493.0 (TID 987) in 10 ms on localhost (1/2)
2017-08-18 12:31:10,093 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 493.0 (TID 986) in 17 ms on localhost (2/2)
2017-08-18 12:31:10,093 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 493.0, whose tasks have all completed, from pool 
2017-08-18 12:31:10,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 493 (foreachPartition at streamingProcessNew.scala:50) finished in 0.017 s
2017-08-18 12:31:10,094 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 493 finished: foreachPartition at streamingProcessNew.scala:50, took 0.033729 s
2017-08-18 12:31:10,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030670000 ms.0 from job set of time 1503030670000 ms
2017-08-18 12:31:10,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.094 s for time 1503030670000 ms (execution: 0.073 s)
2017-08-18 12:31:10,095 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 985 from persistence list
2017-08-18 12:31:10,095 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 985
2017-08-18 12:31:10,095 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 984 from persistence list
2017-08-18 12:31:10,095 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 984
2017-08-18 12:31:10,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:31:10,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030660000 ms
2017-08-18 12:31:15,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030675000 ms
2017-08-18 12:31:15,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030675000 ms.0 from job set of time 1503030675000 ms
2017-08-18 12:31:15,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:31:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 494 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:31:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 494 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:31:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:31:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:31:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 494 (MapPartitionsRDD[989] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:31:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_494 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:31:15,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_494_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:31:15,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_494_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:31:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 494 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:31:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 494 (MapPartitionsRDD[989] at map at streamingProcessNew.scala:49)
2017-08-18 12:31:15,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 494.0 with 2 tasks
2017-08-18 12:31:15,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 494.0 (TID 988, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:31:15,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 494.0 (TID 989, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:31:15,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 494.0 (TID 989)
2017-08-18 12:31:15,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 494.0 (TID 988)
2017-08-18 12:31:15,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:31:15,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:31:15,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 494.0 (TID 989). 714 bytes result sent to driver
2017-08-18 12:31:15,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 494.0 (TID 988). 714 bytes result sent to driver
2017-08-18 12:31:15,088 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 494.0 (TID 989) in 14 ms on localhost (1/2)
2017-08-18 12:31:15,089 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 494.0 (TID 988) in 19 ms on localhost (2/2)
2017-08-18 12:31:15,089 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 494.0, whose tasks have all completed, from pool 
2017-08-18 12:31:15,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 494 (foreachPartition at streamingProcessNew.scala:50) finished in 0.019 s
2017-08-18 12:31:15,090 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 494 finished: foreachPartition at streamingProcessNew.scala:50, took 0.035522 s
2017-08-18 12:31:15,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030675000 ms.0 from job set of time 1503030675000 ms
2017-08-18 12:31:15,091 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 987 from persistence list
2017-08-18 12:31:15,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1503030675000 ms (execution: 0.069 s)
2017-08-18 12:31:15,091 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 987
2017-08-18 12:31:15,091 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 986 from persistence list
2017-08-18 12:31:15,092 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 986
2017-08-18 12:31:15,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:31:15,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030665000 ms
2017-08-18 12:31:20,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030680000 ms
2017-08-18 12:31:20,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030680000 ms.0 from job set of time 1503030680000 ms
2017-08-18 12:31:20,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:31:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 495 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:31:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 495 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:31:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:31:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:31:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 495 (MapPartitionsRDD[991] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:31:20,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_495 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:31:20,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_495_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:31:20,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_495_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:31:20,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 495 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:31:20,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 495 (MapPartitionsRDD[991] at map at streamingProcessNew.scala:49)
2017-08-18 12:31:20,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 495.0 with 2 tasks
2017-08-18 12:31:20,078 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 495.0 (TID 990, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:31:20,079 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 495.0 (TID 991, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:31:20,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 495.0 (TID 990)
2017-08-18 12:31:20,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 495.0 (TID 991)
2017-08-18 12:31:20,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:31:20,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:31:20,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 495.0 (TID 991). 714 bytes result sent to driver
2017-08-18 12:31:20,086 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 495.0 (TID 990). 714 bytes result sent to driver
2017-08-18 12:31:20,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 495.0 (TID 991) in 10 ms on localhost (1/2)
2017-08-18 12:31:20,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 495.0 (TID 990) in 14 ms on localhost (2/2)
2017-08-18 12:31:20,089 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 495.0, whose tasks have all completed, from pool 
2017-08-18 12:31:20,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 495 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:31:20,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 495 finished: foreachPartition at streamingProcessNew.scala:50, took 0.028935 s
2017-08-18 12:31:20,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030680000 ms.0 from job set of time 1503030680000 ms
2017-08-18 12:31:20,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1503030680000 ms (execution: 0.067 s)
2017-08-18 12:31:20,089 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 989 from persistence list
2017-08-18 12:31:20,090 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 989
2017-08-18 12:31:20,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 988 from persistence list
2017-08-18 12:31:20,090 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 988
2017-08-18 12:31:20,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:31:20,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030670000 ms
2017-08-18 12:31:25,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030685000 ms
2017-08-18 12:31:25,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030685000 ms.0 from job set of time 1503030685000 ms
2017-08-18 12:31:25,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:31:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 496 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:31:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 496 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:31:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:31:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:31:25,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 496 (MapPartitionsRDD[993] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:31:25,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_496 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:31:25,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_496_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:31:25,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_496_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:31:25,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 496 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:31:25,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 496 (MapPartitionsRDD[993] at map at streamingProcessNew.scala:49)
2017-08-18 12:31:25,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 496.0 with 2 tasks
2017-08-18 12:31:25,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 496.0 (TID 992, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:31:25,080 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 496.0 (TID 993, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:31:25,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 496.0 (TID 992)
2017-08-18 12:31:25,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 496.0 (TID 993)
2017-08-18 12:31:25,083 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:31:25,083 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:31:25,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 496.0 (TID 993). 714 bytes result sent to driver
2017-08-18 12:31:25,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 496.0 (TID 992). 714 bytes result sent to driver
2017-08-18 12:31:25,090 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 496.0 (TID 993) in 11 ms on localhost (1/2)
2017-08-18 12:31:25,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 496.0 (TID 992) in 16 ms on localhost (2/2)
2017-08-18 12:31:25,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 496 (foreachPartition at streamingProcessNew.scala:50) finished in 0.017 s
2017-08-18 12:31:25,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 496.0, whose tasks have all completed, from pool 
2017-08-18 12:31:25,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 496 finished: foreachPartition at streamingProcessNew.scala:50, took 0.031755 s
2017-08-18 12:31:25,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030685000 ms.0 from job set of time 1503030685000 ms
2017-08-18 12:31:25,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.093 s for time 1503030685000 ms (execution: 0.071 s)
2017-08-18 12:31:25,093 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 991 from persistence list
2017-08-18 12:31:25,094 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 991
2017-08-18 12:31:25,094 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 990 from persistence list
2017-08-18 12:31:25,094 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 990
2017-08-18 12:31:25,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:31:25,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030675000 ms
2017-08-18 12:31:30,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030690000 ms
2017-08-18 12:31:30,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030690000 ms.0 from job set of time 1503030690000 ms
2017-08-18 12:31:30,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:31:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 497 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:31:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 497 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:31:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:31:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:31:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 497 (MapPartitionsRDD[995] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:31:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_497 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:31:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_497_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:31:30,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_497_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:31:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 497 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:31:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 497 (MapPartitionsRDD[995] at map at streamingProcessNew.scala:49)
2017-08-18 12:31:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 497.0 with 2 tasks
2017-08-18 12:31:30,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 497.0 (TID 994, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:31:30,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 497.0 (TID 995, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:31:30,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 497.0 (TID 995)
2017-08-18 12:31:30,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 497.0 (TID 994)
2017-08-18 12:31:30,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:31:30,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:31:30,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 497.0 (TID 995). 714 bytes result sent to driver
2017-08-18 12:31:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 497.0 (TID 994). 714 bytes result sent to driver
2017-08-18 12:31:30,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 497.0 (TID 995) in 10 ms on localhost (1/2)
2017-08-18 12:31:30,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 497.0 (TID 994) in 13 ms on localhost (2/2)
2017-08-18 12:31:30,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 497.0, whose tasks have all completed, from pool 
2017-08-18 12:31:30,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 497 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:31:30,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 497 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024462 s
2017-08-18 12:31:30,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030690000 ms.0 from job set of time 1503030690000 ms
2017-08-18 12:31:30,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1503030690000 ms (execution: 0.055 s)
2017-08-18 12:31:30,076 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 993 from persistence list
2017-08-18 12:31:30,077 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 993
2017-08-18 12:31:30,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 992 from persistence list
2017-08-18 12:31:30,077 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 992
2017-08-18 12:31:30,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:31:30,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030680000 ms
2017-08-18 12:31:35,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030695000 ms
2017-08-18 12:31:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030695000 ms.0 from job set of time 1503030695000 ms
2017-08-18 12:31:35,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:31:35,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 498 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:31:35,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 498 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:31:35,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:31:35,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:31:35,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 498 (MapPartitionsRDD[997] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:31:35,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_498 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:31:35,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_498_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:31:35,086 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_498_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:31:35,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 498 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:31:35,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 498 (MapPartitionsRDD[997] at map at streamingProcessNew.scala:49)
2017-08-18 12:31:35,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 498.0 with 2 tasks
2017-08-18 12:31:35,091 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 498.0 (TID 996, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:31:35,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 498.0 (TID 997, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:31:35,092 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 498.0 (TID 997)
2017-08-18 12:31:35,092 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 498.0 (TID 996)
2017-08-18 12:31:35,115 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_483_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:31:35,115 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:31:35,115 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:31:35,119 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_484_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:31:35,122 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 498.0 (TID 996). 787 bytes result sent to driver
2017-08-18 12:31:35,122 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 498.0 (TID 997). 787 bytes result sent to driver
2017-08-18 12:31:35,122 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_485_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:31:35,124 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_486_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:31:35,126 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 498.0 (TID 997) in 35 ms on localhost (1/2)
2017-08-18 12:31:35,126 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 498.0 (TID 996) in 38 ms on localhost (2/2)
2017-08-18 12:31:35,126 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 498.0, whose tasks have all completed, from pool 
2017-08-18 12:31:35,126 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 498 (foreachPartition at streamingProcessNew.scala:50) finished in 0.038 s
2017-08-18 12:31:35,127 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 498 finished: foreachPartition at streamingProcessNew.scala:50, took 0.064932 s
2017-08-18 12:31:35,128 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030695000 ms.0 from job set of time 1503030695000 ms
2017-08-18 12:31:35,129 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.128 s for time 1503030695000 ms (execution: 0.107 s)
2017-08-18 12:31:35,129 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 995 from persistence list
2017-08-18 12:31:35,130 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_487_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:31:35,130 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 995
2017-08-18 12:31:35,130 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 994 from persistence list
2017-08-18 12:31:35,130 [block-manager-slave-async-thread-pool-41] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 994
2017-08-18 12:31:35,130 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:31:35,130 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030685000 ms
2017-08-18 12:31:35,133 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_488_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:31:35,135 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_489_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:31:35,137 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_490_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:31:35,142 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_491_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:31:35,144 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_492_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:31:35,147 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_493_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:31:35,151 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_494_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:31:35,152 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_495_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:31:35,153 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_496_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:31:35,154 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_497_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:31:40,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030700000 ms
2017-08-18 12:31:40,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030700000 ms.0 from job set of time 1503030700000 ms
2017-08-18 12:31:40,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:31:40,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 499 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:31:40,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 499 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:31:40,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:31:40,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:31:40,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 499 (MapPartitionsRDD[999] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:31:40,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_499 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:31:40,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_499_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:31:40,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_499_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:31:40,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 499 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:31:40,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 499 (MapPartitionsRDD[999] at map at streamingProcessNew.scala:49)
2017-08-18 12:31:40,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 499.0 with 2 tasks
2017-08-18 12:31:40,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 499.0 (TID 998, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:31:40,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 499.0 (TID 999, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:31:40,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 499.0 (TID 999)
2017-08-18 12:31:40,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 499.0 (TID 998)
2017-08-18 12:31:40,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:31:40,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:31:40,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 499.0 (TID 998). 714 bytes result sent to driver
2017-08-18 12:31:40,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 499.0 (TID 999). 714 bytes result sent to driver
2017-08-18 12:31:40,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 499.0 (TID 998) in 9 ms on localhost (1/2)
2017-08-18 12:31:40,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 499.0 (TID 999) in 8 ms on localhost (2/2)
2017-08-18 12:31:40,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 499.0, whose tasks have all completed, from pool 
2017-08-18 12:31:40,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 499 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:31:40,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 499 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024977 s
2017-08-18 12:31:40,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030700000 ms.0 from job set of time 1503030700000 ms
2017-08-18 12:31:40,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1503030700000 ms (execution: 0.057 s)
2017-08-18 12:31:40,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 997 from persistence list
2017-08-18 12:31:40,079 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 997
2017-08-18 12:31:40,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 996 from persistence list
2017-08-18 12:31:40,080 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 996
2017-08-18 12:31:40,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:31:40,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030690000 ms
2017-08-18 12:31:45,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030705000 ms
2017-08-18 12:31:45,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030705000 ms.0 from job set of time 1503030705000 ms
2017-08-18 12:31:45,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:31:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 500 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:31:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 500 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:31:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:31:45,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:31:45,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 500 (MapPartitionsRDD[1001] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:31:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_500 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:31:45,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_500_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:31:45,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_500_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:31:45,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 500 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:31:45,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 500 (MapPartitionsRDD[1001] at map at streamingProcessNew.scala:49)
2017-08-18 12:31:45,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 500.0 with 2 tasks
2017-08-18 12:31:45,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 500.0 (TID 1000, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:31:45,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 500.0 (TID 1001, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:31:45,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 500.0 (TID 1001)
2017-08-18 12:31:45,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 500.0 (TID 1000)
2017-08-18 12:31:45,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:31:45,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:31:45,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 500.0 (TID 1001). 714 bytes result sent to driver
2017-08-18 12:31:45,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 500.0 (TID 1000). 714 bytes result sent to driver
2017-08-18 12:31:45,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 500.0 (TID 1001) in 10 ms on localhost (1/2)
2017-08-18 12:31:45,084 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 500.0 (TID 1000) in 15 ms on localhost (2/2)
2017-08-18 12:31:45,085 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 500.0, whose tasks have all completed, from pool 
2017-08-18 12:31:45,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 500 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:31:45,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 500 finished: foreachPartition at streamingProcessNew.scala:50, took 0.033918 s
2017-08-18 12:31:45,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030705000 ms.0 from job set of time 1503030705000 ms
2017-08-18 12:31:45,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1503030705000 ms (execution: 0.063 s)
2017-08-18 12:31:45,086 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 999 from persistence list
2017-08-18 12:31:45,086 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 999
2017-08-18 12:31:45,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 998 from persistence list
2017-08-18 12:31:45,087 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 998
2017-08-18 12:31:45,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:31:45,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030695000 ms
2017-08-18 12:31:50,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030710000 ms
2017-08-18 12:31:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030710000 ms.0 from job set of time 1503030710000 ms
2017-08-18 12:31:50,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:31:50,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 501 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:31:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 501 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:31:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:31:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:31:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 501 (MapPartitionsRDD[1003] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:31:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_501 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:31:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_501_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:31:50,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_501_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:31:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 501 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:31:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 501 (MapPartitionsRDD[1003] at map at streamingProcessNew.scala:49)
2017-08-18 12:31:50,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 501.0 with 2 tasks
2017-08-18 12:31:50,075 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 501.0 (TID 1002, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:31:50,076 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 501.0 (TID 1003, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:31:50,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 501.0 (TID 1003)
2017-08-18 12:31:50,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 501.0 (TID 1002)
2017-08-18 12:31:50,081 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:31:50,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:31:50,086 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 501.0 (TID 1002). 714 bytes result sent to driver
2017-08-18 12:31:50,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 501.0 (TID 1003). 714 bytes result sent to driver
2017-08-18 12:31:50,089 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 501.0 (TID 1002) in 17 ms on localhost (1/2)
2017-08-18 12:31:50,089 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 501.0 (TID 1003) in 13 ms on localhost (2/2)
2017-08-18 12:31:50,089 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 501.0, whose tasks have all completed, from pool 
2017-08-18 12:31:50,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 501 (foreachPartition at streamingProcessNew.scala:50) finished in 0.019 s
2017-08-18 12:31:50,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 501 finished: foreachPartition at streamingProcessNew.scala:50, took 0.037758 s
2017-08-18 12:31:50,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030710000 ms.0 from job set of time 1503030710000 ms
2017-08-18 12:31:50,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1503030710000 ms (execution: 0.069 s)
2017-08-18 12:31:50,090 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1001 from persistence list
2017-08-18 12:31:50,090 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1001
2017-08-18 12:31:50,091 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1000 from persistence list
2017-08-18 12:31:50,091 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1000
2017-08-18 12:31:50,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:31:50,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030700000 ms
2017-08-18 12:31:55,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030715000 ms
2017-08-18 12:31:55,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030715000 ms.0 from job set of time 1503030715000 ms
2017-08-18 12:31:55,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:31:55,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 502 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:31:55,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 502 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:31:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:31:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:31:55,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 502 (MapPartitionsRDD[1005] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:31:55,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_502 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:31:55,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_502_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:31:55,085 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_502_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:31:55,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 502 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:31:55,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 502 (MapPartitionsRDD[1005] at map at streamingProcessNew.scala:49)
2017-08-18 12:31:55,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 502.0 with 2 tasks
2017-08-18 12:31:55,094 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 502.0 (TID 1004, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:31:55,096 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 502.0 (TID 1005, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:31:55,096 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 502.0 (TID 1005)
2017-08-18 12:31:55,096 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 502.0 (TID 1004)
2017-08-18 12:31:55,100 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:31:55,100 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:31:55,105 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 502.0 (TID 1005). 714 bytes result sent to driver
2017-08-18 12:31:55,105 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 502.0 (TID 1004). 714 bytes result sent to driver
2017-08-18 12:31:55,109 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 502.0 (TID 1004) in 21 ms on localhost (1/2)
2017-08-18 12:31:55,110 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 502.0 (TID 1005) in 15 ms on localhost (2/2)
2017-08-18 12:31:55,110 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 502.0, whose tasks have all completed, from pool 
2017-08-18 12:31:55,110 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 502 (foreachPartition at streamingProcessNew.scala:50) finished in 0.022 s
2017-08-18 12:31:55,111 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 502 finished: foreachPartition at streamingProcessNew.scala:50, took 0.049719 s
2017-08-18 12:31:55,111 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030715000 ms.0 from job set of time 1503030715000 ms
2017-08-18 12:31:55,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.111 s for time 1503030715000 ms (execution: 0.090 s)
2017-08-18 12:31:55,112 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1003 from persistence list
2017-08-18 12:31:55,112 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1003
2017-08-18 12:31:55,112 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1002 from persistence list
2017-08-18 12:31:55,113 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1002
2017-08-18 12:31:55,113 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:31:55,113 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030705000 ms
2017-08-18 12:32:00,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030720000 ms
2017-08-18 12:32:00,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030720000 ms.0 from job set of time 1503030720000 ms
2017-08-18 12:32:00,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:32:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 503 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:32:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 503 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:32:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:32:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:32:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 503 (MapPartitionsRDD[1007] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:32:00,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_503 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:32:00,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_503_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:32:00,078 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_503_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:32:00,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 503 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:32:00,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 503 (MapPartitionsRDD[1007] at map at streamingProcessNew.scala:49)
2017-08-18 12:32:00,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 503.0 with 2 tasks
2017-08-18 12:32:00,083 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 503.0 (TID 1006, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:32:00,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 503.0 (TID 1007, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:32:00,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 503.0 (TID 1007)
2017-08-18 12:32:00,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 503.0 (TID 1006)
2017-08-18 12:32:00,087 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:32:00,087 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:32:00,091 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 503.0 (TID 1007). 714 bytes result sent to driver
2017-08-18 12:32:00,092 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 503.0 (TID 1006). 714 bytes result sent to driver
2017-08-18 12:32:00,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 503.0 (TID 1007) in 10 ms on localhost (1/2)
2017-08-18 12:32:00,094 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 503.0 (TID 1006) in 14 ms on localhost (2/2)
2017-08-18 12:32:00,094 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 503.0, whose tasks have all completed, from pool 
2017-08-18 12:32:00,094 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 503 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:32:00,095 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 503 finished: foreachPartition at streamingProcessNew.scala:50, took 0.034809 s
2017-08-18 12:32:00,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030720000 ms.0 from job set of time 1503030720000 ms
2017-08-18 12:32:00,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.095 s for time 1503030720000 ms (execution: 0.074 s)
2017-08-18 12:32:00,096 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1005 from persistence list
2017-08-18 12:32:00,096 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1005
2017-08-18 12:32:00,096 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1004 from persistence list
2017-08-18 12:32:00,096 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1004
2017-08-18 12:32:00,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:32:00,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030710000 ms
2017-08-18 12:32:05,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030725000 ms
2017-08-18 12:32:05,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030725000 ms.0 from job set of time 1503030725000 ms
2017-08-18 12:32:05,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:32:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 504 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:32:05,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 504 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:32:05,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:32:05,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:32:05,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 504 (MapPartitionsRDD[1009] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:32:05,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_504 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:32:05,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_504_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:32:05,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_504_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:32:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 504 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:32:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 504 (MapPartitionsRDD[1009] at map at streamingProcessNew.scala:49)
2017-08-18 12:32:05,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 504.0 with 2 tasks
2017-08-18 12:32:05,080 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 504.0 (TID 1008, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:32:05,081 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 504.0 (TID 1009, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:32:05,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 504.0 (TID 1009)
2017-08-18 12:32:05,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 504.0 (TID 1008)
2017-08-18 12:32:05,085 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:32:05,085 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:32:05,089 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 504.0 (TID 1008). 714 bytes result sent to driver
2017-08-18 12:32:05,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 504.0 (TID 1009). 714 bytes result sent to driver
2017-08-18 12:32:05,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 504.0 (TID 1008) in 15 ms on localhost (1/2)
2017-08-18 12:32:05,091 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 504.0 (TID 1009) in 11 ms on localhost (2/2)
2017-08-18 12:32:05,091 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 504.0, whose tasks have all completed, from pool 
2017-08-18 12:32:05,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 504 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:32:05,091 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 504 finished: foreachPartition at streamingProcessNew.scala:50, took 0.032479 s
2017-08-18 12:32:05,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030725000 ms.0 from job set of time 1503030725000 ms
2017-08-18 12:32:05,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1503030725000 ms (execution: 0.070 s)
2017-08-18 12:32:05,092 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1007 from persistence list
2017-08-18 12:32:05,092 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1007
2017-08-18 12:32:05,093 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1006 from persistence list
2017-08-18 12:32:05,093 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1006
2017-08-18 12:32:05,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:32:05,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030715000 ms
2017-08-18 12:32:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030730000 ms
2017-08-18 12:32:10,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030730000 ms.0 from job set of time 1503030730000 ms
2017-08-18 12:32:10,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:32:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 505 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:32:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 505 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:32:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:32:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:32:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 505 (MapPartitionsRDD[1011] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:32:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_505 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:32:10,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_505_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:32:10,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_505_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:32:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 505 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:32:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 505 (MapPartitionsRDD[1011] at map at streamingProcessNew.scala:49)
2017-08-18 12:32:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 505.0 with 2 tasks
2017-08-18 12:32:10,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 505.0 (TID 1010, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:32:10,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 505.0 (TID 1011, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:32:10,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 505.0 (TID 1011)
2017-08-18 12:32:10,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 505.0 (TID 1010)
2017-08-18 12:32:10,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:32:10,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:32:10,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 505.0 (TID 1010). 714 bytes result sent to driver
2017-08-18 12:32:10,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 505.0 (TID 1011). 714 bytes result sent to driver
2017-08-18 12:32:10,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 505.0 (TID 1010) in 13 ms on localhost (1/2)
2017-08-18 12:32:10,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 505.0 (TID 1011) in 8 ms on localhost (2/2)
2017-08-18 12:32:10,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 505.0, whose tasks have all completed, from pool 
2017-08-18 12:32:10,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 505 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:32:10,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 505 finished: foreachPartition at streamingProcessNew.scala:50, took 0.027546 s
2017-08-18 12:32:10,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030730000 ms.0 from job set of time 1503030730000 ms
2017-08-18 12:32:10,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1503030730000 ms (execution: 0.055 s)
2017-08-18 12:32:10,076 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1009 from persistence list
2017-08-18 12:32:10,077 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1009
2017-08-18 12:32:10,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1008 from persistence list
2017-08-18 12:32:10,077 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1008
2017-08-18 12:32:10,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:32:10,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030720000 ms
2017-08-18 12:32:15,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030735000 ms
2017-08-18 12:32:15,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030735000 ms.0 from job set of time 1503030735000 ms
2017-08-18 12:32:15,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:32:15,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 506 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:32:15,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 506 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:32:15,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:32:15,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:32:15,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 506 (MapPartitionsRDD[1013] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:32:15,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_506 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:32:15,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_506_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:32:15,042 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_506_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:32:15,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 506 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:32:15,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 506 (MapPartitionsRDD[1013] at map at streamingProcessNew.scala:49)
2017-08-18 12:32:15,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 506.0 with 2 tasks
2017-08-18 12:32:15,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 506.0 (TID 1012, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:32:15,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 506.0 (TID 1013, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:32:15,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 506.0 (TID 1012)
2017-08-18 12:32:15,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 506.0 (TID 1013)
2017-08-18 12:32:15,050 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:32:15,050 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:32:15,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 506.0 (TID 1013). 714 bytes result sent to driver
2017-08-18 12:32:15,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 506.0 (TID 1012). 714 bytes result sent to driver
2017-08-18 12:32:15,054 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 506.0 (TID 1013) in 8 ms on localhost (1/2)
2017-08-18 12:32:15,054 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 506.0 (TID 1012) in 11 ms on localhost (2/2)
2017-08-18 12:32:15,055 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 506.0, whose tasks have all completed, from pool 
2017-08-18 12:32:15,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 506 (foreachPartition at streamingProcessNew.scala:50) finished in 0.012 s
2017-08-18 12:32:15,055 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 506 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024523 s
2017-08-18 12:32:15,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030735000 ms.0 from job set of time 1503030735000 ms
2017-08-18 12:32:15,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.055 s for time 1503030735000 ms (execution: 0.041 s)
2017-08-18 12:32:15,055 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1011 from persistence list
2017-08-18 12:32:15,056 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1011
2017-08-18 12:32:15,056 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1010 from persistence list
2017-08-18 12:32:15,056 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1010
2017-08-18 12:32:15,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:32:15,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030725000 ms
2017-08-18 12:32:20,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030740000 ms
2017-08-18 12:32:20,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030740000 ms.0 from job set of time 1503030740000 ms
2017-08-18 12:32:20,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:32:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 507 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:32:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 507 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:32:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:32:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:32:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 507 (MapPartitionsRDD[1015] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:32:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_507 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:32:20,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_507_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:32:20,058 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_507_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:32:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 507 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:32:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 507 (MapPartitionsRDD[1015] at map at streamingProcessNew.scala:49)
2017-08-18 12:32:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 507.0 with 2 tasks
2017-08-18 12:32:20,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 507.0 (TID 1014, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:32:20,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 507.0 (TID 1015, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:32:20,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 507.0 (TID 1015)
2017-08-18 12:32:20,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 507.0 (TID 1014)
2017-08-18 12:32:20,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:32:20,066 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:32:20,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 507.0 (TID 1015). 714 bytes result sent to driver
2017-08-18 12:32:20,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 507.0 (TID 1014). 714 bytes result sent to driver
2017-08-18 12:32:20,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 507.0 (TID 1015) in 10 ms on localhost (1/2)
2017-08-18 12:32:20,071 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 507.0 (TID 1014) in 12 ms on localhost (2/2)
2017-08-18 12:32:20,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 507.0, whose tasks have all completed, from pool 
2017-08-18 12:32:20,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 507 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:32:20,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 507 finished: foreachPartition at streamingProcessNew.scala:50, took 0.026682 s
2017-08-18 12:32:20,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030740000 ms.0 from job set of time 1503030740000 ms
2017-08-18 12:32:20,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1503030740000 ms (execution: 0.049 s)
2017-08-18 12:32:20,073 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1013 from persistence list
2017-08-18 12:32:20,073 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1013
2017-08-18 12:32:20,073 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1012 from persistence list
2017-08-18 12:32:20,073 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1012
2017-08-18 12:32:20,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:32:20,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030730000 ms
2017-08-18 12:32:25,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030745000 ms
2017-08-18 12:32:25,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030745000 ms.0 from job set of time 1503030745000 ms
2017-08-18 12:32:25,067 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:32:25,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 508 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:32:25,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 508 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:32:25,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:32:25,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:32:25,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 508 (MapPartitionsRDD[1017] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:32:25,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_508 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:32:25,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_508_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:32:25,088 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_508_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:32:25,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 508 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:32:25,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 508 (MapPartitionsRDD[1017] at map at streamingProcessNew.scala:49)
2017-08-18 12:32:25,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 508.0 with 2 tasks
2017-08-18 12:32:25,094 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 508.0 (TID 1016, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:32:25,095 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 508.0 (TID 1017, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:32:25,095 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 508.0 (TID 1016)
2017-08-18 12:32:25,096 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 508.0 (TID 1017)
2017-08-18 12:32:25,098 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:32:25,098 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:32:25,102 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 508.0 (TID 1017). 714 bytes result sent to driver
2017-08-18 12:32:25,102 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 508.0 (TID 1016). 714 bytes result sent to driver
2017-08-18 12:32:25,105 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 508.0 (TID 1017) in 11 ms on localhost (1/2)
2017-08-18 12:32:25,105 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 508.0 (TID 1016) in 15 ms on localhost (2/2)
2017-08-18 12:32:25,105 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 508.0, whose tasks have all completed, from pool 
2017-08-18 12:32:25,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 508 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:32:25,105 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 508 finished: foreachPartition at streamingProcessNew.scala:50, took 0.037854 s
2017-08-18 12:32:25,106 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030745000 ms.0 from job set of time 1503030745000 ms
2017-08-18 12:32:25,106 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.106 s for time 1503030745000 ms (execution: 0.082 s)
2017-08-18 12:32:25,106 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1015 from persistence list
2017-08-18 12:32:25,107 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1015
2017-08-18 12:32:25,107 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1014 from persistence list
2017-08-18 12:32:25,107 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:32:25,108 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030735000 ms
2017-08-18 12:32:25,108 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1014
2017-08-18 12:32:30,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030750000 ms
2017-08-18 12:32:30,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030750000 ms.0 from job set of time 1503030750000 ms
2017-08-18 12:32:30,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:32:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 509 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:32:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 509 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:32:30,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:32:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:32:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 509 (MapPartitionsRDD[1019] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:32:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_509 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:32:30,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_509_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:32:30,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_509_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:32:30,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 509 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:32:30,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 509 (MapPartitionsRDD[1019] at map at streamingProcessNew.scala:49)
2017-08-18 12:32:30,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 509.0 with 2 tasks
2017-08-18 12:32:30,078 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 509.0 (TID 1018, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:32:30,079 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 509.0 (TID 1019, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:32:30,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 509.0 (TID 1018)
2017-08-18 12:32:30,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 509.0 (TID 1019)
2017-08-18 12:32:30,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:32:30,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:32:30,086 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 509.0 (TID 1019). 801 bytes result sent to driver
2017-08-18 12:32:30,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 509.0 (TID 1018). 801 bytes result sent to driver
2017-08-18 12:32:30,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 509.0 (TID 1019) in 10 ms on localhost (1/2)
2017-08-18 12:32:30,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 509.0 (TID 1018) in 13 ms on localhost (2/2)
2017-08-18 12:32:30,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 509.0, whose tasks have all completed, from pool 
2017-08-18 12:32:30,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 509 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:32:30,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 509 finished: foreachPartition at streamingProcessNew.scala:50, took 0.033650 s
2017-08-18 12:32:30,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030750000 ms.0 from job set of time 1503030750000 ms
2017-08-18 12:32:30,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1503030750000 ms (execution: 0.067 s)
2017-08-18 12:32:30,090 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1017 from persistence list
2017-08-18 12:32:30,090 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1017
2017-08-18 12:32:30,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1016 from persistence list
2017-08-18 12:32:30,091 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1016
2017-08-18 12:32:30,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:32:30,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030740000 ms
2017-08-18 12:32:35,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030755000 ms
2017-08-18 12:32:35,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030755000 ms.0 from job set of time 1503030755000 ms
2017-08-18 12:32:35,056 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:32:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 510 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:32:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 510 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:32:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:32:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:32:35,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 510 (MapPartitionsRDD[1021] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:32:35,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_510 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:32:35,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_510_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:32:35,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_510_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:32:35,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 510 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:32:35,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 510 (MapPartitionsRDD[1021] at map at streamingProcessNew.scala:49)
2017-08-18 12:32:35,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 510.0 with 2 tasks
2017-08-18 12:32:35,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 510.0 (TID 1020, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:32:35,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 510.0 (TID 1021, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:32:35,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 510.0 (TID 1020)
2017-08-18 12:32:35,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 510.0 (TID 1021)
2017-08-18 12:32:35,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:32:35,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:32:35,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 510.0 (TID 1020). 714 bytes result sent to driver
2017-08-18 12:32:35,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 510.0 (TID 1021). 714 bytes result sent to driver
2017-08-18 12:32:35,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 510.0 (TID 1020) in 13 ms on localhost (1/2)
2017-08-18 12:32:35,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 510.0 (TID 1021) in 9 ms on localhost (2/2)
2017-08-18 12:32:35,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 510.0, whose tasks have all completed, from pool 
2017-08-18 12:32:35,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 510 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:32:35,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 510 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025052 s
2017-08-18 12:32:35,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030755000 ms.0 from job set of time 1503030755000 ms
2017-08-18 12:32:35,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1503030755000 ms (execution: 0.061 s)
2017-08-18 12:32:35,082 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1019 from persistence list
2017-08-18 12:32:35,082 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1019
2017-08-18 12:32:35,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1018 from persistence list
2017-08-18 12:32:35,083 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1018
2017-08-18 12:32:35,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:32:35,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030745000 ms
2017-08-18 12:32:40,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030760000 ms
2017-08-18 12:32:40,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030760000 ms.0 from job set of time 1503030760000 ms
2017-08-18 12:32:40,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:32:40,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 511 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:32:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 511 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:32:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:32:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:32:40,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 511 (MapPartitionsRDD[1023] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:32:40,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_511 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:32:40,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_511_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:32:40,029 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_511_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:32:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 511 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:32:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 511 (MapPartitionsRDD[1023] at map at streamingProcessNew.scala:49)
2017-08-18 12:32:40,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 511.0 with 2 tasks
2017-08-18 12:32:40,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 511.0 (TID 1022, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:32:40,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 511.0 (TID 1023, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:32:40,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 511.0 (TID 1023)
2017-08-18 12:32:40,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 511.0 (TID 1022)
2017-08-18 12:32:40,035 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:32:40,035 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:32:40,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 511.0 (TID 1023). 714 bytes result sent to driver
2017-08-18 12:32:40,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 511.0 (TID 1022). 714 bytes result sent to driver
2017-08-18 12:32:40,040 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 511.0 (TID 1023) in 6 ms on localhost (1/2)
2017-08-18 12:32:40,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 511.0 (TID 1022) in 10 ms on localhost (2/2)
2017-08-18 12:32:40,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 511.0, whose tasks have all completed, from pool 
2017-08-18 12:32:40,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 511 (foreachPartition at streamingProcessNew.scala:50) finished in 0.010 s
2017-08-18 12:32:40,040 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 511 finished: foreachPartition at streamingProcessNew.scala:50, took 0.017089 s
2017-08-18 12:32:40,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030760000 ms.0 from job set of time 1503030760000 ms
2017-08-18 12:32:40,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1503030760000 ms (execution: 0.029 s)
2017-08-18 12:32:40,041 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1021 from persistence list
2017-08-18 12:32:40,042 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1021
2017-08-18 12:32:40,042 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1020 from persistence list
2017-08-18 12:32:40,042 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1020
2017-08-18 12:32:40,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:32:40,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030750000 ms
2017-08-18 12:32:45,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030765000 ms
2017-08-18 12:32:45,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030765000 ms.0 from job set of time 1503030765000 ms
2017-08-18 12:32:45,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:32:45,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 512 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:32:45,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 512 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:32:45,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:32:45,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:32:45,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 512 (MapPartitionsRDD[1025] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:32:45,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_512 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:32:45,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_512_piece0 stored as bytes in memory (estimated size 25.2 KB, free 413.1 MB)
2017-08-18 12:32:45,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_512_piece0 in memory on 192.168.31.111:60809 (size: 25.2 KB, free: 413.6 MB)
2017-08-18 12:32:45,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 512 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:32:45,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 512 (MapPartitionsRDD[1025] at map at streamingProcessNew.scala:49)
2017-08-18 12:32:45,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 512.0 with 2 tasks
2017-08-18 12:32:45,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 512.0 (TID 1024, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:32:45,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 512.0 (TID 1025, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:32:45,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 512.0 (TID 1025)
2017-08-18 12:32:45,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 512.0 (TID 1024)
2017-08-18 12:32:45,055 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:32:45,055 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:32:45,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 512.0 (TID 1025). 714 bytes result sent to driver
2017-08-18 12:32:45,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 512.0 (TID 1024). 714 bytes result sent to driver
2017-08-18 12:32:45,060 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 512.0 (TID 1024) in 10 ms on localhost (1/2)
2017-08-18 12:32:45,061 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 512.0 (TID 1025) in 8 ms on localhost (2/2)
2017-08-18 12:32:45,061 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 512.0, whose tasks have all completed, from pool 
2017-08-18 12:32:45,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 512 (foreachPartition at streamingProcessNew.scala:50) finished in 0.011 s
2017-08-18 12:32:45,061 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 512 finished: foreachPartition at streamingProcessNew.scala:50, took 0.022159 s
2017-08-18 12:32:45,061 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030765000 ms.0 from job set of time 1503030765000 ms
2017-08-18 12:32:45,062 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1023 from persistence list
2017-08-18 12:32:45,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.061 s for time 1503030765000 ms (execution: 0.043 s)
2017-08-18 12:32:45,062 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1023
2017-08-18 12:32:45,062 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1022 from persistence list
2017-08-18 12:32:45,062 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1022
2017-08-18 12:32:45,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:32:45,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030755000 ms
2017-08-18 12:32:50,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030770000 ms
2017-08-18 12:32:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030770000 ms.0 from job set of time 1503030770000 ms
2017-08-18 12:32:50,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:32:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 513 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:32:50,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 513 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:32:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:32:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:32:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 513 (MapPartitionsRDD[1027] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:32:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_513 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:32:50,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_513_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:32:50,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_513_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:32:50,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 513 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:32:50,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 513 (MapPartitionsRDD[1027] at map at streamingProcessNew.scala:49)
2017-08-18 12:32:50,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 513.0 with 2 tasks
2017-08-18 12:32:50,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 513.0 (TID 1026, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:32:50,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 513.0 (TID 1027, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:32:50,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 513.0 (TID 1027)
2017-08-18 12:32:50,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 513.0 (TID 1026)
2017-08-18 12:32:50,081 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:32:50,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:32:50,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 513.0 (TID 1026). 714 bytes result sent to driver
2017-08-18 12:32:50,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 513.0 (TID 1027). 714 bytes result sent to driver
2017-08-18 12:32:50,087 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 513.0 (TID 1026) in 13 ms on localhost (1/2)
2017-08-18 12:32:50,087 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 513.0 (TID 1027) in 10 ms on localhost (2/2)
2017-08-18 12:32:50,087 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 513.0, whose tasks have all completed, from pool 
2017-08-18 12:32:50,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 513 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:32:50,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 513 finished: foreachPartition at streamingProcessNew.scala:50, took 0.027027 s
2017-08-18 12:32:50,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030770000 ms.0 from job set of time 1503030770000 ms
2017-08-18 12:32:50,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1503030770000 ms (execution: 0.067 s)
2017-08-18 12:32:50,088 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1025 from persistence list
2017-08-18 12:32:50,089 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1025
2017-08-18 12:32:50,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1024 from persistence list
2017-08-18 12:32:50,089 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1024
2017-08-18 12:32:50,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:32:50,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030760000 ms
2017-08-18 12:32:55,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030775000 ms
2017-08-18 12:32:55,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030775000 ms.0 from job set of time 1503030775000 ms
2017-08-18 12:32:55,081 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:32:55,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 514 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:32:55,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 514 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:32:55,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:32:55,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:32:55,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 514 (MapPartitionsRDD[1029] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:32:55,085 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_513_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:32:55,087 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_498_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:32:55,092 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_499_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:32:55,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_514 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:32:55,096 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_500_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:32:55,099 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_501_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:32:55,102 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_514_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:32:55,102 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_514_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:32:55,102 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 514 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:32:55,103 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 514 (MapPartitionsRDD[1029] at map at streamingProcessNew.scala:49)
2017-08-18 12:32:55,103 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 514.0 with 2 tasks
2017-08-18 12:32:55,106 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_502_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:32:55,107 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 514.0 (TID 1028, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:32:55,109 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 514.0 (TID 1029, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:32:55,109 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 514.0 (TID 1029)
2017-08-18 12:32:55,109 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 514.0 (TID 1028)
2017-08-18 12:32:55,112 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_503_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:32:55,113 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:32:55,113 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:32:55,117 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_504_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:32:55,119 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 514.0 (TID 1029). 714 bytes result sent to driver
2017-08-18 12:32:55,119 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 514.0 (TID 1028). 714 bytes result sent to driver
2017-08-18 12:32:55,119 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_505_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:32:55,124 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 514.0 (TID 1029) in 17 ms on localhost (1/2)
2017-08-18 12:32:55,124 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_506_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:32:55,124 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 514.0 (TID 1028) in 20 ms on localhost (2/2)
2017-08-18 12:32:55,124 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 514.0, whose tasks have all completed, from pool 
2017-08-18 12:32:55,124 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 514 (foreachPartition at streamingProcessNew.scala:50) finished in 0.021 s
2017-08-18 12:32:55,125 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 514 finished: foreachPartition at streamingProcessNew.scala:50, took 0.043527 s
2017-08-18 12:32:55,126 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030775000 ms.0 from job set of time 1503030775000 ms
2017-08-18 12:32:55,126 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.126 s for time 1503030775000 ms (execution: 0.105 s)
2017-08-18 12:32:55,126 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1027 from persistence list
2017-08-18 12:32:55,131 [block-manager-slave-async-thread-pool-43] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1027
2017-08-18 12:32:55,131 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1026 from persistence list
2017-08-18 12:32:55,131 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_507_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:32:55,131 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:32:55,131 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030765000 ms
2017-08-18 12:32:55,131 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1026
2017-08-18 12:32:55,132 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_508_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:32:55,133 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_509_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:32:55,134 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_510_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:32:55,136 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_511_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:32:55,137 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_512_piece0 on 192.168.31.111:60809 in memory (size: 25.2 KB, free: 413.9 MB)
2017-08-18 12:33:00,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030780000 ms
2017-08-18 12:33:00,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030780000 ms.0 from job set of time 1503030780000 ms
2017-08-18 12:33:00,063 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:33:00,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 515 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:33:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 515 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:33:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:33:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:33:00,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 515 (MapPartitionsRDD[1031] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:33:00,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_515 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:33:00,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_515_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:33:00,081 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_515_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:33:00,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 515 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:33:00,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 515 (MapPartitionsRDD[1031] at map at streamingProcessNew.scala:49)
2017-08-18 12:33:00,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 515.0 with 2 tasks
2017-08-18 12:33:00,086 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 515.0 (TID 1030, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:33:00,087 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 515.0 (TID 1031, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:33:00,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 515.0 (TID 1031)
2017-08-18 12:33:00,087 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 515.0 (TID 1030)
2017-08-18 12:33:00,090 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:33:00,090 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:33:00,095 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 515.0 (TID 1030). 714 bytes result sent to driver
2017-08-18 12:33:00,095 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 515.0 (TID 1031). 714 bytes result sent to driver
2017-08-18 12:33:00,097 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 515.0 (TID 1030) in 14 ms on localhost (1/2)
2017-08-18 12:33:00,097 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 515.0 (TID 1031) in 11 ms on localhost (2/2)
2017-08-18 12:33:00,098 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 515.0, whose tasks have all completed, from pool 
2017-08-18 12:33:00,098 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 515 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:33:00,098 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 515 finished: foreachPartition at streamingProcessNew.scala:50, took 0.035103 s
2017-08-18 12:33:00,099 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030780000 ms.0 from job set of time 1503030780000 ms
2017-08-18 12:33:00,099 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.099 s for time 1503030780000 ms (execution: 0.077 s)
2017-08-18 12:33:00,099 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1029 from persistence list
2017-08-18 12:33:00,100 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1029
2017-08-18 12:33:00,100 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1028 from persistence list
2017-08-18 12:33:00,100 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1028
2017-08-18 12:33:00,101 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:33:00,101 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030770000 ms
2017-08-18 12:33:05,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030785000 ms
2017-08-18 12:33:05,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030785000 ms.0 from job set of time 1503030785000 ms
2017-08-18 12:33:05,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:33:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 516 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:33:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 516 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:33:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:33:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:33:05,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 516 (MapPartitionsRDD[1033] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:33:05,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_516 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:33:05,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_516_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:33:05,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_516_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:33:05,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 516 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:33:05,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 516 (MapPartitionsRDD[1033] at map at streamingProcessNew.scala:49)
2017-08-18 12:33:05,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 516.0 with 2 tasks
2017-08-18 12:33:05,078 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 516.0 (TID 1032, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:33:05,079 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 516.0 (TID 1033, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:33:05,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 516.0 (TID 1033)
2017-08-18 12:33:05,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 516.0 (TID 1032)
2017-08-18 12:33:05,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:33:05,084 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:33:05,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 516.0 (TID 1033). 714 bytes result sent to driver
2017-08-18 12:33:05,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 516.0 (TID 1032). 801 bytes result sent to driver
2017-08-18 12:33:05,091 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 516.0 (TID 1033) in 12 ms on localhost (1/2)
2017-08-18 12:33:05,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 516.0 (TID 1032) in 17 ms on localhost (2/2)
2017-08-18 12:33:05,092 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 516.0, whose tasks have all completed, from pool 
2017-08-18 12:33:05,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 516 (foreachPartition at streamingProcessNew.scala:50) finished in 0.018 s
2017-08-18 12:33:05,092 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 516 finished: foreachPartition at streamingProcessNew.scala:50, took 0.034020 s
2017-08-18 12:33:05,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030785000 ms.0 from job set of time 1503030785000 ms
2017-08-18 12:33:05,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.093 s for time 1503030785000 ms (execution: 0.072 s)
2017-08-18 12:33:05,093 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1031 from persistence list
2017-08-18 12:33:05,094 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1031
2017-08-18 12:33:05,094 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1030 from persistence list
2017-08-18 12:33:05,094 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1030
2017-08-18 12:33:05,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:33:05,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030775000 ms
2017-08-18 12:33:10,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030790000 ms
2017-08-18 12:33:10,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030790000 ms.0 from job set of time 1503030790000 ms
2017-08-18 12:33:10,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:33:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 517 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:33:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 517 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:33:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:33:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:33:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 517 (MapPartitionsRDD[1035] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:33:10,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_517 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:33:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_517_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:33:10,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_517_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:33:10,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 517 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:33:10,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 517 (MapPartitionsRDD[1035] at map at streamingProcessNew.scala:49)
2017-08-18 12:33:10,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 517.0 with 2 tasks
2017-08-18 12:33:10,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 517.0 (TID 1034, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:33:10,076 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 517.0 (TID 1035, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:33:10,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 517.0 (TID 1034)
2017-08-18 12:33:10,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 517.0 (TID 1035)
2017-08-18 12:33:10,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:33:10,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:33:10,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 517.0 (TID 1034). 714 bytes result sent to driver
2017-08-18 12:33:10,090 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 517.0 (TID 1035). 714 bytes result sent to driver
2017-08-18 12:33:10,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 517.0 (TID 1034) in 23 ms on localhost (1/2)
2017-08-18 12:33:10,094 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 517.0 (TID 1035) in 19 ms on localhost (2/2)
2017-08-18 12:33:10,094 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 517.0, whose tasks have all completed, from pool 
2017-08-18 12:33:10,094 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 517 (foreachPartition at streamingProcessNew.scala:50) finished in 0.024 s
2017-08-18 12:33:10,095 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 517 finished: foreachPartition at streamingProcessNew.scala:50, took 0.045410 s
2017-08-18 12:33:10,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030790000 ms.0 from job set of time 1503030790000 ms
2017-08-18 12:33:10,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.096 s for time 1503030790000 ms (execution: 0.076 s)
2017-08-18 12:33:10,096 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1033 from persistence list
2017-08-18 12:33:10,096 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1033
2017-08-18 12:33:10,096 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1032 from persistence list
2017-08-18 12:33:10,097 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1032
2017-08-18 12:33:10,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:33:10,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030780000 ms
2017-08-18 12:33:15,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030795000 ms
2017-08-18 12:33:15,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030795000 ms.0 from job set of time 1503030795000 ms
2017-08-18 12:33:15,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:33:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 518 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:33:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 518 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:33:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:33:15,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:33:15,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 518 (MapPartitionsRDD[1037] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:33:15,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_518 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:33:15,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_518_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:33:15,080 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_518_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:33:15,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 518 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:33:15,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 518 (MapPartitionsRDD[1037] at map at streamingProcessNew.scala:49)
2017-08-18 12:33:15,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 518.0 with 2 tasks
2017-08-18 12:33:15,085 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 518.0 (TID 1036, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:33:15,085 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 518.0 (TID 1037, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:33:15,086 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 518.0 (TID 1037)
2017-08-18 12:33:15,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 518.0 (TID 1036)
2017-08-18 12:33:15,088 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:33:15,088 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:33:15,093 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 518.0 (TID 1036). 714 bytes result sent to driver
2017-08-18 12:33:15,093 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 518.0 (TID 1037). 714 bytes result sent to driver
2017-08-18 12:33:15,095 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 518.0 (TID 1036) in 13 ms on localhost (1/2)
2017-08-18 12:33:15,095 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 518.0 (TID 1037) in 10 ms on localhost (2/2)
2017-08-18 12:33:15,095 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 518.0, whose tasks have all completed, from pool 
2017-08-18 12:33:15,095 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 518 (foreachPartition at streamingProcessNew.scala:50) finished in 0.013 s
2017-08-18 12:33:15,096 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 518 finished: foreachPartition at streamingProcessNew.scala:50, took 0.036500 s
2017-08-18 12:33:15,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030795000 ms.0 from job set of time 1503030795000 ms
2017-08-18 12:33:15,097 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1035 from persistence list
2017-08-18 12:33:15,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.096 s for time 1503030795000 ms (execution: 0.076 s)
2017-08-18 12:33:15,097 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1035
2017-08-18 12:33:15,097 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1034 from persistence list
2017-08-18 12:33:15,097 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1034
2017-08-18 12:33:15,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:33:15,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030785000 ms
2017-08-18 12:33:20,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030800000 ms
2017-08-18 12:33:20,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030800000 ms.0 from job set of time 1503030800000 ms
2017-08-18 12:33:20,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:33:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 519 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:33:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 519 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:33:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:33:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:33:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 519 (MapPartitionsRDD[1039] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:33:20,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_519 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:33:20,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_519_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:33:20,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_519_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:33:20,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 519 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:33:20,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 519 (MapPartitionsRDD[1039] at map at streamingProcessNew.scala:49)
2017-08-18 12:33:20,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 519.0 with 2 tasks
2017-08-18 12:33:20,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 519.0 (TID 1038, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:33:20,078 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 519.0 (TID 1039, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:33:20,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 519.0 (TID 1039)
2017-08-18 12:33:20,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 519.0 (TID 1038)
2017-08-18 12:33:20,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:33:20,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:33:20,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 519.0 (TID 1039). 714 bytes result sent to driver
2017-08-18 12:33:20,086 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 519.0 (TID 1038). 714 bytes result sent to driver
2017-08-18 12:33:20,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 519.0 (TID 1039) in 11 ms on localhost (1/2)
2017-08-18 12:33:20,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 519.0 (TID 1038) in 15 ms on localhost (2/2)
2017-08-18 12:33:20,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 519.0, whose tasks have all completed, from pool 
2017-08-18 12:33:20,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 519 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:33:20,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 519 finished: foreachPartition at streamingProcessNew.scala:50, took 0.030066 s
2017-08-18 12:33:20,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030800000 ms.0 from job set of time 1503030800000 ms
2017-08-18 12:33:20,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1503030800000 ms (execution: 0.068 s)
2017-08-18 12:33:20,089 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1037 from persistence list
2017-08-18 12:33:20,090 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1037
2017-08-18 12:33:20,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1036 from persistence list
2017-08-18 12:33:20,090 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1036
2017-08-18 12:33:20,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:33:20,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030790000 ms
2017-08-18 12:33:25,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030805000 ms
2017-08-18 12:33:25,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030805000 ms.0 from job set of time 1503030805000 ms
2017-08-18 12:33:25,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:33:25,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 520 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:33:25,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 520 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:33:25,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:33:25,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:33:25,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 520 (MapPartitionsRDD[1041] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:33:25,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_520 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:33:25,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_520_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:33:25,050 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_520_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:33:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 520 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:33:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 520 (MapPartitionsRDD[1041] at map at streamingProcessNew.scala:49)
2017-08-18 12:33:25,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 520.0 with 2 tasks
2017-08-18 12:33:25,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 520.0 (TID 1040, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:33:25,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 520.0 (TID 1041, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:33:25,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 520.0 (TID 1041)
2017-08-18 12:33:25,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 520.0 (TID 1040)
2017-08-18 12:33:25,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:33:25,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:33:25,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 520.0 (TID 1040). 714 bytes result sent to driver
2017-08-18 12:33:25,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 520.0 (TID 1041). 714 bytes result sent to driver
2017-08-18 12:33:25,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 520.0 (TID 1040) in 15 ms on localhost (1/2)
2017-08-18 12:33:25,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 520.0 (TID 1041) in 11 ms on localhost (2/2)
2017-08-18 12:33:25,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 520.0, whose tasks have all completed, from pool 
2017-08-18 12:33:25,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 520 (foreachPartition at streamingProcessNew.scala:50) finished in 0.016 s
2017-08-18 12:33:25,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 520 finished: foreachPartition at streamingProcessNew.scala:50, took 0.029558 s
2017-08-18 12:33:25,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030805000 ms.0 from job set of time 1503030805000 ms
2017-08-18 12:33:25,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1503030805000 ms (execution: 0.051 s)
2017-08-18 12:33:25,068 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1039 from persistence list
2017-08-18 12:33:25,069 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1039
2017-08-18 12:33:25,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1038 from persistence list
2017-08-18 12:33:25,069 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1038
2017-08-18 12:33:25,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:33:25,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030795000 ms
2017-08-18 12:33:30,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030810000 ms
2017-08-18 12:33:30,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030810000 ms.0 from job set of time 1503030810000 ms
2017-08-18 12:33:30,066 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:33:30,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 521 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:33:30,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 521 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:33:30,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:33:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:33:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 521 (MapPartitionsRDD[1043] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:33:30,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_521 stored as values in memory (estimated size 34.5 KB, free 413.5 MB)
2017-08-18 12:33:30,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_521_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.5 MB)
2017-08-18 12:33:30,084 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_521_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:33:30,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 521 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:33:30,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 521 (MapPartitionsRDD[1043] at map at streamingProcessNew.scala:49)
2017-08-18 12:33:30,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 521.0 with 2 tasks
2017-08-18 12:33:30,091 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 521.0 (TID 1042, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:33:30,091 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 521.0 (TID 1043, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:33:30,092 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 521.0 (TID 1042)
2017-08-18 12:33:30,092 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 521.0 (TID 1043)
2017-08-18 12:33:30,096 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:33:30,096 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:33:30,101 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 521.0 (TID 1043). 714 bytes result sent to driver
2017-08-18 12:33:30,101 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 521.0 (TID 1042). 714 bytes result sent to driver
2017-08-18 12:33:30,104 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 521.0 (TID 1043) in 13 ms on localhost (1/2)
2017-08-18 12:33:30,104 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 521.0 (TID 1042) in 18 ms on localhost (2/2)
2017-08-18 12:33:30,105 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 521.0, whose tasks have all completed, from pool 
2017-08-18 12:33:30,105 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 521 (foreachPartition at streamingProcessNew.scala:50) finished in 0.019 s
2017-08-18 12:33:30,105 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 521 finished: foreachPartition at streamingProcessNew.scala:50, took 0.039413 s
2017-08-18 12:33:30,106 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030810000 ms.0 from job set of time 1503030810000 ms
2017-08-18 12:33:30,106 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.106 s for time 1503030810000 ms (execution: 0.083 s)
2017-08-18 12:33:30,106 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1041 from persistence list
2017-08-18 12:33:30,107 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1041
2017-08-18 12:33:30,107 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1040 from persistence list
2017-08-18 12:33:30,107 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1040
2017-08-18 12:33:30,107 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:33:30,108 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030800000 ms
2017-08-18 12:33:38,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030815000 ms
2017-08-18 12:33:38,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030815000 ms.0 from job set of time 1503030815000 ms
2017-08-18 12:33:38,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:33:38,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 522 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:33:38,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 522 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:33:38,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:33:38,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:33:38,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 522 (MapPartitionsRDD[1045] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:33:38,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_522 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:33:38,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_522_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.4 MB)
2017-08-18 12:33:38,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_522_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:33:38,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 522 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:33:38,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 522 (MapPartitionsRDD[1045] at map at streamingProcessNew.scala:49)
2017-08-18 12:33:38,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 522.0 with 2 tasks
2017-08-18 12:33:38,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 522.0 (TID 1044, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:33:38,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 522.0 (TID 1045, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:33:38,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 522.0 (TID 1045)
2017-08-18 12:33:38,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 522.0 (TID 1044)
2017-08-18 12:33:38,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:33:38,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:33:38,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 522.0 (TID 1044). 714 bytes result sent to driver
2017-08-18 12:33:38,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 522.0 (TID 1045). 714 bytes result sent to driver
2017-08-18 12:33:38,083 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 522.0 (TID 1045) in 11 ms on localhost (1/2)
2017-08-18 12:33:38,083 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 522.0 (TID 1044) in 15 ms on localhost (2/2)
2017-08-18 12:33:38,083 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 522.0, whose tasks have all completed, from pool 
2017-08-18 12:33:38,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 522 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:33:38,084 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 522 finished: foreachPartition at streamingProcessNew.scala:50, took 0.028984 s
2017-08-18 12:33:38,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030815000 ms.0 from job set of time 1503030815000 ms
2017-08-18 12:33:38,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 3.085 s for time 1503030815000 ms (execution: 0.069 s)
2017-08-18 12:33:38,085 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1043 from persistence list
2017-08-18 12:33:38,086 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1043
2017-08-18 12:33:38,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1042 from persistence list
2017-08-18 12:33:38,086 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1042
2017-08-18 12:33:38,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:33:38,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030805000 ms
2017-08-18 12:33:40,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030820000 ms
2017-08-18 12:33:40,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030820000 ms.0 from job set of time 1503030820000 ms
2017-08-18 12:33:40,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:33:40,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 523 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:33:40,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 523 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:33:40,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:33:40,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:33:40,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 523 (MapPartitionsRDD[1047] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:33:40,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_523 stored as values in memory (estimated size 34.5 KB, free 413.4 MB)
2017-08-18 12:33:40,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_523_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:33:40,083 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_523_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:33:40,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 523 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:33:40,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 523 (MapPartitionsRDD[1047] at map at streamingProcessNew.scala:49)
2017-08-18 12:33:40,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 523.0 with 2 tasks
2017-08-18 12:33:40,089 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 523.0 (TID 1046, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:33:40,090 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 523.0 (TID 1047, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:33:40,090 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 523.0 (TID 1047)
2017-08-18 12:33:40,090 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 523.0 (TID 1046)
2017-08-18 12:33:40,094 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:33:40,094 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:33:40,098 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 523.0 (TID 1047). 714 bytes result sent to driver
2017-08-18 12:33:40,098 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 523.0 (TID 1046). 714 bytes result sent to driver
2017-08-18 12:33:40,099 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 523.0 (TID 1046) in 14 ms on localhost (1/2)
2017-08-18 12:33:40,100 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 523.0 (TID 1047) in 11 ms on localhost (2/2)
2017-08-18 12:33:40,100 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 523.0, whose tasks have all completed, from pool 
2017-08-18 12:33:40,100 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 523 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:33:40,100 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 523 finished: foreachPartition at streamingProcessNew.scala:50, took 0.038297 s
2017-08-18 12:33:40,100 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030820000 ms.0 from job set of time 1503030820000 ms
2017-08-18 12:33:40,101 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.100 s for time 1503030820000 ms (execution: 0.078 s)
2017-08-18 12:33:40,101 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1045 from persistence list
2017-08-18 12:33:40,101 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1045
2017-08-18 12:33:40,101 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1044 from persistence list
2017-08-18 12:33:40,101 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1044
2017-08-18 12:33:40,102 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:33:40,102 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030810000 ms
2017-08-18 12:33:45,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030825000 ms
2017-08-18 12:33:45,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030825000 ms.0 from job set of time 1503030825000 ms
2017-08-18 12:33:45,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:33:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 524 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:33:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 524 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:33:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:33:45,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:33:45,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 524 (MapPartitionsRDD[1049] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:33:45,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_524 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:33:45,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_524_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.3 MB)
2017-08-18 12:33:45,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_524_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:33:45,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 524 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:33:45,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 524 (MapPartitionsRDD[1049] at map at streamingProcessNew.scala:49)
2017-08-18 12:33:45,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 524.0 with 2 tasks
2017-08-18 12:33:45,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 524.0 (TID 1048, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:33:45,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 524.0 (TID 1049, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:33:45,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 524.0 (TID 1049)
2017-08-18 12:33:45,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 524.0 (TID 1048)
2017-08-18 12:33:45,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:33:45,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:33:45,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 524.0 (TID 1048). 714 bytes result sent to driver
2017-08-18 12:33:45,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 524.0 (TID 1049). 714 bytes result sent to driver
2017-08-18 12:33:45,079 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 524.0 (TID 1048) in 17 ms on localhost (1/2)
2017-08-18 12:33:45,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 524.0 (TID 1049) in 12 ms on localhost (2/2)
2017-08-18 12:33:45,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 524.0, whose tasks have all completed, from pool 
2017-08-18 12:33:45,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 524 (foreachPartition at streamingProcessNew.scala:50) finished in 0.017 s
2017-08-18 12:33:45,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 524 finished: foreachPartition at streamingProcessNew.scala:50, took 0.034732 s
2017-08-18 12:33:45,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030825000 ms.0 from job set of time 1503030825000 ms
2017-08-18 12:33:45,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1503030825000 ms (execution: 0.061 s)
2017-08-18 12:33:45,080 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1047 from persistence list
2017-08-18 12:33:45,081 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1047
2017-08-18 12:33:45,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1046 from persistence list
2017-08-18 12:33:45,081 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1046
2017-08-18 12:33:45,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:33:45,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030815000 ms
2017-08-18 12:33:50,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030830000 ms
2017-08-18 12:33:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030830000 ms.0 from job set of time 1503030830000 ms
2017-08-18 12:33:50,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:33:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 525 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:33:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 525 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:33:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:33:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:33:50,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 525 (MapPartitionsRDD[1051] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:33:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_525 stored as values in memory (estimated size 34.5 KB, free 413.3 MB)
2017-08-18 12:33:50,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_525_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:33:50,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_525_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:33:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 525 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:33:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 525 (MapPartitionsRDD[1051] at map at streamingProcessNew.scala:49)
2017-08-18 12:33:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 525.0 with 2 tasks
2017-08-18 12:33:50,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 525.0 (TID 1050, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:33:50,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 525.0 (TID 1051, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:33:50,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 525.0 (TID 1051)
2017-08-18 12:33:50,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 525.0 (TID 1050)
2017-08-18 12:33:50,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:33:50,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:33:50,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 525.0 (TID 1050). 714 bytes result sent to driver
2017-08-18 12:33:50,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 525.0 (TID 1051). 714 bytes result sent to driver
2017-08-18 12:33:50,080 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 525.0 (TID 1050) in 12 ms on localhost (1/2)
2017-08-18 12:33:50,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 525.0 (TID 1051) in 9 ms on localhost (2/2)
2017-08-18 12:33:50,081 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 525.0, whose tasks have all completed, from pool 
2017-08-18 12:33:50,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 525 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:33:50,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 525 finished: foreachPartition at streamingProcessNew.scala:50, took 0.024621 s
2017-08-18 12:33:50,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030830000 ms.0 from job set of time 1503030830000 ms
2017-08-18 12:33:50,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1503030830000 ms (execution: 0.061 s)
2017-08-18 12:33:50,082 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1049 from persistence list
2017-08-18 12:33:50,083 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1049
2017-08-18 12:33:50,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1048 from persistence list
2017-08-18 12:33:50,083 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1048
2017-08-18 12:33:50,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:33:50,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030820000 ms
2017-08-18 12:33:55,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030835000 ms
2017-08-18 12:33:55,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030835000 ms.0 from job set of time 1503030835000 ms
2017-08-18 12:33:55,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:33:55,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 526 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:33:55,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 526 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:33:55,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:33:55,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:33:55,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 526 (MapPartitionsRDD[1053] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:33:55,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_526 stored as values in memory (estimated size 34.5 KB, free 413.2 MB)
2017-08-18 12:33:55,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_526_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.2 MB)
2017-08-18 12:33:55,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_526_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:33:55,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 526 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:33:55,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 526 (MapPartitionsRDD[1053] at map at streamingProcessNew.scala:49)
2017-08-18 12:33:55,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 526.0 with 2 tasks
2017-08-18 12:33:55,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 526.0 (TID 1052, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:33:55,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 526.0 (TID 1053, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:33:55,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 526.0 (TID 1053)
2017-08-18 12:33:55,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 526.0 (TID 1052)
2017-08-18 12:33:55,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:33:55,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:33:55,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 526.0 (TID 1053). 714 bytes result sent to driver
2017-08-18 12:33:55,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 526.0 (TID 1052). 714 bytes result sent to driver
2017-08-18 12:33:55,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 526.0 (TID 1053) in 10 ms on localhost (1/2)
2017-08-18 12:33:55,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 526.0 (TID 1052) in 15 ms on localhost (2/2)
2017-08-18 12:33:55,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 526.0, whose tasks have all completed, from pool 
2017-08-18 12:33:55,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 526 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:33:55,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 526 finished: foreachPartition at streamingProcessNew.scala:50, took 0.029459 s
2017-08-18 12:33:55,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030835000 ms.0 from job set of time 1503030835000 ms
2017-08-18 12:33:55,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1503030835000 ms (execution: 0.057 s)
2017-08-18 12:33:55,079 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1051 from persistence list
2017-08-18 12:33:55,080 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1051
2017-08-18 12:33:55,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1050 from persistence list
2017-08-18 12:33:55,080 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1050
2017-08-18 12:33:55,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:33:55,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030825000 ms
2017-08-18 12:34:00,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030840000 ms
2017-08-18 12:34:00,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030840000 ms.0 from job set of time 1503030840000 ms
2017-08-18 12:34:00,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:34:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 527 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:34:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 527 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:34:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:34:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:34:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 527 (MapPartitionsRDD[1055] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:34:00,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_527 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:34:00,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_527_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:34:00,082 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_527_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:34:00,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 527 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:34:00,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 527 (MapPartitionsRDD[1055] at map at streamingProcessNew.scala:49)
2017-08-18 12:34:00,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 527.0 with 2 tasks
2017-08-18 12:34:00,091 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 527.0 (TID 1054, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:34:00,093 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 527.0 (TID 1055, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:34:00,094 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 527.0 (TID 1055)
2017-08-18 12:34:00,094 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 527.0 (TID 1054)
2017-08-18 12:34:00,102 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:34:00,102 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:34:00,108 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 527.0 (TID 1055). 801 bytes result sent to driver
2017-08-18 12:34:00,108 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 527.0 (TID 1054). 714 bytes result sent to driver
2017-08-18 12:34:00,110 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 527.0 (TID 1055) in 18 ms on localhost (1/2)
2017-08-18 12:34:00,111 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 527.0 (TID 1054) in 26 ms on localhost (2/2)
2017-08-18 12:34:00,111 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 527.0, whose tasks have all completed, from pool 
2017-08-18 12:34:00,111 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 527 (foreachPartition at streamingProcessNew.scala:50) finished in 0.026 s
2017-08-18 12:34:00,112 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 527 finished: foreachPartition at streamingProcessNew.scala:50, took 0.051320 s
2017-08-18 12:34:00,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030840000 ms.0 from job set of time 1503030840000 ms
2017-08-18 12:34:00,113 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.112 s for time 1503030840000 ms (execution: 0.091 s)
2017-08-18 12:34:00,113 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1053 from persistence list
2017-08-18 12:34:00,113 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1053
2017-08-18 12:34:00,113 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1052 from persistence list
2017-08-18 12:34:00,114 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1052
2017-08-18 12:34:00,114 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:34:00,114 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030830000 ms
2017-08-18 12:34:05,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030845000 ms
2017-08-18 12:34:05,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030845000 ms.0 from job set of time 1503030845000 ms
2017-08-18 12:34:05,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:34:05,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 528 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:34:05,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 528 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:34:05,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:34:05,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:34:05,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 528 (MapPartitionsRDD[1057] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:34:05,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_528 stored as values in memory (estimated size 34.5 KB, free 413.1 MB)
2017-08-18 12:34:05,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_528_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.1 MB)
2017-08-18 12:34:05,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_528_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:34:05,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 528 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:34:05,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 528 (MapPartitionsRDD[1057] at map at streamingProcessNew.scala:49)
2017-08-18 12:34:05,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 528.0 with 2 tasks
2017-08-18 12:34:05,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 528.0 (TID 1056, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:34:05,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 528.0 (TID 1057, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:34:05,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 528.0 (TID 1057)
2017-08-18 12:34:05,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 528.0 (TID 1056)
2017-08-18 12:34:05,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:34:05,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:34:05,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 528.0 (TID 1056). 714 bytes result sent to driver
2017-08-18 12:34:05,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 528.0 (TID 1057). 714 bytes result sent to driver
2017-08-18 12:34:05,085 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 528.0 (TID 1056) in 14 ms on localhost (1/2)
2017-08-18 12:34:05,085 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 528.0 (TID 1057) in 11 ms on localhost (2/2)
2017-08-18 12:34:05,085 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 528.0, whose tasks have all completed, from pool 
2017-08-18 12:34:05,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 528 (foreachPartition at streamingProcessNew.scala:50) finished in 0.015 s
2017-08-18 12:34:05,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 528 finished: foreachPartition at streamingProcessNew.scala:50, took 0.028367 s
2017-08-18 12:34:05,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030845000 ms.0 from job set of time 1503030845000 ms
2017-08-18 12:34:05,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.086 s for time 1503030845000 ms (execution: 0.065 s)
2017-08-18 12:34:05,087 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1055 from persistence list
2017-08-18 12:34:05,087 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1055
2017-08-18 12:34:05,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1054 from persistence list
2017-08-18 12:34:05,088 [block-manager-slave-async-thread-pool-35] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1054
2017-08-18 12:34:05,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:34:05,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030835000 ms
2017-08-18 12:34:10,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030850000 ms
2017-08-18 12:34:10,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030850000 ms.0 from job set of time 1503030850000 ms
2017-08-18 12:34:10,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:34:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 529 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:34:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 529 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:34:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:34:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:34:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 529 (MapPartitionsRDD[1059] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:34:10,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_529 stored as values in memory (estimated size 34.5 KB, free 413.0 MB)
2017-08-18 12:34:10,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_529_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.0 MB)
2017-08-18 12:34:10,087 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_529_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.5 MB)
2017-08-18 12:34:10,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 529 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:34:10,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 529 (MapPartitionsRDD[1059] at map at streamingProcessNew.scala:49)
2017-08-18 12:34:10,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 529.0 with 2 tasks
2017-08-18 12:34:10,102 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 529.0 (TID 1058, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:34:10,104 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 529.0 (TID 1059, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:34:10,104 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 529.0 (TID 1058)
2017-08-18 12:34:10,105 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 529.0 (TID 1059)
2017-08-18 12:34:10,142 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:34:10,143 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:34:10,144 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_514_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:34:10,154 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 529.0 (TID 1058). 787 bytes result sent to driver
2017-08-18 12:34:10,155 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_515_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:34:10,155 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 529.0 (TID 1059). 787 bytes result sent to driver
2017-08-18 12:34:10,160 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_516_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:34:10,160 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 529.0 (TID 1058) in 69 ms on localhost (1/2)
2017-08-18 12:34:10,160 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 529.0 (TID 1059) in 57 ms on localhost (2/2)
2017-08-18 12:34:10,161 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 529.0, whose tasks have all completed, from pool 
2017-08-18 12:34:10,161 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 529 (foreachPartition at streamingProcessNew.scala:50) finished in 0.071 s
2017-08-18 12:34:10,161 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 529 finished: foreachPartition at streamingProcessNew.scala:50, took 0.100463 s
2017-08-18 12:34:10,163 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_517_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.6 MB)
2017-08-18 12:34:10,163 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030850000 ms.0 from job set of time 1503030850000 ms
2017-08-18 12:34:10,163 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.163 s for time 1503030850000 ms (execution: 0.142 s)
2017-08-18 12:34:10,163 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1057 from persistence list
2017-08-18 12:34:10,165 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1057
2017-08-18 12:34:10,165 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1056 from persistence list
2017-08-18 12:34:10,165 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1056
2017-08-18 12:34:10,165 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:34:10,167 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030840000 ms
2017-08-18 12:34:10,167 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_518_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:34:10,173 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_519_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:34:10,176 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_520_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:34:10,180 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_521_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.7 MB)
2017-08-18 12:34:10,184 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_522_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:34:10,188 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_523_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:34:10,193 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_524_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:34:10,194 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_525_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:34:10,195 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_526_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:34:10,197 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_527_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:34:10,198 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_528_piece0 on 192.168.31.111:60809 in memory (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:34:15,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030855000 ms
2017-08-18 12:34:15,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030855000 ms.0 from job set of time 1503030855000 ms
2017-08-18 12:34:15,061 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:34:15,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 530 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:34:15,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 530 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:34:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:34:15,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:34:15,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 530 (MapPartitionsRDD[1061] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:34:15,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_530 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:34:15,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_530_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:34:15,080 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_530_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:34:15,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 530 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:34:15,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 530 (MapPartitionsRDD[1061] at map at streamingProcessNew.scala:49)
2017-08-18 12:34:15,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 530.0 with 2 tasks
2017-08-18 12:34:15,085 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 530.0 (TID 1060, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:34:15,086 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 530.0 (TID 1061, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:34:15,086 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 530.0 (TID 1060)
2017-08-18 12:34:15,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 530.0 (TID 1061)
2017-08-18 12:34:15,090 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:34:15,090 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:34:15,095 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 530.0 (TID 1060). 801 bytes result sent to driver
2017-08-18 12:34:15,095 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 530.0 (TID 1061). 801 bytes result sent to driver
2017-08-18 12:34:15,098 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 530.0 (TID 1060) in 15 ms on localhost (1/2)
2017-08-18 12:34:15,098 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 530.0 (TID 1061) in 12 ms on localhost (2/2)
2017-08-18 12:34:15,098 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 530.0, whose tasks have all completed, from pool 
2017-08-18 12:34:15,098 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 530 (foreachPartition at streamingProcessNew.scala:50) finished in 0.017 s
2017-08-18 12:34:15,099 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 530 finished: foreachPartition at streamingProcessNew.scala:50, took 0.037611 s
2017-08-18 12:34:15,099 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030855000 ms.0 from job set of time 1503030855000 ms
2017-08-18 12:34:15,100 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.099 s for time 1503030855000 ms (execution: 0.078 s)
2017-08-18 12:34:15,100 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1059 from persistence list
2017-08-18 12:34:15,100 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1059
2017-08-18 12:34:15,100 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1058 from persistence list
2017-08-18 12:34:15,101 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1058
2017-08-18 12:34:15,101 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:34:15,101 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030845000 ms
2017-08-18 12:34:20,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030860000 ms
2017-08-18 12:34:20,023 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030860000 ms.0 from job set of time 1503030860000 ms
2017-08-18 12:34:20,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:34:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 531 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:34:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 531 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:34:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:34:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:34:20,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 531 (MapPartitionsRDD[1063] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:34:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_531 stored as values in memory (estimated size 34.5 KB, free 413.8 MB)
2017-08-18 12:34:20,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_531_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.8 MB)
2017-08-18 12:34:20,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_531_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.9 MB)
2017-08-18 12:34:20,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 531 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:34:20,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 531 (MapPartitionsRDD[1063] at map at streamingProcessNew.scala:49)
2017-08-18 12:34:20,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 531.0 with 2 tasks
2017-08-18 12:34:20,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 531.0 (TID 1062, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:34:20,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 531.0 (TID 1063, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:34:20,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 531.0 (TID 1063)
2017-08-18 12:34:20,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 531.0 (TID 1062)
2017-08-18 12:34:20,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:34:20,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:34:20,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 531.0 (TID 1062). 714 bytes result sent to driver
2017-08-18 12:34:20,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 531.0 (TID 1063). 714 bytes result sent to driver
2017-08-18 12:34:20,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 531.0 (TID 1063) in 10 ms on localhost (1/2)
2017-08-18 12:34:20,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 531.0 (TID 1062) in 14 ms on localhost (2/2)
2017-08-18 12:34:20,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 531.0, whose tasks have all completed, from pool 
2017-08-18 12:34:20,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 531 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:34:20,084 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 531 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025565 s
2017-08-18 12:34:20,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030860000 ms.0 from job set of time 1503030860000 ms
2017-08-18 12:34:20,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1503030860000 ms (execution: 0.061 s)
2017-08-18 12:34:20,085 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1061 from persistence list
2017-08-18 12:34:20,085 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1061
2017-08-18 12:34:20,085 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1060 from persistence list
2017-08-18 12:34:20,086 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1060
2017-08-18 12:34:20,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:34:20,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030850000 ms
2017-08-18 12:34:25,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030865000 ms
2017-08-18 12:34:25,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030865000 ms.0 from job set of time 1503030865000 ms
2017-08-18 12:34:25,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:34:25,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 532 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:34:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 532 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:34:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:34:25,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:34:25,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 532 (MapPartitionsRDD[1065] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:34:25,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_532 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:34:25,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_532_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.7 MB)
2017-08-18 12:34:25,082 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_532_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:34:25,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 532 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:34:25,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 532 (MapPartitionsRDD[1065] at map at streamingProcessNew.scala:49)
2017-08-18 12:34:25,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 532.0 with 2 tasks
2017-08-18 12:34:25,091 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 532.0 (TID 1064, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:34:25,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 532.0 (TID 1065, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:34:25,093 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 532.0 (TID 1064)
2017-08-18 12:34:25,093 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 532.0 (TID 1065)
2017-08-18 12:34:25,097 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:34:25,098 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:34:25,102 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 532.0 (TID 1065). 714 bytes result sent to driver
2017-08-18 12:34:25,102 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 532.0 (TID 1064). 714 bytes result sent to driver
2017-08-18 12:34:25,105 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 532.0 (TID 1064) in 20 ms on localhost (1/2)
2017-08-18 12:34:25,105 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 532.0 (TID 1065) in 14 ms on localhost (2/2)
2017-08-18 12:34:25,105 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 532.0, whose tasks have all completed, from pool 
2017-08-18 12:34:25,106 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 532 (foreachPartition at streamingProcessNew.scala:50) finished in 0.020 s
2017-08-18 12:34:25,106 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 532 finished: foreachPartition at streamingProcessNew.scala:50, took 0.047067 s
2017-08-18 12:34:25,107 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030865000 ms.0 from job set of time 1503030865000 ms
2017-08-18 12:34:25,107 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.107 s for time 1503030865000 ms (execution: 0.085 s)
2017-08-18 12:34:25,107 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1063 from persistence list
2017-08-18 12:34:25,107 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1063
2017-08-18 12:34:25,107 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1062 from persistence list
2017-08-18 12:34:25,108 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1062
2017-08-18 12:34:25,108 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:34:25,108 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030855000 ms
2017-08-18 12:34:30,023 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030870000 ms
2017-08-18 12:34:30,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030870000 ms.0 from job set of time 1503030870000 ms
2017-08-18 12:34:30,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:34:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 533 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:34:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 533 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:34:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:34:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:34:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 533 (MapPartitionsRDD[1067] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:34:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_533 stored as values in memory (estimated size 34.5 KB, free 413.7 MB)
2017-08-18 12:34:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_533_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:34:30,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_533_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:34:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 533 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:34:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 533 (MapPartitionsRDD[1067] at map at streamingProcessNew.scala:49)
2017-08-18 12:34:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 533.0 with 2 tasks
2017-08-18 12:34:30,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 533.0 (TID 1066, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:34:30,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 533.0 (TID 1067, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:34:30,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 533.0 (TID 1067)
2017-08-18 12:34:30,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 533.0 (TID 1066)
2017-08-18 12:34:30,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:34:30,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:34:30,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 533.0 (TID 1067). 714 bytes result sent to driver
2017-08-18 12:34:30,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 533.0 (TID 1066). 714 bytes result sent to driver
2017-08-18 12:34:30,076 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 533.0 (TID 1067) in 10 ms on localhost (1/2)
2017-08-18 12:34:30,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 533.0 (TID 1066) in 13 ms on localhost (2/2)
2017-08-18 12:34:30,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 533.0, whose tasks have all completed, from pool 
2017-08-18 12:34:30,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 533 (foreachPartition at streamingProcessNew.scala:50) finished in 0.014 s
2017-08-18 12:34:30,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 533 finished: foreachPartition at streamingProcessNew.scala:50, took 0.025166 s
2017-08-18 12:34:30,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1503030870000 ms.0 from job set of time 1503030870000 ms
2017-08-18 12:34:30,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1503030870000 ms (execution: 0.053 s)
2017-08-18 12:34:30,077 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1065 from persistence list
2017-08-18 12:34:30,078 [block-manager-slave-async-thread-pool-44] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1065
2017-08-18 12:34:30,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1064 from persistence list
2017-08-18 12:34:30,078 [block-manager-slave-async-thread-pool-38] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1064
2017-08-18 12:34:30,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-08-18 12:34:30,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1503030860000 ms
2017-08-18 12:34:35,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503030875000 ms
2017-08-18 12:34:35,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503030875000 ms.0 from job set of time 1503030875000 ms
2017-08-18 12:34:35,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:50
2017-08-18 12:34:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 534 (foreachPartition at streamingProcessNew.scala:50) with 2 output partitions
2017-08-18 12:34:35,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 534 (foreachPartition at streamingProcessNew.scala:50)
2017-08-18 12:34:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-18 12:34:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-18 12:34:35,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 534 (MapPartitionsRDD[1069] at map at streamingProcessNew.scala:49), which has no missing parents
2017-08-18 12:34:35,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_534 stored as values in memory (estimated size 34.5 KB, free 413.6 MB)
2017-08-18 12:34:35,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_534_piece0 stored as bytes in memory (estimated size 25.1 KB, free 413.6 MB)
2017-08-18 12:34:35,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_534_piece0 in memory on 192.168.31.111:60809 (size: 25.1 KB, free: 413.8 MB)
2017-08-18 12:34:35,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 534 from broadcast at DAGScheduler.scala:1012
2017-08-18 12:34:35,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 534 (MapPartitionsRDD[1069] at map at streamingProcessNew.scala:49)
2017-08-18 12:34:35,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 534.0 with 2 tasks
2017-08-18 12:34:35,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 534.0 (TID 1068, localhost, partition 0, ANY, 5658 bytes)
2017-08-18 12:34:35,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 534.0 (TID 1069, localhost, partition 1, ANY, 5658 bytes)
2017-08-18 12:34:35,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 534.0 (TID 1069)
2017-08-18 12:34:35,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 534.0 (TID 1068)
2017-08-18 12:34:35,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 998 is the same as ending offset skipping test07 1
2017-08-18 12:34:35,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 976 is the same as ending offset skipping test07 0
2017-08-18 12:34:35,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 534.0 (TID 1069). 714 bytes result sent to driver
2017-08-18 12:34:35,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 534.0 (TID 1069) in 9 ms on localhost (1/2)
2017-08-18 12:34:35,089 [Executor task launch worker-0] ERROR [org.apache.spark.executor.Executor] - Exception in task 0.0 in stage 534.0 (TID 1068)
java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method) [na:1.8.0_111]
	at java.lang.Thread.start(Thread.java:714) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:950) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1368) [na:1.8.0_111]
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:112) ~[na:1.8.0_111]
	at com.jolbox.bonecp.BoneCP.watchConnection(BoneCP.java:562) ~[bonecp-0.8.0.RELEASE.jar:na]
	at com.jolbox.bonecp.AbstractConnectionStrategy.postConnection(AbstractConnectionStrategy.java:79) ~[bonecp-0.8.0.RELEASE.jar:na]
	at com.jolbox.bonecp.AbstractConnectionStrategy.getConnection(AbstractConnectionStrategy.java:92) ~[bonecp-0.8.0.RELEASE.jar:na]
	at com.jolbox.bonecp.BoneCP.getConnection(BoneCP.java:553) ~[bonecp-0.8.0.RELEASE.jar:na]
	at cn.datapark.process.education.Util.ConnectionPool$.getConnection(ConnectionPool.scala:32) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:50) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]
2017-08-18 12:34:35,122 [Executor task launch worker-0] ERROR [org.apache.spark.util.SparkUncaughtExceptionHandler] - Uncaught exception in thread Thread[Executor task launch worker-0,5,main]
java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method) [na:1.8.0_111]
	at java.lang.Thread.start(Thread.java:714) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:950) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1368) [na:1.8.0_111]
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:112) ~[na:1.8.0_111]
	at com.jolbox.bonecp.BoneCP.watchConnection(BoneCP.java:562) ~[bonecp-0.8.0.RELEASE.jar:na]
	at com.jolbox.bonecp.AbstractConnectionStrategy.postConnection(AbstractConnectionStrategy.java:79) ~[bonecp-0.8.0.RELEASE.jar:na]
	at com.jolbox.bonecp.AbstractConnectionStrategy.getConnection(AbstractConnectionStrategy.java:92) ~[bonecp-0.8.0.RELEASE.jar:na]
	at com.jolbox.bonecp.BoneCP.getConnection(BoneCP.java:553) ~[bonecp-0.8.0.RELEASE.jar:na]
	at cn.datapark.process.education.Util.ConnectionPool$.getConnection(ConnectionPool.scala:32) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:51) ~[classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew$$anonfun$main$2$$anonfun$apply$1.apply(streamingProcessNew.scala:50) ~[classes/:na]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:883) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:85) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]
2017-08-19 12:03:59,750 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-19 12:04:00,482 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-19 12:04:00,624 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-19 12:04:00,625 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-19 12:04:00,626 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-19 12:04:00,627 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-19 12:04:00,627 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-19 12:04:01,338 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 50407.
2017-08-19 12:04:01,357 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-19 12:04:01,375 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-19 12:04:01,388 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-fda20b32-0f47-4759-bcbe-6206ca958dec
2017-08-19 12:04:01,401 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-19 12:04:01,440 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-19 12:04:01,523 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2895ms
2017-08-19 12:04:01,612 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-19 12:04:01,629 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1c60324{/jobs,null,AVAILABLE}
2017-08-19 12:04:01,629 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1ae31b0{/jobs/json,null,AVAILABLE}
2017-08-19 12:04:01,629 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1a534b0{/jobs/job,null,AVAILABLE}
2017-08-19 12:04:01,630 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@10ca692{/jobs/job/json,null,AVAILABLE}
2017-08-19 12:04:01,630 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@104774{/stages,null,AVAILABLE}
2017-08-19 12:04:01,630 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@b3d0f7{/stages/json,null,AVAILABLE}
2017-08-19 12:04:01,630 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@517165{/stages/stage,null,AVAILABLE}
2017-08-19 12:04:01,630 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e3760b{/stages/stage/json,null,AVAILABLE}
2017-08-19 12:04:01,631 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@66a204{/stages/pool,null,AVAILABLE}
2017-08-19 12:04:01,631 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@60f3d7{/stages/pool/json,null,AVAILABLE}
2017-08-19 12:04:01,631 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@17f7be7{/storage,null,AVAILABLE}
2017-08-19 12:04:01,631 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f3156d{/storage/json,null,AVAILABLE}
2017-08-19 12:04:01,631 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1dae9b5{/storage/rdd,null,AVAILABLE}
2017-08-19 12:04:01,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7b5f92{/storage/rdd/json,null,AVAILABLE}
2017-08-19 12:04:01,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@bdb479{/environment,null,AVAILABLE}
2017-08-19 12:04:01,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3f95fe{/environment/json,null,AVAILABLE}
2017-08-19 12:04:01,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@625ccd{/executors,null,AVAILABLE}
2017-08-19 12:04:01,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7d121c{/executors/json,null,AVAILABLE}
2017-08-19 12:04:01,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1aa6596{/executors/threadDump,null,AVAILABLE}
2017-08-19 12:04:01,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1389cb8{/executors/threadDump/json,null,AVAILABLE}
2017-08-19 12:04:01,639 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@19a20a6{/static,null,AVAILABLE}
2017-08-19 12:04:01,639 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@133377b{/,null,AVAILABLE}
2017-08-19 12:04:01,640 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@183649e{/api,null,AVAILABLE}
2017-08-19 12:04:01,640 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fde30a{/stages/stage/kill,null,AVAILABLE}
2017-08-19 12:04:01,647 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@158ed3c{HTTP/1.1}{0.0.0.0:4040}
2017-08-19 12:04:01,647 [main] INFO  [org.spark_project.jetty.server.Server] - Started @3019ms
2017-08-19 12:04:01,647 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-19 12:04:01,649 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-19 12:04:01,728 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-19 12:04:01,753 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50420.
2017-08-19 12:04:01,754 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:50420
2017-08-19 12:04:01,756 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 50420)
2017-08-19 12:04:01,758 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:50420 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 50420)
2017-08-19 12:04:01,760 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 50420)
2017-08-19 12:04:01,944 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@37595f{/metrics/json,null,AVAILABLE}
2017-08-19 12:04:02,771 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 2000 ms
2017-08-19 12:04:02,771 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-19 12:04:02,772 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-19 12:04:02,772 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 2000 ms
2017-08-19 12:04:02,773 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@f5571c
2017-08-19 12:04:02,773 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 2000 ms
2017-08-19 12:04:02,773 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-19 12:04:02,773 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-19 12:04:02,773 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 2000 ms
2017-08-19 12:04:02,773 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@f77fff
2017-08-19 12:04:02,774 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 2000 ms
2017-08-19 12:04:02,774 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-19 12:04:02,774 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-19 12:04:02,774 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 2000 ms
2017-08-19 12:04:02,774 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@167fbeb
2017-08-19 12:04:02,816 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1503115444000
2017-08-19 12:04:02,817 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1503115444000 ms
2017-08-19 12:04:02,818 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-19 12:04:02,820 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b64941{/streaming,null,AVAILABLE}
2017-08-19 12:04:02,821 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@136207f{/streaming/json,null,AVAILABLE}
2017-08-19 12:04:02,821 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@194e8d{/streaming/batch,null,AVAILABLE}
2017-08-19 12:04:02,822 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@10aeb20{/streaming/batch/json,null,AVAILABLE}
2017-08-19 12:04:02,823 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1496961{/static/streaming,null,AVAILABLE}
2017-08-19 12:04:02,824 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-19 12:04:04,138 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115444000 ms
2017-08-19 12:04:04,142 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503115444000 ms.0 from job set of time 1503115444000 ms
2017-08-19 12:04:04,165 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:31
2017-08-19 12:04:04,177 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreach at streamingExample.scala:31) with 2 output partitions
2017-08-19 12:04:04,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreach at streamingExample.scala:31)
2017-08-19 12:04:04,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-19 12:04:04,180 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-19 12:04:04,188 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingExample.scala:30), which has no missing parents
2017-08-19 12:04:04,245 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-19 12:04:04,253 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.0 KB, free 413.9 MB)
2017-08-19 12:04:04,288 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1842.0 B, free 413.9 MB)
2017-08-19 12:04:04,291 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:50420 (size: 1842.0 B, free: 413.9 MB)
2017-08-19 12:04:04,295 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-19 12:04:04,299 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingExample.scala:30)
2017-08-19 12:04:04,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-19 12:04:04,363 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5646 bytes)
2017-08-19 12:04:04,368 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5646 bytes)
2017-08-19 12:04:04,376 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-19 12:04:04,376 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-19 12:04:04,416 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 698 -> 998
2017-08-19 12:04:04,416 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 653 -> 976
2017-08-19 12:04:06,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115446000 ms
2017-08-19 12:04:08,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115448000 ms
2017-08-19 12:04:10,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115450000 ms
2017-08-19 12:04:12,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115452000 ms
2017-08-19 12:04:14,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115454000 ms
2017-08-19 12:04:16,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115456000 ms
2017-08-19 12:04:18,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115458000 ms
2017-08-19 12:04:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115460000 ms
2017-08-19 12:04:22,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115462000 ms
2017-08-19 12:04:24,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115464000 ms
2017-08-19 12:04:26,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115466000 ms
2017-08-19 12:04:28,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115468000 ms
2017-08-19 12:04:30,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503115470000 ms
2017-08-19 12:04:31,380 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 829 bytes result sent to driver
2017-08-19 12:04:31,387 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 27052 ms on localhost (1/2)
2017-08-19 12:59:51,689 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-08-19 12:59:51,900 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-08-19 12:59:51,990 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-08-19 12:59:51,990 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-08-19 12:59:51,991 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-08-19 12:59:51,992 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-08-19 12:59:51,993 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-08-19 12:59:52,716 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 50880.
2017-08-19 12:59:52,735 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-08-19 12:59:52,753 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-08-19 12:59:52,766 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-692faeb1-0564-4466-a26b-4e8e511979d3
2017-08-19 12:59:52,781 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-08-19 12:59:52,823 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-08-19 12:59:52,909 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2008ms
2017-08-19 12:59:53,004 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-08-19 12:59:53,020 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1a534b0{/jobs,null,AVAILABLE}
2017-08-19 12:59:53,020 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@10ca692{/jobs/json,null,AVAILABLE}
2017-08-19 12:59:53,021 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@104774{/jobs/job,null,AVAILABLE}
2017-08-19 12:59:53,021 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@b3d0f7{/jobs/job/json,null,AVAILABLE}
2017-08-19 12:59:53,021 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@517165{/stages,null,AVAILABLE}
2017-08-19 12:59:53,021 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e3760b{/stages/json,null,AVAILABLE}
2017-08-19 12:59:53,021 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@66a204{/stages/stage,null,AVAILABLE}
2017-08-19 12:59:53,022 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@60f3d7{/stages/stage/json,null,AVAILABLE}
2017-08-19 12:59:53,022 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@17f7be7{/stages/pool,null,AVAILABLE}
2017-08-19 12:59:53,022 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f3156d{/stages/pool/json,null,AVAILABLE}
2017-08-19 12:59:53,022 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1dae9b5{/storage,null,AVAILABLE}
2017-08-19 12:59:53,022 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7b5f92{/storage/json,null,AVAILABLE}
2017-08-19 12:59:53,023 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@bdb479{/storage/rdd,null,AVAILABLE}
2017-08-19 12:59:53,023 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3f95fe{/storage/rdd/json,null,AVAILABLE}
2017-08-19 12:59:53,023 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@625ccd{/environment,null,AVAILABLE}
2017-08-19 12:59:53,023 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7d121c{/environment/json,null,AVAILABLE}
2017-08-19 12:59:53,023 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1aa6596{/executors,null,AVAILABLE}
2017-08-19 12:59:53,024 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1389cb8{/executors/json,null,AVAILABLE}
2017-08-19 12:59:53,024 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@19a20a6{/executors/threadDump,null,AVAILABLE}
2017-08-19 12:59:53,024 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@133377b{/executors/threadDump/json,null,AVAILABLE}
2017-08-19 12:59:53,030 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@183649e{/static,null,AVAILABLE}
2017-08-19 12:59:53,030 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fde30a{/,null,AVAILABLE}
2017-08-19 12:59:53,031 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f27ea3{/api,null,AVAILABLE}
2017-08-19 12:59:53,031 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e61929{/stages/stage/kill,null,AVAILABLE}
2017-08-19 12:59:53,038 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@320ade{HTTP/1.1}{0.0.0.0:4040}
2017-08-19 12:59:53,038 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2137ms
2017-08-19 12:59:53,038 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-08-19 12:59:53,040 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.111:4040
2017-08-19 12:59:53,121 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-08-19 12:59:53,146 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50893.
2017-08-19 12:59:53,147 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.111:50893
2017-08-19 12:59:53,148 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.111, 50893)
2017-08-19 12:59:53,151 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.111:50893 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.111, 50893)
2017-08-19 12:59:53,153 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.111, 50893)
2017-08-19 12:59:53,343 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@37595f{/metrics/json,null,AVAILABLE}
2017-08-19 12:59:53,849 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 2000 ms
2017-08-19 12:59:53,849 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-08-19 12:59:53,850 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-08-19 12:59:53,850 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 2000 ms
2017-08-19 12:59:53,851 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@4e86e8
2017-08-19 12:59:53,851 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 2000 ms
2017-08-19 12:59:53,851 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-08-19 12:59:53,851 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-08-19 12:59:53,851 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 2000 ms
2017-08-19 12:59:53,851 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@1750dfb
2017-08-19 12:59:53,851 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 2000 ms
2017-08-19 12:59:53,851 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-08-19 12:59:53,851 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-08-19 12:59:53,852 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 2000 ms
2017-08-19 12:59:53,852 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@1a3b85f
2017-08-19 12:59:53,891 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1503118794000
2017-08-19 12:59:53,892 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1503118794000 ms
2017-08-19 12:59:53,892 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-08-19 12:59:53,894 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b64941{/streaming,null,AVAILABLE}
2017-08-19 12:59:53,894 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@136207f{/streaming/json,null,AVAILABLE}
2017-08-19 12:59:53,895 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@194e8d{/streaming/batch,null,AVAILABLE}
2017-08-19 12:59:53,895 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@10aeb20{/streaming/batch/json,null,AVAILABLE}
2017-08-19 12:59:53,896 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1496961{/static/streaming,null,AVAILABLE}
2017-08-19 12:59:53,897 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-08-19 12:59:54,133 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118794000 ms
2017-08-19 12:59:54,137 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1503118794000 ms.0 from job set of time 1503118794000 ms
2017-08-19 12:59:54,163 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:31
2017-08-19 12:59:54,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreach at streamingExample.scala:31) with 2 output partitions
2017-08-19 12:59:54,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreach at streamingExample.scala:31)
2017-08-19 12:59:54,179 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-08-19 12:59:54,180 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-08-19 12:59:54,188 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingExample.scala:30), which has no missing parents
2017-08-19 12:59:54,247 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-08-19 12:59:54,255 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.0 KB, free 413.9 MB)
2017-08-19 12:59:54,294 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1842.0 B, free 413.9 MB)
2017-08-19 12:59:54,297 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.111:50893 (size: 1842.0 B, free: 413.9 MB)
2017-08-19 12:59:54,301 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-08-19 12:59:54,305 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingExample.scala:30)
2017-08-19 12:59:54,307 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-08-19 12:59:54,360 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5646 bytes)
2017-08-19 12:59:54,364 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5646 bytes)
2017-08-19 12:59:54,370 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-08-19 12:59:54,370 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-08-19 12:59:54,398 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 0 offsets 653 -> 976
2017-08-19 12:59:54,398 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test07, partition 1 offsets 698 -> 998
2017-08-19 12:59:56,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118796000 ms
2017-08-19 12:59:58,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118798000 ms
2017-08-19 13:00:00,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118800000 ms
2017-08-19 13:00:02,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118802000 ms
2017-08-19 13:00:04,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118804000 ms
2017-08-19 13:00:06,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118806000 ms
2017-08-19 13:00:08,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118808000 ms
2017-08-19 13:00:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118810000 ms
2017-08-19 13:00:12,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118812000 ms
2017-08-19 13:00:14,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118814000 ms
2017-08-19 13:00:16,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118816000 ms
2017-08-19 13:00:18,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118818000 ms
2017-08-19 13:00:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118820000 ms
2017-08-19 13:00:21,614 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 916 bytes result sent to driver
2017-08-19 13:00:21,620 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 27281 ms on localhost (1/2)
2017-08-19 13:00:22,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1503118822000 ms
