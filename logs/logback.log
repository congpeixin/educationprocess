2017-07-26 18:08:02,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:02,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:02,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1890 (KafkaRDD[1890] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:02,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1890 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:08:02,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1890_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.3 MB)
2017-07-26 18:08:02,081 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1890_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:02,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1890 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:02,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1890 (KafkaRDD[1890] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:02,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1890.0 with 2 tasks
2017-07-26 18:08:02,082 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1883_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:02,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1890.0 (TID 3780, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:02,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1890.0 (TID 3781, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:02,085 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1876_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:02,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1890.0 (TID 3781)
2017-07-26 18:08:02,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1890.0 (TID 3780)
2017-07-26 18:08:02,087 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:02,088 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:02,090 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1890.0 (TID 3780). 714 bytes result sent to driver
2017-07-26 18:08:02,090 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1890.0 (TID 3781). 714 bytes result sent to driver
2017-07-26 18:08:02,093 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1877_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:02,093 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1890.0 (TID 3780) in 10 ms on localhost (1/2)
2017-07-26 18:08:02,094 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1890.0 (TID 3781) in 10 ms on localhost (2/2)
2017-07-26 18:08:02,094 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1890.0, whose tasks have all completed, from pool 
2017-07-26 18:08:02,094 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1890 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:08:02,095 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1878_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:02,095 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1890 finished: foreachPartition at streamingProcessTest.scala:48, took 0.044218 s
2017-07-26 18:08:02,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063682000 ms.0 from job set of time 1501063682000 ms
2017-07-26 18:08:02,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.095 s for time 1501063682000 ms (execution: 0.077 s)
2017-07-26 18:08:02,096 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1889 from persistence list
2017-07-26 18:08:02,096 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1889
2017-07-26 18:08:02,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:02,096 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063678000 ms
2017-07-26 18:08:02,097 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1879_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:02,099 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1880_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:02,101 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1881_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:02,102 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1882_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:02,104 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1884_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:02,105 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1885_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:02,107 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1886_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:02,108 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1887_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:02,110 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1888_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:02,112 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1889_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:04,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063684000 ms
2017-07-26 18:08:04,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063684000 ms.0 from job set of time 1501063684000 ms
2017-07-26 18:08:04,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:04,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1891 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:04,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1891 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:04,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:04,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:04,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1891 (KafkaRDD[1891] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:04,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1891 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:08:04,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1891_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:08:04,029 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1891_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:04,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1891 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:04,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1891 (KafkaRDD[1891] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:04,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1891.0 with 2 tasks
2017-07-26 18:08:04,031 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1891.0 (TID 3782, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:04,031 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1891.0 (TID 3783, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:04,032 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1891.0 (TID 3783)
2017-07-26 18:08:04,032 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1891.0 (TID 3782)
2017-07-26 18:08:04,034 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:04,034 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:04,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1891.0 (TID 3783). 714 bytes result sent to driver
2017-07-26 18:08:04,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1891.0 (TID 3782). 714 bytes result sent to driver
2017-07-26 18:08:04,037 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1891.0 (TID 3783) in 6 ms on localhost (1/2)
2017-07-26 18:08:04,038 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1891.0 (TID 3782) in 7 ms on localhost (2/2)
2017-07-26 18:08:04,038 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1891.0, whose tasks have all completed, from pool 
2017-07-26 18:08:04,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1891 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:08:04,038 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1891 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014954 s
2017-07-26 18:08:04,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063684000 ms.0 from job set of time 1501063684000 ms
2017-07-26 18:08:04,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.038 s for time 1501063684000 ms (execution: 0.026 s)
2017-07-26 18:08:04,038 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1890 from persistence list
2017-07-26 18:08:04,039 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1890
2017-07-26 18:08:04,039 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:04,039 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063680000 ms
2017-07-26 18:08:06,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063686000 ms
2017-07-26 18:08:06,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063686000 ms.0 from job set of time 1501063686000 ms
2017-07-26 18:08:06,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:06,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1892 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:06,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1892 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:06,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:06,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:06,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1892 (KafkaRDD[1892] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:06,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1892 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:08:06,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1892_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:08:06,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1892_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:06,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1892 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:06,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1892 (KafkaRDD[1892] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:06,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1892.0 with 2 tasks
2017-07-26 18:08:06,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1892.0 (TID 3784, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:06,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1892.0 (TID 3785, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:06,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1892.0 (TID 3785)
2017-07-26 18:08:06,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1892.0 (TID 3784)
2017-07-26 18:08:06,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:06,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:06,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1892.0 (TID 3784). 714 bytes result sent to driver
2017-07-26 18:08:06,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1892.0 (TID 3785). 714 bytes result sent to driver
2017-07-26 18:08:06,071 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1892.0 (TID 3785) in 8 ms on localhost (1/2)
2017-07-26 18:08:06,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1892.0 (TID 3784) in 10 ms on localhost (2/2)
2017-07-26 18:08:06,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1892.0, whose tasks have all completed, from pool 
2017-07-26 18:08:06,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1892 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:08:06,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1892 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026849 s
2017-07-26 18:08:06,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063686000 ms.0 from job set of time 1501063686000 ms
2017-07-26 18:08:06,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501063686000 ms (execution: 0.056 s)
2017-07-26 18:08:06,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1891 from persistence list
2017-07-26 18:08:06,073 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1891
2017-07-26 18:08:06,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:06,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063682000 ms
2017-07-26 18:08:08,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063688000 ms
2017-07-26 18:08:08,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063688000 ms.0 from job set of time 1501063688000 ms
2017-07-26 18:08:08,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1893 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1893 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:08,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1893 (KafkaRDD[1893] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1893 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:08:08,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1893_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:08:08,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1893_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1893 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1893 (KafkaRDD[1893] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1893.0 with 2 tasks
2017-07-26 18:08:08,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1893.0 (TID 3786, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:08,058 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1893.0 (TID 3787, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:08,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1893.0 (TID 3786)
2017-07-26 18:08:08,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1893.0 (TID 3787)
2017-07-26 18:08:08,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:08,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:08,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1893.0 (TID 3787). 635 bytes result sent to driver
2017-07-26 18:08:08,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1893.0 (TID 3786). 635 bytes result sent to driver
2017-07-26 18:08:08,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1893.0 (TID 3787) in 10 ms on localhost (1/2)
2017-07-26 18:08:08,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1893.0 (TID 3786) in 12 ms on localhost (2/2)
2017-07-26 18:08:08,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1893.0, whose tasks have all completed, from pool 
2017-07-26 18:08:08,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1893 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:08:08,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1893 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025299 s
2017-07-26 18:08:08,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063688000 ms.0 from job set of time 1501063688000 ms
2017-07-26 18:08:08,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501063688000 ms (execution: 0.051 s)
2017-07-26 18:08:08,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1892 from persistence list
2017-07-26 18:08:08,070 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1892
2017-07-26 18:08:08,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:08,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063684000 ms
2017-07-26 18:08:10,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063690000 ms
2017-07-26 18:08:10,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063690000 ms.0 from job set of time 1501063690000 ms
2017-07-26 18:08:10,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:10,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1894 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:10,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1894 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:10,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:10,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:10,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1894 (KafkaRDD[1894] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1894 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:08:10,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1894_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:08:10,043 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1894_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:10,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1894 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:10,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1894 (KafkaRDD[1894] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:10,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1894.0 with 2 tasks
2017-07-26 18:08:10,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1894.0 (TID 3788, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:10,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1894.0 (TID 3789, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:10,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1894.0 (TID 3789)
2017-07-26 18:08:10,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1894.0 (TID 3788)
2017-07-26 18:08:10,049 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:10,049 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:10,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1894.0 (TID 3788). 635 bytes result sent to driver
2017-07-26 18:08:10,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1894.0 (TID 3789). 635 bytes result sent to driver
2017-07-26 18:08:10,054 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1894.0 (TID 3788) in 9 ms on localhost (1/2)
2017-07-26 18:08:10,054 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1894.0 (TID 3789) in 8 ms on localhost (2/2)
2017-07-26 18:08:10,054 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1894.0, whose tasks have all completed, from pool 
2017-07-26 18:08:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1894 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:08:10,055 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1894 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021113 s
2017-07-26 18:08:10,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063690000 ms.0 from job set of time 1501063690000 ms
2017-07-26 18:08:10,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.055 s for time 1501063690000 ms (execution: 0.038 s)
2017-07-26 18:08:10,055 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1893 from persistence list
2017-07-26 18:08:10,056 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1893
2017-07-26 18:08:10,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:10,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063686000 ms
2017-07-26 18:08:12,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063692000 ms
2017-07-26 18:08:12,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063692000 ms.0 from job set of time 1501063692000 ms
2017-07-26 18:08:12,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:12,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1895 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:12,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1895 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:12,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:12,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:12,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1895 (KafkaRDD[1895] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:12,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1895 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:08:12,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1895_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:08:12,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1895_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:12,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1895 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:12,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1895 (KafkaRDD[1895] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:12,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1895.0 with 2 tasks
2017-07-26 18:08:12,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1895.0 (TID 3790, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:12,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1895.0 (TID 3791, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:12,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1895.0 (TID 3791)
2017-07-26 18:08:12,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1895.0 (TID 3790)
2017-07-26 18:08:12,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:12,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:12,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1895.0 (TID 3790). 635 bytes result sent to driver
2017-07-26 18:08:12,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1895.0 (TID 3791). 635 bytes result sent to driver
2017-07-26 18:08:12,085 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1895.0 (TID 3790) in 19 ms on localhost (1/2)
2017-07-26 18:08:12,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1895.0 (TID 3791) in 17 ms on localhost (2/2)
2017-07-26 18:08:12,086 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1895.0, whose tasks have all completed, from pool 
2017-07-26 18:08:12,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1895 (foreachPartition at streamingProcessTest.scala:48) finished in 0.020 s
2017-07-26 18:08:12,087 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1895 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040818 s
2017-07-26 18:08:12,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063692000 ms.0 from job set of time 1501063692000 ms
2017-07-26 18:08:12,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1894 from persistence list
2017-07-26 18:08:12,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1501063692000 ms (execution: 0.071 s)
2017-07-26 18:08:12,088 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1894
2017-07-26 18:08:12,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:12,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063688000 ms
2017-07-26 18:08:14,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063694000 ms
2017-07-26 18:08:14,011 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063694000 ms.0 from job set of time 1501063694000 ms
2017-07-26 18:08:14,021 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:14,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1896 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:14,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1896 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:14,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:14,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:14,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1896 (KafkaRDD[1896] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1896 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:08:14,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1896_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:08:14,026 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1896_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:14,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1896 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:14,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1896 (KafkaRDD[1896] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:14,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1896.0 with 2 tasks
2017-07-26 18:08:14,029 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1896.0 (TID 3792, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:14,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1896.0 (TID 3793, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:14,030 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1896.0 (TID 3793)
2017-07-26 18:08:14,030 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1896.0 (TID 3792)
2017-07-26 18:08:14,032 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:14,032 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:14,034 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1896.0 (TID 3793). 635 bytes result sent to driver
2017-07-26 18:08:14,034 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1896.0 (TID 3792). 635 bytes result sent to driver
2017-07-26 18:08:14,036 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1896.0 (TID 3792) in 8 ms on localhost (1/2)
2017-07-26 18:08:14,036 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1896.0 (TID 3793) in 6 ms on localhost (2/2)
2017-07-26 18:08:14,036 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1896.0, whose tasks have all completed, from pool 
2017-07-26 18:08:14,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1896 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:08:14,037 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1896 finished: foreachPartition at streamingProcessTest.scala:48, took 0.015457 s
2017-07-26 18:08:14,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063694000 ms.0 from job set of time 1501063694000 ms
2017-07-26 18:08:14,037 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1895 from persistence list
2017-07-26 18:08:14,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.037 s for time 1501063694000 ms (execution: 0.026 s)
2017-07-26 18:08:14,038 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1895
2017-07-26 18:08:14,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:14,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063690000 ms
2017-07-26 18:08:16,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063696000 ms
2017-07-26 18:08:16,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063696000 ms.0 from job set of time 1501063696000 ms
2017-07-26 18:08:16,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1897 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1897 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1897 (KafkaRDD[1897] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:16,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1897 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:08:16,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1897_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:08:16,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1897_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1897 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1897 (KafkaRDD[1897] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1897.0 with 2 tasks
2017-07-26 18:08:16,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1897.0 (TID 3794, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:16,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1897.0 (TID 3795, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:16,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1897.0 (TID 3794)
2017-07-26 18:08:16,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1897.0 (TID 3795)
2017-07-26 18:08:16,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:16,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:16,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1897.0 (TID 3794). 714 bytes result sent to driver
2017-07-26 18:08:16,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1897.0 (TID 3795). 714 bytes result sent to driver
2017-07-26 18:08:16,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1897.0 (TID 3794) in 9 ms on localhost (1/2)
2017-07-26 18:08:16,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1897.0 (TID 3795) in 8 ms on localhost (2/2)
2017-07-26 18:08:16,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1897.0, whose tasks have all completed, from pool 
2017-07-26 18:08:16,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1897 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:08:16,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1897 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021445 s
2017-07-26 18:08:16,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063696000 ms.0 from job set of time 1501063696000 ms
2017-07-26 18:08:16,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501063696000 ms (execution: 0.053 s)
2017-07-26 18:08:16,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1896 from persistence list
2017-07-26 18:08:16,070 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1896
2017-07-26 18:08:16,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:16,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063692000 ms
2017-07-26 18:08:18,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063698000 ms
2017-07-26 18:08:18,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063698000 ms.0 from job set of time 1501063698000 ms
2017-07-26 18:08:18,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:18,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1898 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:18,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1898 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:18,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:18,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:18,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1898 (KafkaRDD[1898] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:18,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1898 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:08:18,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1898_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:08:18,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1898_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:18,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1898 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:18,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1898 (KafkaRDD[1898] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:18,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1898.0 with 2 tasks
2017-07-26 18:08:18,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1898.0 (TID 3796, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:18,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1898.0 (TID 3797, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:18,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1898.0 (TID 3797)
2017-07-26 18:08:18,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1898.0 (TID 3796)
2017-07-26 18:08:18,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:18,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:18,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1898.0 (TID 3796). 635 bytes result sent to driver
2017-07-26 18:08:18,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1898.0 (TID 3797). 635 bytes result sent to driver
2017-07-26 18:08:18,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1898.0 (TID 3796) in 15 ms on localhost (1/2)
2017-07-26 18:08:18,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1898.0 (TID 3797) in 13 ms on localhost (2/2)
2017-07-26 18:08:18,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1898.0, whose tasks have all completed, from pool 
2017-07-26 18:08:18,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1898 (foreachPartition at streamingProcessTest.scala:48) finished in 0.016 s
2017-07-26 18:08:18,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1898 finished: foreachPartition at streamingProcessTest.scala:48, took 0.034530 s
2017-07-26 18:08:18,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063698000 ms.0 from job set of time 1501063698000 ms
2017-07-26 18:08:18,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501063698000 ms (execution: 0.064 s)
2017-07-26 18:08:18,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1897 from persistence list
2017-07-26 18:08:18,082 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1897
2017-07-26 18:08:18,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:18,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063694000 ms
2017-07-26 18:08:20,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063700000 ms
2017-07-26 18:08:20,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063700000 ms.0 from job set of time 1501063700000 ms
2017-07-26 18:08:20,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:20,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1899 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:20,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1899 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:20,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:20,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:20,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1899 (KafkaRDD[1899] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1899 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:08:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1899_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:08:20,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1899_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:20,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1899 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:20,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1899 (KafkaRDD[1899] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:20,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1899.0 with 2 tasks
2017-07-26 18:08:20,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1899.0 (TID 3798, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:20,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1899.0 (TID 3799, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:20,052 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1899.0 (TID 3798)
2017-07-26 18:08:20,052 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1899.0 (TID 3799)
2017-07-26 18:08:20,055 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:20,055 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:20,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1899.0 (TID 3798). 635 bytes result sent to driver
2017-07-26 18:08:20,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1899.0 (TID 3799). 635 bytes result sent to driver
2017-07-26 18:08:20,060 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1899.0 (TID 3798) in 8 ms on localhost (1/2)
2017-07-26 18:08:20,060 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1899.0 (TID 3799) in 8 ms on localhost (2/2)
2017-07-26 18:08:20,060 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1899.0, whose tasks have all completed, from pool 
2017-07-26 18:08:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1899 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:08:20,060 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1899 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020041 s
2017-07-26 18:08:20,061 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063700000 ms.0 from job set of time 1501063700000 ms
2017-07-26 18:08:20,061 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.061 s for time 1501063700000 ms (execution: 0.045 s)
2017-07-26 18:08:20,061 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1898 from persistence list
2017-07-26 18:08:20,062 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1898
2017-07-26 18:08:20,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:20,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063696000 ms
2017-07-26 18:08:22,026 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063702000 ms
2017-07-26 18:08:22,027 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063702000 ms.0 from job set of time 1501063702000 ms
2017-07-26 18:08:22,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:22,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1900 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:22,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1900 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:22,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:22,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:22,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1900 (KafkaRDD[1900] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:22,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1900 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:08:22,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1900_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:08:22,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1900_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:22,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1900 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:22,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1900 (KafkaRDD[1900] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:22,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1900.0 with 2 tasks
2017-07-26 18:08:22,079 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1900.0 (TID 3800, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:22,080 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1900.0 (TID 3801, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:22,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1900.0 (TID 3801)
2017-07-26 18:08:22,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1900.0 (TID 3800)
2017-07-26 18:08:22,085 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:22,085 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:22,090 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1900.0 (TID 3801). 635 bytes result sent to driver
2017-07-26 18:08:22,090 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1900.0 (TID 3800). 635 bytes result sent to driver
2017-07-26 18:08:22,094 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1900.0 (TID 3800) in 17 ms on localhost (1/2)
2017-07-26 18:08:22,095 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1900.0 (TID 3801) in 15 ms on localhost (2/2)
2017-07-26 18:08:22,095 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1900.0, whose tasks have all completed, from pool 
2017-07-26 18:08:22,095 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1900 (foreachPartition at streamingProcessTest.scala:48) finished in 0.018 s
2017-07-26 18:08:22,096 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1900 finished: foreachPartition at streamingProcessTest.scala:48, took 0.037631 s
2017-07-26 18:08:22,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063702000 ms.0 from job set of time 1501063702000 ms
2017-07-26 18:08:22,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.097 s for time 1501063702000 ms (execution: 0.070 s)
2017-07-26 18:08:22,097 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1899 from persistence list
2017-07-26 18:08:22,099 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1899
2017-07-26 18:08:22,099 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:22,099 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063698000 ms
2017-07-26 18:08:24,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063704000 ms
2017-07-26 18:08:24,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063704000 ms.0 from job set of time 1501063704000 ms
2017-07-26 18:08:24,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:24,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1901 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:24,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1901 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:24,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:24,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:24,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1901 (KafkaRDD[1901] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:24,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1901 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:08:24,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1901_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:08:24,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1901_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:24,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1901 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:24,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1901 (KafkaRDD[1901] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:24,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1901.0 with 2 tasks
2017-07-26 18:08:24,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1901.0 (TID 3802, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:24,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1901.0 (TID 3803, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:24,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1901.0 (TID 3802)
2017-07-26 18:08:24,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1901.0 (TID 3803)
2017-07-26 18:08:24,058 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:24,058 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:24,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1901.0 (TID 3802). 722 bytes result sent to driver
2017-07-26 18:08:24,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1901.0 (TID 3803). 722 bytes result sent to driver
2017-07-26 18:08:24,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1901.0 (TID 3802) in 9 ms on localhost (1/2)
2017-07-26 18:08:24,062 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1901.0 (TID 3803) in 7 ms on localhost (2/2)
2017-07-26 18:08:24,062 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1901.0, whose tasks have all completed, from pool 
2017-07-26 18:08:24,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1901 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:08:24,062 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1901 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024197 s
2017-07-26 18:08:24,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063704000 ms.0 from job set of time 1501063704000 ms
2017-07-26 18:08:24,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1501063704000 ms (execution: 0.048 s)
2017-07-26 18:08:24,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1900 from persistence list
2017-07-26 18:08:24,063 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1900
2017-07-26 18:08:24,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:24,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063700000 ms
2017-07-26 18:08:26,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063706000 ms
2017-07-26 18:08:26,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063706000 ms.0 from job set of time 1501063706000 ms
2017-07-26 18:08:26,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:26,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1902 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:26,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1902 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:26,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:26,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:26,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1902 (KafkaRDD[1902] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:26,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1902 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:08:26,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1902_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:08:26,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1902_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:26,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1902 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:26,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1902 (KafkaRDD[1902] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:26,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1902.0 with 2 tasks
2017-07-26 18:08:26,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1902.0 (TID 3804, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:26,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1902.0 (TID 3805, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:26,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1902.0 (TID 3805)
2017-07-26 18:08:26,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1902.0 (TID 3804)
2017-07-26 18:08:26,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:26,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:26,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1902.0 (TID 3805). 714 bytes result sent to driver
2017-07-26 18:08:26,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1902.0 (TID 3804). 635 bytes result sent to driver
2017-07-26 18:08:26,083 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1902.0 (TID 3805) in 14 ms on localhost (1/2)
2017-07-26 18:08:26,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1902.0 (TID 3804) in 17 ms on localhost (2/2)
2017-07-26 18:08:26,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1902.0, whose tasks have all completed, from pool 
2017-07-26 18:08:26,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1902 (foreachPartition at streamingProcessTest.scala:48) finished in 0.017 s
2017-07-26 18:08:26,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1902 finished: foreachPartition at streamingProcessTest.scala:48, took 0.038355 s
2017-07-26 18:08:26,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063706000 ms.0 from job set of time 1501063706000 ms
2017-07-26 18:08:26,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1901 from persistence list
2017-07-26 18:08:26,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.086 s for time 1501063706000 ms (execution: 0.070 s)
2017-07-26 18:08:26,087 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1901
2017-07-26 18:08:26,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:26,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063702000 ms
2017-07-26 18:08:28,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063708000 ms
2017-07-26 18:08:28,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063708000 ms.0 from job set of time 1501063708000 ms
2017-07-26 18:08:28,032 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:28,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1903 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:28,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1903 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:28,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:28,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:28,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1903 (KafkaRDD[1903] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:28,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1903 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:08:28,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1903_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:08:28,043 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1903_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:28,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1903 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:28,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1903 (KafkaRDD[1903] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:28,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1903.0 with 2 tasks
2017-07-26 18:08:28,046 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1903.0 (TID 3806, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:28,047 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1903.0 (TID 3807, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:28,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1903.0 (TID 3806)
2017-07-26 18:08:28,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1903.0 (TID 3807)
2017-07-26 18:08:28,049 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:28,049 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:28,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1903.0 (TID 3807). 635 bytes result sent to driver
2017-07-26 18:08:28,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1903.0 (TID 3806). 635 bytes result sent to driver
2017-07-26 18:08:28,052 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1903.0 (TID 3807) in 5 ms on localhost (1/2)
2017-07-26 18:08:28,052 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1903.0 (TID 3806) in 7 ms on localhost (2/2)
2017-07-26 18:08:28,052 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1903.0, whose tasks have all completed, from pool 
2017-07-26 18:08:28,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1903 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:08:28,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1903 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019870 s
2017-07-26 18:08:28,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063708000 ms.0 from job set of time 1501063708000 ms
2017-07-26 18:08:28,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.052 s for time 1501063708000 ms (execution: 0.039 s)
2017-07-26 18:08:28,053 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1902 from persistence list
2017-07-26 18:08:28,053 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1902
2017-07-26 18:08:28,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:28,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063704000 ms
2017-07-26 18:08:30,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063710000 ms
2017-07-26 18:08:30,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063710000 ms.0 from job set of time 1501063710000 ms
2017-07-26 18:08:30,060 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1904 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1904 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:30,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1903_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1904 (KafkaRDD[1904] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:30,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1890_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:30,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1891_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1904 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:08:30,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1892_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:30,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1893_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:30,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1904_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:08:30,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1904_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:30,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1904 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:30,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1904 (KafkaRDD[1904] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:30,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1904.0 with 2 tasks
2017-07-26 18:08:30,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1894_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:30,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1904.0 (TID 3808, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:30,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1904.0 (TID 3809, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1904.0 (TID 3808)
2017-07-26 18:08:30,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1904.0 (TID 3809)
2017-07-26 18:08:30,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1895_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:30,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:30,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1896_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:30,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:30,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1904.0 (TID 3808). 635 bytes result sent to driver
2017-07-26 18:08:30,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1904.0 (TID 3809). 714 bytes result sent to driver
2017-07-26 18:08:30,079 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1897_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:30,081 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1904.0 (TID 3808) in 10 ms on localhost (1/2)
2017-07-26 18:08:30,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1904.0 (TID 3809) in 9 ms on localhost (2/2)
2017-07-26 18:08:30,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1904.0, whose tasks have all completed, from pool 
2017-07-26 18:08:30,081 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1898_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:30,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1904 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:08:30,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1904 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022278 s
2017-07-26 18:08:30,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063710000 ms.0 from job set of time 1501063710000 ms
2017-07-26 18:08:30,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1501063710000 ms (execution: 0.067 s)
2017-07-26 18:08:30,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1903 from persistence list
2017-07-26 18:08:30,083 [block-manager-slave-async-thread-pool-16] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1903
2017-07-26 18:08:30,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:30,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063706000 ms
2017-07-26 18:08:30,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1899_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:30,085 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1900_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:30,087 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1901_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:30,089 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1902_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:32,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063712000 ms
2017-07-26 18:08:32,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063712000 ms.0 from job set of time 1501063712000 ms
2017-07-26 18:08:32,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:32,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1905 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:32,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1905 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:32,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:32,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:32,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1905 (KafkaRDD[1905] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:32,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1905 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:08:32,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1905_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:08:32,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1905_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:32,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1905 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:32,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1905 (KafkaRDD[1905] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:32,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1905.0 with 2 tasks
2017-07-26 18:08:32,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1905.0 (TID 3810, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:32,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1905.0 (TID 3811, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:32,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1905.0 (TID 3810)
2017-07-26 18:08:32,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1905.0 (TID 3811)
2017-07-26 18:08:32,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:32,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:32,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1905.0 (TID 3810). 635 bytes result sent to driver
2017-07-26 18:08:32,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1905.0 (TID 3811). 714 bytes result sent to driver
2017-07-26 18:08:32,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1905.0 (TID 3810) in 10 ms on localhost (1/2)
2017-07-26 18:08:32,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1905.0 (TID 3811) in 8 ms on localhost (2/2)
2017-07-26 18:08:32,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1905 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:08:32,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1905.0, whose tasks have all completed, from pool 
2017-07-26 18:08:32,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1905 finished: foreachPartition at streamingProcessTest.scala:48, took 0.031030 s
2017-07-26 18:08:32,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063712000 ms.0 from job set of time 1501063712000 ms
2017-07-26 18:08:32,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501063712000 ms (execution: 0.061 s)
2017-07-26 18:08:32,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1904 from persistence list
2017-07-26 18:08:32,076 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1904
2017-07-26 18:08:32,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:32,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063708000 ms
2017-07-26 18:08:34,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063714000 ms
2017-07-26 18:08:34,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063714000 ms.0 from job set of time 1501063714000 ms
2017-07-26 18:08:34,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:34,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1906 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:34,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1906 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:34,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:34,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:34,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1906 (KafkaRDD[1906] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1906 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:08:34,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1906_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:08:34,036 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1906_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:34,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1906 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:34,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1906 (KafkaRDD[1906] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:34,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1906.0 with 2 tasks
2017-07-26 18:08:34,038 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1906.0 (TID 3812, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:34,039 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1906.0 (TID 3813, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:34,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1906.0 (TID 3812)
2017-07-26 18:08:34,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1906.0 (TID 3813)
2017-07-26 18:08:34,041 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:34,041 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:34,043 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1906.0 (TID 3812). 635 bytes result sent to driver
2017-07-26 18:08:34,043 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1906.0 (TID 3813). 635 bytes result sent to driver
2017-07-26 18:08:34,045 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1906.0 (TID 3813) in 7 ms on localhost (1/2)
2017-07-26 18:08:34,045 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1906.0 (TID 3812) in 8 ms on localhost (2/2)
2017-07-26 18:08:34,045 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1906.0, whose tasks have all completed, from pool 
2017-07-26 18:08:34,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1906 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:08:34,046 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1906 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018417 s
2017-07-26 18:08:34,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063714000 ms.0 from job set of time 1501063714000 ms
2017-07-26 18:08:34,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.046 s for time 1501063714000 ms (execution: 0.033 s)
2017-07-26 18:08:34,046 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1905 from persistence list
2017-07-26 18:08:34,047 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1905
2017-07-26 18:08:34,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:34,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063710000 ms
2017-07-26 18:08:36,028 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063716000 ms
2017-07-26 18:08:36,029 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063716000 ms.0 from job set of time 1501063716000 ms
2017-07-26 18:08:36,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:36,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1907 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:36,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1907 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:36,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:36,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:36,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1907 (KafkaRDD[1907] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:36,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1907 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:08:36,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1907_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:08:36,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1907_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:36,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1907 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:36,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1907 (KafkaRDD[1907] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:36,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1907.0 with 2 tasks
2017-07-26 18:08:36,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1907.0 (TID 3814, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:36,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1907.0 (TID 3815, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:36,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1907.0 (TID 3815)
2017-07-26 18:08:36,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1907.0 (TID 3814)
2017-07-26 18:08:36,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:36,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:36,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1907.0 (TID 3815). 714 bytes result sent to driver
2017-07-26 18:08:36,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1907.0 (TID 3814). 635 bytes result sent to driver
2017-07-26 18:08:36,079 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1907.0 (TID 3815) in 9 ms on localhost (1/2)
2017-07-26 18:08:36,079 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1907.0 (TID 3814) in 10 ms on localhost (2/2)
2017-07-26 18:08:36,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1907.0, whose tasks have all completed, from pool 
2017-07-26 18:08:36,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1907 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:08:36,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1907 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021673 s
2017-07-26 18:08:36,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063716000 ms.0 from job set of time 1501063716000 ms
2017-07-26 18:08:36,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501063716000 ms (execution: 0.052 s)
2017-07-26 18:08:36,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1906 from persistence list
2017-07-26 18:08:36,081 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1906
2017-07-26 18:08:36,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:36,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063712000 ms
2017-07-26 18:08:38,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063718000 ms
2017-07-26 18:08:38,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063718000 ms.0 from job set of time 1501063718000 ms
2017-07-26 18:08:38,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:38,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1908 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:38,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1908 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:38,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:38,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:38,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1908 (KafkaRDD[1908] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:38,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1908 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:08:38,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1908_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:08:38,050 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1908_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:38,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1908 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:38,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1908 (KafkaRDD[1908] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:38,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1908.0 with 2 tasks
2017-07-26 18:08:38,053 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1908.0 (TID 3816, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:38,053 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1908.0 (TID 3817, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:38,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1908.0 (TID 3817)
2017-07-26 18:08:38,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1908.0 (TID 3816)
2017-07-26 18:08:38,056 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:38,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1908.0 (TID 3817). 635 bytes result sent to driver
2017-07-26 18:08:38,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:38,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1908.0 (TID 3817) in 8 ms on localhost (1/2)
2017-07-26 18:08:38,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1908.0 (TID 3816). 714 bytes result sent to driver
2017-07-26 18:08:38,064 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1908.0 (TID 3816) in 13 ms on localhost (2/2)
2017-07-26 18:08:38,064 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1908.0, whose tasks have all completed, from pool 
2017-07-26 18:08:38,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1908 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:08:38,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1908 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024162 s
2017-07-26 18:08:38,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063718000 ms.0 from job set of time 1501063718000 ms
2017-07-26 18:08:38,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1501063718000 ms (execution: 0.049 s)
2017-07-26 18:08:38,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1907 from persistence list
2017-07-26 18:08:38,066 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1907
2017-07-26 18:08:38,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:38,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063714000 ms
2017-07-26 18:08:40,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063720000 ms
2017-07-26 18:08:40,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063720000 ms.0 from job set of time 1501063720000 ms
2017-07-26 18:08:40,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1909 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1909 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:40,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:40,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:40,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1909 (KafkaRDD[1909] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:40,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1909 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:08:40,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1909_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:08:40,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1909_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:40,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1909 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:40,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1909 (KafkaRDD[1909] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:40,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1909.0 with 2 tasks
2017-07-26 18:08:40,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1909.0 (TID 3818, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:40,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1909.0 (TID 3819, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:40,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1909.0 (TID 3819)
2017-07-26 18:08:40,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1909.0 (TID 3818)
2017-07-26 18:08:40,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:40,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:40,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1909.0 (TID 3818). 635 bytes result sent to driver
2017-07-26 18:08:40,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1909.0 (TID 3819). 635 bytes result sent to driver
2017-07-26 18:08:40,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1909.0 (TID 3818) in 13 ms on localhost (1/2)
2017-07-26 18:08:40,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1909.0 (TID 3819) in 11 ms on localhost (2/2)
2017-07-26 18:08:40,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1909.0, whose tasks have all completed, from pool 
2017-07-26 18:08:40,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1909 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:08:40,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1909 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027455 s
2017-07-26 18:08:40,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063720000 ms.0 from job set of time 1501063720000 ms
2017-07-26 18:08:40,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501063720000 ms (execution: 0.059 s)
2017-07-26 18:08:40,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1908 from persistence list
2017-07-26 18:08:40,077 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1908
2017-07-26 18:08:40,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:40,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063716000 ms
2017-07-26 18:08:42,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063722000 ms
2017-07-26 18:08:42,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063722000 ms.0 from job set of time 1501063722000 ms
2017-07-26 18:08:42,054 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:42,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1910 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:42,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1910 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:42,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:42,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:42,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1910 (KafkaRDD[1910] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:42,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1910 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:08:42,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1910_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:08:42,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1910_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:42,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1910 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:42,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1910 (KafkaRDD[1910] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:42,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1910.0 with 2 tasks
2017-07-26 18:08:42,070 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1910.0 (TID 3820, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:42,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1910.0 (TID 3821, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:42,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1910.0 (TID 3821)
2017-07-26 18:08:42,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1910.0 (TID 3820)
2017-07-26 18:08:42,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:42,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:42,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1910.0 (TID 3820). 714 bytes result sent to driver
2017-07-26 18:08:42,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1910.0 (TID 3821). 714 bytes result sent to driver
2017-07-26 18:08:42,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1910.0 (TID 3820) in 12 ms on localhost (1/2)
2017-07-26 18:08:42,081 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1910.0 (TID 3821) in 10 ms on localhost (2/2)
2017-07-26 18:08:42,081 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1910.0, whose tasks have all completed, from pool 
2017-07-26 18:08:42,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1910 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:08:42,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1910 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027828 s
2017-07-26 18:08:42,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063722000 ms.0 from job set of time 1501063722000 ms
2017-07-26 18:08:42,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1501063722000 ms (execution: 0.066 s)
2017-07-26 18:08:42,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1909 from persistence list
2017-07-26 18:08:42,083 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1909
2017-07-26 18:08:42,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:42,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063718000 ms
2017-07-26 18:08:44,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063724000 ms
2017-07-26 18:08:44,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063724000 ms.0 from job set of time 1501063724000 ms
2017-07-26 18:08:44,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:44,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1911 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:44,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1911 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:44,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:44,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:44,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1911 (KafkaRDD[1911] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:44,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1911 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:08:44,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1911_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:08:44,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1911_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:44,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1911 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:44,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1911 (KafkaRDD[1911] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:44,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1911.0 with 2 tasks
2017-07-26 18:08:44,037 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1911.0 (TID 3822, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:44,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1911.0 (TID 3823, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:44,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1911.0 (TID 3823)
2017-07-26 18:08:44,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1911.0 (TID 3822)
2017-07-26 18:08:44,039 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:44,039 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:44,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1911.0 (TID 3822). 635 bytes result sent to driver
2017-07-26 18:08:44,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1911.0 (TID 3823). 635 bytes result sent to driver
2017-07-26 18:08:44,042 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1911.0 (TID 3822) in 5 ms on localhost (1/2)
2017-07-26 18:08:44,042 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1911.0 (TID 3823) in 5 ms on localhost (2/2)
2017-07-26 18:08:44,042 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1911.0, whose tasks have all completed, from pool 
2017-07-26 18:08:44,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1911 (foreachPartition at streamingProcessTest.scala:48) finished in 0.005 s
2017-07-26 18:08:44,042 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1911 finished: foreachPartition at streamingProcessTest.scala:48, took 0.015526 s
2017-07-26 18:08:44,043 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063724000 ms.0 from job set of time 1501063724000 ms
2017-07-26 18:08:44,043 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.043 s for time 1501063724000 ms (execution: 0.029 s)
2017-07-26 18:08:44,043 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1910 from persistence list
2017-07-26 18:08:44,043 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1910
2017-07-26 18:08:44,044 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:44,044 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063720000 ms
2017-07-26 18:08:46,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063726000 ms
2017-07-26 18:08:46,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063726000 ms.0 from job set of time 1501063726000 ms
2017-07-26 18:08:46,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:46,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1912 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:46,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1912 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:46,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:46,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:46,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1912 (KafkaRDD[1912] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:46,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1912 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:08:46,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1912_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:08:46,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1912_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:46,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1912 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:46,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1912 (KafkaRDD[1912] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:46,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1912.0 with 2 tasks
2017-07-26 18:08:46,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1912.0 (TID 3824, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:46,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1912.0 (TID 3825, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:46,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1912.0 (TID 3825)
2017-07-26 18:08:46,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1912.0 (TID 3824)
2017-07-26 18:08:46,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:46,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:46,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1912.0 (TID 3825). 635 bytes result sent to driver
2017-07-26 18:08:46,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1912.0 (TID 3824). 635 bytes result sent to driver
2017-07-26 18:08:46,072 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1912.0 (TID 3824) in 11 ms on localhost (1/2)
2017-07-26 18:08:46,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1912.0 (TID 3825) in 10 ms on localhost (2/2)
2017-07-26 18:08:46,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1912.0, whose tasks have all completed, from pool 
2017-07-26 18:08:46,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1912 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:08:46,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1912 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026183 s
2017-07-26 18:08:46,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063726000 ms.0 from job set of time 1501063726000 ms
2017-07-26 18:08:46,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1501063726000 ms (execution: 0.058 s)
2017-07-26 18:08:46,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1911 from persistence list
2017-07-26 18:08:46,075 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1911
2017-07-26 18:08:46,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:46,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063722000 ms
2017-07-26 18:08:48,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063728000 ms
2017-07-26 18:08:48,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063728000 ms.0 from job set of time 1501063728000 ms
2017-07-26 18:08:48,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:48,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1913 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:48,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1913 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:48,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:48,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:48,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1913 (KafkaRDD[1913] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:48,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1913 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:08:48,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1913_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:08:48,048 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1913_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1913 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1913 (KafkaRDD[1913] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1913.0 with 2 tasks
2017-07-26 18:08:48,050 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1913.0 (TID 3826, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:48,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1913.0 (TID 3827, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:48,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1913.0 (TID 3826)
2017-07-26 18:08:48,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1913.0 (TID 3827)
2017-07-26 18:08:48,054 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:48,054 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:48,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1913.0 (TID 3826). 635 bytes result sent to driver
2017-07-26 18:08:48,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1913.0 (TID 3827). 635 bytes result sent to driver
2017-07-26 18:08:48,058 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1913.0 (TID 3826) in 9 ms on localhost (1/2)
2017-07-26 18:08:48,059 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1913.0 (TID 3827) in 8 ms on localhost (2/2)
2017-07-26 18:08:48,059 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1913.0, whose tasks have all completed, from pool 
2017-07-26 18:08:48,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1913 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:08:48,059 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1913 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020641 s
2017-07-26 18:08:48,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063728000 ms.0 from job set of time 1501063728000 ms
2017-07-26 18:08:48,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.059 s for time 1501063728000 ms (execution: 0.042 s)
2017-07-26 18:08:48,059 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1912 from persistence list
2017-07-26 18:08:48,060 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1912
2017-07-26 18:08:48,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:48,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063724000 ms
2017-07-26 18:08:50,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063730000 ms
2017-07-26 18:08:50,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063730000 ms.0 from job set of time 1501063730000 ms
2017-07-26 18:08:50,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1914 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:50,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1914 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:50,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:50,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:50,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1914 (KafkaRDD[1914] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1914 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:08:50,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1914_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:08:50,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1914_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1914 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1914 (KafkaRDD[1914] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:50,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1914.0 with 2 tasks
2017-07-26 18:08:50,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1914.0 (TID 3828, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:50,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1914.0 (TID 3829, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:50,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1914.0 (TID 3828)
2017-07-26 18:08:50,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1914.0 (TID 3829)
2017-07-26 18:08:50,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:50,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1914.0 (TID 3828). 635 bytes result sent to driver
2017-07-26 18:08:50,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:50,079 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1914.0 (TID 3828) in 9 ms on localhost (1/2)
2017-07-26 18:08:50,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1914.0 (TID 3829). 635 bytes result sent to driver
2017-07-26 18:08:50,082 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1914.0 (TID 3829) in 11 ms on localhost (2/2)
2017-07-26 18:08:50,082 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1914.0, whose tasks have all completed, from pool 
2017-07-26 18:08:50,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1914 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:08:50,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1914 finished: foreachPartition at streamingProcessTest.scala:48, took 0.034388 s
2017-07-26 18:08:50,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063730000 ms.0 from job set of time 1501063730000 ms
2017-07-26 18:08:50,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1913 from persistence list
2017-07-26 18:08:50,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1501063730000 ms (execution: 0.067 s)
2017-07-26 18:08:50,084 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1913
2017-07-26 18:08:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:50,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063726000 ms
2017-07-26 18:08:52,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063732000 ms
2017-07-26 18:08:52,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063732000 ms.0 from job set of time 1501063732000 ms
2017-07-26 18:08:52,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:52,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1915 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:52,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1915 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1915 (KafkaRDD[1915] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:52,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1915 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:08:52,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1915_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:08:52,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1915_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:52,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1915 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:52,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1915 (KafkaRDD[1915] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:52,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1915.0 with 2 tasks
2017-07-26 18:08:52,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1915.0 (TID 3830, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:52,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1915.0 (TID 3831, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:52,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1915.0 (TID 3830)
2017-07-26 18:08:52,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1915.0 (TID 3831)
2017-07-26 18:08:52,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:52,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:52,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1915.0 (TID 3830). 801 bytes result sent to driver
2017-07-26 18:08:52,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1915.0 (TID 3831). 801 bytes result sent to driver
2017-07-26 18:08:52,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1915.0 (TID 3830) in 14 ms on localhost (1/2)
2017-07-26 18:08:52,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1915.0 (TID 3831) in 13 ms on localhost (2/2)
2017-07-26 18:08:52,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1915.0, whose tasks have all completed, from pool 
2017-07-26 18:08:52,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1915 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:08:52,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1915 finished: foreachPartition at streamingProcessTest.scala:48, took 0.029434 s
2017-07-26 18:08:52,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063732000 ms.0 from job set of time 1501063732000 ms
2017-07-26 18:08:52,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1501063732000 ms (execution: 0.062 s)
2017-07-26 18:08:52,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1914 from persistence list
2017-07-26 18:08:52,080 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1914
2017-07-26 18:08:52,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:52,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063728000 ms
2017-07-26 18:08:54,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063734000 ms
2017-07-26 18:08:54,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063734000 ms.0 from job set of time 1501063734000 ms
2017-07-26 18:08:54,024 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:54,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1916 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:54,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1916 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:54,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:54,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:54,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1916 (KafkaRDD[1916] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:54,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1916 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:08:54,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1916_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:08:54,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1916_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:54,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1916 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:54,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1916 (KafkaRDD[1916] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:54,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1916.0 with 2 tasks
2017-07-26 18:08:54,034 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1916.0 (TID 3832, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:54,034 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1916.0 (TID 3833, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:54,034 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1916.0 (TID 3832)
2017-07-26 18:08:54,034 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1916.0 (TID 3833)
2017-07-26 18:08:54,036 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:54,036 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:54,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1916.0 (TID 3833). 635 bytes result sent to driver
2017-07-26 18:08:54,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1916.0 (TID 3832). 635 bytes result sent to driver
2017-07-26 18:08:54,040 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1916.0 (TID 3833) in 5 ms on localhost (1/2)
2017-07-26 18:08:54,040 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1916.0 (TID 3832) in 7 ms on localhost (2/2)
2017-07-26 18:08:54,040 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1916.0, whose tasks have all completed, from pool 
2017-07-26 18:08:54,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1916 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:08:54,040 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1916 finished: foreachPartition at streamingProcessTest.scala:48, took 0.016398 s
2017-07-26 18:08:54,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063734000 ms.0 from job set of time 1501063734000 ms
2017-07-26 18:08:54,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.040 s for time 1501063734000 ms (execution: 0.028 s)
2017-07-26 18:08:54,041 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1915 from persistence list
2017-07-26 18:08:54,041 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1915
2017-07-26 18:08:54,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:54,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063730000 ms
2017-07-26 18:08:56,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063736000 ms
2017-07-26 18:08:56,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063736000 ms.0 from job set of time 1501063736000 ms
2017-07-26 18:08:56,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:56,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1917 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:56,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1917 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:56,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:56,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1917 (KafkaRDD[1917] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:56,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1917 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:08:56,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1917_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:08:56,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1917_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:56,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1917 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:56,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1917 (KafkaRDD[1917] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:56,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1917.0 with 2 tasks
2017-07-26 18:08:56,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1917.0 (TID 3834, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:56,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1917.0 (TID 3835, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:56,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1917.0 (TID 3834)
2017-07-26 18:08:56,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1917.0 (TID 3835)
2017-07-26 18:08:56,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:56,063 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:56,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1917.0 (TID 3835). 714 bytes result sent to driver
2017-07-26 18:08:56,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1917.0 (TID 3834). 714 bytes result sent to driver
2017-07-26 18:08:56,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1917.0 (TID 3835) in 12 ms on localhost (1/2)
2017-07-26 18:08:56,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1917.0 (TID 3834) in 13 ms on localhost (2/2)
2017-07-26 18:08:56,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1917.0, whose tasks have all completed, from pool 
2017-07-26 18:08:56,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1917 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:08:56,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1917 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026974 s
2017-07-26 18:08:56,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063736000 ms.0 from job set of time 1501063736000 ms
2017-07-26 18:08:56,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501063736000 ms (execution: 0.055 s)
2017-07-26 18:08:56,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1916 from persistence list
2017-07-26 18:08:56,071 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1916
2017-07-26 18:08:56,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:56,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063732000 ms
2017-07-26 18:08:58,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063738000 ms
2017-07-26 18:08:58,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063738000 ms.0 from job set of time 1501063738000 ms
2017-07-26 18:08:58,050 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1917_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:58,052 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1904_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:58,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1905_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:08:58,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1906_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:58,058 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:08:58,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1918 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:08:58,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1918 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:08:58,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:08:58,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:08:58,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1918 (KafkaRDD[1918] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:08:58,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1907_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:58,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1908_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:58,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1918 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:08:58,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1909_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:58,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1910_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:58,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1911_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:58,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1918_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:08:58,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1918_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:58,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1918 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:08:58,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1918 (KafkaRDD[1918] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:08:58,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1918.0 with 2 tasks
2017-07-26 18:08:58,070 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1912_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:08:58,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1918.0 (TID 3836, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:08:58,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1918.0 (TID 3837, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:08:58,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1918.0 (TID 3837)
2017-07-26 18:08:58,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1918.0 (TID 3836)
2017-07-26 18:08:58,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1913_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:58,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1914_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:58,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:08:58,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:08:58,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1915_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:58,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1918.0 (TID 3836). 635 bytes result sent to driver
2017-07-26 18:08:58,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1918.0 (TID 3837). 635 bytes result sent to driver
2017-07-26 18:08:58,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1916_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:08:58,079 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1918.0 (TID 3836) in 9 ms on localhost (1/2)
2017-07-26 18:08:58,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1918.0 (TID 3837) in 9 ms on localhost (2/2)
2017-07-26 18:08:58,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1918.0, whose tasks have all completed, from pool 
2017-07-26 18:08:58,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1918 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:08:58,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1918 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022538 s
2017-07-26 18:08:58,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063738000 ms.0 from job set of time 1501063738000 ms
2017-07-26 18:08:58,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501063738000 ms (execution: 0.063 s)
2017-07-26 18:08:58,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1917 from persistence list
2017-07-26 18:08:58,081 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1917
2017-07-26 18:08:58,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:08:58,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063734000 ms
2017-07-26 18:09:00,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063740000 ms
2017-07-26 18:09:00,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063740000 ms.0 from job set of time 1501063740000 ms
2017-07-26 18:09:00,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:00,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1919 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:00,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1919 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:00,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:00,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:00,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1919 (KafkaRDD[1919] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:00,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1919 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:09:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1919_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:09:00,052 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1919_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1919 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1919 (KafkaRDD[1919] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1919.0 with 2 tasks
2017-07-26 18:09:00,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1919.0 (TID 3838, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:00,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1919.0 (TID 3839, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:00,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1919.0 (TID 3838)
2017-07-26 18:09:00,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1919.0 (TID 3839)
2017-07-26 18:09:00,058 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:00,058 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:00,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1919.0 (TID 3839). 635 bytes result sent to driver
2017-07-26 18:09:00,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1919.0 (TID 3838). 635 bytes result sent to driver
2017-07-26 18:09:00,061 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1919.0 (TID 3839) in 6 ms on localhost (1/2)
2017-07-26 18:09:00,062 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1919.0 (TID 3838) in 8 ms on localhost (2/2)
2017-07-26 18:09:00,062 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1919.0, whose tasks have all completed, from pool 
2017-07-26 18:09:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1919 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:09:00,062 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1919 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024481 s
2017-07-26 18:09:00,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063740000 ms.0 from job set of time 1501063740000 ms
2017-07-26 18:09:00,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1501063740000 ms (execution: 0.044 s)
2017-07-26 18:09:00,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1918 from persistence list
2017-07-26 18:09:00,063 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1918
2017-07-26 18:09:00,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:00,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063736000 ms
2017-07-26 18:09:02,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063742000 ms
2017-07-26 18:09:02,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063742000 ms.0 from job set of time 1501063742000 ms
2017-07-26 18:09:02,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:02,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1920 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:02,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1920 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:02,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:02,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:02,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1920 (KafkaRDD[1920] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:02,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1920 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:09:02,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1920_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:09:02,039 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1920_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:02,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1920 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:02,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1920 (KafkaRDD[1920] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:02,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1920.0 with 2 tasks
2017-07-26 18:09:02,042 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1920.0 (TID 3840, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:02,042 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1920.0 (TID 3841, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:02,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1920.0 (TID 3840)
2017-07-26 18:09:02,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1920.0 (TID 3841)
2017-07-26 18:09:02,045 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:02,045 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:02,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1920.0 (TID 3841). 635 bytes result sent to driver
2017-07-26 18:09:02,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1920.0 (TID 3840). 635 bytes result sent to driver
2017-07-26 18:09:02,050 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1920.0 (TID 3840) in 9 ms on localhost (1/2)
2017-07-26 18:09:02,050 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1920.0 (TID 3841) in 8 ms on localhost (2/2)
2017-07-26 18:09:02,050 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1920.0, whose tasks have all completed, from pool 
2017-07-26 18:09:02,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1920 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:09:02,051 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1920 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020859 s
2017-07-26 18:09:02,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063742000 ms.0 from job set of time 1501063742000 ms
2017-07-26 18:09:02,052 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1919 from persistence list
2017-07-26 18:09:02,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.052 s for time 1501063742000 ms (execution: 0.037 s)
2017-07-26 18:09:02,053 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1919
2017-07-26 18:09:02,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:02,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063738000 ms
2017-07-26 18:09:04,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063744000 ms
2017-07-26 18:09:04,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063744000 ms.0 from job set of time 1501063744000 ms
2017-07-26 18:09:04,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:04,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1921 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:04,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1921 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:04,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:04,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:04,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1921 (KafkaRDD[1921] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:04,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1921 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:09:04,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1921_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:09:04,029 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1921_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:04,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1921 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:04,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1921 (KafkaRDD[1921] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:04,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1921.0 with 2 tasks
2017-07-26 18:09:04,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1921.0 (TID 3842, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:04,031 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1921.0 (TID 3843, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:04,031 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1921.0 (TID 3842)
2017-07-26 18:09:04,031 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1921.0 (TID 3843)
2017-07-26 18:09:04,033 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:04,033 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:04,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1921.0 (TID 3843). 635 bytes result sent to driver
2017-07-26 18:09:04,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1921.0 (TID 3842). 635 bytes result sent to driver
2017-07-26 18:09:04,037 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1921.0 (TID 3843) in 6 ms on localhost (1/2)
2017-07-26 18:09:04,037 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1921.0 (TID 3842) in 7 ms on localhost (2/2)
2017-07-26 18:09:04,037 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1921.0, whose tasks have all completed, from pool 
2017-07-26 18:09:04,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1921 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:09:04,038 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1921 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014610 s
2017-07-26 18:09:04,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063744000 ms.0 from job set of time 1501063744000 ms
2017-07-26 18:09:04,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.038 s for time 1501063744000 ms (execution: 0.026 s)
2017-07-26 18:09:04,038 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1920 from persistence list
2017-07-26 18:09:04,039 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1920
2017-07-26 18:09:04,039 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:04,039 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063740000 ms
2017-07-26 18:09:06,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063746000 ms
2017-07-26 18:09:06,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063746000 ms.0 from job set of time 1501063746000 ms
2017-07-26 18:09:06,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:06,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1922 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:06,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1922 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:06,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:06,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:06,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1922 (KafkaRDD[1922] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:06,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1922 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:09:06,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1922_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:09:06,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1922_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:06,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1922 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:06,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1922 (KafkaRDD[1922] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:06,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1922.0 with 2 tasks
2017-07-26 18:09:06,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1922.0 (TID 3844, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:06,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1922.0 (TID 3845, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:06,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1922.0 (TID 3844)
2017-07-26 18:09:06,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1922.0 (TID 3845)
2017-07-26 18:09:06,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:06,081 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:06,086 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1922.0 (TID 3844). 635 bytes result sent to driver
2017-07-26 18:09:06,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1922.0 (TID 3845). 635 bytes result sent to driver
2017-07-26 18:09:06,090 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1922.0 (TID 3845) in 15 ms on localhost (1/2)
2017-07-26 18:09:06,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1922.0 (TID 3844) in 18 ms on localhost (2/2)
2017-07-26 18:09:06,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1922.0, whose tasks have all completed, from pool 
2017-07-26 18:09:06,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1922 (foreachPartition at streamingProcessTest.scala:48) finished in 0.018 s
2017-07-26 18:09:06,091 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1922 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040142 s
2017-07-26 18:09:06,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063746000 ms.0 from job set of time 1501063746000 ms
2017-07-26 18:09:06,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.091 s for time 1501063746000 ms (execution: 0.074 s)
2017-07-26 18:09:06,091 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1921 from persistence list
2017-07-26 18:09:06,092 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1921
2017-07-26 18:09:06,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:06,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063742000 ms
2017-07-26 18:09:08,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063748000 ms
2017-07-26 18:09:08,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063748000 ms.0 from job set of time 1501063748000 ms
2017-07-26 18:09:08,036 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:08,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1923 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:08,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1923 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:08,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:08,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:08,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1923 (KafkaRDD[1923] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:08,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1923 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:09:08,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1923_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:09:08,043 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1923_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1923 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1923 (KafkaRDD[1923] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1923.0 with 2 tasks
2017-07-26 18:09:08,045 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1923.0 (TID 3846, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:08,046 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1923.0 (TID 3847, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:08,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1923.0 (TID 3846)
2017-07-26 18:09:08,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1923.0 (TID 3847)
2017-07-26 18:09:08,048 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:08,048 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:08,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1923.0 (TID 3847). 722 bytes result sent to driver
2017-07-26 18:09:08,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1923.0 (TID 3846). 722 bytes result sent to driver
2017-07-26 18:09:08,052 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1923.0 (TID 3847) in 6 ms on localhost (1/2)
2017-07-26 18:09:08,052 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1923.0 (TID 3846) in 8 ms on localhost (2/2)
2017-07-26 18:09:08,052 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1923.0, whose tasks have all completed, from pool 
2017-07-26 18:09:08,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1923 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:09:08,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1923 finished: foreachPartition at streamingProcessTest.scala:48, took 0.016367 s
2017-07-26 18:09:08,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063748000 ms.0 from job set of time 1501063748000 ms
2017-07-26 18:09:08,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.053 s for time 1501063748000 ms (execution: 0.038 s)
2017-07-26 18:09:08,053 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1922 from persistence list
2017-07-26 18:09:08,053 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1922
2017-07-26 18:09:08,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:08,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063744000 ms
2017-07-26 18:09:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063750000 ms
2017-07-26 18:09:10,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063750000 ms.0 from job set of time 1501063750000 ms
2017-07-26 18:09:10,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1924 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1924 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1924 (KafkaRDD[1924] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1924 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:09:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1924_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:09:10,052 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1924_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:10,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1924 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1924 (KafkaRDD[1924] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1924.0 with 2 tasks
2017-07-26 18:09:10,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1924.0 (TID 3848, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:10,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1924.0 (TID 3849, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:10,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1924.0 (TID 3848)
2017-07-26 18:09:10,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1924.0 (TID 3849)
2017-07-26 18:09:10,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:10,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:10,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1924.0 (TID 3849). 635 bytes result sent to driver
2017-07-26 18:09:10,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1924.0 (TID 3848). 714 bytes result sent to driver
2017-07-26 18:09:10,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1924.0 (TID 3849) in 11 ms on localhost (1/2)
2017-07-26 18:09:10,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1924.0 (TID 3848) in 13 ms on localhost (2/2)
2017-07-26 18:09:10,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1924.0, whose tasks have all completed, from pool 
2017-07-26 18:09:10,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1924 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:09:10,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1924 finished: foreachPartition at streamingProcessTest.scala:48, took 0.030726 s
2017-07-26 18:09:10,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063750000 ms.0 from job set of time 1501063750000 ms
2017-07-26 18:09:10,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501063750000 ms (execution: 0.055 s)
2017-07-26 18:09:10,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1923 from persistence list
2017-07-26 18:09:10,071 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1923
2017-07-26 18:09:10,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:10,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063746000 ms
2017-07-26 18:09:12,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063752000 ms
2017-07-26 18:09:12,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063752000 ms.0 from job set of time 1501063752000 ms
2017-07-26 18:09:12,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:12,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1925 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:12,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1925 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:12,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:12,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:12,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1925 (KafkaRDD[1925] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:12,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1925 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:09:12,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1925_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:09:12,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1925_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:12,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1925 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:12,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1925 (KafkaRDD[1925] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:12,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1925.0 with 2 tasks
2017-07-26 18:09:12,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1925.0 (TID 3850, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:12,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1925.0 (TID 3851, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:12,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1925.0 (TID 3850)
2017-07-26 18:09:12,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1925.0 (TID 3851)
2017-07-26 18:09:12,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:12,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:12,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1925.0 (TID 3851). 714 bytes result sent to driver
2017-07-26 18:09:12,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1925.0 (TID 3850). 714 bytes result sent to driver
2017-07-26 18:09:12,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1925.0 (TID 3851) in 9 ms on localhost (1/2)
2017-07-26 18:09:12,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1925.0 (TID 3850) in 10 ms on localhost (2/2)
2017-07-26 18:09:12,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1925.0, whose tasks have all completed, from pool 
2017-07-26 18:09:12,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1925 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:09:12,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1925 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025555 s
2017-07-26 18:09:12,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063752000 ms.0 from job set of time 1501063752000 ms
2017-07-26 18:09:12,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1501063752000 ms (execution: 0.049 s)
2017-07-26 18:09:12,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1924 from persistence list
2017-07-26 18:09:12,069 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1924
2017-07-26 18:09:12,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:12,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063748000 ms
2017-07-26 18:09:14,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063754000 ms
2017-07-26 18:09:14,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063754000 ms.0 from job set of time 1501063754000 ms
2017-07-26 18:09:14,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:14,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1926 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:14,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1926 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:14,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:14,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:14,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1926 (KafkaRDD[1926] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:14,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1926 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:09:14,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1926_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:09:14,039 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1926_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:14,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1926 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:14,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1926 (KafkaRDD[1926] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:14,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1926.0 with 2 tasks
2017-07-26 18:09:14,040 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1926.0 (TID 3852, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:14,041 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1926.0 (TID 3853, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:14,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1926.0 (TID 3852)
2017-07-26 18:09:14,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1926.0 (TID 3853)
2017-07-26 18:09:14,042 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:14,042 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:14,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1926.0 (TID 3852). 714 bytes result sent to driver
2017-07-26 18:09:14,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1926.0 (TID 3853). 635 bytes result sent to driver
2017-07-26 18:09:14,046 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1926.0 (TID 3853) in 6 ms on localhost (1/2)
2017-07-26 18:09:14,047 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1926.0 (TID 3852) in 6 ms on localhost (2/2)
2017-07-26 18:09:14,047 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1926.0, whose tasks have all completed, from pool 
2017-07-26 18:09:14,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1926 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:09:14,047 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1926 finished: foreachPartition at streamingProcessTest.scala:48, took 0.017794 s
2017-07-26 18:09:14,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063754000 ms.0 from job set of time 1501063754000 ms
2017-07-26 18:09:14,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1501063754000 ms (execution: 0.035 s)
2017-07-26 18:09:14,048 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1925 from persistence list
2017-07-26 18:09:14,049 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1925
2017-07-26 18:09:14,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:14,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063750000 ms
2017-07-26 18:09:16,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063756000 ms
2017-07-26 18:09:16,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063756000 ms.0 from job set of time 1501063756000 ms
2017-07-26 18:09:16,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1927 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1927 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1927 (KafkaRDD[1927] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:16,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1927 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:09:16,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1927_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:09:16,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1927_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:16,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1927 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:16,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1927 (KafkaRDD[1927] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:16,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1927.0 with 2 tasks
2017-07-26 18:09:16,058 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1927.0 (TID 3854, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:16,058 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1927.0 (TID 3855, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:16,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1927.0 (TID 3854)
2017-07-26 18:09:16,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1927.0 (TID 3855)
2017-07-26 18:09:16,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:16,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:16,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1927.0 (TID 3855). 635 bytes result sent to driver
2017-07-26 18:09:16,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1927.0 (TID 3854). 714 bytes result sent to driver
2017-07-26 18:09:16,065 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1927.0 (TID 3855) in 7 ms on localhost (1/2)
2017-07-26 18:09:16,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1927.0 (TID 3854) in 8 ms on localhost (2/2)
2017-07-26 18:09:16,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1927.0, whose tasks have all completed, from pool 
2017-07-26 18:09:16,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1927 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:09:16,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1927 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018491 s
2017-07-26 18:09:16,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063756000 ms.0 from job set of time 1501063756000 ms
2017-07-26 18:09:16,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501063756000 ms (execution: 0.050 s)
2017-07-26 18:09:16,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1926 from persistence list
2017-07-26 18:09:16,067 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1926
2017-07-26 18:09:16,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:16,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063752000 ms
2017-07-26 18:09:18,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063758000 ms
2017-07-26 18:09:18,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063758000 ms.0 from job set of time 1501063758000 ms
2017-07-26 18:09:18,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:18,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1928 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:18,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1928 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:18,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:18,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:18,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1928 (KafkaRDD[1928] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:18,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1928 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:09:18,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1928_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:09:18,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1928_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:18,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1928 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:18,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1928 (KafkaRDD[1928] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:18,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1928.0 with 2 tasks
2017-07-26 18:09:18,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1928.0 (TID 3856, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:18,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1928.0 (TID 3857, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:18,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1928.0 (TID 3857)
2017-07-26 18:09:18,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1928.0 (TID 3856)
2017-07-26 18:09:18,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:18,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:18,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1928.0 (TID 3857). 635 bytes result sent to driver
2017-07-26 18:09:18,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1928.0 (TID 3856). 635 bytes result sent to driver
2017-07-26 18:09:18,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1928.0 (TID 3857) in 7 ms on localhost (1/2)
2017-07-26 18:09:18,070 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1928.0 (TID 3856) in 9 ms on localhost (2/2)
2017-07-26 18:09:18,070 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1928.0, whose tasks have all completed, from pool 
2017-07-26 18:09:18,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1928 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:09:18,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1928 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026922 s
2017-07-26 18:09:18,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063758000 ms.0 from job set of time 1501063758000 ms
2017-07-26 18:09:18,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1501063758000 ms (execution: 0.054 s)
2017-07-26 18:09:18,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1927 from persistence list
2017-07-26 18:09:18,072 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1927
2017-07-26 18:09:18,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:18,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063754000 ms
2017-07-26 18:09:20,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063760000 ms
2017-07-26 18:09:20,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063760000 ms.0 from job set of time 1501063760000 ms
2017-07-26 18:09:20,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1929 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1929 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:20,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1929 (KafkaRDD[1929] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:20,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1929 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:09:20,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1929_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:09:20,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1929_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:20,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1929 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:20,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1929 (KafkaRDD[1929] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:20,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1929.0 with 2 tasks
2017-07-26 18:09:20,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1929.0 (TID 3858, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:20,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1929.0 (TID 3859, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:20,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1929.0 (TID 3859)
2017-07-26 18:09:20,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1929.0 (TID 3858)
2017-07-26 18:09:20,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:20,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:20,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1929.0 (TID 3859). 635 bytes result sent to driver
2017-07-26 18:09:20,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1929.0 (TID 3858). 635 bytes result sent to driver
2017-07-26 18:09:20,079 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1929.0 (TID 3859) in 10 ms on localhost (1/2)
2017-07-26 18:09:20,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1929.0 (TID 3858) in 11 ms on localhost (2/2)
2017-07-26 18:09:20,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1929.0, whose tasks have all completed, from pool 
2017-07-26 18:09:20,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1929 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:09:20,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1929 finished: foreachPartition at streamingProcessTest.scala:48, took 0.032522 s
2017-07-26 18:09:20,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063760000 ms.0 from job set of time 1501063760000 ms
2017-07-26 18:09:20,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501063760000 ms (execution: 0.064 s)
2017-07-26 18:09:20,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1928 from persistence list
2017-07-26 18:09:20,081 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1928
2017-07-26 18:09:20,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:20,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063756000 ms
2017-07-26 18:09:22,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063762000 ms
2017-07-26 18:09:22,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063762000 ms.0 from job set of time 1501063762000 ms
2017-07-26 18:09:22,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1930 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1930 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:22,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:22,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:22,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1930 (KafkaRDD[1930] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:22,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1930 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:09:22,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1930_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:09:22,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1930_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:22,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1930 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:22,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1930 (KafkaRDD[1930] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:22,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1930.0 with 2 tasks
2017-07-26 18:09:22,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1930.0 (TID 3860, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:22,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1930.0 (TID 3861, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:22,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1930.0 (TID 3861)
2017-07-26 18:09:22,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1930.0 (TID 3860)
2017-07-26 18:09:22,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:22,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:22,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1930.0 (TID 3861). 635 bytes result sent to driver
2017-07-26 18:09:22,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1930.0 (TID 3860). 714 bytes result sent to driver
2017-07-26 18:09:22,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1930.0 (TID 3860) in 9 ms on localhost (1/2)
2017-07-26 18:09:22,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1930.0 (TID 3861) in 9 ms on localhost (2/2)
2017-07-26 18:09:22,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1930.0, whose tasks have all completed, from pool 
2017-07-26 18:09:22,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1930 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:09:22,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1930 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019843 s
2017-07-26 18:09:22,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063762000 ms.0 from job set of time 1501063762000 ms
2017-07-26 18:09:22,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1501063762000 ms (execution: 0.050 s)
2017-07-26 18:09:22,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1929 from persistence list
2017-07-26 18:09:22,069 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1929
2017-07-26 18:09:22,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:22,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063758000 ms
2017-07-26 18:09:24,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063764000 ms
2017-07-26 18:09:24,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063764000 ms.0 from job set of time 1501063764000 ms
2017-07-26 18:09:24,028 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:24,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1931 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:24,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1931 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:24,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:24,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:24,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1931 (KafkaRDD[1931] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:24,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1931 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:09:24,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1931_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:09:24,036 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1931_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:24,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1931 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:24,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1931 (KafkaRDD[1931] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:24,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1931.0 with 2 tasks
2017-07-26 18:09:24,037 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1931.0 (TID 3862, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:24,038 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1931.0 (TID 3863, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:24,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1931.0 (TID 3862)
2017-07-26 18:09:24,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1931.0 (TID 3863)
2017-07-26 18:09:24,042 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:24,042 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:24,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1931.0 (TID 3862). 787 bytes result sent to driver
2017-07-26 18:09:24,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1931.0 (TID 3863). 787 bytes result sent to driver
2017-07-26 18:09:24,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1918_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:24,052 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1931.0 (TID 3862) in 15 ms on localhost (1/2)
2017-07-26 18:09:24,053 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1931.0 (TID 3863) in 15 ms on localhost (2/2)
2017-07-26 18:09:24,053 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1931.0, whose tasks have all completed, from pool 
2017-07-26 18:09:24,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1931 (foreachPartition at streamingProcessTest.scala:48) finished in 0.016 s
2017-07-26 18:09:24,053 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1931 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024924 s
2017-07-26 18:09:24,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063764000 ms.0 from job set of time 1501063764000 ms
2017-07-26 18:09:24,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1919_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:24,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.053 s for time 1501063764000 ms (execution: 0.036 s)
2017-07-26 18:09:24,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1930 from persistence list
2017-07-26 18:09:24,054 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1930
2017-07-26 18:09:24,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:24,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063760000 ms
2017-07-26 18:09:24,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1920_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:24,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1921_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:24,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1922_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:24,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1923_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:24,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1924_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:24,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1925_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:24,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1926_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:24,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1927_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:24,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1928_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:24,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1929_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:24,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1930_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:26,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063766000 ms
2017-07-26 18:09:26,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063766000 ms.0 from job set of time 1501063766000 ms
2017-07-26 18:09:26,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:26,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1932 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:26,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1932 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:26,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:26,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:26,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1932 (KafkaRDD[1932] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:26,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1932 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:09:26,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1932_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:09:26,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1932_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:26,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1932 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:26,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1932 (KafkaRDD[1932] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:26,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1932.0 with 2 tasks
2017-07-26 18:09:26,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1932.0 (TID 3864, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:26,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1932.0 (TID 3865, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:26,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1932.0 (TID 3865)
2017-07-26 18:09:26,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1932.0 (TID 3864)
2017-07-26 18:09:26,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:26,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:26,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1932.0 (TID 3865). 635 bytes result sent to driver
2017-07-26 18:09:26,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1932.0 (TID 3864). 635 bytes result sent to driver
2017-07-26 18:09:26,071 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1932.0 (TID 3865) in 11 ms on localhost (1/2)
2017-07-26 18:09:26,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1932.0 (TID 3864) in 12 ms on localhost (2/2)
2017-07-26 18:09:26,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1932.0, whose tasks have all completed, from pool 
2017-07-26 18:09:26,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1932 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:09:26,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1932 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027659 s
2017-07-26 18:09:26,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063766000 ms.0 from job set of time 1501063766000 ms
2017-07-26 18:09:26,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1501063766000 ms (execution: 0.055 s)
2017-07-26 18:09:26,073 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1931 from persistence list
2017-07-26 18:09:26,074 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1931
2017-07-26 18:09:26,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:26,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063762000 ms
2017-07-26 18:09:28,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063768000 ms
2017-07-26 18:09:28,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063768000 ms.0 from job set of time 1501063768000 ms
2017-07-26 18:09:28,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:28,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1933 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:28,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1933 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:28,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:28,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1933 (KafkaRDD[1933] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:28,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1933 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:09:28,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1933_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:09:28,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1933_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:28,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1933 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:28,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1933 (KafkaRDD[1933] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:28,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1933.0 with 2 tasks
2017-07-26 18:09:28,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1933.0 (TID 3866, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:28,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1933.0 (TID 3867, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:28,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1933.0 (TID 3867)
2017-07-26 18:09:28,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1933.0 (TID 3866)
2017-07-26 18:09:28,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:28,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:28,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1933.0 (TID 3867). 635 bytes result sent to driver
2017-07-26 18:09:28,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1933.0 (TID 3866). 714 bytes result sent to driver
2017-07-26 18:09:28,081 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1933.0 (TID 3867) in 13 ms on localhost (1/2)
2017-07-26 18:09:28,081 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1933.0 (TID 3866) in 15 ms on localhost (2/2)
2017-07-26 18:09:28,081 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1933.0, whose tasks have all completed, from pool 
2017-07-26 18:09:28,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1933 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:09:28,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1933 finished: foreachPartition at streamingProcessTest.scala:48, took 0.036219 s
2017-07-26 18:09:28,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063768000 ms.0 from job set of time 1501063768000 ms
2017-07-26 18:09:28,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1501063768000 ms (execution: 0.064 s)
2017-07-26 18:09:28,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1932 from persistence list
2017-07-26 18:09:28,083 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1932
2017-07-26 18:09:28,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:28,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063764000 ms
2017-07-26 18:09:30,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063770000 ms
2017-07-26 18:09:30,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063770000 ms.0 from job set of time 1501063770000 ms
2017-07-26 18:09:30,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1934 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1934 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:30,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1934 (KafkaRDD[1934] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:30,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1934 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:09:30,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1934_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:09:30,046 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1934_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:30,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1934 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:30,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1934 (KafkaRDD[1934] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:30,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1934.0 with 2 tasks
2017-07-26 18:09:30,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1934.0 (TID 3868, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:30,050 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1934.0 (TID 3869, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:30,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1934.0 (TID 3868)
2017-07-26 18:09:30,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1934.0 (TID 3869)
2017-07-26 18:09:30,054 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:30,055 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:30,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1934.0 (TID 3868). 714 bytes result sent to driver
2017-07-26 18:09:30,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1934.0 (TID 3869). 714 bytes result sent to driver
2017-07-26 18:09:30,059 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1934.0 (TID 3868) in 11 ms on localhost (1/2)
2017-07-26 18:09:30,060 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1934.0 (TID 3869) in 10 ms on localhost (2/2)
2017-07-26 18:09:30,060 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1934.0, whose tasks have all completed, from pool 
2017-07-26 18:09:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1934 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:09:30,060 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1934 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026673 s
2017-07-26 18:09:30,060 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063770000 ms.0 from job set of time 1501063770000 ms
2017-07-26 18:09:30,061 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.060 s for time 1501063770000 ms (execution: 0.046 s)
2017-07-26 18:09:30,061 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1933 from persistence list
2017-07-26 18:09:30,061 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1933
2017-07-26 18:09:30,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:30,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063766000 ms
2017-07-26 18:09:32,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063772000 ms
2017-07-26 18:09:32,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063772000 ms.0 from job set of time 1501063772000 ms
2017-07-26 18:09:32,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:32,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1935 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:32,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1935 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:32,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:32,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:32,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1935 (KafkaRDD[1935] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:32,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1935 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:09:32,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1935_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:09:32,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1935_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:32,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1935 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:32,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1935 (KafkaRDD[1935] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:32,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1935.0 with 2 tasks
2017-07-26 18:09:32,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1935.0 (TID 3870, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:32,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1935.0 (TID 3871, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:32,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1935.0 (TID 3871)
2017-07-26 18:09:32,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1935.0 (TID 3870)
2017-07-26 18:09:32,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:32,066 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:32,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1935.0 (TID 3870). 635 bytes result sent to driver
2017-07-26 18:09:32,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1935.0 (TID 3871). 635 bytes result sent to driver
2017-07-26 18:09:32,070 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1935.0 (TID 3871) in 9 ms on localhost (1/2)
2017-07-26 18:09:32,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1935.0 (TID 3870) in 10 ms on localhost (2/2)
2017-07-26 18:09:32,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1935.0, whose tasks have all completed, from pool 
2017-07-26 18:09:32,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1935 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:09:32,071 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1935 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025422 s
2017-07-26 18:09:32,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063772000 ms.0 from job set of time 1501063772000 ms
2017-07-26 18:09:32,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1501063772000 ms (execution: 0.055 s)
2017-07-26 18:09:32,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1934 from persistence list
2017-07-26 18:09:32,072 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1934
2017-07-26 18:09:32,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:32,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063768000 ms
2017-07-26 18:09:34,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063774000 ms
2017-07-26 18:09:34,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063774000 ms.0 from job set of time 1501063774000 ms
2017-07-26 18:09:34,022 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:34,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1936 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:34,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1936 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:34,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:34,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:34,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1936 (KafkaRDD[1936] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:34,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1936 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:09:34,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1936_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:09:34,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1936_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:34,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1936 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:34,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1936 (KafkaRDD[1936] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:34,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1936.0 with 2 tasks
2017-07-26 18:09:34,032 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1936.0 (TID 3872, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:34,033 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1936.0 (TID 3873, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:34,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1936.0 (TID 3872)
2017-07-26 18:09:34,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1936.0 (TID 3873)
2017-07-26 18:09:34,036 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:34,036 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:34,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1936.0 (TID 3872). 714 bytes result sent to driver
2017-07-26 18:09:34,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1936.0 (TID 3873). 714 bytes result sent to driver
2017-07-26 18:09:34,039 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1936.0 (TID 3873) in 6 ms on localhost (1/2)
2017-07-26 18:09:34,040 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1936.0 (TID 3872) in 8 ms on localhost (2/2)
2017-07-26 18:09:34,040 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1936.0, whose tasks have all completed, from pool 
2017-07-26 18:09:34,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1936 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:09:34,040 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1936 finished: foreachPartition at streamingProcessTest.scala:48, took 0.017248 s
2017-07-26 18:09:34,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063774000 ms.0 from job set of time 1501063774000 ms
2017-07-26 18:09:34,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.040 s for time 1501063774000 ms (execution: 0.028 s)
2017-07-26 18:09:34,040 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1935 from persistence list
2017-07-26 18:09:34,041 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1935
2017-07-26 18:09:34,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:34,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063770000 ms
2017-07-26 18:09:36,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063776000 ms
2017-07-26 18:09:36,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063776000 ms.0 from job set of time 1501063776000 ms
2017-07-26 18:09:36,035 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:36,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1937 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:36,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1937 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:36,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:36,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:36,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1937 (KafkaRDD[1937] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:36,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1937 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:09:36,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1937_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:09:36,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1937_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:36,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1937 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:36,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1937 (KafkaRDD[1937] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:36,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1937.0 with 2 tasks
2017-07-26 18:09:36,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1937.0 (TID 3874, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:36,047 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1937.0 (TID 3875, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:36,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1937.0 (TID 3875)
2017-07-26 18:09:36,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1937.0 (TID 3874)
2017-07-26 18:09:36,049 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:36,049 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:36,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1937.0 (TID 3875). 714 bytes result sent to driver
2017-07-26 18:09:36,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1937.0 (TID 3874). 714 bytes result sent to driver
2017-07-26 18:09:36,054 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1937.0 (TID 3875) in 7 ms on localhost (1/2)
2017-07-26 18:09:36,054 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1937.0 (TID 3874) in 9 ms on localhost (2/2)
2017-07-26 18:09:36,054 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1937.0, whose tasks have all completed, from pool 
2017-07-26 18:09:36,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1937 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:09:36,054 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1937 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019316 s
2017-07-26 18:09:36,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063776000 ms.0 from job set of time 1501063776000 ms
2017-07-26 18:09:36,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.055 s for time 1501063776000 ms (execution: 0.040 s)
2017-07-26 18:09:36,055 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1936 from persistence list
2017-07-26 18:09:36,055 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1936
2017-07-26 18:09:36,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:36,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063772000 ms
2017-07-26 18:09:38,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063778000 ms
2017-07-26 18:09:38,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063778000 ms.0 from job set of time 1501063778000 ms
2017-07-26 18:09:38,036 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:38,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1938 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:38,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1938 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:38,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:38,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:38,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1938 (KafkaRDD[1938] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1938 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:09:38,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1938_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:09:38,049 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1938_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:38,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1938 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:38,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1938 (KafkaRDD[1938] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:38,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1938.0 with 2 tasks
2017-07-26 18:09:38,052 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1938.0 (TID 3876, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:38,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1938.0 (TID 3877, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:38,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1938.0 (TID 3877)
2017-07-26 18:09:38,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1938.0 (TID 3876)
2017-07-26 18:09:38,058 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:38,058 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:38,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1938.0 (TID 3877). 714 bytes result sent to driver
2017-07-26 18:09:38,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1938.0 (TID 3876). 714 bytes result sent to driver
2017-07-26 18:09:38,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1938.0 (TID 3876) in 14 ms on localhost (1/2)
2017-07-26 18:09:38,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1938.0 (TID 3877) in 12 ms on localhost (2/2)
2017-07-26 18:09:38,066 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1938.0, whose tasks have all completed, from pool 
2017-07-26 18:09:38,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1938 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:09:38,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1938 finished: foreachPartition at streamingProcessTest.scala:48, took 0.030134 s
2017-07-26 18:09:38,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063778000 ms.0 from job set of time 1501063778000 ms
2017-07-26 18:09:38,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1501063778000 ms (execution: 0.052 s)
2017-07-26 18:09:38,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1937 from persistence list
2017-07-26 18:09:38,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:38,067 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1937
2017-07-26 18:09:38,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063774000 ms
2017-07-26 18:09:40,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063780000 ms
2017-07-26 18:09:40,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063780000 ms.0 from job set of time 1501063780000 ms
2017-07-26 18:09:40,031 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:40,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1939 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:40,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1939 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:40,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:40,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:40,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1939 (KafkaRDD[1939] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:40,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1939 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:09:40,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1939_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:09:40,047 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1939_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1939 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1939 (KafkaRDD[1939] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1939.0 with 2 tasks
2017-07-26 18:09:40,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1939.0 (TID 3878, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:40,052 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1939.0 (TID 3879, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:40,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1939.0 (TID 3879)
2017-07-26 18:09:40,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1939.0 (TID 3878)
2017-07-26 18:09:40,054 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:40,055 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:40,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1939.0 (TID 3879). 714 bytes result sent to driver
2017-07-26 18:09:40,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1939.0 (TID 3878). 635 bytes result sent to driver
2017-07-26 18:09:40,058 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1939.0 (TID 3879) in 6 ms on localhost (1/2)
2017-07-26 18:09:40,059 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1939.0 (TID 3878) in 10 ms on localhost (2/2)
2017-07-26 18:09:40,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1939 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:09:40,059 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1939.0, whose tasks have all completed, from pool 
2017-07-26 18:09:40,060 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1939 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028267 s
2017-07-26 18:09:40,060 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063780000 ms.0 from job set of time 1501063780000 ms
2017-07-26 18:09:40,060 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.060 s for time 1501063780000 ms (execution: 0.042 s)
2017-07-26 18:09:40,060 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1938 from persistence list
2017-07-26 18:09:40,061 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1938
2017-07-26 18:09:40,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:40,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063776000 ms
2017-07-26 18:09:42,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063782000 ms
2017-07-26 18:09:42,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063782000 ms.0 from job set of time 1501063782000 ms
2017-07-26 18:09:42,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:42,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1940 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:42,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1940 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:42,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:42,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:42,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1940 (KafkaRDD[1940] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:42,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1940 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:09:42,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1940_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:09:42,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1940_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:42,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1940 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:42,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1940 (KafkaRDD[1940] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:42,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1940.0 with 2 tasks
2017-07-26 18:09:42,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1940.0 (TID 3880, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:42,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1940.0 (TID 3881, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:42,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1940.0 (TID 3881)
2017-07-26 18:09:42,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1940.0 (TID 3880)
2017-07-26 18:09:42,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:42,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:42,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1940.0 (TID 3881). 714 bytes result sent to driver
2017-07-26 18:09:42,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1940.0 (TID 3880). 714 bytes result sent to driver
2017-07-26 18:09:42,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1940.0 (TID 3881) in 9 ms on localhost (1/2)
2017-07-26 18:09:42,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1940.0 (TID 3880) in 10 ms on localhost (2/2)
2017-07-26 18:09:42,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1940.0, whose tasks have all completed, from pool 
2017-07-26 18:09:42,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1940 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:09:42,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1940 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024310 s
2017-07-26 18:09:42,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063782000 ms.0 from job set of time 1501063782000 ms
2017-07-26 18:09:42,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501063782000 ms (execution: 0.058 s)
2017-07-26 18:09:42,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1939 from persistence list
2017-07-26 18:09:42,076 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1939
2017-07-26 18:09:42,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:42,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063778000 ms
2017-07-26 18:09:44,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063784000 ms
2017-07-26 18:09:44,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063784000 ms.0 from job set of time 1501063784000 ms
2017-07-26 18:09:44,032 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:44,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1941 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:44,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1941 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:44,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:44,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:44,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1941 (KafkaRDD[1941] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:44,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1941 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:09:44,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1941_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:09:44,042 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1941_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:44,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1941 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:44,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1941 (KafkaRDD[1941] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:44,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1941.0 with 2 tasks
2017-07-26 18:09:44,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1941.0 (TID 3882, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:44,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1941.0 (TID 3883, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:44,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1941.0 (TID 3883)
2017-07-26 18:09:44,045 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1941.0 (TID 3882)
2017-07-26 18:09:44,047 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:44,047 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:44,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1941.0 (TID 3882). 635 bytes result sent to driver
2017-07-26 18:09:44,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1941.0 (TID 3883). 714 bytes result sent to driver
2017-07-26 18:09:44,052 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1941.0 (TID 3882) in 9 ms on localhost (1/2)
2017-07-26 18:09:44,052 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1941.0 (TID 3883) in 8 ms on localhost (2/2)
2017-07-26 18:09:44,053 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1941.0, whose tasks have all completed, from pool 
2017-07-26 18:09:44,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1941 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:09:44,053 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1941 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020622 s
2017-07-26 18:09:44,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063784000 ms.0 from job set of time 1501063784000 ms
2017-07-26 18:09:44,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1940 from persistence list
2017-07-26 18:09:44,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.053 s for time 1501063784000 ms (execution: 0.035 s)
2017-07-26 18:09:44,054 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1940
2017-07-26 18:09:44,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:44,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063780000 ms
2017-07-26 18:09:46,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063786000 ms
2017-07-26 18:09:46,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063786000 ms.0 from job set of time 1501063786000 ms
2017-07-26 18:09:46,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1942 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1942 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1942 (KafkaRDD[1942] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:46,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1942 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:09:46,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1942_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:09:46,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1942_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:46,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1942 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:46,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1942 (KafkaRDD[1942] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:46,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1942.0 with 2 tasks
2017-07-26 18:09:46,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1942.0 (TID 3884, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:46,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1942.0 (TID 3885, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:46,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1942.0 (TID 3884)
2017-07-26 18:09:46,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1942.0 (TID 3885)
2017-07-26 18:09:46,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:46,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:46,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1942.0 (TID 3885). 635 bytes result sent to driver
2017-07-26 18:09:46,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1942.0 (TID 3884). 635 bytes result sent to driver
2017-07-26 18:09:46,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1942.0 (TID 3885) in 10 ms on localhost (1/2)
2017-07-26 18:09:46,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1942.0 (TID 3884) in 11 ms on localhost (2/2)
2017-07-26 18:09:46,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1942.0, whose tasks have all completed, from pool 
2017-07-26 18:09:46,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1942 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:09:46,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1942 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025091 s
2017-07-26 18:09:46,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063786000 ms.0 from job set of time 1501063786000 ms
2017-07-26 18:09:46,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1941 from persistence list
2017-07-26 18:09:46,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501063786000 ms (execution: 0.056 s)
2017-07-26 18:09:46,076 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1941
2017-07-26 18:09:46,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:46,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063782000 ms
2017-07-26 18:09:48,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063788000 ms
2017-07-26 18:09:48,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063788000 ms.0 from job set of time 1501063788000 ms
2017-07-26 18:09:48,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1943 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1943 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:48,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:48,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1943 (KafkaRDD[1943] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:48,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1943 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:09:48,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1943_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:09:48,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1943_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:48,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1943 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:48,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1943 (KafkaRDD[1943] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:48,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1943.0 with 2 tasks
2017-07-26 18:09:48,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1943.0 (TID 3886, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:48,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1943.0 (TID 3887, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:48,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1943.0 (TID 3887)
2017-07-26 18:09:48,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1943.0 (TID 3886)
2017-07-26 18:09:48,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:48,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:48,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1943.0 (TID 3887). 714 bytes result sent to driver
2017-07-26 18:09:48,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1943.0 (TID 3886). 714 bytes result sent to driver
2017-07-26 18:09:48,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1943.0 (TID 3887) in 8 ms on localhost (1/2)
2017-07-26 18:09:48,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1943.0 (TID 3886) in 9 ms on localhost (2/2)
2017-07-26 18:09:48,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1943.0, whose tasks have all completed, from pool 
2017-07-26 18:09:48,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1943 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:09:48,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1943 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026957 s
2017-07-26 18:09:48,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063788000 ms.0 from job set of time 1501063788000 ms
2017-07-26 18:09:48,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501063788000 ms (execution: 0.058 s)
2017-07-26 18:09:48,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1942 from persistence list
2017-07-26 18:09:48,076 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1942
2017-07-26 18:09:48,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:48,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063784000 ms
2017-07-26 18:09:50,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063790000 ms
2017-07-26 18:09:50,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063790000 ms.0 from job set of time 1501063790000 ms
2017-07-26 18:09:50,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1944 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1944 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1944 (KafkaRDD[1944] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1944 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:09:50,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1944_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:09:50,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1944_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:50,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1944 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1944 (KafkaRDD[1944] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:50,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1944.0 with 2 tasks
2017-07-26 18:09:50,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1944.0 (TID 3888, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:50,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1944.0 (TID 3889, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:50,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1944.0 (TID 3888)
2017-07-26 18:09:50,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1944.0 (TID 3889)
2017-07-26 18:09:50,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:50,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:50,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1944.0 (TID 3888). 635 bytes result sent to driver
2017-07-26 18:09:50,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1944.0 (TID 3889). 714 bytes result sent to driver
2017-07-26 18:09:50,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1944.0 (TID 3889) in 10 ms on localhost (1/2)
2017-07-26 18:09:50,079 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1944.0 (TID 3888) in 11 ms on localhost (2/2)
2017-07-26 18:09:50,079 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1944.0, whose tasks have all completed, from pool 
2017-07-26 18:09:50,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1944 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:09:50,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1944 finished: foreachPartition at streamingProcessTest.scala:48, took 0.032699 s
2017-07-26 18:09:50,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063790000 ms.0 from job set of time 1501063790000 ms
2017-07-26 18:09:50,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501063790000 ms (execution: 0.064 s)
2017-07-26 18:09:50,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1943 from persistence list
2017-07-26 18:09:50,082 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1943
2017-07-26 18:09:50,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:50,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063786000 ms
2017-07-26 18:09:52,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063792000 ms
2017-07-26 18:09:52,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063792000 ms.0 from job set of time 1501063792000 ms
2017-07-26 18:09:52,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:52,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1945 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1945 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:52,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1945 (KafkaRDD[1945] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:52,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1945 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:09:52,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1945_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.3 MB)
2017-07-26 18:09:52,080 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1945_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:52,080 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1931_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:52,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1945 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:52,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1945 (KafkaRDD[1945] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:52,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1945.0 with 2 tasks
2017-07-26 18:09:52,083 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1945.0 (TID 3890, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:52,083 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1932_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:52,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1945.0 (TID 3891, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:52,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1945.0 (TID 3890)
2017-07-26 18:09:52,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1945.0 (TID 3891)
2017-07-26 18:09:52,085 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1933_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:52,087 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:52,087 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:52,088 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1934_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:09:52,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1945.0 (TID 3890). 635 bytes result sent to driver
2017-07-26 18:09:52,090 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1945.0 (TID 3891). 714 bytes result sent to driver
2017-07-26 18:09:52,090 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1935_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:52,092 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1945.0 (TID 3890) in 10 ms on localhost (1/2)
2017-07-26 18:09:52,092 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1945.0 (TID 3891) in 9 ms on localhost (2/2)
2017-07-26 18:09:52,092 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1936_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:52,092 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1945.0, whose tasks have all completed, from pool 
2017-07-26 18:09:52,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1945 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:09:52,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1945 finished: foreachPartition at streamingProcessTest.scala:48, took 0.046325 s
2017-07-26 18:09:52,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063792000 ms.0 from job set of time 1501063792000 ms
2017-07-26 18:09:52,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.093 s for time 1501063792000 ms (execution: 0.077 s)
2017-07-26 18:09:52,094 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1944 from persistence list
2017-07-26 18:09:52,094 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1944
2017-07-26 18:09:52,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:52,094 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1937_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:52,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063788000 ms
2017-07-26 18:09:52,096 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1938_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:52,099 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1939_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:52,101 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1940_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:09:52,103 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1941_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:52,105 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1942_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:52,107 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1943_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:52,108 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1944_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:54,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063794000 ms
2017-07-26 18:09:54,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063794000 ms.0 from job set of time 1501063794000 ms
2017-07-26 18:09:54,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:54,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1946 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:54,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1946 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:54,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:54,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:54,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1946 (KafkaRDD[1946] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:54,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1946 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:09:54,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1946_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:09:54,035 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1946_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:54,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1946 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:54,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1946 (KafkaRDD[1946] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:54,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1946.0 with 2 tasks
2017-07-26 18:09:54,038 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1946.0 (TID 3892, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:54,038 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1946.0 (TID 3893, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:54,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1946.0 (TID 3893)
2017-07-26 18:09:54,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1946.0 (TID 3892)
2017-07-26 18:09:54,041 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:54,041 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:54,043 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1946.0 (TID 3892). 635 bytes result sent to driver
2017-07-26 18:09:54,043 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1946.0 (TID 3893). 635 bytes result sent to driver
2017-07-26 18:09:54,045 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1946.0 (TID 3892) in 8 ms on localhost (1/2)
2017-07-26 18:09:54,046 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1946.0 (TID 3893) in 7 ms on localhost (2/2)
2017-07-26 18:09:54,046 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1946.0, whose tasks have all completed, from pool 
2017-07-26 18:09:54,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1946 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:09:54,046 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1946 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019561 s
2017-07-26 18:09:54,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063794000 ms.0 from job set of time 1501063794000 ms
2017-07-26 18:09:54,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.047 s for time 1501063794000 ms (execution: 0.034 s)
2017-07-26 18:09:54,047 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1945 from persistence list
2017-07-26 18:09:54,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:54,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063790000 ms
2017-07-26 18:09:54,048 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1945
2017-07-26 18:09:56,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063796000 ms
2017-07-26 18:09:56,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063796000 ms.0 from job set of time 1501063796000 ms
2017-07-26 18:09:56,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:56,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1947 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1947 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1947 (KafkaRDD[1947] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:56,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1947 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:09:56,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1947_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:09:56,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1947_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:56,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1947 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:56,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1947 (KafkaRDD[1947] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:56,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1947.0 with 2 tasks
2017-07-26 18:09:56,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1947.0 (TID 3894, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:56,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1947.0 (TID 3895, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:56,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1947.0 (TID 3894)
2017-07-26 18:09:56,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1947.0 (TID 3895)
2017-07-26 18:09:56,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:56,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:56,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1947.0 (TID 3895). 714 bytes result sent to driver
2017-07-26 18:09:56,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1947.0 (TID 3894). 714 bytes result sent to driver
2017-07-26 18:09:56,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1947.0 (TID 3895) in 10 ms on localhost (1/2)
2017-07-26 18:09:56,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1947.0 (TID 3894) in 13 ms on localhost (2/2)
2017-07-26 18:09:56,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1947 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:09:56,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1947.0, whose tasks have all completed, from pool 
2017-07-26 18:09:56,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1947 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025699 s
2017-07-26 18:09:56,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063796000 ms.0 from job set of time 1501063796000 ms
2017-07-26 18:09:56,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501063796000 ms (execution: 0.053 s)
2017-07-26 18:09:56,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1946 from persistence list
2017-07-26 18:09:56,069 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1946
2017-07-26 18:09:56,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:56,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063792000 ms
2017-07-26 18:09:58,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063798000 ms
2017-07-26 18:09:58,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063798000 ms.0 from job set of time 1501063798000 ms
2017-07-26 18:09:58,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:09:58,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1948 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:09:58,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1948 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:09:58,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:09:58,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:09:58,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1948 (KafkaRDD[1948] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:09:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1948 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:09:58,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1948_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:09:58,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1948_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:09:58,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1948 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:09:58,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1948 (KafkaRDD[1948] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:09:58,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1948.0 with 2 tasks
2017-07-26 18:09:58,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1948.0 (TID 3896, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:09:58,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1948.0 (TID 3897, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:09:58,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1948.0 (TID 3896)
2017-07-26 18:09:58,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1948.0 (TID 3897)
2017-07-26 18:09:58,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:09:58,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:09:58,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1948.0 (TID 3896). 635 bytes result sent to driver
2017-07-26 18:09:58,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1948.0 (TID 3897). 714 bytes result sent to driver
2017-07-26 18:09:58,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1948.0 (TID 3896) in 9 ms on localhost (1/2)
2017-07-26 18:09:58,064 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1948.0 (TID 3897) in 8 ms on localhost (2/2)
2017-07-26 18:09:58,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1948.0, whose tasks have all completed, from pool 
2017-07-26 18:09:58,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1948 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:09:58,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1948 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021513 s
2017-07-26 18:09:58,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063798000 ms.0 from job set of time 1501063798000 ms
2017-07-26 18:09:58,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1501063798000 ms (execution: 0.049 s)
2017-07-26 18:09:58,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1947 from persistence list
2017-07-26 18:09:58,066 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1947
2017-07-26 18:09:58,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:09:58,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063794000 ms
2017-07-26 18:10:00,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063800000 ms
2017-07-26 18:10:00,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063800000 ms.0 from job set of time 1501063800000 ms
2017-07-26 18:10:00,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1949 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1949 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1949 (KafkaRDD[1949] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1949 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:10:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1949_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:10:00,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1949_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1949 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:00,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1949 (KafkaRDD[1949] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:00,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1949.0 with 2 tasks
2017-07-26 18:10:00,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1949.0 (TID 3898, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:00,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1949.0 (TID 3899, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:00,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1949.0 (TID 3898)
2017-07-26 18:10:00,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1949.0 (TID 3899)
2017-07-26 18:10:00,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:00,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:00,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1949.0 (TID 3899). 635 bytes result sent to driver
2017-07-26 18:10:00,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1949.0 (TID 3898). 635 bytes result sent to driver
2017-07-26 18:10:00,086 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1949.0 (TID 3899) in 16 ms on localhost (1/2)
2017-07-26 18:10:00,086 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1949.0 (TID 3898) in 18 ms on localhost (2/2)
2017-07-26 18:10:00,087 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1949.0, whose tasks have all completed, from pool 
2017-07-26 18:10:00,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1949 (foreachPartition at streamingProcessTest.scala:48) finished in 0.019 s
2017-07-26 18:10:00,087 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1949 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040568 s
2017-07-26 18:10:00,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063800000 ms.0 from job set of time 1501063800000 ms
2017-07-26 18:10:00,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1501063800000 ms (execution: 0.072 s)
2017-07-26 18:10:00,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1948 from persistence list
2017-07-26 18:10:00,088 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1948
2017-07-26 18:10:00,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:00,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063796000 ms
2017-07-26 18:10:02,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063802000 ms
2017-07-26 18:10:02,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063802000 ms.0 from job set of time 1501063802000 ms
2017-07-26 18:10:02,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:02,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1950 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:02,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1950 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:02,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1950 (KafkaRDD[1950] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:02,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1950 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:10:02,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1950_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:10:02,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1950_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:02,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1950 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:02,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1950 (KafkaRDD[1950] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:02,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1950.0 with 2 tasks
2017-07-26 18:10:02,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1950.0 (TID 3900, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:02,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1950.0 (TID 3901, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:02,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1950.0 (TID 3901)
2017-07-26 18:10:02,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1950.0 (TID 3900)
2017-07-26 18:10:02,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:02,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:02,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1950.0 (TID 3901). 714 bytes result sent to driver
2017-07-26 18:10:02,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1950.0 (TID 3900). 714 bytes result sent to driver
2017-07-26 18:10:02,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1950.0 (TID 3900) in 12 ms on localhost (1/2)
2017-07-26 18:10:02,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1950.0 (TID 3901) in 12 ms on localhost (2/2)
2017-07-26 18:10:02,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1950.0, whose tasks have all completed, from pool 
2017-07-26 18:10:02,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1950 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:10:02,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1950 finished: foreachPartition at streamingProcessTest.scala:48, took 0.034163 s
2017-07-26 18:10:02,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063802000 ms.0 from job set of time 1501063802000 ms
2017-07-26 18:10:02,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1949 from persistence list
2017-07-26 18:10:02,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1501063802000 ms (execution: 0.067 s)
2017-07-26 18:10:02,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:02,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063798000 ms
2017-07-26 18:10:02,084 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1949
2017-07-26 18:10:04,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063804000 ms
2017-07-26 18:10:04,011 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063804000 ms.0 from job set of time 1501063804000 ms
2017-07-26 18:10:04,022 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:04,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1951 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:04,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1951 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:04,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:04,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:04,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1951 (KafkaRDD[1951] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:04,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1951 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:10:04,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1951_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:10:04,028 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1951_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:04,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1951 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:04,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1951 (KafkaRDD[1951] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:04,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1951.0 with 2 tasks
2017-07-26 18:10:04,030 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1951.0 (TID 3902, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:04,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1951.0 (TID 3903, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:04,031 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1951.0 (TID 3902)
2017-07-26 18:10:04,031 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1951.0 (TID 3903)
2017-07-26 18:10:04,033 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:04,033 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:04,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1951.0 (TID 3903). 635 bytes result sent to driver
2017-07-26 18:10:04,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1951.0 (TID 3902). 635 bytes result sent to driver
2017-07-26 18:10:04,036 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1951.0 (TID 3903) in 5 ms on localhost (1/2)
2017-07-26 18:10:04,037 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1951.0 (TID 3902) in 7 ms on localhost (2/2)
2017-07-26 18:10:04,037 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1951.0, whose tasks have all completed, from pool 
2017-07-26 18:10:04,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1951 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:10:04,037 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1951 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014520 s
2017-07-26 18:10:04,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063804000 ms.0 from job set of time 1501063804000 ms
2017-07-26 18:10:04,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.037 s for time 1501063804000 ms (execution: 0.026 s)
2017-07-26 18:10:04,037 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1950 from persistence list
2017-07-26 18:10:04,038 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1950
2017-07-26 18:10:04,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:04,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063800000 ms
2017-07-26 18:10:06,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063806000 ms
2017-07-26 18:10:06,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063806000 ms.0 from job set of time 1501063806000 ms
2017-07-26 18:10:06,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:06,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1952 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:06,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1952 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:06,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:06,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:06,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1952 (KafkaRDD[1952] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:06,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1952 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:10:06,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1952_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:10:06,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1952_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:06,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1952 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:06,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1952 (KafkaRDD[1952] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:06,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1952.0 with 2 tasks
2017-07-26 18:10:06,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1952.0 (TID 3904, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:06,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1952.0 (TID 3905, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:06,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1952.0 (TID 3905)
2017-07-26 18:10:06,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1952.0 (TID 3904)
2017-07-26 18:10:06,055 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:06,055 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:06,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1952.0 (TID 3905). 714 bytes result sent to driver
2017-07-26 18:10:06,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1952.0 (TID 3904). 714 bytes result sent to driver
2017-07-26 18:10:06,061 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1952.0 (TID 3905) in 8 ms on localhost (1/2)
2017-07-26 18:10:06,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1952.0 (TID 3904) in 10 ms on localhost (2/2)
2017-07-26 18:10:06,062 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1952.0, whose tasks have all completed, from pool 
2017-07-26 18:10:06,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1952 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:10:06,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1952 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023691 s
2017-07-26 18:10:06,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063806000 ms.0 from job set of time 1501063806000 ms
2017-07-26 18:10:06,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1501063806000 ms (execution: 0.044 s)
2017-07-26 18:10:06,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1951 from persistence list
2017-07-26 18:10:06,064 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1951
2017-07-26 18:10:06,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:06,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063802000 ms
2017-07-26 18:10:08,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063808000 ms
2017-07-26 18:10:08,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063808000 ms.0 from job set of time 1501063808000 ms
2017-07-26 18:10:08,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1953 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1953 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:08,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1953 (KafkaRDD[1953] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:08,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1953 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:10:08,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1953_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:10:08,053 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1953_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:08,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1953 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:08,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1953 (KafkaRDD[1953] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:08,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1953.0 with 2 tasks
2017-07-26 18:10:08,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1953.0 (TID 3906, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:08,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1953.0 (TID 3907, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:08,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1953.0 (TID 3906)
2017-07-26 18:10:08,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1953.0 (TID 3907)
2017-07-26 18:10:08,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:08,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:08,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1953.0 (TID 3907). 714 bytes result sent to driver
2017-07-26 18:10:08,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1953.0 (TID 3906). 714 bytes result sent to driver
2017-07-26 18:10:08,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1953.0 (TID 3907) in 8 ms on localhost (1/2)
2017-07-26 18:10:08,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1953.0 (TID 3906) in 9 ms on localhost (2/2)
2017-07-26 18:10:08,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1953.0, whose tasks have all completed, from pool 
2017-07-26 18:10:08,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1953 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:10:08,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1953 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020284 s
2017-07-26 18:10:08,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063808000 ms.0 from job set of time 1501063808000 ms
2017-07-26 18:10:08,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1501063808000 ms (execution: 0.045 s)
2017-07-26 18:10:08,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1952 from persistence list
2017-07-26 18:10:08,065 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1952
2017-07-26 18:10:08,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:08,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063804000 ms
2017-07-26 18:10:10,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063810000 ms
2017-07-26 18:10:10,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063810000 ms.0 from job set of time 1501063810000 ms
2017-07-26 18:10:10,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:10,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1954 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:10,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1954 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:10,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:10,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:10,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1954 (KafkaRDD[1954] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:10,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1954 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:10:10,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1954_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:10:10,032 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1954_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:10,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1954 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:10,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1954 (KafkaRDD[1954] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:10,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1954.0 with 2 tasks
2017-07-26 18:10:10,034 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1954.0 (TID 3908, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:10,035 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1954.0 (TID 3909, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:10,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1954.0 (TID 3909)
2017-07-26 18:10:10,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1954.0 (TID 3908)
2017-07-26 18:10:10,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:10,037 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:10,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1954.0 (TID 3909). 635 bytes result sent to driver
2017-07-26 18:10:10,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1954.0 (TID 3908). 635 bytes result sent to driver
2017-07-26 18:10:10,040 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1954.0 (TID 3908) in 7 ms on localhost (1/2)
2017-07-26 18:10:10,040 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1954.0 (TID 3909) in 6 ms on localhost (2/2)
2017-07-26 18:10:10,040 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1954.0, whose tasks have all completed, from pool 
2017-07-26 18:10:10,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1954 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:10:10,041 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1954 finished: foreachPartition at streamingProcessTest.scala:48, took 0.017263 s
2017-07-26 18:10:10,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063810000 ms.0 from job set of time 1501063810000 ms
2017-07-26 18:10:10,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1501063810000 ms (execution: 0.027 s)
2017-07-26 18:10:10,041 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1953 from persistence list
2017-07-26 18:10:10,042 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1953
2017-07-26 18:10:10,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:10,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063806000 ms
2017-07-26 18:10:12,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063812000 ms
2017-07-26 18:10:12,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063812000 ms.0 from job set of time 1501063812000 ms
2017-07-26 18:10:12,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:12,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1955 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:12,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1955 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:12,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:12,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:12,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1955 (KafkaRDD[1955] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:12,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1955 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:10:12,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1955_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:10:12,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1955_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:12,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1955 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:12,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1955 (KafkaRDD[1955] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:12,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1955.0 with 2 tasks
2017-07-26 18:10:12,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1955.0 (TID 3910, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:12,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1955.0 (TID 3911, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:12,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1955.0 (TID 3910)
2017-07-26 18:10:12,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1955.0 (TID 3911)
2017-07-26 18:10:12,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:12,040 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:12,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1955.0 (TID 3910). 635 bytes result sent to driver
2017-07-26 18:10:12,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1955.0 (TID 3911). 635 bytes result sent to driver
2017-07-26 18:10:12,043 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1955.0 (TID 3910) in 6 ms on localhost (1/2)
2017-07-26 18:10:12,043 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1955.0 (TID 3911) in 5 ms on localhost (2/2)
2017-07-26 18:10:12,043 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1955.0, whose tasks have all completed, from pool 
2017-07-26 18:10:12,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1955 (foreachPartition at streamingProcessTest.scala:48) finished in 0.006 s
2017-07-26 18:10:12,043 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1955 finished: foreachPartition at streamingProcessTest.scala:48, took 0.017821 s
2017-07-26 18:10:12,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063812000 ms.0 from job set of time 1501063812000 ms
2017-07-26 18:10:12,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.044 s for time 1501063812000 ms (execution: 0.029 s)
2017-07-26 18:10:12,045 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1954 from persistence list
2017-07-26 18:10:12,046 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1954
2017-07-26 18:10:12,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:12,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063808000 ms
2017-07-26 18:10:14,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063814000 ms
2017-07-26 18:10:14,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063814000 ms.0 from job set of time 1501063814000 ms
2017-07-26 18:10:14,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:14,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1956 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:14,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1956 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:14,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:14,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:14,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1956 (KafkaRDD[1956] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:14,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1956 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:10:14,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1956_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:10:14,041 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1956_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:14,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1956 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:14,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1956 (KafkaRDD[1956] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:14,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1956.0 with 2 tasks
2017-07-26 18:10:14,043 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1956.0 (TID 3912, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:14,043 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1956.0 (TID 3913, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:14,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1956.0 (TID 3912)
2017-07-26 18:10:14,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1956.0 (TID 3913)
2017-07-26 18:10:14,046 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:14,046 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:14,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1956.0 (TID 3913). 635 bytes result sent to driver
2017-07-26 18:10:14,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1956.0 (TID 3912). 635 bytes result sent to driver
2017-07-26 18:10:14,050 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1956.0 (TID 3912) in 8 ms on localhost (1/2)
2017-07-26 18:10:14,050 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1956.0 (TID 3913) in 7 ms on localhost (2/2)
2017-07-26 18:10:14,050 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1956.0, whose tasks have all completed, from pool 
2017-07-26 18:10:14,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1956 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:10:14,051 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1956 finished: foreachPartition at streamingProcessTest.scala:48, took 0.017398 s
2017-07-26 18:10:14,051 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063814000 ms.0 from job set of time 1501063814000 ms
2017-07-26 18:10:14,051 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.051 s for time 1501063814000 ms (execution: 0.036 s)
2017-07-26 18:10:14,051 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1955 from persistence list
2017-07-26 18:10:14,052 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1955
2017-07-26 18:10:14,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:14,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063810000 ms
2017-07-26 18:10:16,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063816000 ms
2017-07-26 18:10:16,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063816000 ms.0 from job set of time 1501063816000 ms
2017-07-26 18:10:16,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:16,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1957 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:16,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1957 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:16,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:16,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:16,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1957 (KafkaRDD[1957] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:16,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1957 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:10:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1957_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:10:16,049 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1957_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1957 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1957 (KafkaRDD[1957] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:16,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1957.0 with 2 tasks
2017-07-26 18:10:16,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1957.0 (TID 3914, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:16,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1957.0 (TID 3915, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:16,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1957.0 (TID 3915)
2017-07-26 18:10:16,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1957.0 (TID 3914)
2017-07-26 18:10:16,053 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:16,053 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:16,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1957.0 (TID 3914). 714 bytes result sent to driver
2017-07-26 18:10:16,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1957.0 (TID 3915). 714 bytes result sent to driver
2017-07-26 18:10:16,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1957.0 (TID 3914) in 7 ms on localhost (1/2)
2017-07-26 18:10:16,058 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1957.0 (TID 3915) in 6 ms on localhost (2/2)
2017-07-26 18:10:16,058 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1957.0, whose tasks have all completed, from pool 
2017-07-26 18:10:16,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1957 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:10:16,058 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1957 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020238 s
2017-07-26 18:10:16,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063816000 ms.0 from job set of time 1501063816000 ms
2017-07-26 18:10:16,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.058 s for time 1501063816000 ms (execution: 0.043 s)
2017-07-26 18:10:16,059 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1956 from persistence list
2017-07-26 18:10:16,060 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1956
2017-07-26 18:10:16,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:16,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063812000 ms
2017-07-26 18:10:18,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063818000 ms
2017-07-26 18:10:18,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063818000 ms.0 from job set of time 1501063818000 ms
2017-07-26 18:10:18,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:18,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1958 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:18,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1958 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:18,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:18,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:18,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1958 (KafkaRDD[1958] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:18,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1958 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:10:18,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1958_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:10:18,045 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1958_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:18,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1958 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:18,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1958 (KafkaRDD[1958] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:18,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1958.0 with 2 tasks
2017-07-26 18:10:18,048 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1958.0 (TID 3916, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:18,049 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1958.0 (TID 3917, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:18,049 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1958.0 (TID 3917)
2017-07-26 18:10:18,049 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1958.0 (TID 3916)
2017-07-26 18:10:18,052 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:18,052 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:18,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1958.0 (TID 3917). 635 bytes result sent to driver
2017-07-26 18:10:18,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1958.0 (TID 3916). 635 bytes result sent to driver
2017-07-26 18:10:18,057 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1958.0 (TID 3917) in 9 ms on localhost (1/2)
2017-07-26 18:10:18,058 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1958.0 (TID 3916) in 11 ms on localhost (2/2)
2017-07-26 18:10:18,058 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1958.0, whose tasks have all completed, from pool 
2017-07-26 18:10:18,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1958 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:10:18,060 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1958 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026219 s
2017-07-26 18:10:18,060 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063818000 ms.0 from job set of time 1501063818000 ms
2017-07-26 18:10:18,060 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.060 s for time 1501063818000 ms (execution: 0.047 s)
2017-07-26 18:10:18,061 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1957 from persistence list
2017-07-26 18:10:18,061 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1957
2017-07-26 18:10:18,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:18,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063814000 ms
2017-07-26 18:10:20,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063820000 ms
2017-07-26 18:10:20,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063820000 ms.0 from job set of time 1501063820000 ms
2017-07-26 18:10:20,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1959 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1959 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1959 (KafkaRDD[1959] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:20,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1958_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:20,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1959 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:10:20,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1945_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:20,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1946_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:20,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1959_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:10:20,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1959_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:20,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1959 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:20,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1959 (KafkaRDD[1959] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:20,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1959.0 with 2 tasks
2017-07-26 18:10:20,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1947_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:20,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1959.0 (TID 3918, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:20,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1959.0 (TID 3919, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:20,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1959.0 (TID 3918)
2017-07-26 18:10:20,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1948_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:20,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1959.0 (TID 3919)
2017-07-26 18:10:20,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1949_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:20,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:20,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:20,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1950_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:20,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1959.0 (TID 3919). 635 bytes result sent to driver
2017-07-26 18:10:20,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1959.0 (TID 3918). 635 bytes result sent to driver
2017-07-26 18:10:20,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1951_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:20,077 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1959.0 (TID 3919) in 8 ms on localhost (1/2)
2017-07-26 18:10:20,077 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1959.0 (TID 3918) in 8 ms on localhost (2/2)
2017-07-26 18:10:20,077 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1959.0, whose tasks have all completed, from pool 
2017-07-26 18:10:20,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1959 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:10:20,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1952_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:20,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1959 finished: foreachPartition at streamingProcessTest.scala:48, took 0.032273 s
2017-07-26 18:10:20,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063820000 ms.0 from job set of time 1501063820000 ms
2017-07-26 18:10:20,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1501063820000 ms (execution: 0.060 s)
2017-07-26 18:10:20,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1958 from persistence list
2017-07-26 18:10:20,079 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1958
2017-07-26 18:10:20,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:20,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063816000 ms
2017-07-26 18:10:20,079 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1953_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:20,081 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1954_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:20,082 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1955_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:20,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1956_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:20,085 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1957_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:22,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063822000 ms
2017-07-26 18:10:22,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063822000 ms.0 from job set of time 1501063822000 ms
2017-07-26 18:10:22,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:22,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1960 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:22,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1960 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:22,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:22,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:22,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1960 (KafkaRDD[1960] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:22,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1960 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:10:22,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1960_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:10:22,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1960_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:22,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1960 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:22,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1960 (KafkaRDD[1960] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:22,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1960.0 with 2 tasks
2017-07-26 18:10:22,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1960.0 (TID 3920, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:22,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1960.0 (TID 3921, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:22,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1960.0 (TID 3921)
2017-07-26 18:10:22,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1960.0 (TID 3920)
2017-07-26 18:10:22,058 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:22,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:22,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1960.0 (TID 3921). 714 bytes result sent to driver
2017-07-26 18:10:22,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1960.0 (TID 3920). 635 bytes result sent to driver
2017-07-26 18:10:22,064 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1960.0 (TID 3921) in 9 ms on localhost (1/2)
2017-07-26 18:10:22,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1960.0 (TID 3920) in 10 ms on localhost (2/2)
2017-07-26 18:10:22,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1960.0, whose tasks have all completed, from pool 
2017-07-26 18:10:22,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1960 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:10:22,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1960 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027784 s
2017-07-26 18:10:22,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063822000 ms.0 from job set of time 1501063822000 ms
2017-07-26 18:10:22,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501063822000 ms (execution: 0.048 s)
2017-07-26 18:10:22,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1959 from persistence list
2017-07-26 18:10:22,067 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1959
2017-07-26 18:10:22,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:22,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063818000 ms
2017-07-26 18:10:24,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063824000 ms
2017-07-26 18:10:24,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063824000 ms.0 from job set of time 1501063824000 ms
2017-07-26 18:10:24,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:24,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1961 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:24,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1961 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:24,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:24,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:24,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1961 (KafkaRDD[1961] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:24,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1961 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:10:24,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1961_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:10:24,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1961_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:24,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1961 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:24,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1961 (KafkaRDD[1961] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:24,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1961.0 with 2 tasks
2017-07-26 18:10:24,037 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1961.0 (TID 3922, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:24,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1961.0 (TID 3923, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:24,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1961.0 (TID 3923)
2017-07-26 18:10:24,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1961.0 (TID 3922)
2017-07-26 18:10:24,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:24,040 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:24,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1961.0 (TID 3922). 635 bytes result sent to driver
2017-07-26 18:10:24,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1961.0 (TID 3923). 714 bytes result sent to driver
2017-07-26 18:10:24,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1961.0 (TID 3923) in 6 ms on localhost (1/2)
2017-07-26 18:10:24,045 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1961.0 (TID 3922) in 7 ms on localhost (2/2)
2017-07-26 18:10:24,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1961 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:10:24,045 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1961.0, whose tasks have all completed, from pool 
2017-07-26 18:10:24,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1961 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018836 s
2017-07-26 18:10:24,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063824000 ms.0 from job set of time 1501063824000 ms
2017-07-26 18:10:24,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.046 s for time 1501063824000 ms (execution: 0.034 s)
2017-07-26 18:10:24,046 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1960 from persistence list
2017-07-26 18:10:24,046 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1960
2017-07-26 18:10:24,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:24,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063820000 ms
2017-07-26 18:10:26,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063826000 ms
2017-07-26 18:10:26,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063826000 ms.0 from job set of time 1501063826000 ms
2017-07-26 18:10:26,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:26,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1962 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:26,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1962 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:26,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:26,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:26,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1962 (KafkaRDD[1962] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:26,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1962 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:10:26,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1962_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:10:26,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1962_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:26,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1962 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:26,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1962 (KafkaRDD[1962] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:26,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1962.0 with 2 tasks
2017-07-26 18:10:26,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1962.0 (TID 3924, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:26,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1962.0 (TID 3925, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:26,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1962.0 (TID 3924)
2017-07-26 18:10:26,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1962.0 (TID 3925)
2017-07-26 18:10:26,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:26,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:26,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1962.0 (TID 3925). 722 bytes result sent to driver
2017-07-26 18:10:26,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1962.0 (TID 3924). 801 bytes result sent to driver
2017-07-26 18:10:26,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1962.0 (TID 3924) in 13 ms on localhost (1/2)
2017-07-26 18:10:26,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1962.0 (TID 3925) in 12 ms on localhost (2/2)
2017-07-26 18:10:26,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1962.0, whose tasks have all completed, from pool 
2017-07-26 18:10:26,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1962 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:10:26,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1962 finished: foreachPartition at streamingProcessTest.scala:48, took 0.029174 s
2017-07-26 18:10:26,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063826000 ms.0 from job set of time 1501063826000 ms
2017-07-26 18:10:26,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501063826000 ms (execution: 0.054 s)
2017-07-26 18:10:26,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1961 from persistence list
2017-07-26 18:10:26,070 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1961
2017-07-26 18:10:26,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:26,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063822000 ms
2017-07-26 18:10:28,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063828000 ms
2017-07-26 18:10:28,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063828000 ms.0 from job set of time 1501063828000 ms
2017-07-26 18:10:28,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:28,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1963 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:28,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1963 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:28,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:28,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:28,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1963 (KafkaRDD[1963] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:28,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1963 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:10:28,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1963_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:10:28,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1963_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:28,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1963 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:28,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1963 (KafkaRDD[1963] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:28,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1963.0 with 2 tasks
2017-07-26 18:10:28,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1963.0 (TID 3926, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:28,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1963.0 (TID 3927, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:28,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1963.0 (TID 3927)
2017-07-26 18:10:28,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1963.0 (TID 3926)
2017-07-26 18:10:28,055 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:28,055 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:28,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1963.0 (TID 3927). 635 bytes result sent to driver
2017-07-26 18:10:28,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1963.0 (TID 3926). 635 bytes result sent to driver
2017-07-26 18:10:28,059 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1963.0 (TID 3927) in 6 ms on localhost (1/2)
2017-07-26 18:10:28,060 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1963.0 (TID 3926) in 8 ms on localhost (2/2)
2017-07-26 18:10:28,060 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1963.0, whose tasks have all completed, from pool 
2017-07-26 18:10:28,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1963 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:10:28,060 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1963 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021179 s
2017-07-26 18:10:28,061 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063828000 ms.0 from job set of time 1501063828000 ms
2017-07-26 18:10:28,061 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.061 s for time 1501063828000 ms (execution: 0.045 s)
2017-07-26 18:10:28,061 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1962 from persistence list
2017-07-26 18:10:28,061 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1962
2017-07-26 18:10:28,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:28,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063824000 ms
2017-07-26 18:10:30,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063830000 ms
2017-07-26 18:10:30,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063830000 ms.0 from job set of time 1501063830000 ms
2017-07-26 18:10:30,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:30,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1964 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:30,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1964 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:30,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1964 (KafkaRDD[1964] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1964 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:10:30,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1964_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:10:30,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1964_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:30,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1964 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1964 (KafkaRDD[1964] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1964.0 with 2 tasks
2017-07-26 18:10:30,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1964.0 (TID 3928, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:30,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1964.0 (TID 3929, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:30,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1964.0 (TID 3929)
2017-07-26 18:10:30,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1964.0 (TID 3928)
2017-07-26 18:10:30,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:30,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:30,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1964.0 (TID 3928). 635 bytes result sent to driver
2017-07-26 18:10:30,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1964.0 (TID 3929). 635 bytes result sent to driver
2017-07-26 18:10:30,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1964.0 (TID 3928) in 8 ms on localhost (1/2)
2017-07-26 18:10:30,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1964.0 (TID 3929) in 8 ms on localhost (2/2)
2017-07-26 18:10:30,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1964.0, whose tasks have all completed, from pool 
2017-07-26 18:10:30,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1964 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:10:30,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1964 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025489 s
2017-07-26 18:10:30,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063830000 ms.0 from job set of time 1501063830000 ms
2017-07-26 18:10:30,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501063830000 ms (execution: 0.054 s)
2017-07-26 18:10:30,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1963 from persistence list
2017-07-26 18:10:30,071 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1963
2017-07-26 18:10:30,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:30,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063826000 ms
2017-07-26 18:10:32,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063832000 ms
2017-07-26 18:10:32,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063832000 ms.0 from job set of time 1501063832000 ms
2017-07-26 18:10:32,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:32,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1965 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:32,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1965 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:32,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:32,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:32,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1965 (KafkaRDD[1965] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:32,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1965 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:10:32,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1965_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:10:32,070 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1965_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:32,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1965 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:32,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1965 (KafkaRDD[1965] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:32,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1965.0 with 2 tasks
2017-07-26 18:10:32,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1965.0 (TID 3930, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:32,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1965.0 (TID 3931, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:32,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1965.0 (TID 3930)
2017-07-26 18:10:32,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1965.0 (TID 3931)
2017-07-26 18:10:32,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:32,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:32,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1965.0 (TID 3931). 635 bytes result sent to driver
2017-07-26 18:10:32,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1965.0 (TID 3930). 714 bytes result sent to driver
2017-07-26 18:10:32,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1965.0 (TID 3931) in 9 ms on localhost (1/2)
2017-07-26 18:10:32,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1965.0 (TID 3930) in 10 ms on localhost (2/2)
2017-07-26 18:10:32,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1965.0, whose tasks have all completed, from pool 
2017-07-26 18:10:32,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1965 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:10:32,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1965 finished: foreachPartition at streamingProcessTest.scala:48, took 0.029684 s
2017-07-26 18:10:32,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063832000 ms.0 from job set of time 1501063832000 ms
2017-07-26 18:10:32,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1501063832000 ms (execution: 0.068 s)
2017-07-26 18:10:32,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1964 from persistence list
2017-07-26 18:10:32,085 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1964
2017-07-26 18:10:32,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:32,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063828000 ms
2017-07-26 18:10:34,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063834000 ms
2017-07-26 18:10:34,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063834000 ms.0 from job set of time 1501063834000 ms
2017-07-26 18:10:34,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1966 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1966 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1966 (KafkaRDD[1966] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:34,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1966 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:10:34,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1966_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:10:34,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1966_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:34,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1966 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:34,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1966 (KafkaRDD[1966] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:34,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1966.0 with 2 tasks
2017-07-26 18:10:34,041 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1966.0 (TID 3932, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:34,041 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1966.0 (TID 3933, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:34,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1966.0 (TID 3932)
2017-07-26 18:10:34,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1966.0 (TID 3933)
2017-07-26 18:10:34,044 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:34,045 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:34,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1966.0 (TID 3932). 714 bytes result sent to driver
2017-07-26 18:10:34,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1966.0 (TID 3933). 635 bytes result sent to driver
2017-07-26 18:10:34,048 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1966.0 (TID 3932) in 8 ms on localhost (1/2)
2017-07-26 18:10:34,049 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1966.0 (TID 3933) in 8 ms on localhost (2/2)
2017-07-26 18:10:34,049 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1966.0, whose tasks have all completed, from pool 
2017-07-26 18:10:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1966 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:10:34,050 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1966 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019792 s
2017-07-26 18:10:34,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063834000 ms.0 from job set of time 1501063834000 ms
2017-07-26 18:10:34,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.050 s for time 1501063834000 ms (execution: 0.035 s)
2017-07-26 18:10:34,050 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1965 from persistence list
2017-07-26 18:10:34,051 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1965
2017-07-26 18:10:34,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:34,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063830000 ms
2017-07-26 18:10:36,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063836000 ms
2017-07-26 18:10:36,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063836000 ms.0 from job set of time 1501063836000 ms
2017-07-26 18:10:36,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:36,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1967 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:36,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1967 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:36,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:36,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:36,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1967 (KafkaRDD[1967] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:36,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1967 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:10:36,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1967_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:10:36,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1967_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:36,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1967 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:36,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1967 (KafkaRDD[1967] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:36,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1967.0 with 2 tasks
2017-07-26 18:10:36,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1967.0 (TID 3934, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:36,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1967.0 (TID 3935, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:36,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1967.0 (TID 3935)
2017-07-26 18:10:36,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1967.0 (TID 3934)
2017-07-26 18:10:36,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:36,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:36,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1967.0 (TID 3935). 635 bytes result sent to driver
2017-07-26 18:10:36,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1967.0 (TID 3934). 635 bytes result sent to driver
2017-07-26 18:10:36,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1967.0 (TID 3935) in 10 ms on localhost (1/2)
2017-07-26 18:10:36,076 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1967.0 (TID 3934) in 12 ms on localhost (2/2)
2017-07-26 18:10:36,076 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1967.0, whose tasks have all completed, from pool 
2017-07-26 18:10:36,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1967 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:10:36,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1967 finished: foreachPartition at streamingProcessTest.scala:48, took 0.029380 s
2017-07-26 18:10:36,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063836000 ms.0 from job set of time 1501063836000 ms
2017-07-26 18:10:36,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1501063836000 ms (execution: 0.061 s)
2017-07-26 18:10:36,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1966 from persistence list
2017-07-26 18:10:36,078 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1966
2017-07-26 18:10:36,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:36,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063832000 ms
2017-07-26 18:10:38,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063838000 ms
2017-07-26 18:10:38,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063838000 ms.0 from job set of time 1501063838000 ms
2017-07-26 18:10:38,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:38,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1968 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:38,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1968 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:38,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:38,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:38,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1968 (KafkaRDD[1968] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:38,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1968 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:10:38,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1968_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:10:38,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1968_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:38,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1968 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:38,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1968 (KafkaRDD[1968] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:38,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1968.0 with 2 tasks
2017-07-26 18:10:38,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1968.0 (TID 3936, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:38,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1968.0 (TID 3937, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:38,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1968.0 (TID 3936)
2017-07-26 18:10:38,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1968.0 (TID 3937)
2017-07-26 18:10:38,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:38,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:38,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1968.0 (TID 3937). 635 bytes result sent to driver
2017-07-26 18:10:38,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1968.0 (TID 3936). 635 bytes result sent to driver
2017-07-26 18:10:38,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1968.0 (TID 3937) in 7 ms on localhost (1/2)
2017-07-26 18:10:38,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1968.0 (TID 3936) in 9 ms on localhost (2/2)
2017-07-26 18:10:38,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1968 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:10:38,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1968.0, whose tasks have all completed, from pool 
2017-07-26 18:10:38,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1968 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027729 s
2017-07-26 18:10:38,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063838000 ms.0 from job set of time 1501063838000 ms
2017-07-26 18:10:38,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1501063838000 ms (execution: 0.060 s)
2017-07-26 18:10:38,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1967 from persistence list
2017-07-26 18:10:38,078 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1967
2017-07-26 18:10:38,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:38,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063834000 ms
2017-07-26 18:10:40,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063840000 ms
2017-07-26 18:10:40,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063840000 ms.0 from job set of time 1501063840000 ms
2017-07-26 18:10:40,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1969 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1969 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1969 (KafkaRDD[1969] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1969 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:10:40,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1969_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:10:40,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1969_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:40,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1969 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:40,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1969 (KafkaRDD[1969] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:40,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1969.0 with 2 tasks
2017-07-26 18:10:40,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1969.0 (TID 3938, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:40,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1969.0 (TID 3939, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:40,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1969.0 (TID 3939)
2017-07-26 18:10:40,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1969.0 (TID 3938)
2017-07-26 18:10:40,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:40,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:40,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1969.0 (TID 3939). 635 bytes result sent to driver
2017-07-26 18:10:40,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1969.0 (TID 3938). 635 bytes result sent to driver
2017-07-26 18:10:40,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1969.0 (TID 3939) in 8 ms on localhost (1/2)
2017-07-26 18:10:40,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1969.0 (TID 3938) in 10 ms on localhost (2/2)
2017-07-26 18:10:40,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1969.0, whose tasks have all completed, from pool 
2017-07-26 18:10:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1969 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:10:40,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1969 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022927 s
2017-07-26 18:10:40,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063840000 ms.0 from job set of time 1501063840000 ms
2017-07-26 18:10:40,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1501063840000 ms (execution: 0.055 s)
2017-07-26 18:10:40,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1968 from persistence list
2017-07-26 18:10:40,074 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1968
2017-07-26 18:10:40,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:40,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063836000 ms
2017-07-26 18:10:42,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063842000 ms
2017-07-26 18:10:42,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063842000 ms.0 from job set of time 1501063842000 ms
2017-07-26 18:10:42,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:42,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1970 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:42,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1970 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:42,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:42,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:42,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1970 (KafkaRDD[1970] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:42,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1970 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:10:42,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1970_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:10:42,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1970_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:42,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1970 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:42,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1970 (KafkaRDD[1970] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:42,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1970.0 with 2 tasks
2017-07-26 18:10:42,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1970.0 (TID 3940, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:42,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1970.0 (TID 3941, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:42,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1970.0 (TID 3941)
2017-07-26 18:10:42,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1970.0 (TID 3940)
2017-07-26 18:10:42,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:42,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:42,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1970.0 (TID 3941). 635 bytes result sent to driver
2017-07-26 18:10:42,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1970.0 (TID 3940). 714 bytes result sent to driver
2017-07-26 18:10:42,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1970.0 (TID 3941) in 14 ms on localhost (1/2)
2017-07-26 18:10:42,084 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1970.0 (TID 3940) in 15 ms on localhost (2/2)
2017-07-26 18:10:42,084 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1970.0, whose tasks have all completed, from pool 
2017-07-26 18:10:42,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1970 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:10:42,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1970 finished: foreachPartition at streamingProcessTest.scala:48, took 0.031782 s
2017-07-26 18:10:42,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063842000 ms.0 from job set of time 1501063842000 ms
2017-07-26 18:10:42,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1501063842000 ms (execution: 0.068 s)
2017-07-26 18:10:42,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1969 from persistence list
2017-07-26 18:10:42,086 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1969
2017-07-26 18:10:42,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:42,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063838000 ms
2017-07-26 18:10:44,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063844000 ms
2017-07-26 18:10:44,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063844000 ms.0 from job set of time 1501063844000 ms
2017-07-26 18:10:44,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:44,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1971 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:44,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1971 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:44,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:44,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:44,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1971 (KafkaRDD[1971] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:44,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1971 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:10:44,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1971_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:10:44,037 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1971_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:44,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1971 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:44,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1971 (KafkaRDD[1971] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:44,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1971.0 with 2 tasks
2017-07-26 18:10:44,039 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1971.0 (TID 3942, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:44,039 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1971.0 (TID 3943, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:44,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1971.0 (TID 3942)
2017-07-26 18:10:44,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1971.0 (TID 3943)
2017-07-26 18:10:44,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:44,041 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:44,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1971.0 (TID 3942). 714 bytes result sent to driver
2017-07-26 18:10:44,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1971.0 (TID 3943). 635 bytes result sent to driver
2017-07-26 18:10:44,043 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1971.0 (TID 3943) in 4 ms on localhost (1/2)
2017-07-26 18:10:44,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1971.0 (TID 3942) in 5 ms on localhost (2/2)
2017-07-26 18:10:44,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1971.0, whose tasks have all completed, from pool 
2017-07-26 18:10:44,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1971 (foreachPartition at streamingProcessTest.scala:48) finished in 0.006 s
2017-07-26 18:10:44,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1971 finished: foreachPartition at streamingProcessTest.scala:48, took 0.017926 s
2017-07-26 18:10:44,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063844000 ms.0 from job set of time 1501063844000 ms
2017-07-26 18:10:44,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.045 s for time 1501063844000 ms (execution: 0.033 s)
2017-07-26 18:10:44,045 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1970 from persistence list
2017-07-26 18:10:44,046 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1970
2017-07-26 18:10:44,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:44,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063840000 ms
2017-07-26 18:10:46,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063846000 ms
2017-07-26 18:10:46,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063846000 ms.0 from job set of time 1501063846000 ms
2017-07-26 18:10:46,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:46,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1972 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1972 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1972 (KafkaRDD[1972] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:46,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1972 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:10:46,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1972_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:10:46,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1972_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:46,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1972 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:46,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1972 (KafkaRDD[1972] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:46,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1972.0 with 2 tasks
2017-07-26 18:10:46,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1972.0 (TID 3944, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:46,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1972.0 (TID 3945, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:46,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1972.0 (TID 3944)
2017-07-26 18:10:46,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1972.0 (TID 3945)
2017-07-26 18:10:46,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:46,066 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:46,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1972.0 (TID 3945). 635 bytes result sent to driver
2017-07-26 18:10:46,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1972.0 (TID 3944). 635 bytes result sent to driver
2017-07-26 18:10:46,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1972.0 (TID 3945) in 9 ms on localhost (1/2)
2017-07-26 18:10:46,071 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1972.0 (TID 3944) in 10 ms on localhost (2/2)
2017-07-26 18:10:46,071 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1972.0, whose tasks have all completed, from pool 
2017-07-26 18:10:46,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1972 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:10:46,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1972 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022818 s
2017-07-26 18:10:46,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063846000 ms.0 from job set of time 1501063846000 ms
2017-07-26 18:10:46,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501063846000 ms (execution: 0.055 s)
2017-07-26 18:10:46,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1971 from persistence list
2017-07-26 18:10:46,073 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1971
2017-07-26 18:10:46,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:46,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063842000 ms
2017-07-26 18:10:48,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063848000 ms
2017-07-26 18:10:48,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063848000 ms.0 from job set of time 1501063848000 ms
2017-07-26 18:10:48,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1972_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:48,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1959_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:48,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1960_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:10:48,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:48,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1961_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:48,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1973 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:48,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1973 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:48,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:48,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:48,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1973 (KafkaRDD[1973] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:48,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1962_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:48,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1963_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:48,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1973 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:10:48,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1964_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:48,070 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1965_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:48,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1973_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:10:48,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1973_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:48,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1973 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:48,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1973 (KafkaRDD[1973] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:48,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1973.0 with 2 tasks
2017-07-26 18:10:48,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1966_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:48,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1973.0 (TID 3946, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:48,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1973.0 (TID 3947, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:48,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1973.0 (TID 3947)
2017-07-26 18:10:48,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1973.0 (TID 3946)
2017-07-26 18:10:48,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1967_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:48,080 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1968_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:48,081 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:48,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:48,083 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1969_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:48,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1973.0 (TID 3947). 635 bytes result sent to driver
2017-07-26 18:10:48,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1973.0 (TID 3946). 635 bytes result sent to driver
2017-07-26 18:10:48,087 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1970_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:48,087 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1973.0 (TID 3947) in 12 ms on localhost (1/2)
2017-07-26 18:10:48,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1973.0 (TID 3946) in 15 ms on localhost (2/2)
2017-07-26 18:10:48,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1973.0, whose tasks have all completed, from pool 
2017-07-26 18:10:48,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1973 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:10:48,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1973 finished: foreachPartition at streamingProcessTest.scala:48, took 0.031253 s
2017-07-26 18:10:48,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063848000 ms.0 from job set of time 1501063848000 ms
2017-07-26 18:10:48,089 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1971_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:48,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1501063848000 ms (execution: 0.070 s)
2017-07-26 18:10:48,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1972 from persistence list
2017-07-26 18:10:48,089 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1972
2017-07-26 18:10:48,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:48,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063844000 ms
2017-07-26 18:10:50,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063850000 ms
2017-07-26 18:10:50,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063850000 ms.0 from job set of time 1501063850000 ms
2017-07-26 18:10:50,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1974 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1974 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1974 (KafkaRDD[1974] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:50,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1974 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:10:50,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1974_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:10:50,045 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1974_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:50,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1974 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:50,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1974 (KafkaRDD[1974] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:50,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1974.0 with 2 tasks
2017-07-26 18:10:50,047 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1974.0 (TID 3948, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:50,048 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1974.0 (TID 3949, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:50,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1974.0 (TID 3948)
2017-07-26 18:10:50,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1974.0 (TID 3949)
2017-07-26 18:10:50,050 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:50,050 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:50,052 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1974.0 (TID 3948). 635 bytes result sent to driver
2017-07-26 18:10:50,052 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1974.0 (TID 3949). 635 bytes result sent to driver
2017-07-26 18:10:50,054 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1974.0 (TID 3948) in 8 ms on localhost (1/2)
2017-07-26 18:10:50,054 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1974.0 (TID 3949) in 7 ms on localhost (2/2)
2017-07-26 18:10:50,054 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1974.0, whose tasks have all completed, from pool 
2017-07-26 18:10:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1974 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:10:50,055 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1974 finished: foreachPartition at streamingProcessTest.scala:48, took 0.016820 s
2017-07-26 18:10:50,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063850000 ms.0 from job set of time 1501063850000 ms
2017-07-26 18:10:50,056 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.055 s for time 1501063850000 ms (execution: 0.038 s)
2017-07-26 18:10:50,056 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1973 from persistence list
2017-07-26 18:10:50,056 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1973
2017-07-26 18:10:50,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:50,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063846000 ms
2017-07-26 18:10:52,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063852000 ms
2017-07-26 18:10:52,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063852000 ms.0 from job set of time 1501063852000 ms
2017-07-26 18:10:52,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:52,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1975 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:52,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1975 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:52,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:52,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:52,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1975 (KafkaRDD[1975] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:52,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1975 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:10:52,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1975_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:10:52,050 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1975_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1975 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1975 (KafkaRDD[1975] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1975.0 with 2 tasks
2017-07-26 18:10:52,052 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1975.0 (TID 3950, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:52,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1975.0 (TID 3951, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:52,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1975.0 (TID 3951)
2017-07-26 18:10:52,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1975.0 (TID 3950)
2017-07-26 18:10:52,056 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:52,056 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:52,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1975.0 (TID 3951). 635 bytes result sent to driver
2017-07-26 18:10:52,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1975.0 (TID 3950). 635 bytes result sent to driver
2017-07-26 18:10:52,060 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1975.0 (TID 3951) in 8 ms on localhost (1/2)
2017-07-26 18:10:52,060 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1975.0 (TID 3950) in 9 ms on localhost (2/2)
2017-07-26 18:10:52,060 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1975.0, whose tasks have all completed, from pool 
2017-07-26 18:10:52,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1975 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:10:52,061 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1975 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021510 s
2017-07-26 18:10:52,061 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063852000 ms.0 from job set of time 1501063852000 ms
2017-07-26 18:10:52,061 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.061 s for time 1501063852000 ms (execution: 0.045 s)
2017-07-26 18:10:52,061 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1974 from persistence list
2017-07-26 18:10:52,062 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1974
2017-07-26 18:10:52,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:52,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063848000 ms
2017-07-26 18:10:54,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063854000 ms
2017-07-26 18:10:54,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063854000 ms.0 from job set of time 1501063854000 ms
2017-07-26 18:10:54,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:54,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1976 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:54,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1976 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:54,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:54,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:54,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1976 (KafkaRDD[1976] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:54,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1976 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:10:54,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1976_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:10:54,046 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1976_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:10:54,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1976 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:54,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1976 (KafkaRDD[1976] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:54,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1976.0 with 2 tasks
2017-07-26 18:10:54,048 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1976.0 (TID 3952, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:54,049 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1976.0 (TID 3953, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:54,049 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1976.0 (TID 3953)
2017-07-26 18:10:54,049 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1976.0 (TID 3952)
2017-07-26 18:10:54,051 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:54,051 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:54,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1976.0 (TID 3953). 714 bytes result sent to driver
2017-07-26 18:10:54,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1976.0 (TID 3952). 714 bytes result sent to driver
2017-07-26 18:10:54,056 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1976.0 (TID 3953) in 8 ms on localhost (1/2)
2017-07-26 18:10:54,056 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1976.0 (TID 3952) in 9 ms on localhost (2/2)
2017-07-26 18:10:54,057 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1976.0, whose tasks have all completed, from pool 
2017-07-26 18:10:54,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1976 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:10:54,057 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1976 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019451 s
2017-07-26 18:10:54,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063854000 ms.0 from job set of time 1501063854000 ms
2017-07-26 18:10:54,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.057 s for time 1501063854000 ms (execution: 0.038 s)
2017-07-26 18:10:54,057 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1975 from persistence list
2017-07-26 18:10:54,058 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1975
2017-07-26 18:10:54,058 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:54,058 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063850000 ms
2017-07-26 18:10:56,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063856000 ms
2017-07-26 18:10:56,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063856000 ms.0 from job set of time 1501063856000 ms
2017-07-26 18:10:56,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1977 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1977 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:56,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1977 (KafkaRDD[1977] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:56,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1977 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:10:56,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1977_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:10:56,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1977_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:56,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1977 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:56,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1977 (KafkaRDD[1977] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:56,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1977.0 with 2 tasks
2017-07-26 18:10:56,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1977.0 (TID 3954, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:56,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1977.0 (TID 3955, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:56,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1977.0 (TID 3955)
2017-07-26 18:10:56,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1977.0 (TID 3954)
2017-07-26 18:10:56,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:56,063 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:56,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1977.0 (TID 3955). 714 bytes result sent to driver
2017-07-26 18:10:56,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1977.0 (TID 3954). 714 bytes result sent to driver
2017-07-26 18:10:56,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1977.0 (TID 3955) in 9 ms on localhost (1/2)
2017-07-26 18:10:56,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1977.0 (TID 3954) in 11 ms on localhost (2/2)
2017-07-26 18:10:56,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1977.0, whose tasks have all completed, from pool 
2017-07-26 18:10:56,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1977 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:10:56,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1977 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025192 s
2017-07-26 18:10:56,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063856000 ms.0 from job set of time 1501063856000 ms
2017-07-26 18:10:56,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501063856000 ms (execution: 0.051 s)
2017-07-26 18:10:56,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1976 from persistence list
2017-07-26 18:10:56,070 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1976
2017-07-26 18:10:56,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:56,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063852000 ms
2017-07-26 18:10:58,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063858000 ms
2017-07-26 18:10:58,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063858000 ms.0 from job set of time 1501063858000 ms
2017-07-26 18:10:58,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:10:58,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1978 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:10:58,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1978 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:10:58,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:10:58,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:10:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1978 (KafkaRDD[1978] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:10:58,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1978 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:10:58,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1978_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:10:58,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1978_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:10:58,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1978 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:10:58,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1978 (KafkaRDD[1978] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:10:58,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1978.0 with 2 tasks
2017-07-26 18:10:58,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1978.0 (TID 3956, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:10:58,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1978.0 (TID 3957, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:10:58,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1978.0 (TID 3957)
2017-07-26 18:10:58,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1978.0 (TID 3956)
2017-07-26 18:10:58,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:10:58,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:10:58,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1978.0 (TID 3956). 635 bytes result sent to driver
2017-07-26 18:10:58,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1978.0 (TID 3957). 714 bytes result sent to driver
2017-07-26 18:10:58,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1978.0 (TID 3956) in 9 ms on localhost (1/2)
2017-07-26 18:10:58,070 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1978.0 (TID 3957) in 9 ms on localhost (2/2)
2017-07-26 18:10:58,070 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1978.0, whose tasks have all completed, from pool 
2017-07-26 18:10:58,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1978 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:10:58,071 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1978 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023221 s
2017-07-26 18:10:58,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063858000 ms.0 from job set of time 1501063858000 ms
2017-07-26 18:10:58,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1501063858000 ms (execution: 0.055 s)
2017-07-26 18:10:58,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1977 from persistence list
2017-07-26 18:10:58,072 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1977
2017-07-26 18:10:58,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:10:58,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063854000 ms
2017-07-26 18:11:00,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063860000 ms
2017-07-26 18:11:00,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063860000 ms.0 from job set of time 1501063860000 ms
2017-07-26 18:11:00,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1979 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1979 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:00,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1979 (KafkaRDD[1979] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1979 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:11:00,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1979_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:11:00,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1979_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:00,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1979 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:00,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1979 (KafkaRDD[1979] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:00,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1979.0 with 2 tasks
2017-07-26 18:11:00,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1979.0 (TID 3958, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:00,078 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1979.0 (TID 3959, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:00,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1979.0 (TID 3958)
2017-07-26 18:11:00,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1979.0 (TID 3959)
2017-07-26 18:11:00,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:00,084 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:00,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1979.0 (TID 3958). 714 bytes result sent to driver
2017-07-26 18:11:00,088 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1979.0 (TID 3959). 714 bytes result sent to driver
2017-07-26 18:11:00,091 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1979.0 (TID 3958) in 15 ms on localhost (1/2)
2017-07-26 18:11:00,091 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1979.0 (TID 3959) in 14 ms on localhost (2/2)
2017-07-26 18:11:00,091 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1979.0, whose tasks have all completed, from pool 
2017-07-26 18:11:00,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1979 (foreachPartition at streamingProcessTest.scala:48) finished in 0.016 s
2017-07-26 18:11:00,092 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1979 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040142 s
2017-07-26 18:11:00,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063860000 ms.0 from job set of time 1501063860000 ms
2017-07-26 18:11:00,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1501063860000 ms (execution: 0.074 s)
2017-07-26 18:11:00,093 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1978 from persistence list
2017-07-26 18:11:00,093 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1978
2017-07-26 18:11:00,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:00,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063856000 ms
2017-07-26 18:11:02,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063862000 ms
2017-07-26 18:11:02,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063862000 ms.0 from job set of time 1501063862000 ms
2017-07-26 18:11:02,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:02,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1980 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:02,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1980 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:02,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:02,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:02,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1980 (KafkaRDD[1980] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:02,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1980 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:11:02,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1980_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:11:02,052 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1980_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:02,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1980 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:02,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1980 (KafkaRDD[1980] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:02,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1980.0 with 2 tasks
2017-07-26 18:11:02,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1980.0 (TID 3960, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:02,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1980.0 (TID 3961, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:02,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1980.0 (TID 3961)
2017-07-26 18:11:02,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1980.0 (TID 3960)
2017-07-26 18:11:02,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:02,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:02,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1980.0 (TID 3961). 714 bytes result sent to driver
2017-07-26 18:11:02,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1980.0 (TID 3960). 714 bytes result sent to driver
2017-07-26 18:11:02,066 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1980.0 (TID 3961) in 10 ms on localhost (1/2)
2017-07-26 18:11:02,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1980.0 (TID 3960) in 13 ms on localhost (2/2)
2017-07-26 18:11:02,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1980.0, whose tasks have all completed, from pool 
2017-07-26 18:11:02,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1980 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:11:02,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1980 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025036 s
2017-07-26 18:11:02,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063862000 ms.0 from job set of time 1501063862000 ms
2017-07-26 18:11:02,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1501063862000 ms (execution: 0.052 s)
2017-07-26 18:11:02,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1979 from persistence list
2017-07-26 18:11:02,068 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1979
2017-07-26 18:11:02,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:02,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063858000 ms
2017-07-26 18:11:04,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063864000 ms
2017-07-26 18:11:04,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063864000 ms.0 from job set of time 1501063864000 ms
2017-07-26 18:11:04,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:04,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1981 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:04,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1981 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:04,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:04,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:04,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1981 (KafkaRDD[1981] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:04,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1981 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:11:04,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1981_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:11:04,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1981_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:04,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1981 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:04,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1981 (KafkaRDD[1981] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:04,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1981.0 with 2 tasks
2017-07-26 18:11:04,040 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1981.0 (TID 3962, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:04,041 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1981.0 (TID 3963, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:04,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1981.0 (TID 3962)
2017-07-26 18:11:04,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1981.0 (TID 3963)
2017-07-26 18:11:04,043 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:04,043 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:04,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1981.0 (TID 3963). 714 bytes result sent to driver
2017-07-26 18:11:04,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1981.0 (TID 3962). 635 bytes result sent to driver
2017-07-26 18:11:04,048 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1981.0 (TID 3962) in 9 ms on localhost (1/2)
2017-07-26 18:11:04,048 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1981.0 (TID 3963) in 8 ms on localhost (2/2)
2017-07-26 18:11:04,048 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1981.0, whose tasks have all completed, from pool 
2017-07-26 18:11:04,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1981 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:11:04,049 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1981 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019427 s
2017-07-26 18:11:04,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063864000 ms.0 from job set of time 1501063864000 ms
2017-07-26 18:11:04,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.049 s for time 1501063864000 ms (execution: 0.034 s)
2017-07-26 18:11:04,050 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1980 from persistence list
2017-07-26 18:11:04,050 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1980
2017-07-26 18:11:04,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:04,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063860000 ms
2017-07-26 18:11:06,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063866000 ms
2017-07-26 18:11:06,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063866000 ms.0 from job set of time 1501063866000 ms
2017-07-26 18:11:06,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:06,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1982 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:06,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1982 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:06,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:06,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:06,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1982 (KafkaRDD[1982] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:06,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1982 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:11:06,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1982_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:11:06,064 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1982_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:06,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1982 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:06,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1982 (KafkaRDD[1982] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:06,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1982.0 with 2 tasks
2017-07-26 18:11:06,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1982.0 (TID 3964, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:06,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1982.0 (TID 3965, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:06,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1982.0 (TID 3964)
2017-07-26 18:11:06,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1982.0 (TID 3965)
2017-07-26 18:11:06,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:06,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:06,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1982.0 (TID 3964). 714 bytes result sent to driver
2017-07-26 18:11:06,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1982.0 (TID 3965). 714 bytes result sent to driver
2017-07-26 18:11:06,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1982.0 (TID 3965) in 9 ms on localhost (1/2)
2017-07-26 18:11:06,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1982.0 (TID 3964) in 10 ms on localhost (2/2)
2017-07-26 18:11:06,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1982.0, whose tasks have all completed, from pool 
2017-07-26 18:11:06,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1982 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:11:06,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1982 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027903 s
2017-07-26 18:11:06,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063866000 ms.0 from job set of time 1501063866000 ms
2017-07-26 18:11:06,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1501063866000 ms (execution: 0.060 s)
2017-07-26 18:11:06,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1981 from persistence list
2017-07-26 18:11:06,078 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1981
2017-07-26 18:11:06,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:06,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063862000 ms
2017-07-26 18:11:08,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063868000 ms
2017-07-26 18:11:08,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063868000 ms.0 from job set of time 1501063868000 ms
2017-07-26 18:11:08,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1983 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1983 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:08,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:08,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1983 (KafkaRDD[1983] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:08,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1983 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:11:08,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1983_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:11:08,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1983_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:08,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1983 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:08,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1983 (KafkaRDD[1983] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:08,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1983.0 with 2 tasks
2017-07-26 18:11:08,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1983.0 (TID 3966, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:08,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1983.0 (TID 3967, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:08,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1983.0 (TID 3967)
2017-07-26 18:11:08,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1983.0 (TID 3966)
2017-07-26 18:11:08,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:08,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:08,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1983.0 (TID 3966). 714 bytes result sent to driver
2017-07-26 18:11:08,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1983.0 (TID 3967). 635 bytes result sent to driver
2017-07-26 18:11:08,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1983.0 (TID 3966) in 13 ms on localhost (1/2)
2017-07-26 18:11:08,084 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1983.0 (TID 3967) in 12 ms on localhost (2/2)
2017-07-26 18:11:08,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1983 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:11:08,084 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1983.0, whose tasks have all completed, from pool 
2017-07-26 18:11:08,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1983 finished: foreachPartition at streamingProcessTest.scala:48, took 0.036453 s
2017-07-26 18:11:08,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063868000 ms.0 from job set of time 1501063868000 ms
2017-07-26 18:11:08,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1501063868000 ms (execution: 0.069 s)
2017-07-26 18:11:08,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1982 from persistence list
2017-07-26 18:11:08,086 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1982
2017-07-26 18:11:08,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:08,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063864000 ms
2017-07-26 18:11:10,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063870000 ms
2017-07-26 18:11:10,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063870000 ms.0 from job set of time 1501063870000 ms
2017-07-26 18:11:10,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1984 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1984 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1984 (KafkaRDD[1984] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:10,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1984 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:11:10,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1984_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:11:10,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1984_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:10,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1984 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:10,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1984 (KafkaRDD[1984] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:10,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1984.0 with 2 tasks
2017-07-26 18:11:10,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1984.0 (TID 3968, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:10,073 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1984.0 (TID 3969, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:10,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1984.0 (TID 3969)
2017-07-26 18:11:10,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1984.0 (TID 3968)
2017-07-26 18:11:10,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:10,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:10,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1984.0 (TID 3969). 722 bytes result sent to driver
2017-07-26 18:11:10,086 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1984.0 (TID 3968). 714 bytes result sent to driver
2017-07-26 18:11:10,091 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1984.0 (TID 3969) in 18 ms on localhost (1/2)
2017-07-26 18:11:10,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1984.0 (TID 3968) in 21 ms on localhost (2/2)
2017-07-26 18:11:10,092 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1984.0, whose tasks have all completed, from pool 
2017-07-26 18:11:10,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1984 (foreachPartition at streamingProcessTest.scala:48) finished in 0.022 s
2017-07-26 18:11:10,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1984 finished: foreachPartition at streamingProcessTest.scala:48, took 0.043963 s
2017-07-26 18:11:10,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063870000 ms.0 from job set of time 1501063870000 ms
2017-07-26 18:11:10,094 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1983 from persistence list
2017-07-26 18:11:10,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.093 s for time 1501063870000 ms (execution: 0.076 s)
2017-07-26 18:11:10,095 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1983
2017-07-26 18:11:10,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:10,095 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063866000 ms
2017-07-26 18:11:12,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063872000 ms
2017-07-26 18:11:12,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063872000 ms.0 from job set of time 1501063872000 ms
2017-07-26 18:11:12,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:12,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1985 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:12,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1985 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:12,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:12,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:12,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1985 (KafkaRDD[1985] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:12,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1985 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:11:12,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1985_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:11:12,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1985_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:12,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1985 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:12,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1985 (KafkaRDD[1985] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:12,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1985.0 with 2 tasks
2017-07-26 18:11:12,050 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1985.0 (TID 3970, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:12,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1985.0 (TID 3971, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:12,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1985.0 (TID 3971)
2017-07-26 18:11:12,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1985.0 (TID 3970)
2017-07-26 18:11:12,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:12,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:12,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1985.0 (TID 3970). 714 bytes result sent to driver
2017-07-26 18:11:12,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1985.0 (TID 3971). 714 bytes result sent to driver
2017-07-26 18:11:12,061 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1985.0 (TID 3971) in 11 ms on localhost (1/2)
2017-07-26 18:11:12,061 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1985.0 (TID 3970) in 12 ms on localhost (2/2)
2017-07-26 18:11:12,061 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1985.0, whose tasks have all completed, from pool 
2017-07-26 18:11:12,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1985 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:11:12,061 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1985 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027701 s
2017-07-26 18:11:12,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063872000 ms.0 from job set of time 1501063872000 ms
2017-07-26 18:11:12,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.062 s for time 1501063872000 ms (execution: 0.048 s)
2017-07-26 18:11:12,062 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1984 from persistence list
2017-07-26 18:11:12,063 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1984
2017-07-26 18:11:12,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:12,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063868000 ms
2017-07-26 18:11:14,010 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063874000 ms
2017-07-26 18:11:14,010 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063874000 ms.0 from job set of time 1501063874000 ms
2017-07-26 18:11:14,021 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:14,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1986 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:14,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1986 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:14,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:14,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:14,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1986 (KafkaRDD[1986] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1986 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:11:14,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1986_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:11:14,026 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1986_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:14,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1986 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:14,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1986 (KafkaRDD[1986] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:14,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1986.0 with 2 tasks
2017-07-26 18:11:14,028 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1986.0 (TID 3972, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:14,029 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1986.0 (TID 3973, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:14,029 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1986.0 (TID 3972)
2017-07-26 18:11:14,029 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1986.0 (TID 3973)
2017-07-26 18:11:14,031 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:14,031 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:14,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1986.0 (TID 3973). 635 bytes result sent to driver
2017-07-26 18:11:14,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1986.0 (TID 3972). 635 bytes result sent to driver
2017-07-26 18:11:14,043 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1986.0 (TID 3972) in 15 ms on localhost (1/2)
2017-07-26 18:11:14,043 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1986.0 (TID 3973) in 14 ms on localhost (2/2)
2017-07-26 18:11:14,043 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1973_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:14,043 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1986.0, whose tasks have all completed, from pool 
2017-07-26 18:11:14,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1986 (foreachPartition at streamingProcessTest.scala:48) finished in 0.016 s
2017-07-26 18:11:14,044 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1986 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023066 s
2017-07-26 18:11:14,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063874000 ms.0 from job set of time 1501063874000 ms
2017-07-26 18:11:14,044 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1985 from persistence list
2017-07-26 18:11:14,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.044 s for time 1501063874000 ms (execution: 0.034 s)
2017-07-26 18:11:14,045 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1985
2017-07-26 18:11:14,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:14,045 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1974_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:14,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063870000 ms
2017-07-26 18:11:14,047 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1975_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:14,049 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1976_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:14,051 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1977_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:14,052 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1978_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:14,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1979_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:14,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1980_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:14,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1981_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:14,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1982_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:14,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1983_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:14,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1984_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:14,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1985_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:16,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063876000 ms
2017-07-26 18:11:16,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063876000 ms.0 from job set of time 1501063876000 ms
2017-07-26 18:11:16,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:16,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1987 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:16,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1987 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:16,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:16,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:16,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1987 (KafkaRDD[1987] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1987 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:11:16,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1987_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:11:16,064 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1987_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:16,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1987 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:16,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1987 (KafkaRDD[1987] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:16,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1987.0 with 2 tasks
2017-07-26 18:11:16,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1987.0 (TID 3974, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:16,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1987.0 (TID 3975, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:16,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1987.0 (TID 3975)
2017-07-26 18:11:16,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1987.0 (TID 3974)
2017-07-26 18:11:16,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:16,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:16,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1987.0 (TID 3975). 714 bytes result sent to driver
2017-07-26 18:11:16,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1987.0 (TID 3974). 714 bytes result sent to driver
2017-07-26 18:11:16,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1987.0 (TID 3975) in 12 ms on localhost (1/2)
2017-07-26 18:11:16,080 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1987.0 (TID 3974) in 13 ms on localhost (2/2)
2017-07-26 18:11:16,080 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1987.0, whose tasks have all completed, from pool 
2017-07-26 18:11:16,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1987 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:11:16,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1987 finished: foreachPartition at streamingProcessTest.scala:48, took 0.041000 s
2017-07-26 18:11:16,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063876000 ms.0 from job set of time 1501063876000 ms
2017-07-26 18:11:16,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501063876000 ms (execution: 0.065 s)
2017-07-26 18:11:16,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1986 from persistence list
2017-07-26 18:11:16,082 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1986
2017-07-26 18:11:16,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:16,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063872000 ms
2017-07-26 18:11:18,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063878000 ms
2017-07-26 18:11:18,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063878000 ms.0 from job set of time 1501063878000 ms
2017-07-26 18:11:18,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1988 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1988 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:18,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:18,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:18,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1988 (KafkaRDD[1988] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:18,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1988 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:11:18,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1988_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:11:18,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1988_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:18,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1988 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:18,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1988 (KafkaRDD[1988] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:18,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1988.0 with 2 tasks
2017-07-26 18:11:18,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1988.0 (TID 3976, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:18,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1988.0 (TID 3977, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:18,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1988.0 (TID 3977)
2017-07-26 18:11:18,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1988.0 (TID 3976)
2017-07-26 18:11:18,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:18,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:18,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1988.0 (TID 3976). 635 bytes result sent to driver
2017-07-26 18:11:18,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1988.0 (TID 3977). 635 bytes result sent to driver
2017-07-26 18:11:18,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1988.0 (TID 3976) in 10 ms on localhost (1/2)
2017-07-26 18:11:18,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1988.0 (TID 3977) in 8 ms on localhost (2/2)
2017-07-26 18:11:18,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1988.0, whose tasks have all completed, from pool 
2017-07-26 18:11:18,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1988 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:11:18,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1988 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025578 s
2017-07-26 18:11:18,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063878000 ms.0 from job set of time 1501063878000 ms
2017-07-26 18:11:18,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501063878000 ms (execution: 0.057 s)
2017-07-26 18:11:18,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1987 from persistence list
2017-07-26 18:11:18,076 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1987
2017-07-26 18:11:18,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:18,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063874000 ms
2017-07-26 18:11:20,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063880000 ms
2017-07-26 18:11:20,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063880000 ms.0 from job set of time 1501063880000 ms
2017-07-26 18:11:20,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:20,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1989 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:20,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1989 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:20,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:20,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:20,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1989 (KafkaRDD[1989] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:20,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1989 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:11:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1989_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:11:20,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1989_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:20,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1989 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:20,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1989 (KafkaRDD[1989] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:20,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1989.0 with 2 tasks
2017-07-26 18:11:20,038 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1989.0 (TID 3978, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:20,039 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1989.0 (TID 3979, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:20,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1989.0 (TID 3978)
2017-07-26 18:11:20,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1989.0 (TID 3979)
2017-07-26 18:11:20,043 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:20,043 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:20,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1989.0 (TID 3979). 635 bytes result sent to driver
2017-07-26 18:11:20,045 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1989.0 (TID 3978). 635 bytes result sent to driver
2017-07-26 18:11:20,047 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1989.0 (TID 3978) in 9 ms on localhost (1/2)
2017-07-26 18:11:20,048 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1989.0 (TID 3979) in 9 ms on localhost (2/2)
2017-07-26 18:11:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1989 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:11:20,048 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1989.0, whose tasks have all completed, from pool 
2017-07-26 18:11:20,048 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1989 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020808 s
2017-07-26 18:11:20,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063880000 ms.0 from job set of time 1501063880000 ms
2017-07-26 18:11:20,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.049 s for time 1501063880000 ms (execution: 0.035 s)
2017-07-26 18:11:20,049 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1988 from persistence list
2017-07-26 18:11:20,049 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1988
2017-07-26 18:11:20,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:20,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063876000 ms
2017-07-26 18:11:22,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063882000 ms
2017-07-26 18:11:22,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063882000 ms.0 from job set of time 1501063882000 ms
2017-07-26 18:11:22,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1990 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1990 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1990 (KafkaRDD[1990] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:22,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1990 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:11:22,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1990_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:11:22,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1990_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:22,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1990 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:22,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1990 (KafkaRDD[1990] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:22,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1990.0 with 2 tasks
2017-07-26 18:11:22,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1990.0 (TID 3980, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:22,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1990.0 (TID 3981, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:22,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1990.0 (TID 3980)
2017-07-26 18:11:22,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1990.0 (TID 3981)
2017-07-26 18:11:22,063 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:22,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:22,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1990.0 (TID 3980). 635 bytes result sent to driver
2017-07-26 18:11:22,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1990.0 (TID 3981). 714 bytes result sent to driver
2017-07-26 18:11:22,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1990.0 (TID 3980) in 10 ms on localhost (1/2)
2017-07-26 18:11:22,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1990.0 (TID 3981) in 10 ms on localhost (2/2)
2017-07-26 18:11:22,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1990.0, whose tasks have all completed, from pool 
2017-07-26 18:11:22,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1990 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:11:22,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1990 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022559 s
2017-07-26 18:11:22,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063882000 ms.0 from job set of time 1501063882000 ms
2017-07-26 18:11:22,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501063882000 ms (execution: 0.052 s)
2017-07-26 18:11:22,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1989 from persistence list
2017-07-26 18:11:22,071 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1989
2017-07-26 18:11:22,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:22,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063878000 ms
2017-07-26 18:11:24,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063884000 ms
2017-07-26 18:11:24,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063884000 ms.0 from job set of time 1501063884000 ms
2017-07-26 18:11:24,028 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:24,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1991 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:24,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1991 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:24,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:24,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:24,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1991 (KafkaRDD[1991] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:24,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1991 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:11:24,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1991_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:11:24,035 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1991_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:24,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1991 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:24,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1991 (KafkaRDD[1991] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:24,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1991.0 with 2 tasks
2017-07-26 18:11:24,037 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1991.0 (TID 3982, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:24,037 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1991.0 (TID 3983, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:24,037 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1991.0 (TID 3982)
2017-07-26 18:11:24,037 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1991.0 (TID 3983)
2017-07-26 18:11:24,039 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:24,039 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:24,040 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1991.0 (TID 3982). 635 bytes result sent to driver
2017-07-26 18:11:24,040 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1991.0 (TID 3983). 635 bytes result sent to driver
2017-07-26 18:11:24,042 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1991.0 (TID 3983) in 5 ms on localhost (1/2)
2017-07-26 18:11:24,042 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1991.0 (TID 3982) in 6 ms on localhost (2/2)
2017-07-26 18:11:24,043 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1991.0, whose tasks have all completed, from pool 
2017-07-26 18:11:24,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1991 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:11:24,043 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1991 finished: foreachPartition at streamingProcessTest.scala:48, took 0.015086 s
2017-07-26 18:11:24,043 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063884000 ms.0 from job set of time 1501063884000 ms
2017-07-26 18:11:24,044 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1990 from persistence list
2017-07-26 18:11:24,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.043 s for time 1501063884000 ms (execution: 0.030 s)
2017-07-26 18:11:24,044 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:24,044 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1990
2017-07-26 18:11:24,044 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063880000 ms
2017-07-26 18:11:26,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063886000 ms
2017-07-26 18:11:26,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063886000 ms.0 from job set of time 1501063886000 ms
2017-07-26 18:11:26,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:26,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1992 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:26,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1992 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:26,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:26,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:26,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1992 (KafkaRDD[1992] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:26,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1992 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:11:26,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1992_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:11:26,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1992_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:26,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1992 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:26,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1992 (KafkaRDD[1992] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:26,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1992.0 with 2 tasks
2017-07-26 18:11:26,076 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1992.0 (TID 3984, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:26,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1992.0 (TID 3985, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:26,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1992.0 (TID 3985)
2017-07-26 18:11:26,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1992.0 (TID 3984)
2017-07-26 18:11:26,083 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:26,083 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:26,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1992.0 (TID 3984). 714 bytes result sent to driver
2017-07-26 18:11:26,089 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1992.0 (TID 3985). 714 bytes result sent to driver
2017-07-26 18:11:26,093 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1992.0 (TID 3985) in 17 ms on localhost (1/2)
2017-07-26 18:11:26,094 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1992.0 (TID 3984) in 20 ms on localhost (2/2)
2017-07-26 18:11:26,095 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1992.0, whose tasks have all completed, from pool 
2017-07-26 18:11:26,095 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1992 (foreachPartition at streamingProcessTest.scala:48) finished in 0.021 s
2017-07-26 18:11:26,096 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1992 finished: foreachPartition at streamingProcessTest.scala:48, took 0.043780 s
2017-07-26 18:11:26,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063886000 ms.0 from job set of time 1501063886000 ms
2017-07-26 18:11:26,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.097 s for time 1501063886000 ms (execution: 0.078 s)
2017-07-26 18:11:26,097 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1991 from persistence list
2017-07-26 18:11:26,098 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1991
2017-07-26 18:11:26,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:26,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063882000 ms
2017-07-26 18:11:28,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063888000 ms
2017-07-26 18:11:28,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063888000 ms.0 from job set of time 1501063888000 ms
2017-07-26 18:11:28,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:28,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1993 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:28,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1993 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:28,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:28,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:28,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1993 (KafkaRDD[1993] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:28,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1993 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:11:28,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1993_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:11:28,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1993_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:28,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1993 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:28,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1993 (KafkaRDD[1993] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:28,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1993.0 with 2 tasks
2017-07-26 18:11:28,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1993.0 (TID 3986, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:28,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1993.0 (TID 3987, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:28,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1993.0 (TID 3987)
2017-07-26 18:11:28,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1993.0 (TID 3986)
2017-07-26 18:11:28,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:28,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:28,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1993.0 (TID 3987). 635 bytes result sent to driver
2017-07-26 18:11:28,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1993.0 (TID 3986). 635 bytes result sent to driver
2017-07-26 18:11:28,077 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1993.0 (TID 3987) in 12 ms on localhost (1/2)
2017-07-26 18:11:28,077 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1993.0 (TID 3986) in 13 ms on localhost (2/2)
2017-07-26 18:11:28,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1993.0, whose tasks have all completed, from pool 
2017-07-26 18:11:28,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1993 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:11:28,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1993 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028319 s
2017-07-26 18:11:28,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063888000 ms.0 from job set of time 1501063888000 ms
2017-07-26 18:11:28,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1501063888000 ms (execution: 0.059 s)
2017-07-26 18:11:28,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1992 from persistence list
2017-07-26 18:11:28,079 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1992
2017-07-26 18:11:28,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:28,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063884000 ms
2017-07-26 18:11:30,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063890000 ms
2017-07-26 18:11:30,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063890000 ms.0 from job set of time 1501063890000 ms
2017-07-26 18:11:30,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:30,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1994 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:30,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1994 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:30,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:30,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:30,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1994 (KafkaRDD[1994] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1994 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:11:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1994_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:11:30,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1994_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1994 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1994 (KafkaRDD[1994] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1994.0 with 2 tasks
2017-07-26 18:11:30,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1994.0 (TID 3988, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:30,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1994.0 (TID 3989, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:30,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1994.0 (TID 3988)
2017-07-26 18:11:30,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1994.0 (TID 3989)
2017-07-26 18:11:30,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:30,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:30,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1994.0 (TID 3988). 635 bytes result sent to driver
2017-07-26 18:11:30,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1994.0 (TID 3989). 635 bytes result sent to driver
2017-07-26 18:11:30,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1994.0 (TID 3989) in 10 ms on localhost (1/2)
2017-07-26 18:11:30,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1994.0 (TID 3988) in 12 ms on localhost (2/2)
2017-07-26 18:11:30,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1994 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:11:30,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1994.0, whose tasks have all completed, from pool 
2017-07-26 18:11:30,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1994 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028808 s
2017-07-26 18:11:30,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063890000 ms.0 from job set of time 1501063890000 ms
2017-07-26 18:11:30,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501063890000 ms (execution: 0.057 s)
2017-07-26 18:11:30,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1993 from persistence list
2017-07-26 18:11:30,076 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1993
2017-07-26 18:11:30,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:30,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063886000 ms
2017-07-26 18:11:32,010 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063892000 ms
2017-07-26 18:11:32,011 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063892000 ms.0 from job set of time 1501063892000 ms
2017-07-26 18:11:32,020 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:32,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1995 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:32,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1995 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:32,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:32,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:32,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1995 (KafkaRDD[1995] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:32,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1995 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:11:32,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1995_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:11:32,026 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1995_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:32,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1995 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:32,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1995 (KafkaRDD[1995] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:32,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1995.0 with 2 tasks
2017-07-26 18:11:32,028 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1995.0 (TID 3990, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:32,029 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1995.0 (TID 3991, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:32,029 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1995.0 (TID 3991)
2017-07-26 18:11:32,029 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1995.0 (TID 3990)
2017-07-26 18:11:32,031 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:32,031 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:32,032 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1995.0 (TID 3990). 635 bytes result sent to driver
2017-07-26 18:11:32,032 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1995.0 (TID 3991). 635 bytes result sent to driver
2017-07-26 18:11:32,034 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1995.0 (TID 3990) in 6 ms on localhost (1/2)
2017-07-26 18:11:32,034 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1995.0 (TID 3991) in 6 ms on localhost (2/2)
2017-07-26 18:11:32,034 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1995.0, whose tasks have all completed, from pool 
2017-07-26 18:11:32,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1995 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:11:32,034 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1995 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014320 s
2017-07-26 18:11:32,035 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063892000 ms.0 from job set of time 1501063892000 ms
2017-07-26 18:11:32,035 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.035 s for time 1501063892000 ms (execution: 0.024 s)
2017-07-26 18:11:32,035 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1994 from persistence list
2017-07-26 18:11:32,035 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1994
2017-07-26 18:11:32,036 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:32,036 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063888000 ms
2017-07-26 18:11:34,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063894000 ms
2017-07-26 18:11:34,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063894000 ms.0 from job set of time 1501063894000 ms
2017-07-26 18:11:34,031 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:34,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1996 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:34,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1996 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:34,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:34,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:34,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1996 (KafkaRDD[1996] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:34,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1996 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:11:34,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1996_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:11:34,042 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1996_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:34,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1996 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:34,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1996 (KafkaRDD[1996] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:34,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1996.0 with 2 tasks
2017-07-26 18:11:34,044 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1996.0 (TID 3992, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:34,045 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1996.0 (TID 3993, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:34,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1996.0 (TID 3992)
2017-07-26 18:11:34,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1996.0 (TID 3993)
2017-07-26 18:11:34,048 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:34,048 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:34,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1996.0 (TID 3993). 635 bytes result sent to driver
2017-07-26 18:11:34,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1996.0 (TID 3992). 635 bytes result sent to driver
2017-07-26 18:11:34,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1996.0 (TID 3992) in 7 ms on localhost (1/2)
2017-07-26 18:11:34,051 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1996.0 (TID 3993) in 6 ms on localhost (2/2)
2017-07-26 18:11:34,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1996.0, whose tasks have all completed, from pool 
2017-07-26 18:11:34,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1996 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:11:34,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1996 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020281 s
2017-07-26 18:11:34,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063894000 ms.0 from job set of time 1501063894000 ms
2017-07-26 18:11:34,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.052 s for time 1501063894000 ms (execution: 0.038 s)
2017-07-26 18:11:34,052 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1995 from persistence list
2017-07-26 18:11:34,053 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1995
2017-07-26 18:11:34,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:34,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063890000 ms
2017-07-26 18:11:36,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063896000 ms
2017-07-26 18:11:36,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063896000 ms.0 from job set of time 1501063896000 ms
2017-07-26 18:11:36,021 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:36,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1997 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:36,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1997 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:36,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:36,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:36,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1997 (KafkaRDD[1997] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:36,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1997 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:11:36,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1997_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:11:36,026 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1997_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:36,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1997 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:36,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1997 (KafkaRDD[1997] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:36,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1997.0 with 2 tasks
2017-07-26 18:11:36,029 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1997.0 (TID 3994, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:36,029 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1997.0 (TID 3995, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:36,030 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1997.0 (TID 3994)
2017-07-26 18:11:36,030 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1997.0 (TID 3995)
2017-07-26 18:11:36,031 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:36,031 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:36,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1997.0 (TID 3995). 635 bytes result sent to driver
2017-07-26 18:11:36,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1997.0 (TID 3994). 722 bytes result sent to driver
2017-07-26 18:11:36,034 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1997.0 (TID 3995) in 5 ms on localhost (1/2)
2017-07-26 18:11:36,035 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1997.0 (TID 3994) in 7 ms on localhost (2/2)
2017-07-26 18:11:36,035 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1997.0, whose tasks have all completed, from pool 
2017-07-26 18:11:36,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1997 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:11:36,035 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1997 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014392 s
2017-07-26 18:11:36,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063896000 ms.0 from job set of time 1501063896000 ms
2017-07-26 18:11:36,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.036 s for time 1501063896000 ms (execution: 0.024 s)
2017-07-26 18:11:36,036 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1996 from persistence list
2017-07-26 18:11:36,036 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1996
2017-07-26 18:11:36,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:36,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063892000 ms
2017-07-26 18:11:38,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063898000 ms
2017-07-26 18:11:38,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063898000 ms.0 from job set of time 1501063898000 ms
2017-07-26 18:11:38,036 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:38,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1998 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:38,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1998 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:38,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:38,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:38,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1998 (KafkaRDD[1998] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:38,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1998 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:11:38,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1998_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:11:38,043 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1998_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:38,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1998 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:38,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1998 (KafkaRDD[1998] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:38,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1998.0 with 2 tasks
2017-07-26 18:11:38,045 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1998.0 (TID 3996, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:38,046 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1998.0 (TID 3997, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:38,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1998.0 (TID 3996)
2017-07-26 18:11:38,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1998.0 (TID 3997)
2017-07-26 18:11:38,048 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:38,048 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:38,049 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1998.0 (TID 3996). 635 bytes result sent to driver
2017-07-26 18:11:38,049 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1998.0 (TID 3997). 635 bytes result sent to driver
2017-07-26 18:11:38,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1998.0 (TID 3996) in 6 ms on localhost (1/2)
2017-07-26 18:11:38,051 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1998.0 (TID 3997) in 6 ms on localhost (2/2)
2017-07-26 18:11:38,051 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1998.0, whose tasks have all completed, from pool 
2017-07-26 18:11:38,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1998 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:11:38,051 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1998 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014933 s
2017-07-26 18:11:38,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063898000 ms.0 from job set of time 1501063898000 ms
2017-07-26 18:11:38,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.052 s for time 1501063898000 ms (execution: 0.036 s)
2017-07-26 18:11:38,052 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1997 from persistence list
2017-07-26 18:11:38,052 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1997
2017-07-26 18:11:38,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:38,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063894000 ms
2017-07-26 18:11:40,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063900000 ms
2017-07-26 18:11:40,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063900000 ms.0 from job set of time 1501063900000 ms
2017-07-26 18:11:40,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:40,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1999 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:40,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1999 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:40,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1999 (KafkaRDD[1999] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:40,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1999 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:11:40,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1999_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:11:40,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1999_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:40,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1999 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:40,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1999 (KafkaRDD[1999] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:40,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1999.0 with 2 tasks
2017-07-26 18:11:40,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1999.0 (TID 3998, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:40,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1999.0 (TID 3999, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:40,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1999.0 (TID 3999)
2017-07-26 18:11:40,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1999.0 (TID 3998)
2017-07-26 18:11:40,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:40,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:40,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1999.0 (TID 3998). 714 bytes result sent to driver
2017-07-26 18:11:40,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1999.0 (TID 3999). 714 bytes result sent to driver
2017-07-26 18:11:40,085 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1999.0 (TID 3999) in 12 ms on localhost (1/2)
2017-07-26 18:11:40,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1999.0 (TID 3998) in 14 ms on localhost (2/2)
2017-07-26 18:11:40,086 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1999.0, whose tasks have all completed, from pool 
2017-07-26 18:11:40,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1999 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:11:40,087 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1999 finished: foreachPartition at streamingProcessTest.scala:48, took 0.037852 s
2017-07-26 18:11:40,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063900000 ms.0 from job set of time 1501063900000 ms
2017-07-26 18:11:40,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1998 from persistence list
2017-07-26 18:11:40,088 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1998
2017-07-26 18:11:40,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:40,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063896000 ms
2017-07-26 18:11:40,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1501063900000 ms (execution: 0.071 s)
2017-07-26 18:11:42,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063902000 ms
2017-07-26 18:11:42,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063902000 ms.0 from job set of time 1501063902000 ms
2017-07-26 18:11:42,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:42,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2000 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:42,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2000 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:42,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2000 (KafkaRDD[2000] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:42,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2000 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:11:42,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2000_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.3 MB)
2017-07-26 18:11:42,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2000_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:42,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2000 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:42,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2000 (KafkaRDD[2000] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:42,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2000.0 with 2 tasks
2017-07-26 18:11:42,064 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2000.0 (TID 4000, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:42,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2000.0 (TID 4001, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:42,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2000.0 (TID 4001)
2017-07-26 18:11:42,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2000.0 (TID 4000)
2017-07-26 18:11:42,077 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1986_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:42,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:42,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:42,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2000.0 (TID 4000). 635 bytes result sent to driver
2017-07-26 18:11:42,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2000.0 (TID 4001). 635 bytes result sent to driver
2017-07-26 18:11:42,082 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1987_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:42,083 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2000.0 (TID 4000) in 20 ms on localhost (1/2)
2017-07-26 18:11:42,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2000.0 (TID 4001) in 20 ms on localhost (2/2)
2017-07-26 18:11:42,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2000 (foreachPartition at streamingProcessTest.scala:48) finished in 0.021 s
2017-07-26 18:11:42,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2000.0, whose tasks have all completed, from pool 
2017-07-26 18:11:42,084 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1988_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:42,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2000 finished: foreachPartition at streamingProcessTest.scala:48, took 0.035418 s
2017-07-26 18:11:42,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063902000 ms.0 from job set of time 1501063902000 ms
2017-07-26 18:11:42,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1999 from persistence list
2017-07-26 18:11:42,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1501063902000 ms (execution: 0.068 s)
2017-07-26 18:11:42,086 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1999
2017-07-26 18:11:42,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:42,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063898000 ms
2017-07-26 18:11:42,087 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1989_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:11:42,088 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1990_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:42,090 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1991_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:42,091 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1992_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:42,093 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1993_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:42,095 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1994_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:42,097 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1995_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:42,098 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1996_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:42,100 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1997_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:42,101 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1998_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:42,103 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1999_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:44,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063904000 ms
2017-07-26 18:11:44,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063904000 ms.0 from job set of time 1501063904000 ms
2017-07-26 18:11:44,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:44,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2001 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:44,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2001 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:44,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:44,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:44,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2001 (KafkaRDD[2001] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:44,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2001 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:11:44,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2001_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:11:44,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2001_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:44,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2001 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:44,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2001 (KafkaRDD[2001] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:44,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2001.0 with 2 tasks
2017-07-26 18:11:44,033 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2001.0 (TID 4002, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:44,033 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2001.0 (TID 4003, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:44,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2001.0 (TID 4002)
2017-07-26 18:11:44,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2001.0 (TID 4003)
2017-07-26 18:11:44,035 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:44,035 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:44,037 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2001.0 (TID 4003). 635 bytes result sent to driver
2017-07-26 18:11:44,037 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2001.0 (TID 4002). 714 bytes result sent to driver
2017-07-26 18:11:44,038 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2001.0 (TID 4003) in 5 ms on localhost (1/2)
2017-07-26 18:11:44,038 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2001.0 (TID 4002) in 6 ms on localhost (2/2)
2017-07-26 18:11:44,038 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2001.0, whose tasks have all completed, from pool 
2017-07-26 18:11:44,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2001 (foreachPartition at streamingProcessTest.scala:48) finished in 0.006 s
2017-07-26 18:11:44,039 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2001 finished: foreachPartition at streamingProcessTest.scala:48, took 0.015299 s
2017-07-26 18:11:44,039 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063904000 ms.0 from job set of time 1501063904000 ms
2017-07-26 18:11:44,039 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.039 s for time 1501063904000 ms (execution: 0.025 s)
2017-07-26 18:11:44,039 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2000 from persistence list
2017-07-26 18:11:44,039 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2000
2017-07-26 18:11:44,040 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:44,040 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063900000 ms
2017-07-26 18:11:46,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063906000 ms
2017-07-26 18:11:46,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063906000 ms.0 from job set of time 1501063906000 ms
2017-07-26 18:11:46,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:46,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2002 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:46,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2002 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:46,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:46,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:46,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2002 (KafkaRDD[2002] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:46,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2002 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:11:46,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2002_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:11:46,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2002_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:46,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2002 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:46,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2002 (KafkaRDD[2002] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:46,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2002.0 with 2 tasks
2017-07-26 18:11:46,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2002.0 (TID 4004, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:46,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2002.0 (TID 4005, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:46,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2002.0 (TID 4005)
2017-07-26 18:11:46,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2002.0 (TID 4004)
2017-07-26 18:11:46,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:46,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:46,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2002.0 (TID 4005). 635 bytes result sent to driver
2017-07-26 18:11:46,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2002.0 (TID 4004). 714 bytes result sent to driver
2017-07-26 18:11:46,062 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2002.0 (TID 4005) in 8 ms on localhost (1/2)
2017-07-26 18:11:46,062 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2002.0 (TID 4004) in 9 ms on localhost (2/2)
2017-07-26 18:11:46,062 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2002.0, whose tasks have all completed, from pool 
2017-07-26 18:11:46,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2002 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:11:46,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2002 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023137 s
2017-07-26 18:11:46,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063906000 ms.0 from job set of time 1501063906000 ms
2017-07-26 18:11:46,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1501063906000 ms (execution: 0.047 s)
2017-07-26 18:11:46,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2001 from persistence list
2017-07-26 18:11:46,064 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2001
2017-07-26 18:11:46,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:46,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063902000 ms
2017-07-26 18:11:48,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063908000 ms
2017-07-26 18:11:48,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063908000 ms.0 from job set of time 1501063908000 ms
2017-07-26 18:11:48,055 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:48,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2003 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:48,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2003 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:48,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:48,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:48,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2003 (KafkaRDD[2003] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:48,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2003 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:11:48,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2003_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:11:48,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2003_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:11:48,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2003 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:48,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2003 (KafkaRDD[2003] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:48,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2003.0 with 2 tasks
2017-07-26 18:11:48,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2003.0 (TID 4006, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:48,073 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2003.0 (TID 4007, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:48,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2003.0 (TID 4007)
2017-07-26 18:11:48,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2003.0 (TID 4006)
2017-07-26 18:11:48,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:48,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:48,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2003.0 (TID 4006). 635 bytes result sent to driver
2017-07-26 18:11:48,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2003.0 (TID 4007). 635 bytes result sent to driver
2017-07-26 18:11:48,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2003.0 (TID 4007) in 11 ms on localhost (1/2)
2017-07-26 18:11:48,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2003.0 (TID 4006) in 13 ms on localhost (2/2)
2017-07-26 18:11:48,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2003.0, whose tasks have all completed, from pool 
2017-07-26 18:11:48,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2003 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:11:48,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2003 finished: foreachPartition at streamingProcessTest.scala:48, took 0.029208 s
2017-07-26 18:11:48,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063908000 ms.0 from job set of time 1501063908000 ms
2017-07-26 18:11:48,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2002 from persistence list
2017-07-26 18:11:48,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1501063908000 ms (execution: 0.063 s)
2017-07-26 18:11:48,086 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2002
2017-07-26 18:11:48,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:48,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063904000 ms
2017-07-26 18:11:50,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063910000 ms
2017-07-26 18:11:50,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063910000 ms.0 from job set of time 1501063910000 ms
2017-07-26 18:11:50,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2004 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2004 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2004 (KafkaRDD[2004] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2004 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:11:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2004_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:11:50,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2004_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2004 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2004 (KafkaRDD[2004] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2004.0 with 2 tasks
2017-07-26 18:11:50,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2004.0 (TID 4008, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:50,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2004.0 (TID 4009, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:50,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2004.0 (TID 4008)
2017-07-26 18:11:50,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2004.0 (TID 4009)
2017-07-26 18:11:50,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:50,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:50,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2004.0 (TID 4008). 714 bytes result sent to driver
2017-07-26 18:11:50,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2004.0 (TID 4009). 714 bytes result sent to driver
2017-07-26 18:11:50,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2004.0 (TID 4008) in 11 ms on localhost (1/2)
2017-07-26 18:11:50,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2004.0 (TID 4009) in 10 ms on localhost (2/2)
2017-07-26 18:11:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2004 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:11:50,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2004.0, whose tasks have all completed, from pool 
2017-07-26 18:11:50,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2004 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026091 s
2017-07-26 18:11:50,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063910000 ms.0 from job set of time 1501063910000 ms
2017-07-26 18:11:50,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501063910000 ms (execution: 0.054 s)
2017-07-26 18:11:50,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2003 from persistence list
2017-07-26 18:11:50,070 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2003
2017-07-26 18:11:50,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:50,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063906000 ms
2017-07-26 18:11:52,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063912000 ms
2017-07-26 18:11:52,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063912000 ms.0 from job set of time 1501063912000 ms
2017-07-26 18:11:52,021 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:52,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2005 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:52,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2005 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:52,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:52,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:52,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2005 (KafkaRDD[2005] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:52,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2005 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:11:52,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2005_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:11:52,028 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2005_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:52,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2005 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:52,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2005 (KafkaRDD[2005] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:52,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2005.0 with 2 tasks
2017-07-26 18:11:52,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2005.0 (TID 4010, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:52,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2005.0 (TID 4011, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:52,032 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2005.0 (TID 4010)
2017-07-26 18:11:52,032 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2005.0 (TID 4011)
2017-07-26 18:11:52,033 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:52,033 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:52,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2005.0 (TID 4011). 635 bytes result sent to driver
2017-07-26 18:11:52,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2005.0 (TID 4010). 635 bytes result sent to driver
2017-07-26 18:11:52,036 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2005.0 (TID 4011) in 5 ms on localhost (1/2)
2017-07-26 18:11:52,037 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2005.0 (TID 4010) in 8 ms on localhost (2/2)
2017-07-26 18:11:52,037 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2005.0, whose tasks have all completed, from pool 
2017-07-26 18:11:52,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2005 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:11:52,037 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2005 finished: foreachPartition at streamingProcessTest.scala:48, took 0.015635 s
2017-07-26 18:11:52,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063912000 ms.0 from job set of time 1501063912000 ms
2017-07-26 18:11:52,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.037 s for time 1501063912000 ms (execution: 0.025 s)
2017-07-26 18:11:52,038 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2004 from persistence list
2017-07-26 18:11:52,038 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2004
2017-07-26 18:11:52,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:52,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063908000 ms
2017-07-26 18:11:54,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063914000 ms
2017-07-26 18:11:54,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063914000 ms.0 from job set of time 1501063914000 ms
2017-07-26 18:11:54,024 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:54,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2006 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:54,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2006 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:54,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:54,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:54,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2006 (KafkaRDD[2006] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:54,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2006 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:11:54,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2006_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:11:54,032 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2006_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:54,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2006 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:54,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2006 (KafkaRDD[2006] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:54,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2006.0 with 2 tasks
2017-07-26 18:11:54,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2006.0 (TID 4012, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:54,036 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2006.0 (TID 4013, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:54,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2006.0 (TID 4013)
2017-07-26 18:11:54,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2006.0 (TID 4012)
2017-07-26 18:11:54,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:54,038 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:54,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2006.0 (TID 4013). 635 bytes result sent to driver
2017-07-26 18:11:54,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2006.0 (TID 4012). 714 bytes result sent to driver
2017-07-26 18:11:54,040 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2006.0 (TID 4013) in 4 ms on localhost (1/2)
2017-07-26 18:11:54,041 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2006.0 (TID 4012) in 7 ms on localhost (2/2)
2017-07-26 18:11:54,041 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2006.0, whose tasks have all completed, from pool 
2017-07-26 18:11:54,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2006 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:11:54,041 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2006 finished: foreachPartition at streamingProcessTest.scala:48, took 0.017544 s
2017-07-26 18:11:54,042 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063914000 ms.0 from job set of time 1501063914000 ms
2017-07-26 18:11:54,042 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.042 s for time 1501063914000 ms (execution: 0.028 s)
2017-07-26 18:11:54,042 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2005 from persistence list
2017-07-26 18:11:54,042 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2005
2017-07-26 18:11:54,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:54,043 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063910000 ms
2017-07-26 18:11:56,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063916000 ms
2017-07-26 18:11:56,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063916000 ms.0 from job set of time 1501063916000 ms
2017-07-26 18:11:56,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2007 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2007 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2007 (KafkaRDD[2007] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:56,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2007 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:11:56,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2007_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:11:56,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2007_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:56,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2007 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:56,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2007 (KafkaRDD[2007] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:56,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2007.0 with 2 tasks
2017-07-26 18:11:56,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2007.0 (TID 4014, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:56,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2007.0 (TID 4015, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:56,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2007.0 (TID 4015)
2017-07-26 18:11:56,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2007.0 (TID 4014)
2017-07-26 18:11:56,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:56,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:56,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2007.0 (TID 4014). 635 bytes result sent to driver
2017-07-26 18:11:56,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2007.0 (TID 4015). 635 bytes result sent to driver
2017-07-26 18:11:56,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2007.0 (TID 4015) in 8 ms on localhost (1/2)
2017-07-26 18:11:56,062 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2007.0 (TID 4014) in 9 ms on localhost (2/2)
2017-07-26 18:11:56,062 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2007.0, whose tasks have all completed, from pool 
2017-07-26 18:11:56,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2007 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:11:56,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2007 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019429 s
2017-07-26 18:11:56,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063916000 ms.0 from job set of time 1501063916000 ms
2017-07-26 18:11:56,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1501063916000 ms (execution: 0.045 s)
2017-07-26 18:11:56,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2006 from persistence list
2017-07-26 18:11:56,064 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2006
2017-07-26 18:11:56,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:56,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063912000 ms
2017-07-26 18:11:58,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063918000 ms
2017-07-26 18:11:58,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063918000 ms.0 from job set of time 1501063918000 ms
2017-07-26 18:11:58,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:11:58,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2008 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:11:58,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2008 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:11:58,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:11:58,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:11:58,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2008 (KafkaRDD[2008] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:11:58,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2008 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:11:58,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2008_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:11:58,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2008_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:11:58,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2008 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:11:58,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2008 (KafkaRDD[2008] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:11:58,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2008.0 with 2 tasks
2017-07-26 18:11:58,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2008.0 (TID 4016, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:11:58,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2008.0 (TID 4017, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:11:58,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2008.0 (TID 4016)
2017-07-26 18:11:58,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2008.0 (TID 4017)
2017-07-26 18:11:58,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:11:58,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:11:58,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2008.0 (TID 4016). 635 bytes result sent to driver
2017-07-26 18:11:58,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2008.0 (TID 4017). 714 bytes result sent to driver
2017-07-26 18:11:58,089 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2008.0 (TID 4016) in 18 ms on localhost (1/2)
2017-07-26 18:11:58,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2008.0 (TID 4017) in 16 ms on localhost (2/2)
2017-07-26 18:11:58,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2008.0, whose tasks have all completed, from pool 
2017-07-26 18:11:58,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2008 (foreachPartition at streamingProcessTest.scala:48) finished in 0.019 s
2017-07-26 18:11:58,091 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2008 finished: foreachPartition at streamingProcessTest.scala:48, took 0.041686 s
2017-07-26 18:11:58,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063918000 ms.0 from job set of time 1501063918000 ms
2017-07-26 18:11:58,092 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2007 from persistence list
2017-07-26 18:11:58,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1501063918000 ms (execution: 0.075 s)
2017-07-26 18:11:58,093 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2007
2017-07-26 18:11:58,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:11:58,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063914000 ms
2017-07-26 18:12:00,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063920000 ms
2017-07-26 18:12:00,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063920000 ms.0 from job set of time 1501063920000 ms
2017-07-26 18:12:00,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2009 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2009 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:00,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2009 (KafkaRDD[2009] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2009 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:12:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2009_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:12:00,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2009_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:00,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2009 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2009 (KafkaRDD[2009] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2009.0 with 2 tasks
2017-07-26 18:12:00,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2009.0 (TID 4018, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:00,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2009.0 (TID 4019, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:00,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2009.0 (TID 4018)
2017-07-26 18:12:00,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2009.0 (TID 4019)
2017-07-26 18:12:00,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:00,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:00,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2009.0 (TID 4019). 635 bytes result sent to driver
2017-07-26 18:12:00,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2009.0 (TID 4018). 635 bytes result sent to driver
2017-07-26 18:12:00,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2009.0 (TID 4018) in 9 ms on localhost (1/2)
2017-07-26 18:12:00,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2009.0 (TID 4019) in 8 ms on localhost (2/2)
2017-07-26 18:12:00,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2009.0, whose tasks have all completed, from pool 
2017-07-26 18:12:00,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2009 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:12:00,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2009 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025675 s
2017-07-26 18:12:00,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063920000 ms.0 from job set of time 1501063920000 ms
2017-07-26 18:12:00,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1501063920000 ms (execution: 0.051 s)
2017-07-26 18:12:00,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2008 from persistence list
2017-07-26 18:12:00,068 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2008
2017-07-26 18:12:00,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:00,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063916000 ms
2017-07-26 18:12:02,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063922000 ms
2017-07-26 18:12:02,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063922000 ms.0 from job set of time 1501063922000 ms
2017-07-26 18:12:02,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2010 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2010 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:02,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2010 (KafkaRDD[2010] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:02,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2010 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:12:02,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2010_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:12:02,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2010_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:02,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2010 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:02,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2010 (KafkaRDD[2010] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:02,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2010.0 with 2 tasks
2017-07-26 18:12:02,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2010.0 (TID 4020, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:02,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2010.0 (TID 4021, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:02,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2010.0 (TID 4021)
2017-07-26 18:12:02,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2010.0 (TID 4020)
2017-07-26 18:12:02,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:02,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:02,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2010.0 (TID 4021). 635 bytes result sent to driver
2017-07-26 18:12:02,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2010.0 (TID 4020). 635 bytes result sent to driver
2017-07-26 18:12:02,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2010.0 (TID 4020) in 8 ms on localhost (1/2)
2017-07-26 18:12:02,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2010.0 (TID 4021) in 7 ms on localhost (2/2)
2017-07-26 18:12:02,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2010.0, whose tasks have all completed, from pool 
2017-07-26 18:12:02,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2010 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:12:02,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2010 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028146 s
2017-07-26 18:12:02,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063922000 ms.0 from job set of time 1501063922000 ms
2017-07-26 18:12:02,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2009 from persistence list
2017-07-26 18:12:02,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1501063922000 ms (execution: 0.058 s)
2017-07-26 18:12:02,078 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2009
2017-07-26 18:12:02,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:02,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063918000 ms
2017-07-26 18:12:04,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063924000 ms
2017-07-26 18:12:04,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063924000 ms.0 from job set of time 1501063924000 ms
2017-07-26 18:12:04,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:04,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2011 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:04,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2011 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:04,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:04,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:04,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2011 (KafkaRDD[2011] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:04,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2011 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:12:04,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2011_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:12:04,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2011_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:04,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2011 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:04,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2011 (KafkaRDD[2011] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:04,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2011.0 with 2 tasks
2017-07-26 18:12:04,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2011.0 (TID 4022, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:04,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2011.0 (TID 4023, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:04,037 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2011.0 (TID 4023)
2017-07-26 18:12:04,037 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2011.0 (TID 4022)
2017-07-26 18:12:04,039 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:04,039 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:04,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2011.0 (TID 4023). 635 bytes result sent to driver
2017-07-26 18:12:04,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2011.0 (TID 4022). 635 bytes result sent to driver
2017-07-26 18:12:04,043 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2011.0 (TID 4023) in 6 ms on localhost (1/2)
2017-07-26 18:12:04,043 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2011.0 (TID 4022) in 7 ms on localhost (2/2)
2017-07-26 18:12:04,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2011.0, whose tasks have all completed, from pool 
2017-07-26 18:12:04,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2011 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:12:04,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2011 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018321 s
2017-07-26 18:12:04,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063924000 ms.0 from job set of time 1501063924000 ms
2017-07-26 18:12:04,045 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2010 from persistence list
2017-07-26 18:12:04,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.045 s for time 1501063924000 ms (execution: 0.031 s)
2017-07-26 18:12:04,046 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2010
2017-07-26 18:12:04,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:04,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063920000 ms
2017-07-26 18:12:06,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063926000 ms
2017-07-26 18:12:06,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063926000 ms.0 from job set of time 1501063926000 ms
2017-07-26 18:12:06,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:06,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2012 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:06,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2012 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:06,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:06,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:06,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2012 (KafkaRDD[2012] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:06,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2012 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:12:06,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2012_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:12:06,053 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2012_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:06,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2012 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:06,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2012 (KafkaRDD[2012] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:06,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2012.0 with 2 tasks
2017-07-26 18:12:06,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2012.0 (TID 4024, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:06,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2012.0 (TID 4025, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:06,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2012.0 (TID 4025)
2017-07-26 18:12:06,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2012.0 (TID 4024)
2017-07-26 18:12:06,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:06,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:06,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2012.0 (TID 4025). 714 bytes result sent to driver
2017-07-26 18:12:06,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2012.0 (TID 4024). 714 bytes result sent to driver
2017-07-26 18:12:06,063 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2012.0 (TID 4025) in 7 ms on localhost (1/2)
2017-07-26 18:12:06,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2012.0 (TID 4024) in 8 ms on localhost (2/2)
2017-07-26 18:12:06,064 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2012.0, whose tasks have all completed, from pool 
2017-07-26 18:12:06,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2012 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:12:06,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2012 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022940 s
2017-07-26 18:12:06,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063926000 ms.0 from job set of time 1501063926000 ms
2017-07-26 18:12:06,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1501063926000 ms (execution: 0.047 s)
2017-07-26 18:12:06,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2011 from persistence list
2017-07-26 18:12:06,065 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2011
2017-07-26 18:12:06,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:06,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063922000 ms
2017-07-26 18:12:08,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063928000 ms
2017-07-26 18:12:08,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063928000 ms.0 from job set of time 1501063928000 ms
2017-07-26 18:12:08,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:08,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2013 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:08,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2013 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:08,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2013 (KafkaRDD[2013] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:08,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2013 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:12:08,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2013_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:12:08,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2013_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:08,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2013 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:08,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2013 (KafkaRDD[2013] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:08,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2013.0 with 2 tasks
2017-07-26 18:12:08,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2013.0 (TID 4026, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:08,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2013.0 (TID 4027, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:08,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2013.0 (TID 4027)
2017-07-26 18:12:08,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2013.0 (TID 4026)
2017-07-26 18:12:08,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:08,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:08,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2013.0 (TID 4026). 635 bytes result sent to driver
2017-07-26 18:12:08,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2013.0 (TID 4027). 714 bytes result sent to driver
2017-07-26 18:12:08,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2013.0 (TID 4026) in 9 ms on localhost (1/2)
2017-07-26 18:12:08,080 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2013.0 (TID 4027) in 9 ms on localhost (2/2)
2017-07-26 18:12:08,080 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2013.0, whose tasks have all completed, from pool 
2017-07-26 18:12:08,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2013 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:12:08,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2013 finished: foreachPartition at streamingProcessTest.scala:48, took 0.033771 s
2017-07-26 18:12:08,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063928000 ms.0 from job set of time 1501063928000 ms
2017-07-26 18:12:08,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2012 from persistence list
2017-07-26 18:12:08,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501063928000 ms (execution: 0.064 s)
2017-07-26 18:12:08,082 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2012
2017-07-26 18:12:08,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:08,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063924000 ms
2017-07-26 18:12:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063930000 ms
2017-07-26 18:12:10,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063930000 ms.0 from job set of time 1501063930000 ms
2017-07-26 18:12:10,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:10,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2014 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:10,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2014 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:10,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:10,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2014 (KafkaRDD[2014] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2014 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:12:10,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2014_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.3 MB)
2017-07-26 18:12:10,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2014_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:10,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2000_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:10,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2014 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:10,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2014 (KafkaRDD[2014] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:10,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2014.0 with 2 tasks
2017-07-26 18:12:10,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2014.0 (TID 4028, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:10,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2001_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:10,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2014.0 (TID 4029, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:10,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2014.0 (TID 4028)
2017-07-26 18:12:10,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2014.0 (TID 4029)
2017-07-26 18:12:10,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2002_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:10,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:10,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:10,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2014.0 (TID 4028). 714 bytes result sent to driver
2017-07-26 18:12:10,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2014.0 (TID 4029). 635 bytes result sent to driver
2017-07-26 18:12:10,079 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2003_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:10,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2014.0 (TID 4028) in 10 ms on localhost (1/2)
2017-07-26 18:12:10,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2014.0 (TID 4029) in 9 ms on localhost (2/2)
2017-07-26 18:12:10,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2014.0, whose tasks have all completed, from pool 
2017-07-26 18:12:10,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2014 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:12:10,080 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2004_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:10,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2014 finished: foreachPartition at streamingProcessTest.scala:48, took 0.033839 s
2017-07-26 18:12:10,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063930000 ms.0 from job set of time 1501063930000 ms
2017-07-26 18:12:10,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501063930000 ms (execution: 0.065 s)
2017-07-26 18:12:10,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2013 from persistence list
2017-07-26 18:12:10,081 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2013
2017-07-26 18:12:10,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:10,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063926000 ms
2017-07-26 18:12:10,082 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2005_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:10,083 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2006_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:10,084 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2007_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:10,086 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2008_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:10,087 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2009_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:10,088 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2010_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:10,090 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2011_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:10,091 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2012_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:10,092 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2013_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:12,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063932000 ms
2017-07-26 18:12:12,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063932000 ms.0 from job set of time 1501063932000 ms
2017-07-26 18:12:12,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:12,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2015 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:12,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2015 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:12,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:12,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:12,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2015 (KafkaRDD[2015] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:12,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2015 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:12:12,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2015_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:12:12,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2015_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:12,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2015 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:12,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2015 (KafkaRDD[2015] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:12,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2015.0 with 2 tasks
2017-07-26 18:12:12,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2015.0 (TID 4030, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:12,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2015.0 (TID 4031, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:12,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2015.0 (TID 4031)
2017-07-26 18:12:12,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2015.0 (TID 4030)
2017-07-26 18:12:12,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:12,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:12,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2015.0 (TID 4031). 635 bytes result sent to driver
2017-07-26 18:12:12,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2015.0 (TID 4030). 722 bytes result sent to driver
2017-07-26 18:12:12,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2015.0 (TID 4031) in 8 ms on localhost (1/2)
2017-07-26 18:12:12,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2015.0 (TID 4030) in 9 ms on localhost (2/2)
2017-07-26 18:12:12,073 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2015.0, whose tasks have all completed, from pool 
2017-07-26 18:12:12,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2015 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:12:12,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2015 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026119 s
2017-07-26 18:12:12,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063932000 ms.0 from job set of time 1501063932000 ms
2017-07-26 18:12:12,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1501063932000 ms (execution: 0.058 s)
2017-07-26 18:12:12,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2014 from persistence list
2017-07-26 18:12:12,074 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2014
2017-07-26 18:12:12,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:12,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063928000 ms
2017-07-26 18:12:14,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063934000 ms
2017-07-26 18:12:14,011 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063934000 ms.0 from job set of time 1501063934000 ms
2017-07-26 18:12:14,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2016 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2016 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2016 (KafkaRDD[2016] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:14,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2016 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:12:14,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2016_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:12:14,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2016_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:14,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2016 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:14,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2016 (KafkaRDD[2016] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:14,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2016.0 with 2 tasks
2017-07-26 18:12:14,033 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2016.0 (TID 4032, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:14,034 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2016.0 (TID 4033, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:14,034 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2016.0 (TID 4033)
2017-07-26 18:12:14,034 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2016.0 (TID 4032)
2017-07-26 18:12:14,036 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:14,036 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:14,037 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2016.0 (TID 4033). 635 bytes result sent to driver
2017-07-26 18:12:14,037 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2016.0 (TID 4032). 635 bytes result sent to driver
2017-07-26 18:12:14,039 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2016.0 (TID 4033) in 5 ms on localhost (1/2)
2017-07-26 18:12:14,039 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2016.0 (TID 4032) in 6 ms on localhost (2/2)
2017-07-26 18:12:14,039 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2016.0, whose tasks have all completed, from pool 
2017-07-26 18:12:14,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2016 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:12:14,039 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2016 finished: foreachPartition at streamingProcessTest.scala:48, took 0.015552 s
2017-07-26 18:12:14,039 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063934000 ms.0 from job set of time 1501063934000 ms
2017-07-26 18:12:14,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.039 s for time 1501063934000 ms (execution: 0.028 s)
2017-07-26 18:12:14,040 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2015 from persistence list
2017-07-26 18:12:14,040 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2015
2017-07-26 18:12:14,040 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:14,040 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063930000 ms
2017-07-26 18:12:16,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063936000 ms
2017-07-26 18:12:16,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063936000 ms.0 from job set of time 1501063936000 ms
2017-07-26 18:12:16,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2017 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2017 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:16,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2017 (KafkaRDD[2017] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:16,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2017 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:12:16,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2017_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:12:16,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2017_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:16,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2017 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:16,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2017 (KafkaRDD[2017] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:16,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2017.0 with 2 tasks
2017-07-26 18:12:16,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2017.0 (TID 4034, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:16,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2017.0 (TID 4035, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:16,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2017.0 (TID 4035)
2017-07-26 18:12:16,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2017.0 (TID 4034)
2017-07-26 18:12:16,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:16,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:16,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2017.0 (TID 4035). 714 bytes result sent to driver
2017-07-26 18:12:16,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2017.0 (TID 4034). 714 bytes result sent to driver
2017-07-26 18:12:16,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2017.0 (TID 4035) in 9 ms on localhost (1/2)
2017-07-26 18:12:16,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2017.0 (TID 4034) in 12 ms on localhost (2/2)
2017-07-26 18:12:16,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2017.0, whose tasks have all completed, from pool 
2017-07-26 18:12:16,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2017 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:12:16,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2017 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027264 s
2017-07-26 18:12:16,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063936000 ms.0 from job set of time 1501063936000 ms
2017-07-26 18:12:16,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501063936000 ms (execution: 0.060 s)
2017-07-26 18:12:16,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2016 from persistence list
2017-07-26 18:12:16,077 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2016
2017-07-26 18:12:16,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:16,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063932000 ms
2017-07-26 18:12:18,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063938000 ms
2017-07-26 18:12:18,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063938000 ms.0 from job set of time 1501063938000 ms
2017-07-26 18:12:18,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:18,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2018 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2018 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2018 (KafkaRDD[2018] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:18,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2018 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:12:18,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2018_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:12:18,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2018_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:18,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2018 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:18,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2018 (KafkaRDD[2018] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:18,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2018.0 with 2 tasks
2017-07-26 18:12:18,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2018.0 (TID 4036, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:18,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2018.0 (TID 4037, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:18,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2018.0 (TID 4037)
2017-07-26 18:12:18,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2018.0 (TID 4036)
2017-07-26 18:12:18,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:18,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:18,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2018.0 (TID 4036). 635 bytes result sent to driver
2017-07-26 18:12:18,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2018.0 (TID 4037). 635 bytes result sent to driver
2017-07-26 18:12:18,087 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2018.0 (TID 4036) in 17 ms on localhost (1/2)
2017-07-26 18:12:18,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2018.0 (TID 4037) in 15 ms on localhost (2/2)
2017-07-26 18:12:18,088 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2018.0, whose tasks have all completed, from pool 
2017-07-26 18:12:18,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2018 (foreachPartition at streamingProcessTest.scala:48) finished in 0.018 s
2017-07-26 18:12:18,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2018 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040932 s
2017-07-26 18:12:18,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063938000 ms.0 from job set of time 1501063938000 ms
2017-07-26 18:12:18,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1501063938000 ms (execution: 0.072 s)
2017-07-26 18:12:18,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2017 from persistence list
2017-07-26 18:12:18,090 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2017
2017-07-26 18:12:18,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:18,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063934000 ms
2017-07-26 18:12:20,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063940000 ms
2017-07-26 18:12:20,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063940000 ms.0 from job set of time 1501063940000 ms
2017-07-26 18:12:20,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2019 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2019 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2019 (KafkaRDD[2019] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2019 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:12:20,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2019_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:12:20,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2019_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2019 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2019 (KafkaRDD[2019] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:20,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2019.0 with 2 tasks
2017-07-26 18:12:20,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2019.0 (TID 4038, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:20,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2019.0 (TID 4039, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:20,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2019.0 (TID 4039)
2017-07-26 18:12:20,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2019.0 (TID 4038)
2017-07-26 18:12:20,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:20,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:20,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2019.0 (TID 4038). 635 bytes result sent to driver
2017-07-26 18:12:20,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2019.0 (TID 4039). 714 bytes result sent to driver
2017-07-26 18:12:20,065 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2019.0 (TID 4038) in 10 ms on localhost (1/2)
2017-07-26 18:12:20,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2019.0 (TID 4039) in 10 ms on localhost (2/2)
2017-07-26 18:12:20,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2019.0, whose tasks have all completed, from pool 
2017-07-26 18:12:20,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2019 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:12:20,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2019 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022986 s
2017-07-26 18:12:20,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063940000 ms.0 from job set of time 1501063940000 ms
2017-07-26 18:12:20,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1501063940000 ms (execution: 0.049 s)
2017-07-26 18:12:20,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2018 from persistence list
2017-07-26 18:12:20,068 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2018
2017-07-26 18:12:20,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:20,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063936000 ms
2017-07-26 18:12:22,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063942000 ms
2017-07-26 18:12:22,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063942000 ms.0 from job set of time 1501063942000 ms
2017-07-26 18:12:22,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2020 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2020 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:22,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2020 (KafkaRDD[2020] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:22,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2020 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:12:22,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2020_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:12:22,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2020_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:22,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2020 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:22,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2020 (KafkaRDD[2020] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:22,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2020.0 with 2 tasks
2017-07-26 18:12:22,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2020.0 (TID 4040, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:22,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2020.0 (TID 4041, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:22,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2020.0 (TID 4041)
2017-07-26 18:12:22,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2020.0 (TID 4040)
2017-07-26 18:12:22,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:22,066 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:22,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2020.0 (TID 4041). 635 bytes result sent to driver
2017-07-26 18:12:22,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2020.0 (TID 4040). 635 bytes result sent to driver
2017-07-26 18:12:22,072 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2020.0 (TID 4040) in 12 ms on localhost (1/2)
2017-07-26 18:12:22,072 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2020.0 (TID 4041) in 10 ms on localhost (2/2)
2017-07-26 18:12:22,072 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2020.0, whose tasks have all completed, from pool 
2017-07-26 18:12:22,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2020 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:12:22,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2020 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026687 s
2017-07-26 18:12:22,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063942000 ms.0 from job set of time 1501063942000 ms
2017-07-26 18:12:22,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1501063942000 ms (execution: 0.057 s)
2017-07-26 18:12:22,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2019 from persistence list
2017-07-26 18:12:22,075 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2019
2017-07-26 18:12:22,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:22,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063938000 ms
2017-07-26 18:12:24,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063944000 ms
2017-07-26 18:12:24,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063944000 ms.0 from job set of time 1501063944000 ms
2017-07-26 18:12:24,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:24,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2021 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:24,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2021 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:24,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:24,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:24,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2021 (KafkaRDD[2021] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:24,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2021 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:12:24,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2021_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:12:24,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2021_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:24,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2021 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:24,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2021 (KafkaRDD[2021] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:24,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2021.0 with 2 tasks
2017-07-26 18:12:24,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2021.0 (TID 4042, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:24,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2021.0 (TID 4043, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:24,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2021.0 (TID 4043)
2017-07-26 18:12:24,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2021.0 (TID 4042)
2017-07-26 18:12:24,039 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:24,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:24,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2021.0 (TID 4042). 635 bytes result sent to driver
2017-07-26 18:12:24,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2021.0 (TID 4043). 714 bytes result sent to driver
2017-07-26 18:12:24,042 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2021.0 (TID 4042) in 5 ms on localhost (1/2)
2017-07-26 18:12:24,042 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2021.0 (TID 4043) in 4 ms on localhost (2/2)
2017-07-26 18:12:24,042 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2021.0, whose tasks have all completed, from pool 
2017-07-26 18:12:24,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2021 (foreachPartition at streamingProcessTest.scala:48) finished in 0.005 s
2017-07-26 18:12:24,043 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2021 finished: foreachPartition at streamingProcessTest.scala:48, took 0.013654 s
2017-07-26 18:12:24,043 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063944000 ms.0 from job set of time 1501063944000 ms
2017-07-26 18:12:24,043 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.043 s for time 1501063944000 ms (execution: 0.029 s)
2017-07-26 18:12:24,043 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2020 from persistence list
2017-07-26 18:12:24,044 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2020
2017-07-26 18:12:24,044 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:24,044 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063940000 ms
2017-07-26 18:12:26,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063946000 ms
2017-07-26 18:12:26,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063946000 ms.0 from job set of time 1501063946000 ms
2017-07-26 18:12:26,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:26,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2022 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:26,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2022 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:26,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:26,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:26,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2022 (KafkaRDD[2022] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:26,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2022 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:12:26,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2022_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:12:26,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2022_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:26,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2022 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:26,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2022 (KafkaRDD[2022] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:26,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2022.0 with 2 tasks
2017-07-26 18:12:26,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2022.0 (TID 4044, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:26,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2022.0 (TID 4045, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:26,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2022.0 (TID 4044)
2017-07-26 18:12:26,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2022.0 (TID 4045)
2017-07-26 18:12:26,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:26,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:26,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2022.0 (TID 4045). 635 bytes result sent to driver
2017-07-26 18:12:26,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2022.0 (TID 4044). 722 bytes result sent to driver
2017-07-26 18:12:26,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2022.0 (TID 4045) in 11 ms on localhost (1/2)
2017-07-26 18:12:26,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2022.0 (TID 4044) in 12 ms on localhost (2/2)
2017-07-26 18:12:26,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2022.0, whose tasks have all completed, from pool 
2017-07-26 18:12:26,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2022 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:12:26,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2022 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027619 s
2017-07-26 18:12:26,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063946000 ms.0 from job set of time 1501063946000 ms
2017-07-26 18:12:26,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2021 from persistence list
2017-07-26 18:12:26,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1501063946000 ms (execution: 0.057 s)
2017-07-26 18:12:26,074 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2021
2017-07-26 18:12:26,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:26,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063942000 ms
2017-07-26 18:12:28,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063948000 ms
2017-07-26 18:12:28,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063948000 ms.0 from job set of time 1501063948000 ms
2017-07-26 18:12:28,130 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:28,131 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2023 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:28,132 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2023 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:28,132 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:28,132 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:28,133 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2023 (KafkaRDD[2023] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:28,138 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2023 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:12:28,143 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2023_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:12:28,144 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2023_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:28,144 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2023 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:28,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2023 (KafkaRDD[2023] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:28,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2023.0 with 2 tasks
2017-07-26 18:12:28,146 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2023.0 (TID 4046, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:28,147 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2023.0 (TID 4047, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:28,147 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2023.0 (TID 4046)
2017-07-26 18:12:28,147 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2023.0 (TID 4047)
2017-07-26 18:12:28,150 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:28,150 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:28,153 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2023.0 (TID 4047). 635 bytes result sent to driver
2017-07-26 18:12:28,153 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2023.0 (TID 4046). 635 bytes result sent to driver
2017-07-26 18:12:28,156 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2023.0 (TID 4046) in 11 ms on localhost (1/2)
2017-07-26 18:12:28,157 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2023.0 (TID 4047) in 10 ms on localhost (2/2)
2017-07-26 18:12:28,157 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2023.0, whose tasks have all completed, from pool 
2017-07-26 18:12:28,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2023 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:12:28,157 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2023 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026830 s
2017-07-26 18:12:28,158 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063948000 ms.0 from job set of time 1501063948000 ms
2017-07-26 18:12:28,158 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.158 s for time 1501063948000 ms (execution: 0.142 s)
2017-07-26 18:12:28,158 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2022 from persistence list
2017-07-26 18:12:28,158 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2022
2017-07-26 18:12:28,159 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:28,159 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063944000 ms
2017-07-26 18:12:30,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063950000 ms
2017-07-26 18:12:30,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063950000 ms.0 from job set of time 1501063950000 ms
2017-07-26 18:12:30,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2024 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2024 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2024 (KafkaRDD[2024] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:30,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2024 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:12:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2024_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:12:30,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2024_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2024 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2024 (KafkaRDD[2024] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2024.0 with 2 tasks
2017-07-26 18:12:30,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2024.0 (TID 4048, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:30,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2024.0 (TID 4049, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:30,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2024.0 (TID 4049)
2017-07-26 18:12:30,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2024.0 (TID 4048)
2017-07-26 18:12:30,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:30,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:30,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2024.0 (TID 4048). 714 bytes result sent to driver
2017-07-26 18:12:30,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2024.0 (TID 4049). 714 bytes result sent to driver
2017-07-26 18:12:30,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2024.0 (TID 4048) in 9 ms on localhost (1/2)
2017-07-26 18:12:30,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2024.0 (TID 4049) in 8 ms on localhost (2/2)
2017-07-26 18:12:30,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2024.0, whose tasks have all completed, from pool 
2017-07-26 18:12:30,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2024 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:12:30,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2024 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024451 s
2017-07-26 18:12:30,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063950000 ms.0 from job set of time 1501063950000 ms
2017-07-26 18:12:30,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501063950000 ms (execution: 0.056 s)
2017-07-26 18:12:30,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2023 from persistence list
2017-07-26 18:12:30,076 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2023
2017-07-26 18:12:30,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:30,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063946000 ms
2017-07-26 18:12:32,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063952000 ms
2017-07-26 18:12:32,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063952000 ms.0 from job set of time 1501063952000 ms
2017-07-26 18:12:32,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:32,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2025 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:32,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2025 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:32,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:32,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:32,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2025 (KafkaRDD[2025] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:32,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2025 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:12:32,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2025_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:12:32,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2025_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:32,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2025 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:32,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2025 (KafkaRDD[2025] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:32,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2025.0 with 2 tasks
2017-07-26 18:12:32,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2025.0 (TID 4050, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:32,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2025.0 (TID 4051, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:32,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2025.0 (TID 4051)
2017-07-26 18:12:32,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2025.0 (TID 4050)
2017-07-26 18:12:32,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:32,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:32,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2025.0 (TID 4051). 635 bytes result sent to driver
2017-07-26 18:12:32,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2025.0 (TID 4050). 635 bytes result sent to driver
2017-07-26 18:12:32,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2025.0 (TID 4051) in 10 ms on localhost (1/2)
2017-07-26 18:12:32,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2025.0 (TID 4050) in 11 ms on localhost (2/2)
2017-07-26 18:12:32,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2025.0, whose tasks have all completed, from pool 
2017-07-26 18:12:32,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2025 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:12:32,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2025 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024610 s
2017-07-26 18:12:32,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063952000 ms.0 from job set of time 1501063952000 ms
2017-07-26 18:12:32,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501063952000 ms (execution: 0.053 s)
2017-07-26 18:12:32,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2024 from persistence list
2017-07-26 18:12:32,070 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2024
2017-07-26 18:12:32,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:32,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063948000 ms
2017-07-26 18:12:34,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063954000 ms
2017-07-26 18:12:34,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063954000 ms.0 from job set of time 1501063954000 ms
2017-07-26 18:12:34,032 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:34,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2026 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:34,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2026 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:34,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:34,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:34,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2026 (KafkaRDD[2026] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:34,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2026 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:12:34,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2026_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:12:34,043 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2026_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:34,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2026 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:34,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2026 (KafkaRDD[2026] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:34,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2026.0 with 2 tasks
2017-07-26 18:12:34,046 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2026.0 (TID 4052, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:34,047 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2026.0 (TID 4053, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:34,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2026.0 (TID 4053)
2017-07-26 18:12:34,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2026.0 (TID 4052)
2017-07-26 18:12:34,049 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:34,049 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:34,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2026.0 (TID 4052). 635 bytes result sent to driver
2017-07-26 18:12:34,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2026.0 (TID 4053). 714 bytes result sent to driver
2017-07-26 18:12:34,053 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2026.0 (TID 4053) in 6 ms on localhost (1/2)
2017-07-26 18:12:34,053 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2026.0 (TID 4052) in 8 ms on localhost (2/2)
2017-07-26 18:12:34,053 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2026.0, whose tasks have all completed, from pool 
2017-07-26 18:12:34,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2026 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:12:34,053 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2026 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021443 s
2017-07-26 18:12:34,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063954000 ms.0 from job set of time 1501063954000 ms
2017-07-26 18:12:34,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.054 s for time 1501063954000 ms (execution: 0.040 s)
2017-07-26 18:12:34,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2025 from persistence list
2017-07-26 18:12:34,054 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2025
2017-07-26 18:12:34,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:34,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063950000 ms
2017-07-26 18:12:36,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063956000 ms
2017-07-26 18:12:36,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063956000 ms.0 from job set of time 1501063956000 ms
2017-07-26 18:12:36,021 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:36,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2027 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:36,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2027 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:36,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:36,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:36,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2027 (KafkaRDD[2027] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:36,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2027 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:12:36,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2027_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:12:36,027 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2027_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:36,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2027 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:36,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2027 (KafkaRDD[2027] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:36,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2027.0 with 2 tasks
2017-07-26 18:12:36,029 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2027.0 (TID 4054, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:36,029 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2027.0 (TID 4055, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:36,030 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2027.0 (TID 4055)
2017-07-26 18:12:36,030 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2027.0 (TID 4054)
2017-07-26 18:12:36,031 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:36,031 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:36,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2027.0 (TID 4054). 714 bytes result sent to driver
2017-07-26 18:12:36,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2027.0 (TID 4055). 714 bytes result sent to driver
2017-07-26 18:12:36,035 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2027.0 (TID 4054) in 7 ms on localhost (1/2)
2017-07-26 18:12:36,035 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2027.0 (TID 4055) in 6 ms on localhost (2/2)
2017-07-26 18:12:36,035 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2027.0, whose tasks have all completed, from pool 
2017-07-26 18:12:36,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2027 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:12:36,036 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2027 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014516 s
2017-07-26 18:12:36,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063956000 ms.0 from job set of time 1501063956000 ms
2017-07-26 18:12:36,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.036 s for time 1501063956000 ms (execution: 0.023 s)
2017-07-26 18:12:36,036 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2026 from persistence list
2017-07-26 18:12:36,037 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2026
2017-07-26 18:12:36,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:36,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063952000 ms
2017-07-26 18:12:38,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063958000 ms
2017-07-26 18:12:38,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063958000 ms.0 from job set of time 1501063958000 ms
2017-07-26 18:12:38,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:38,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2028 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:38,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2028 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:38,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:38,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:38,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2028 (KafkaRDD[2028] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:38,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2027_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:38,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2028 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:12:38,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2014_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:38,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2015_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:38,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2028_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:12:38,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2016_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:38,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2028_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:38,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2028 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:38,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2028 (KafkaRDD[2028] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:38,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2028.0 with 2 tasks
2017-07-26 18:12:38,077 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2017_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:38,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2028.0 (TID 4056, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:38,078 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2028.0 (TID 4057, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:38,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2028.0 (TID 4057)
2017-07-26 18:12:38,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2028.0 (TID 4056)
2017-07-26 18:12:38,079 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2018_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:38,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:38,080 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:38,081 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2019_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:38,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2028.0 (TID 4056). 635 bytes result sent to driver
2017-07-26 18:12:38,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2028.0 (TID 4057). 635 bytes result sent to driver
2017-07-26 18:12:38,084 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2020_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:38,085 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2028.0 (TID 4057) in 8 ms on localhost (1/2)
2017-07-26 18:12:38,085 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2028.0 (TID 4056) in 8 ms on localhost (2/2)
2017-07-26 18:12:38,086 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2028.0, whose tasks have all completed, from pool 
2017-07-26 18:12:38,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2028 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:12:38,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2028 finished: foreachPartition at streamingProcessTest.scala:48, took 0.039820 s
2017-07-26 18:12:38,086 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2021_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:38,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063958000 ms.0 from job set of time 1501063958000 ms
2017-07-26 18:12:38,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2027 from persistence list
2017-07-26 18:12:38,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:38,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063954000 ms
2017-07-26 18:12:38,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.086 s for time 1501063958000 ms (execution: 0.069 s)
2017-07-26 18:12:38,087 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2027
2017-07-26 18:12:38,088 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2022_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:38,090 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2023_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:38,092 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2024_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:38,094 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2025_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:38,095 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2026_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:40,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063960000 ms
2017-07-26 18:12:40,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063960000 ms.0 from job set of time 1501063960000 ms
2017-07-26 18:12:40,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:40,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2029 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:40,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2029 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:40,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:40,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:40,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2029 (KafkaRDD[2029] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:40,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2029 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:12:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2029_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:12:40,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2029_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:40,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2029 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:40,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2029 (KafkaRDD[2029] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:40,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2029.0 with 2 tasks
2017-07-26 18:12:40,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2029.0 (TID 4058, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:40,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2029.0 (TID 4059, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:40,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2029.0 (TID 4058)
2017-07-26 18:12:40,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2029.0 (TID 4059)
2017-07-26 18:12:40,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:40,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:40,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2029.0 (TID 4058). 714 bytes result sent to driver
2017-07-26 18:12:40,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2029.0 (TID 4059). 714 bytes result sent to driver
2017-07-26 18:12:40,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2029.0 (TID 4058) in 9 ms on localhost (1/2)
2017-07-26 18:12:40,070 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2029.0 (TID 4059) in 9 ms on localhost (2/2)
2017-07-26 18:12:40,070 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2029.0, whose tasks have all completed, from pool 
2017-07-26 18:12:40,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2029 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:12:40,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2029 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020910 s
2017-07-26 18:12:40,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063960000 ms.0 from job set of time 1501063960000 ms
2017-07-26 18:12:40,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1501063960000 ms (execution: 0.051 s)
2017-07-26 18:12:40,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2028 from persistence list
2017-07-26 18:12:40,071 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2028
2017-07-26 18:12:40,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:40,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063956000 ms
2017-07-26 18:12:42,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063962000 ms
2017-07-26 18:12:42,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063962000 ms.0 from job set of time 1501063962000 ms
2017-07-26 18:12:42,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2030 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2030 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:42,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2030 (KafkaRDD[2030] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:42,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2030 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:12:42,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2030_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:12:42,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2030_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:42,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2030 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:42,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2030 (KafkaRDD[2030] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:42,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2030.0 with 2 tasks
2017-07-26 18:12:42,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2030.0 (TID 4060, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:42,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2030.0 (TID 4061, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:42,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2030.0 (TID 4060)
2017-07-26 18:12:42,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2030.0 (TID 4061)
2017-07-26 18:12:42,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:42,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:42,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2030.0 (TID 4061). 635 bytes result sent to driver
2017-07-26 18:12:42,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2030.0 (TID 4060). 635 bytes result sent to driver
2017-07-26 18:12:42,071 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2030.0 (TID 4061) in 7 ms on localhost (1/2)
2017-07-26 18:12:42,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2030.0 (TID 4060) in 8 ms on localhost (2/2)
2017-07-26 18:12:42,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2030.0, whose tasks have all completed, from pool 
2017-07-26 18:12:42,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2030 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:12:42,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2030 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022072 s
2017-07-26 18:12:42,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063962000 ms.0 from job set of time 1501063962000 ms
2017-07-26 18:12:42,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501063962000 ms (execution: 0.054 s)
2017-07-26 18:12:42,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2029 from persistence list
2017-07-26 18:12:42,073 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2029
2017-07-26 18:12:42,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:42,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063958000 ms
2017-07-26 18:12:44,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063964000 ms
2017-07-26 18:12:44,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063964000 ms.0 from job set of time 1501063964000 ms
2017-07-26 18:12:44,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:44,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2031 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:44,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2031 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:44,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:44,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:44,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2031 (KafkaRDD[2031] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:44,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2031 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:12:44,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2031_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:12:44,035 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2031_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:12:44,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2031 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:44,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2031 (KafkaRDD[2031] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:44,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2031.0 with 2 tasks
2017-07-26 18:12:44,037 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2031.0 (TID 4062, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:44,037 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2031.0 (TID 4063, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:44,037 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2031.0 (TID 4062)
2017-07-26 18:12:44,037 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2031.0 (TID 4063)
2017-07-26 18:12:44,040 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:44,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:44,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2031.0 (TID 4062). 635 bytes result sent to driver
2017-07-26 18:12:44,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2031.0 (TID 4063). 635 bytes result sent to driver
2017-07-26 18:12:44,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2031.0 (TID 4062) in 8 ms on localhost (1/2)
2017-07-26 18:12:44,044 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2031.0 (TID 4063) in 7 ms on localhost (2/2)
2017-07-26 18:12:44,045 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2031.0, whose tasks have all completed, from pool 
2017-07-26 18:12:44,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2031 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:12:44,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2031 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019192 s
2017-07-26 18:12:44,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063964000 ms.0 from job set of time 1501063964000 ms
2017-07-26 18:12:44,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.045 s for time 1501063964000 ms (execution: 0.032 s)
2017-07-26 18:12:44,046 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2030 from persistence list
2017-07-26 18:12:44,046 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2030
2017-07-26 18:12:44,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:44,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063960000 ms
2017-07-26 18:12:46,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063966000 ms
2017-07-26 18:12:46,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063966000 ms.0 from job set of time 1501063966000 ms
2017-07-26 18:12:46,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:46,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2032 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:46,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2032 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:46,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:46,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:46,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2032 (KafkaRDD[2032] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:46,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2032 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:12:46,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2032_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:12:46,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2032_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:46,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2032 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:46,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2032 (KafkaRDD[2032] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:46,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2032.0 with 2 tasks
2017-07-26 18:12:46,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2032.0 (TID 4064, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:46,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2032.0 (TID 4065, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:46,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2032.0 (TID 4064)
2017-07-26 18:12:46,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2032.0 (TID 4065)
2017-07-26 18:12:46,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:46,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:46,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2032.0 (TID 4064). 714 bytes result sent to driver
2017-07-26 18:12:46,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2032.0 (TID 4065). 714 bytes result sent to driver
2017-07-26 18:12:46,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2032.0 (TID 4064) in 10 ms on localhost (1/2)
2017-07-26 18:12:46,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2032.0 (TID 4065) in 9 ms on localhost (2/2)
2017-07-26 18:12:46,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2032.0, whose tasks have all completed, from pool 
2017-07-26 18:12:46,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2032 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:12:46,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2032 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021290 s
2017-07-26 18:12:46,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063966000 ms.0 from job set of time 1501063966000 ms
2017-07-26 18:12:46,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501063966000 ms (execution: 0.053 s)
2017-07-26 18:12:46,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2031 from persistence list
2017-07-26 18:12:46,070 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2031
2017-07-26 18:12:46,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:46,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063962000 ms
2017-07-26 18:12:48,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063968000 ms
2017-07-26 18:12:48,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063968000 ms.0 from job set of time 1501063968000 ms
2017-07-26 18:12:48,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2033 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2033 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:48,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2033 (KafkaRDD[2033] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:48,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2033 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:12:48,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2033_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:12:48,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2033_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:48,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2033 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:48,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2033 (KafkaRDD[2033] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:48,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2033.0 with 2 tasks
2017-07-26 18:12:48,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2033.0 (TID 4066, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:48,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2033.0 (TID 4067, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:48,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2033.0 (TID 4067)
2017-07-26 18:12:48,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2033.0 (TID 4066)
2017-07-26 18:12:48,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:48,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:48,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2033.0 (TID 4067). 714 bytes result sent to driver
2017-07-26 18:12:48,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2033.0 (TID 4066). 635 bytes result sent to driver
2017-07-26 18:12:48,087 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2033.0 (TID 4067) in 16 ms on localhost (1/2)
2017-07-26 18:12:48,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2033.0 (TID 4066) in 19 ms on localhost (2/2)
2017-07-26 18:12:48,088 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2033.0, whose tasks have all completed, from pool 
2017-07-26 18:12:48,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2033 (foreachPartition at streamingProcessTest.scala:48) finished in 0.019 s
2017-07-26 18:12:48,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2033 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040629 s
2017-07-26 18:12:48,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063968000 ms.0 from job set of time 1501063968000 ms
2017-07-26 18:12:48,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1501063968000 ms (execution: 0.072 s)
2017-07-26 18:12:48,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2032 from persistence list
2017-07-26 18:12:48,090 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2032
2017-07-26 18:12:48,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:48,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063964000 ms
2017-07-26 18:12:50,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063970000 ms
2017-07-26 18:12:50,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063970000 ms.0 from job set of time 1501063970000 ms
2017-07-26 18:12:50,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2034 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2034 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2034 (KafkaRDD[2034] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2034 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:12:50,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2034_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:12:50,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2034_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:50,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2034 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:50,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2034 (KafkaRDD[2034] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:50,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2034.0 with 2 tasks
2017-07-26 18:12:50,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2034.0 (TID 4068, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:50,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2034.0 (TID 4069, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:50,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2034.0 (TID 4068)
2017-07-26 18:12:50,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2034.0 (TID 4069)
2017-07-26 18:12:50,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:50,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:50,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2034.0 (TID 4069). 635 bytes result sent to driver
2017-07-26 18:12:50,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2034.0 (TID 4068). 635 bytes result sent to driver
2017-07-26 18:12:50,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2034.0 (TID 4069) in 7 ms on localhost (1/2)
2017-07-26 18:12:50,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2034.0 (TID 4068) in 9 ms on localhost (2/2)
2017-07-26 18:12:50,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2034.0, whose tasks have all completed, from pool 
2017-07-26 18:12:50,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2034 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:12:50,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2034 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028408 s
2017-07-26 18:12:50,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063970000 ms.0 from job set of time 1501063970000 ms
2017-07-26 18:12:50,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501063970000 ms (execution: 0.060 s)
2017-07-26 18:12:50,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2033 from persistence list
2017-07-26 18:12:50,076 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2033
2017-07-26 18:12:50,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:50,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063966000 ms
2017-07-26 18:12:52,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063972000 ms
2017-07-26 18:12:52,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063972000 ms.0 from job set of time 1501063972000 ms
2017-07-26 18:12:52,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:52,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2035 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:52,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2035 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:52,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:52,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:52,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2035 (KafkaRDD[2035] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:52,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2035 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:12:52,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2035_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:12:52,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2035_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:52,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2035 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:52,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2035 (KafkaRDD[2035] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:52,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2035.0 with 2 tasks
2017-07-26 18:12:52,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2035.0 (TID 4070, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:52,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2035.0 (TID 4071, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:52,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2035.0 (TID 4071)
2017-07-26 18:12:52,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2035.0 (TID 4070)
2017-07-26 18:12:52,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:52,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:52,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2035.0 (TID 4070). 714 bytes result sent to driver
2017-07-26 18:12:52,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2035.0 (TID 4071). 714 bytes result sent to driver
2017-07-26 18:12:52,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2035.0 (TID 4070) in 13 ms on localhost (1/2)
2017-07-26 18:12:52,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2035.0 (TID 4071) in 11 ms on localhost (2/2)
2017-07-26 18:12:52,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2035.0, whose tasks have all completed, from pool 
2017-07-26 18:12:52,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2035 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:12:52,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2035 finished: foreachPartition at streamingProcessTest.scala:48, took 0.030164 s
2017-07-26 18:12:52,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063972000 ms.0 from job set of time 1501063972000 ms
2017-07-26 18:12:52,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1501063972000 ms (execution: 0.054 s)
2017-07-26 18:12:52,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2034 from persistence list
2017-07-26 18:12:52,069 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2034
2017-07-26 18:12:52,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:52,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063968000 ms
2017-07-26 18:12:54,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063974000 ms
2017-07-26 18:12:54,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063974000 ms.0 from job set of time 1501063974000 ms
2017-07-26 18:12:54,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:54,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2036 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:54,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2036 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:54,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:54,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:54,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2036 (KafkaRDD[2036] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:54,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2036 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:12:54,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2036_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:12:54,036 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2036_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:54,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2036 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:54,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2036 (KafkaRDD[2036] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:54,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2036.0 with 2 tasks
2017-07-26 18:12:54,038 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2036.0 (TID 4072, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:54,038 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2036.0 (TID 4073, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:54,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2036.0 (TID 4072)
2017-07-26 18:12:54,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2036.0 (TID 4073)
2017-07-26 18:12:54,041 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:54,041 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:54,043 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2036.0 (TID 4073). 635 bytes result sent to driver
2017-07-26 18:12:54,043 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2036.0 (TID 4072). 714 bytes result sent to driver
2017-07-26 18:12:54,045 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2036.0 (TID 4072) in 8 ms on localhost (1/2)
2017-07-26 18:12:54,045 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2036.0 (TID 4073) in 7 ms on localhost (2/2)
2017-07-26 18:12:54,045 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2036.0, whose tasks have all completed, from pool 
2017-07-26 18:12:54,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2036 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:12:54,046 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2036 finished: foreachPartition at streamingProcessTest.scala:48, took 0.016221 s
2017-07-26 18:12:54,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063974000 ms.0 from job set of time 1501063974000 ms
2017-07-26 18:12:54,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.046 s for time 1501063974000 ms (execution: 0.032 s)
2017-07-26 18:12:54,046 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2035 from persistence list
2017-07-26 18:12:54,047 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2035
2017-07-26 18:12:54,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:54,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063970000 ms
2017-07-26 18:12:56,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063976000 ms
2017-07-26 18:12:56,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063976000 ms.0 from job set of time 1501063976000 ms
2017-07-26 18:12:56,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2037 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:56,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2037 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:56,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:56,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:56,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2037 (KafkaRDD[2037] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:56,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2037 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:12:56,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2037_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:12:56,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2037_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:12:56,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2037 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:56,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2037 (KafkaRDD[2037] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:56,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2037.0 with 2 tasks
2017-07-26 18:12:56,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2037.0 (TID 4074, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:56,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2037.0 (TID 4075, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:56,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2037.0 (TID 4075)
2017-07-26 18:12:56,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2037.0 (TID 4074)
2017-07-26 18:12:56,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:56,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:56,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2037.0 (TID 4075). 635 bytes result sent to driver
2017-07-26 18:12:56,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2037.0 (TID 4074). 635 bytes result sent to driver
2017-07-26 18:12:56,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2037.0 (TID 4074) in 12 ms on localhost (1/2)
2017-07-26 18:12:56,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2037.0 (TID 4075) in 11 ms on localhost (2/2)
2017-07-26 18:12:56,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2037.0, whose tasks have all completed, from pool 
2017-07-26 18:12:56,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2037 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:12:56,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2037 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023940 s
2017-07-26 18:12:56,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063976000 ms.0 from job set of time 1501063976000 ms
2017-07-26 18:12:56,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1501063976000 ms (execution: 0.050 s)
2017-07-26 18:12:56,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2036 from persistence list
2017-07-26 18:12:56,068 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2036
2017-07-26 18:12:56,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:56,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063972000 ms
2017-07-26 18:12:58,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063978000 ms
2017-07-26 18:12:58,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063978000 ms.0 from job set of time 1501063978000 ms
2017-07-26 18:12:58,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:12:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2038 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:12:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2038 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:12:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:12:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:12:58,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2038 (KafkaRDD[2038] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:12:58,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2038 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:12:58,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2038_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:12:58,060 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2038_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:12:58,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2038 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:12:58,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2038 (KafkaRDD[2038] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:12:58,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2038.0 with 2 tasks
2017-07-26 18:12:58,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2038.0 (TID 4076, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:12:58,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2038.0 (TID 4077, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:12:58,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2038.0 (TID 4077)
2017-07-26 18:12:58,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2038.0 (TID 4076)
2017-07-26 18:12:58,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:12:58,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:12:58,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2038.0 (TID 4076). 635 bytes result sent to driver
2017-07-26 18:12:58,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2038.0 (TID 4077). 635 bytes result sent to driver
2017-07-26 18:12:58,072 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2038.0 (TID 4076) in 10 ms on localhost (1/2)
2017-07-26 18:12:58,072 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2038.0 (TID 4077) in 9 ms on localhost (2/2)
2017-07-26 18:12:58,072 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2038.0, whose tasks have all completed, from pool 
2017-07-26 18:12:58,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2038 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:12:58,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2038 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024583 s
2017-07-26 18:12:58,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063978000 ms.0 from job set of time 1501063978000 ms
2017-07-26 18:12:58,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1501063978000 ms (execution: 0.057 s)
2017-07-26 18:12:58,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2037 from persistence list
2017-07-26 18:12:58,074 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2037
2017-07-26 18:12:58,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:12:58,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063974000 ms
2017-07-26 18:13:00,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063980000 ms
2017-07-26 18:13:00,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063980000 ms.0 from job set of time 1501063980000 ms
2017-07-26 18:13:00,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:00,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2039 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:00,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2039 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:00,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:00,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:00,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2039 (KafkaRDD[2039] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:00,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2039 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:13:00,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2039_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:13:00,040 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2039_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2039 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2039 (KafkaRDD[2039] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:00,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2039.0 with 2 tasks
2017-07-26 18:13:00,042 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2039.0 (TID 4078, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:00,042 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2039.0 (TID 4079, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:00,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2039.0 (TID 4079)
2017-07-26 18:13:00,043 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2039.0 (TID 4078)
2017-07-26 18:13:00,044 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:00,044 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:00,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2039.0 (TID 4079). 714 bytes result sent to driver
2017-07-26 18:13:00,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2039.0 (TID 4078). 714 bytes result sent to driver
2017-07-26 18:13:00,048 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2039.0 (TID 4078) in 7 ms on localhost (1/2)
2017-07-26 18:13:00,048 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2039.0 (TID 4079) in 6 ms on localhost (2/2)
2017-07-26 18:13:00,048 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2039.0, whose tasks have all completed, from pool 
2017-07-26 18:13:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2039 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:13:00,049 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2039 finished: foreachPartition at streamingProcessTest.scala:48, took 0.015079 s
2017-07-26 18:13:00,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063980000 ms.0 from job set of time 1501063980000 ms
2017-07-26 18:13:00,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.049 s for time 1501063980000 ms (execution: 0.029 s)
2017-07-26 18:13:00,049 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2038 from persistence list
2017-07-26 18:13:00,050 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2038
2017-07-26 18:13:00,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:00,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063976000 ms
2017-07-26 18:13:02,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063982000 ms
2017-07-26 18:13:02,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063982000 ms.0 from job set of time 1501063982000 ms
2017-07-26 18:13:02,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:02,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2040 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:02,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2040 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:02,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:02,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:02,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2040 (KafkaRDD[2040] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:02,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2040 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:13:02,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2040_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:13:02,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2040_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:02,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2040 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:02,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2040 (KafkaRDD[2040] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:02,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2040.0 with 2 tasks
2017-07-26 18:13:02,077 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2040.0 (TID 4080, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:02,078 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2040.0 (TID 4081, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:02,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2040.0 (TID 4080)
2017-07-26 18:13:02,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2040.0 (TID 4081)
2017-07-26 18:13:02,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:02,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:02,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2040.0 (TID 4081). 635 bytes result sent to driver
2017-07-26 18:13:02,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2040.0 (TID 4080). 635 bytes result sent to driver
2017-07-26 18:13:02,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2040.0 (TID 4081) in 10 ms on localhost (1/2)
2017-07-26 18:13:02,088 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2040.0 (TID 4080) in 12 ms on localhost (2/2)
2017-07-26 18:13:02,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2040 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:13:02,089 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2040.0, whose tasks have all completed, from pool 
2017-07-26 18:13:02,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2040 finished: foreachPartition at streamingProcessTest.scala:48, took 0.038881 s
2017-07-26 18:13:02,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063982000 ms.0 from job set of time 1501063982000 ms
2017-07-26 18:13:02,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1501063982000 ms (execution: 0.072 s)
2017-07-26 18:13:02,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2039 from persistence list
2017-07-26 18:13:02,091 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2039
2017-07-26 18:13:02,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:02,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063978000 ms
2017-07-26 18:13:04,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063984000 ms
2017-07-26 18:13:04,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063984000 ms.0 from job set of time 1501063984000 ms
2017-07-26 18:13:04,032 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:04,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2041 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:04,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2041 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:04,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:04,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:04,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2041 (KafkaRDD[2041] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:04,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2041 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:13:04,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2041_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:13:04,043 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2041_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:04,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2041 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:04,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2041 (KafkaRDD[2041] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:04,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2041.0 with 2 tasks
2017-07-26 18:13:04,046 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2041.0 (TID 4082, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:04,047 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2041.0 (TID 4083, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:04,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2041.0 (TID 4082)
2017-07-26 18:13:04,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2041.0 (TID 4083)
2017-07-26 18:13:04,048 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:04,048 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:04,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2041.0 (TID 4082). 635 bytes result sent to driver
2017-07-26 18:13:04,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2041.0 (TID 4083). 714 bytes result sent to driver
2017-07-26 18:13:04,052 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2041.0 (TID 4083) in 6 ms on localhost (1/2)
2017-07-26 18:13:04,052 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2041.0 (TID 4082) in 7 ms on localhost (2/2)
2017-07-26 18:13:04,053 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2041.0, whose tasks have all completed, from pool 
2017-07-26 18:13:04,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2041 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:13:04,053 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2041 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020790 s
2017-07-26 18:13:04,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063984000 ms.0 from job set of time 1501063984000 ms
2017-07-26 18:13:04,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.053 s for time 1501063984000 ms (execution: 0.039 s)
2017-07-26 18:13:04,053 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2040 from persistence list
2017-07-26 18:13:04,054 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2040
2017-07-26 18:13:04,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:04,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063980000 ms
2017-07-26 18:13:06,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063986000 ms
2017-07-26 18:13:06,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063986000 ms.0 from job set of time 1501063986000 ms
2017-07-26 18:13:06,052 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2041_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:06,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2028_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:06,058 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2029_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:06,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2030_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:06,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2031_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:06,063 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:06,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2042 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:06,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2042 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:06,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:06,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:06,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2032_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:06,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2042 (KafkaRDD[2042] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:06,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2033_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:06,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2042 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:13:06,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2034_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:06,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2035_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:06,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2042_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:13:06,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2036_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:06,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2042_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:06,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2042 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:06,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2042 (KafkaRDD[2042] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:06,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2042.0 with 2 tasks
2017-07-26 18:13:06,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2037_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:06,077 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2042.0 (TID 4084, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:06,078 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2042.0 (TID 4085, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:06,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2042.0 (TID 4084)
2017-07-26 18:13:06,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2042.0 (TID 4085)
2017-07-26 18:13:06,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2038_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:06,081 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2039_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:06,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:06,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:06,084 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2040_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:06,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2042.0 (TID 4085). 635 bytes result sent to driver
2017-07-26 18:13:06,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2042.0 (TID 4084). 635 bytes result sent to driver
2017-07-26 18:13:06,087 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2042.0 (TID 4085) in 10 ms on localhost (1/2)
2017-07-26 18:13:06,088 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2042.0 (TID 4084) in 11 ms on localhost (2/2)
2017-07-26 18:13:06,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2042 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:13:06,088 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2042.0, whose tasks have all completed, from pool 
2017-07-26 18:13:06,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2042 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024801 s
2017-07-26 18:13:06,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063986000 ms.0 from job set of time 1501063986000 ms
2017-07-26 18:13:06,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1501063986000 ms (execution: 0.071 s)
2017-07-26 18:13:06,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2041 from persistence list
2017-07-26 18:13:06,089 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2041
2017-07-26 18:13:06,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:06,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063982000 ms
2017-07-26 18:13:08,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063988000 ms
2017-07-26 18:13:08,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063988000 ms.0 from job set of time 1501063988000 ms
2017-07-26 18:13:08,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:08,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2043 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:08,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2043 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:08,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:08,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:08,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2043 (KafkaRDD[2043] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:08,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2043 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:13:08,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2043_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:13:08,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2043_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:08,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2043 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:08,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2043 (KafkaRDD[2043] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:08,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2043.0 with 2 tasks
2017-07-26 18:13:08,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2043.0 (TID 4086, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:08,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2043.0 (TID 4087, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:08,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2043.0 (TID 4087)
2017-07-26 18:13:08,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2043.0 (TID 4086)
2017-07-26 18:13:08,066 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:08,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:08,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2043.0 (TID 4086). 635 bytes result sent to driver
2017-07-26 18:13:08,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2043.0 (TID 4087). 635 bytes result sent to driver
2017-07-26 18:13:08,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2043.0 (TID 4087) in 11 ms on localhost (1/2)
2017-07-26 18:13:08,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2043.0 (TID 4086) in 13 ms on localhost (2/2)
2017-07-26 18:13:08,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2043.0, whose tasks have all completed, from pool 
2017-07-26 18:13:08,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2043 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:13:08,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2043 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027733 s
2017-07-26 18:13:08,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063988000 ms.0 from job set of time 1501063988000 ms
2017-07-26 18:13:08,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1501063988000 ms (execution: 0.055 s)
2017-07-26 18:13:08,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2042 from persistence list
2017-07-26 18:13:08,075 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2042
2017-07-26 18:13:08,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:08,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063984000 ms
2017-07-26 18:13:10,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063990000 ms
2017-07-26 18:13:10,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063990000 ms.0 from job set of time 1501063990000 ms
2017-07-26 18:13:10,036 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:10,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2044 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:10,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2044 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:10,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:10,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:10,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2044 (KafkaRDD[2044] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:10,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2044 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:13:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2044_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:13:10,049 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2044_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2044 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2044 (KafkaRDD[2044] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2044.0 with 2 tasks
2017-07-26 18:13:10,053 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2044.0 (TID 4088, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:10,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2044.0 (TID 4089, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:10,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2044.0 (TID 4089)
2017-07-26 18:13:10,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2044.0 (TID 4088)
2017-07-26 18:13:10,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:10,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:10,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2044.0 (TID 4089). 714 bytes result sent to driver
2017-07-26 18:13:10,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2044.0 (TID 4088). 714 bytes result sent to driver
2017-07-26 18:13:10,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2044.0 (TID 4089) in 11 ms on localhost (1/2)
2017-07-26 18:13:10,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2044.0 (TID 4088) in 12 ms on localhost (2/2)
2017-07-26 18:13:10,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2044.0, whose tasks have all completed, from pool 
2017-07-26 18:13:10,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2044 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:13:10,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2044 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028836 s
2017-07-26 18:13:10,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063990000 ms.0 from job set of time 1501063990000 ms
2017-07-26 18:13:10,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501063990000 ms (execution: 0.051 s)
2017-07-26 18:13:10,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2043 from persistence list
2017-07-26 18:13:10,067 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2043
2017-07-26 18:13:10,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:10,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063986000 ms
2017-07-26 18:13:12,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063992000 ms
2017-07-26 18:13:12,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063992000 ms.0 from job set of time 1501063992000 ms
2017-07-26 18:13:12,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:12,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2045 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:12,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2045 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:12,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:12,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:12,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2045 (KafkaRDD[2045] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:12,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2045 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:13:12,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2045_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:13:12,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2045_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:12,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2045 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:12,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2045 (KafkaRDD[2045] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:12,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2045.0 with 2 tasks
2017-07-26 18:13:12,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2045.0 (TID 4090, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:12,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2045.0 (TID 4091, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:12,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2045.0 (TID 4091)
2017-07-26 18:13:12,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2045.0 (TID 4090)
2017-07-26 18:13:12,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:12,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:12,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2045.0 (TID 4090). 635 bytes result sent to driver
2017-07-26 18:13:12,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2045.0 (TID 4091). 714 bytes result sent to driver
2017-07-26 18:13:12,065 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2045.0 (TID 4090) in 11 ms on localhost (1/2)
2017-07-26 18:13:12,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2045.0 (TID 4091) in 11 ms on localhost (2/2)
2017-07-26 18:13:12,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2045.0, whose tasks have all completed, from pool 
2017-07-26 18:13:12,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2045 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:13:12,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2045 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026652 s
2017-07-26 18:13:12,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063992000 ms.0 from job set of time 1501063992000 ms
2017-07-26 18:13:12,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1501063992000 ms (execution: 0.050 s)
2017-07-26 18:13:12,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2044 from persistence list
2017-07-26 18:13:12,068 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2044
2017-07-26 18:13:12,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:12,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063988000 ms
2017-07-26 18:13:14,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063994000 ms
2017-07-26 18:13:14,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063994000 ms.0 from job set of time 1501063994000 ms
2017-07-26 18:13:14,022 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:14,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2046 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:14,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2046 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:14,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:14,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2046 (KafkaRDD[2046] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:14,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2046 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:13:14,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2046_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:13:14,029 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2046_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:14,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2046 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:14,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2046 (KafkaRDD[2046] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:14,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2046.0 with 2 tasks
2017-07-26 18:13:14,031 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2046.0 (TID 4092, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:14,032 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2046.0 (TID 4093, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:14,032 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2046.0 (TID 4093)
2017-07-26 18:13:14,032 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2046.0 (TID 4092)
2017-07-26 18:13:14,034 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:14,034 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:14,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2046.0 (TID 4093). 635 bytes result sent to driver
2017-07-26 18:13:14,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2046.0 (TID 4092). 635 bytes result sent to driver
2017-07-26 18:13:14,037 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2046.0 (TID 4093) in 5 ms on localhost (1/2)
2017-07-26 18:13:14,037 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2046.0 (TID 4092) in 6 ms on localhost (2/2)
2017-07-26 18:13:14,037 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2046.0, whose tasks have all completed, from pool 
2017-07-26 18:13:14,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2046 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:13:14,038 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2046 finished: foreachPartition at streamingProcessTest.scala:48, took 0.015314 s
2017-07-26 18:13:14,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063994000 ms.0 from job set of time 1501063994000 ms
2017-07-26 18:13:14,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.038 s for time 1501063994000 ms (execution: 0.026 s)
2017-07-26 18:13:14,038 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2045 from persistence list
2017-07-26 18:13:14,038 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2045
2017-07-26 18:13:14,039 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:14,039 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063990000 ms
2017-07-26 18:13:16,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063996000 ms
2017-07-26 18:13:16,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063996000 ms.0 from job set of time 1501063996000 ms
2017-07-26 18:13:16,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:16,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2047 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:16,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2047 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:16,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:16,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:16,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2047 (KafkaRDD[2047] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2047 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:13:16,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2047_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:13:16,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2047_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2047 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2047 (KafkaRDD[2047] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2047.0 with 2 tasks
2017-07-26 18:13:16,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2047.0 (TID 4094, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:16,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2047.0 (TID 4095, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:16,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2047.0 (TID 4094)
2017-07-26 18:13:16,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2047.0 (TID 4095)
2017-07-26 18:13:16,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:16,066 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:16,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2047.0 (TID 4094). 714 bytes result sent to driver
2017-07-26 18:13:16,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2047.0 (TID 4095). 714 bytes result sent to driver
2017-07-26 18:13:16,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2047.0 (TID 4094) in 16 ms on localhost (1/2)
2017-07-26 18:13:16,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2047.0 (TID 4095) in 15 ms on localhost (2/2)
2017-07-26 18:13:16,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2047.0, whose tasks have all completed, from pool 
2017-07-26 18:13:16,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2047 (foreachPartition at streamingProcessTest.scala:48) finished in 0.017 s
2017-07-26 18:13:16,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2047 finished: foreachPartition at streamingProcessTest.scala:48, took 0.034229 s
2017-07-26 18:13:16,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063996000 ms.0 from job set of time 1501063996000 ms
2017-07-26 18:13:16,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1501063996000 ms (execution: 0.061 s)
2017-07-26 18:13:16,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2046 from persistence list
2017-07-26 18:13:16,077 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2046
2017-07-26 18:13:16,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:16,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063992000 ms
2017-07-26 18:13:18,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501063998000 ms
2017-07-26 18:13:18,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501063998000 ms.0 from job set of time 1501063998000 ms
2017-07-26 18:13:18,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:18,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2048 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:18,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2048 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:18,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:18,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:18,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2048 (KafkaRDD[2048] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:18,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2048 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:13:18,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2048_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:13:18,040 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2048_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:18,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2048 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:18,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2048 (KafkaRDD[2048] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:18,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2048.0 with 2 tasks
2017-07-26 18:13:18,041 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2048.0 (TID 4096, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:18,042 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2048.0 (TID 4097, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:18,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2048.0 (TID 4097)
2017-07-26 18:13:18,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2048.0 (TID 4096)
2017-07-26 18:13:18,044 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:18,044 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:18,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2048.0 (TID 4097). 635 bytes result sent to driver
2017-07-26 18:13:18,045 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2048.0 (TID 4096). 635 bytes result sent to driver
2017-07-26 18:13:18,047 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2048.0 (TID 4096) in 6 ms on localhost (1/2)
2017-07-26 18:13:18,047 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2048.0 (TID 4097) in 5 ms on localhost (2/2)
2017-07-26 18:13:18,047 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2048.0, whose tasks have all completed, from pool 
2017-07-26 18:13:18,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2048 (foreachPartition at streamingProcessTest.scala:48) finished in 0.006 s
2017-07-26 18:13:18,048 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2048 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014022 s
2017-07-26 18:13:18,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501063998000 ms.0 from job set of time 1501063998000 ms
2017-07-26 18:13:18,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1501063998000 ms (execution: 0.030 s)
2017-07-26 18:13:18,048 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2047 from persistence list
2017-07-26 18:13:18,048 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2047
2017-07-26 18:13:18,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:18,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063994000 ms
2017-07-26 18:13:20,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064000000 ms
2017-07-26 18:13:20,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064000000 ms.0 from job set of time 1501064000000 ms
2017-07-26 18:13:20,035 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2049 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2049 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:20,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2049 (KafkaRDD[2049] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:20,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2049 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:13:20,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2049_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:13:20,043 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2049_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2049 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2049 (KafkaRDD[2049] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2049.0 with 2 tasks
2017-07-26 18:13:20,045 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2049.0 (TID 4098, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:20,046 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2049.0 (TID 4099, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:20,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2049.0 (TID 4099)
2017-07-26 18:13:20,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2049.0 (TID 4098)
2017-07-26 18:13:20,048 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:20,049 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:20,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2049.0 (TID 4099). 635 bytes result sent to driver
2017-07-26 18:13:20,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2049.0 (TID 4098). 714 bytes result sent to driver
2017-07-26 18:13:20,052 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2049.0 (TID 4099) in 7 ms on localhost (1/2)
2017-07-26 18:13:20,052 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2049.0 (TID 4098) in 8 ms on localhost (2/2)
2017-07-26 18:13:20,052 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2049.0, whose tasks have all completed, from pool 
2017-07-26 18:13:20,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2049 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:13:20,053 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2049 finished: foreachPartition at streamingProcessTest.scala:48, took 0.017443 s
2017-07-26 18:13:20,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064000000 ms.0 from job set of time 1501064000000 ms
2017-07-26 18:13:20,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.053 s for time 1501064000000 ms (execution: 0.039 s)
2017-07-26 18:13:20,053 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2048 from persistence list
2017-07-26 18:13:20,054 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2048
2017-07-26 18:13:20,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:20,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063996000 ms
2017-07-26 18:13:22,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064002000 ms
2017-07-26 18:13:22,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064002000 ms.0 from job set of time 1501064002000 ms
2017-07-26 18:13:22,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:22,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2050 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:22,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2050 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:22,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:22,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:22,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2050 (KafkaRDD[2050] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:22,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2050 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:13:22,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2050_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:13:22,045 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2050_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:22,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2050 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:22,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2050 (KafkaRDD[2050] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2050.0 with 2 tasks
2017-07-26 18:13:22,048 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2050.0 (TID 4100, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:22,048 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2050.0 (TID 4101, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:22,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2050.0 (TID 4101)
2017-07-26 18:13:22,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2050.0 (TID 4100)
2017-07-26 18:13:22,050 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:22,050 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:22,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2050.0 (TID 4101). 635 bytes result sent to driver
2017-07-26 18:13:22,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2050.0 (TID 4100). 635 bytes result sent to driver
2017-07-26 18:13:22,053 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2050.0 (TID 4101) in 5 ms on localhost (1/2)
2017-07-26 18:13:22,053 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2050.0 (TID 4100) in 6 ms on localhost (2/2)
2017-07-26 18:13:22,053 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2050.0, whose tasks have all completed, from pool 
2017-07-26 18:13:22,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2050 (foreachPartition at streamingProcessTest.scala:48) finished in 0.006 s
2017-07-26 18:13:22,053 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2050 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020100 s
2017-07-26 18:13:22,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064002000 ms.0 from job set of time 1501064002000 ms
2017-07-26 18:13:22,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.054 s for time 1501064002000 ms (execution: 0.040 s)
2017-07-26 18:13:22,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2049 from persistence list
2017-07-26 18:13:22,054 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2049
2017-07-26 18:13:22,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:22,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501063998000 ms
2017-07-26 18:13:24,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064004000 ms
2017-07-26 18:13:24,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064004000 ms.0 from job set of time 1501064004000 ms
2017-07-26 18:13:24,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:24,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2051 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:24,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2051 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:24,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:24,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:24,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2051 (KafkaRDD[2051] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:24,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2051 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:13:24,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2051_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:13:24,034 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2051_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:24,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2051 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:24,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2051 (KafkaRDD[2051] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:24,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2051.0 with 2 tasks
2017-07-26 18:13:24,036 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2051.0 (TID 4102, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:24,036 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2051.0 (TID 4103, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:24,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2051.0 (TID 4102)
2017-07-26 18:13:24,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2051.0 (TID 4103)
2017-07-26 18:13:24,038 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:24,038 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:24,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2051.0 (TID 4102). 635 bytes result sent to driver
2017-07-26 18:13:24,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2051.0 (TID 4103). 635 bytes result sent to driver
2017-07-26 18:13:24,041 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2051.0 (TID 4103) in 5 ms on localhost (1/2)
2017-07-26 18:13:24,041 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2051.0 (TID 4102) in 6 ms on localhost (2/2)
2017-07-26 18:13:24,041 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2051.0, whose tasks have all completed, from pool 
2017-07-26 18:13:24,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2051 (foreachPartition at streamingProcessTest.scala:48) finished in 0.006 s
2017-07-26 18:13:24,041 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2051 finished: foreachPartition at streamingProcessTest.scala:48, took 0.015051 s
2017-07-26 18:13:24,042 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064004000 ms.0 from job set of time 1501064004000 ms
2017-07-26 18:13:24,042 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.042 s for time 1501064004000 ms (execution: 0.027 s)
2017-07-26 18:13:24,042 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2050 from persistence list
2017-07-26 18:13:24,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:24,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064000000 ms
2017-07-26 18:13:24,042 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2050
2017-07-26 18:13:26,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064006000 ms
2017-07-26 18:13:26,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064006000 ms.0 from job set of time 1501064006000 ms
2017-07-26 18:13:26,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:26,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2052 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:26,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2052 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:26,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:26,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:26,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2052 (KafkaRDD[2052] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:26,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2052 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:13:26,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2052_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:13:26,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2052_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:26,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2052 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:26,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2052 (KafkaRDD[2052] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:26,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2052.0 with 2 tasks
2017-07-26 18:13:26,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2052.0 (TID 4104, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:26,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2052.0 (TID 4105, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:26,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2052.0 (TID 4104)
2017-07-26 18:13:26,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2052.0 (TID 4105)
2017-07-26 18:13:26,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:26,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:26,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2052.0 (TID 4104). 714 bytes result sent to driver
2017-07-26 18:13:26,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2052.0 (TID 4105). 714 bytes result sent to driver
2017-07-26 18:13:26,089 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2052.0 (TID 4104) in 19 ms on localhost (1/2)
2017-07-26 18:13:26,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2052.0 (TID 4105) in 17 ms on localhost (2/2)
2017-07-26 18:13:26,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2052.0, whose tasks have all completed, from pool 
2017-07-26 18:13:26,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2052 (foreachPartition at streamingProcessTest.scala:48) finished in 0.021 s
2017-07-26 18:13:26,091 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2052 finished: foreachPartition at streamingProcessTest.scala:48, took 0.041729 s
2017-07-26 18:13:26,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064006000 ms.0 from job set of time 1501064006000 ms
2017-07-26 18:13:26,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1501064006000 ms (execution: 0.075 s)
2017-07-26 18:13:26,092 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2051 from persistence list
2017-07-26 18:13:26,093 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2051
2017-07-26 18:13:26,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:26,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064002000 ms
2017-07-26 18:13:28,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064008000 ms
2017-07-26 18:13:28,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064008000 ms.0 from job set of time 1501064008000 ms
2017-07-26 18:13:28,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2053 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2053 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:28,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:28,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:28,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2053 (KafkaRDD[2053] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:28,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2053 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:13:28,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2053_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:13:28,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2053_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:28,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2053 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:28,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2053 (KafkaRDD[2053] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:28,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2053.0 with 2 tasks
2017-07-26 18:13:28,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2053.0 (TID 4106, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:28,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2053.0 (TID 4107, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:28,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2053.0 (TID 4107)
2017-07-26 18:13:28,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2053.0 (TID 4106)
2017-07-26 18:13:28,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:28,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:28,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2053.0 (TID 4106). 635 bytes result sent to driver
2017-07-26 18:13:28,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2053.0 (TID 4107). 635 bytes result sent to driver
2017-07-26 18:13:28,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2053.0 (TID 4106) in 11 ms on localhost (1/2)
2017-07-26 18:13:28,073 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2053.0 (TID 4107) in 11 ms on localhost (2/2)
2017-07-26 18:13:28,073 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2053.0, whose tasks have all completed, from pool 
2017-07-26 18:13:28,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2053 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:13:28,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2053 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026376 s
2017-07-26 18:13:28,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064008000 ms.0 from job set of time 1501064008000 ms
2017-07-26 18:13:28,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501064008000 ms (execution: 0.058 s)
2017-07-26 18:13:28,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2052 from persistence list
2017-07-26 18:13:28,076 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2052
2017-07-26 18:13:28,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:28,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064004000 ms
2017-07-26 18:13:30,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064010000 ms
2017-07-26 18:13:30,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064010000 ms.0 from job set of time 1501064010000 ms
2017-07-26 18:13:30,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2054 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2054 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2054 (KafkaRDD[2054] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2054 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:13:30,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2054_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:13:30,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2054_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:30,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2054 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:30,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2054 (KafkaRDD[2054] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:30,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2054.0 with 2 tasks
2017-07-26 18:13:30,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2054.0 (TID 4108, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:30,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2054.0 (TID 4109, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:30,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2054.0 (TID 4108)
2017-07-26 18:13:30,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2054.0 (TID 4109)
2017-07-26 18:13:30,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:30,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2054.0 (TID 4108). 635 bytes result sent to driver
2017-07-26 18:13:30,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2054.0 (TID 4109). 635 bytes result sent to driver
2017-07-26 18:13:30,078 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2054.0 (TID 4109) in 10 ms on localhost (1/2)
2017-07-26 18:13:30,079 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2054.0 (TID 4108) in 11 ms on localhost (2/2)
2017-07-26 18:13:30,079 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2054.0, whose tasks have all completed, from pool 
2017-07-26 18:13:30,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2054 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:13:30,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2054 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027144 s
2017-07-26 18:13:30,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064010000 ms.0 from job set of time 1501064010000 ms
2017-07-26 18:13:30,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1501064010000 ms (execution: 0.060 s)
2017-07-26 18:13:30,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2053 from persistence list
2017-07-26 18:13:30,081 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2053
2017-07-26 18:13:30,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:30,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064006000 ms
2017-07-26 18:13:32,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064012000 ms
2017-07-26 18:13:32,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064012000 ms.0 from job set of time 1501064012000 ms
2017-07-26 18:13:32,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:32,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2055 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:32,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2055 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:32,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:32,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:32,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2055 (KafkaRDD[2055] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:32,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2055 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:13:32,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2055_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:13:32,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2055_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:32,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2055 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:32,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2055 (KafkaRDD[2055] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:32,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2055.0 with 2 tasks
2017-07-26 18:13:32,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2055.0 (TID 4110, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:32,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2055.0 (TID 4111, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:32,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2055.0 (TID 4111)
2017-07-26 18:13:32,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2055.0 (TID 4110)
2017-07-26 18:13:32,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:32,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:32,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2055.0 (TID 4111). 795 bytes result sent to driver
2017-07-26 18:13:32,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2055.0 (TID 4110). 795 bytes result sent to driver
2017-07-26 18:13:32,086 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2042_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:32,087 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2055.0 (TID 4111) in 22 ms on localhost (1/2)
2017-07-26 18:13:32,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2055.0 (TID 4110) in 24 ms on localhost (2/2)
2017-07-26 18:13:32,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2055.0, whose tasks have all completed, from pool 
2017-07-26 18:13:32,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2055 (foreachPartition at streamingProcessTest.scala:48) finished in 0.025 s
2017-07-26 18:13:32,089 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2043_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:32,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2055 finished: foreachPartition at streamingProcessTest.scala:48, took 0.044578 s
2017-07-26 18:13:32,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064012000 ms.0 from job set of time 1501064012000 ms
2017-07-26 18:13:32,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1501064012000 ms (execution: 0.073 s)
2017-07-26 18:13:32,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2054 from persistence list
2017-07-26 18:13:32,090 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2054
2017-07-26 18:13:32,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:32,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064008000 ms
2017-07-26 18:13:32,091 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2044_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:32,093 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2045_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:32,095 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2046_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:32,097 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2047_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:32,098 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2048_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:32,100 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2049_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:32,101 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2050_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:32,102 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2051_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:32,103 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2052_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:32,104 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2053_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:32,106 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2054_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:34,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064014000 ms
2017-07-26 18:13:34,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064014000 ms.0 from job set of time 1501064014000 ms
2017-07-26 18:13:34,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2056 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2056 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2056 (KafkaRDD[2056] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:34,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2056 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:13:34,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2056_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:13:34,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2056_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:34,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2056 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:34,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2056 (KafkaRDD[2056] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:34,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2056.0 with 2 tasks
2017-07-26 18:13:34,041 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2056.0 (TID 4112, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:34,042 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2056.0 (TID 4113, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:34,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2056.0 (TID 4113)
2017-07-26 18:13:34,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2056.0 (TID 4112)
2017-07-26 18:13:34,044 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:34,044 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:34,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2056.0 (TID 4113). 714 bytes result sent to driver
2017-07-26 18:13:34,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2056.0 (TID 4112). 714 bytes result sent to driver
2017-07-26 18:13:34,049 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2056.0 (TID 4113) in 6 ms on localhost (1/2)
2017-07-26 18:13:34,049 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2056.0 (TID 4112) in 8 ms on localhost (2/2)
2017-07-26 18:13:34,049 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2056.0, whose tasks have all completed, from pool 
2017-07-26 18:13:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2056 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:13:34,050 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2056 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019393 s
2017-07-26 18:13:34,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064014000 ms.0 from job set of time 1501064014000 ms
2017-07-26 18:13:34,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.050 s for time 1501064014000 ms (execution: 0.035 s)
2017-07-26 18:13:34,050 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2055 from persistence list
2017-07-26 18:13:34,051 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2055
2017-07-26 18:13:34,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:34,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064010000 ms
2017-07-26 18:13:36,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064016000 ms
2017-07-26 18:13:36,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064016000 ms.0 from job set of time 1501064016000 ms
2017-07-26 18:13:36,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:36,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2057 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:36,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2057 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:36,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:36,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:36,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2057 (KafkaRDD[2057] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:36,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2057 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:13:36,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2057_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:13:36,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2057_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:36,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2057 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:36,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2057 (KafkaRDD[2057] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:36,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2057.0 with 2 tasks
2017-07-26 18:13:36,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2057.0 (TID 4114, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:36,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2057.0 (TID 4115, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:36,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2057.0 (TID 4114)
2017-07-26 18:13:36,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2057.0 (TID 4115)
2017-07-26 18:13:36,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:36,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:36,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2057.0 (TID 4114). 714 bytes result sent to driver
2017-07-26 18:13:36,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2057.0 (TID 4115). 635 bytes result sent to driver
2017-07-26 18:13:36,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2057.0 (TID 4114) in 17 ms on localhost (1/2)
2017-07-26 18:13:36,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2057.0 (TID 4115) in 17 ms on localhost (2/2)
2017-07-26 18:13:36,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2057.0, whose tasks have all completed, from pool 
2017-07-26 18:13:36,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2057 (foreachPartition at streamingProcessTest.scala:48) finished in 0.019 s
2017-07-26 18:13:36,084 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2057 finished: foreachPartition at streamingProcessTest.scala:48, took 0.038618 s
2017-07-26 18:13:36,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064016000 ms.0 from job set of time 1501064016000 ms
2017-07-26 18:13:36,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1501064016000 ms (execution: 0.069 s)
2017-07-26 18:13:36,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2056 from persistence list
2017-07-26 18:13:36,086 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2056
2017-07-26 18:13:36,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:36,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064012000 ms
2017-07-26 18:13:38,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064018000 ms
2017-07-26 18:13:38,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064018000 ms.0 from job set of time 1501064018000 ms
2017-07-26 18:13:38,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2058 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2058 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2058 (KafkaRDD[2058] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:38,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2058 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:13:38,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2058_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:13:38,052 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2058_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:13:38,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2058 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:38,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2058 (KafkaRDD[2058] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:38,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2058.0 with 2 tasks
2017-07-26 18:13:38,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2058.0 (TID 4116, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:38,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2058.0 (TID 4117, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:38,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2058.0 (TID 4116)
2017-07-26 18:13:38,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2058.0 (TID 4117)
2017-07-26 18:13:38,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:38,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:38,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2058.0 (TID 4116). 714 bytes result sent to driver
2017-07-26 18:13:38,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2058.0 (TID 4117). 714 bytes result sent to driver
2017-07-26 18:13:38,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2058.0 (TID 4116) in 10 ms on localhost (1/2)
2017-07-26 18:13:38,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2058.0 (TID 4117) in 9 ms on localhost (2/2)
2017-07-26 18:13:38,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2058.0, whose tasks have all completed, from pool 
2017-07-26 18:13:38,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2058 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:13:38,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2058 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023029 s
2017-07-26 18:13:38,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064018000 ms.0 from job set of time 1501064018000 ms
2017-07-26 18:13:38,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501064018000 ms (execution: 0.049 s)
2017-07-26 18:13:38,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2057 from persistence list
2017-07-26 18:13:38,067 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2057
2017-07-26 18:13:38,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:38,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064014000 ms
2017-07-26 18:13:40,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064020000 ms
2017-07-26 18:13:40,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064020000 ms.0 from job set of time 1501064020000 ms
2017-07-26 18:13:40,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:40,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2059 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:40,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2059 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:40,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:40,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:40,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2059 (KafkaRDD[2059] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:40,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2059 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:13:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2059_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:13:40,043 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2059_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:40,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2059 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:40,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2059 (KafkaRDD[2059] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:40,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2059.0 with 2 tasks
2017-07-26 18:13:40,046 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2059.0 (TID 4118, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:40,048 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2059.0 (TID 4119, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:40,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2059.0 (TID 4118)
2017-07-26 18:13:40,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2059.0 (TID 4119)
2017-07-26 18:13:40,051 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:40,051 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:40,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2059.0 (TID 4118). 635 bytes result sent to driver
2017-07-26 18:13:40,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2059.0 (TID 4119). 635 bytes result sent to driver
2017-07-26 18:13:40,054 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2059.0 (TID 4119) in 7 ms on localhost (1/2)
2017-07-26 18:13:40,054 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2059.0 (TID 4118) in 9 ms on localhost (2/2)
2017-07-26 18:13:40,054 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2059.0, whose tasks have all completed, from pool 
2017-07-26 18:13:40,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2059 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:13:40,055 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2059 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022066 s
2017-07-26 18:13:40,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064020000 ms.0 from job set of time 1501064020000 ms
2017-07-26 18:13:40,055 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.055 s for time 1501064020000 ms (execution: 0.038 s)
2017-07-26 18:13:40,055 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2058 from persistence list
2017-07-26 18:13:40,056 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2058
2017-07-26 18:13:40,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:40,056 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064016000 ms
2017-07-26 18:13:42,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064022000 ms
2017-07-26 18:13:42,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064022000 ms.0 from job set of time 1501064022000 ms
2017-07-26 18:13:42,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:42,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2060 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:42,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2060 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:42,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:42,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:42,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2060 (KafkaRDD[2060] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:42,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2060 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:13:42,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2060_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:13:42,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2060_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:42,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2060 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:42,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2060 (KafkaRDD[2060] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:42,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2060.0 with 2 tasks
2017-07-26 18:13:42,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2060.0 (TID 4120, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:42,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2060.0 (TID 4121, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:42,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2060.0 (TID 4121)
2017-07-26 18:13:42,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2060.0 (TID 4120)
2017-07-26 18:13:42,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:42,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:42,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2060.0 (TID 4120). 635 bytes result sent to driver
2017-07-26 18:13:42,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2060.0 (TID 4121). 714 bytes result sent to driver
2017-07-26 18:13:42,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2060.0 (TID 4120) in 8 ms on localhost (1/2)
2017-07-26 18:13:42,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2060.0 (TID 4121) in 7 ms on localhost (2/2)
2017-07-26 18:13:42,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2060.0, whose tasks have all completed, from pool 
2017-07-26 18:13:42,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2060 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:13:42,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2060 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022561 s
2017-07-26 18:13:42,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064022000 ms.0 from job set of time 1501064022000 ms
2017-07-26 18:13:42,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501064022000 ms (execution: 0.046 s)
2017-07-26 18:13:42,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2059 from persistence list
2017-07-26 18:13:42,067 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2059
2017-07-26 18:13:42,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:42,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064018000 ms
2017-07-26 18:13:44,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064024000 ms
2017-07-26 18:13:44,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064024000 ms.0 from job set of time 1501064024000 ms
2017-07-26 18:13:44,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:44,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2061 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:44,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2061 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:44,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:44,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:44,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2061 (KafkaRDD[2061] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:44,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2061 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:13:44,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2061_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:13:44,039 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2061_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:44,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2061 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:44,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2061 (KafkaRDD[2061] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:44,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2061.0 with 2 tasks
2017-07-26 18:13:44,042 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2061.0 (TID 4122, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:44,042 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2061.0 (TID 4123, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:44,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2061.0 (TID 4123)
2017-07-26 18:13:44,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2061.0 (TID 4122)
2017-07-26 18:13:44,044 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:44,044 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:44,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2061.0 (TID 4123). 635 bytes result sent to driver
2017-07-26 18:13:44,045 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2061.0 (TID 4122). 635 bytes result sent to driver
2017-07-26 18:13:44,047 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2061.0 (TID 4122) in 6 ms on localhost (1/2)
2017-07-26 18:13:44,048 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2061.0 (TID 4123) in 5 ms on localhost (2/2)
2017-07-26 18:13:44,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2061 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:13:44,048 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2061.0, whose tasks have all completed, from pool 
2017-07-26 18:13:44,048 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2061 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018031 s
2017-07-26 18:13:44,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064024000 ms.0 from job set of time 1501064024000 ms
2017-07-26 18:13:44,049 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2060 from persistence list
2017-07-26 18:13:44,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.049 s for time 1501064024000 ms (execution: 0.037 s)
2017-07-26 18:13:44,049 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2060
2017-07-26 18:13:44,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:44,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064020000 ms
2017-07-26 18:13:46,313 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064026000 ms
2017-07-26 18:13:46,314 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064026000 ms.0 from job set of time 1501064026000 ms
2017-07-26 18:13:46,346 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:46,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2062 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:46,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2062 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:46,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:46,347 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:46,348 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2062 (KafkaRDD[2062] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:46,352 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2062 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:13:46,355 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2062_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:13:46,356 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2062_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:46,357 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2062 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:46,357 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2062 (KafkaRDD[2062] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:46,357 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2062.0 with 2 tasks
2017-07-26 18:13:46,359 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2062.0 (TID 4124, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:46,359 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2062.0 (TID 4125, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:46,360 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2062.0 (TID 4125)
2017-07-26 18:13:46,360 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2062.0 (TID 4124)
2017-07-26 18:13:46,362 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:46,362 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:46,365 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2062.0 (TID 4124). 714 bytes result sent to driver
2017-07-26 18:13:46,365 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2062.0 (TID 4125). 714 bytes result sent to driver
2017-07-26 18:13:46,369 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2062.0 (TID 4125) in 9 ms on localhost (1/2)
2017-07-26 18:13:46,369 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2062.0 (TID 4124) in 11 ms on localhost (2/2)
2017-07-26 18:13:46,369 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2062.0, whose tasks have all completed, from pool 
2017-07-26 18:13:46,369 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2062 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:13:46,370 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2062 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023504 s
2017-07-26 18:13:46,370 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064026000 ms.0 from job set of time 1501064026000 ms
2017-07-26 18:13:46,371 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2061 from persistence list
2017-07-26 18:13:46,371 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.370 s for time 1501064026000 ms (execution: 0.056 s)
2017-07-26 18:13:46,371 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2061
2017-07-26 18:13:46,371 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:46,371 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064022000 ms
2017-07-26 18:13:48,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064028000 ms
2017-07-26 18:13:48,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064028000 ms.0 from job set of time 1501064028000 ms
2017-07-26 18:13:48,035 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:48,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2063 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:48,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2063 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:48,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:48,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:48,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2063 (KafkaRDD[2063] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:48,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2063 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:13:48,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2063_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:13:48,041 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2063_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:48,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2063 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:48,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2063 (KafkaRDD[2063] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:48,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2063.0 with 2 tasks
2017-07-26 18:13:48,043 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2063.0 (TID 4126, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:48,044 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2063.0 (TID 4127, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:48,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2063.0 (TID 4126)
2017-07-26 18:13:48,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2063.0 (TID 4127)
2017-07-26 18:13:48,047 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:48,048 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:48,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2063.0 (TID 4126). 714 bytes result sent to driver
2017-07-26 18:13:48,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2063.0 (TID 4127). 714 bytes result sent to driver
2017-07-26 18:13:48,052 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2063.0 (TID 4126) in 10 ms on localhost (1/2)
2017-07-26 18:13:48,053 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2063.0 (TID 4127) in 9 ms on localhost (2/2)
2017-07-26 18:13:48,053 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2063.0, whose tasks have all completed, from pool 
2017-07-26 18:13:48,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2063 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:13:48,053 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2063 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018244 s
2017-07-26 18:13:48,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064028000 ms.0 from job set of time 1501064028000 ms
2017-07-26 18:13:48,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.054 s for time 1501064028000 ms (execution: 0.036 s)
2017-07-26 18:13:48,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2062 from persistence list
2017-07-26 18:13:48,054 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2062
2017-07-26 18:13:48,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:48,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064024000 ms
2017-07-26 18:13:50,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064030000 ms
2017-07-26 18:13:50,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064030000 ms.0 from job set of time 1501064030000 ms
2017-07-26 18:13:50,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:50,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2064 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:50,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2064 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:50,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:50,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:50,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2064 (KafkaRDD[2064] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:50,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2064 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:13:50,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2064_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:13:50,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2064_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:13:50,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2064 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:50,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2064 (KafkaRDD[2064] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:50,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2064.0 with 2 tasks
2017-07-26 18:13:50,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2064.0 (TID 4128, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:50,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2064.0 (TID 4129, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:50,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2064.0 (TID 4129)
2017-07-26 18:13:50,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2064.0 (TID 4128)
2017-07-26 18:13:50,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:50,037 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:50,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2064.0 (TID 4129). 714 bytes result sent to driver
2017-07-26 18:13:50,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2064.0 (TID 4128). 635 bytes result sent to driver
2017-07-26 18:13:50,040 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2064.0 (TID 4129) in 5 ms on localhost (1/2)
2017-07-26 18:13:50,041 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2064.0 (TID 4128) in 7 ms on localhost (2/2)
2017-07-26 18:13:50,041 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2064.0, whose tasks have all completed, from pool 
2017-07-26 18:13:50,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2064 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:13:50,041 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2064 finished: foreachPartition at streamingProcessTest.scala:48, took 0.016409 s
2017-07-26 18:13:50,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064030000 ms.0 from job set of time 1501064030000 ms
2017-07-26 18:13:50,042 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1501064030000 ms (execution: 0.027 s)
2017-07-26 18:13:50,042 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2063 from persistence list
2017-07-26 18:13:50,042 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2063
2017-07-26 18:13:50,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:50,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064026000 ms
2017-07-26 18:13:52,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064032000 ms
2017-07-26 18:13:52,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064032000 ms.0 from job set of time 1501064032000 ms
2017-07-26 18:13:52,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:52,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2065 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:52,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2065 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:52,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:52,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:52,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2065 (KafkaRDD[2065] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:52,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2065 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:13:52,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2065_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:13:52,053 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2065_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:52,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2065 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:52,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2065 (KafkaRDD[2065] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:52,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2065.0 with 2 tasks
2017-07-26 18:13:52,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2065.0 (TID 4130, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:52,058 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2065.0 (TID 4131, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:52,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2065.0 (TID 4130)
2017-07-26 18:13:52,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2065.0 (TID 4131)
2017-07-26 18:13:52,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:52,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:52,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2065.0 (TID 4130). 635 bytes result sent to driver
2017-07-26 18:13:52,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2065.0 (TID 4131). 635 bytes result sent to driver
2017-07-26 18:13:52,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2065.0 (TID 4130) in 13 ms on localhost (1/2)
2017-07-26 18:13:52,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2065.0 (TID 4131) in 12 ms on localhost (2/2)
2017-07-26 18:13:52,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2065.0, whose tasks have all completed, from pool 
2017-07-26 18:13:52,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2065 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:13:52,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2065 finished: foreachPartition at streamingProcessTest.scala:48, took 0.029698 s
2017-07-26 18:13:52,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064032000 ms.0 from job set of time 1501064032000 ms
2017-07-26 18:13:52,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501064032000 ms (execution: 0.054 s)
2017-07-26 18:13:52,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2064 from persistence list
2017-07-26 18:13:52,071 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2064
2017-07-26 18:13:52,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:52,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064028000 ms
2017-07-26 18:13:54,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064034000 ms
2017-07-26 18:13:54,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064034000 ms.0 from job set of time 1501064034000 ms
2017-07-26 18:13:54,032 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:54,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2066 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:54,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2066 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:54,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:54,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:54,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2066 (KafkaRDD[2066] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:54,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2066 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:13:54,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2066_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:13:54,042 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2066_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:54,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2066 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:54,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2066 (KafkaRDD[2066] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:54,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2066.0 with 2 tasks
2017-07-26 18:13:54,045 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2066.0 (TID 4132, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:54,045 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2066.0 (TID 4133, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:54,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2066.0 (TID 4132)
2017-07-26 18:13:54,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2066.0 (TID 4133)
2017-07-26 18:13:54,048 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:54,049 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:54,052 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2066.0 (TID 4133). 635 bytes result sent to driver
2017-07-26 18:13:54,052 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2066.0 (TID 4132). 714 bytes result sent to driver
2017-07-26 18:13:54,055 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2066.0 (TID 4133) in 10 ms on localhost (1/2)
2017-07-26 18:13:54,055 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2066.0 (TID 4132) in 11 ms on localhost (2/2)
2017-07-26 18:13:54,056 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2066.0, whose tasks have all completed, from pool 
2017-07-26 18:13:54,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2066 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:13:54,056 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2066 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023797 s
2017-07-26 18:13:54,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064034000 ms.0 from job set of time 1501064034000 ms
2017-07-26 18:13:54,057 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2065 from persistence list
2017-07-26 18:13:54,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.056 s for time 1501064034000 ms (execution: 0.041 s)
2017-07-26 18:13:54,065 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2065
2017-07-26 18:13:54,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:54,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064030000 ms
2017-07-26 18:13:56,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064036000 ms
2017-07-26 18:13:56,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064036000 ms.0 from job set of time 1501064036000 ms
2017-07-26 18:13:56,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:56,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2067 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:56,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2067 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:56,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:56,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:56,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2067 (KafkaRDD[2067] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:56,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2067 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:13:56,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2067_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:13:56,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2067_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:56,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2067 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:56,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2067 (KafkaRDD[2067] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:56,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2067.0 with 2 tasks
2017-07-26 18:13:56,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2067.0 (TID 4134, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:56,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2067.0 (TID 4135, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:56,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2067.0 (TID 4134)
2017-07-26 18:13:56,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2067.0 (TID 4135)
2017-07-26 18:13:56,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:56,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:56,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2067.0 (TID 4135). 714 bytes result sent to driver
2017-07-26 18:13:56,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2067.0 (TID 4134). 714 bytes result sent to driver
2017-07-26 18:13:56,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2067.0 (TID 4135) in 8 ms on localhost (1/2)
2017-07-26 18:13:56,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2067.0 (TID 4134) in 11 ms on localhost (2/2)
2017-07-26 18:13:56,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2067.0, whose tasks have all completed, from pool 
2017-07-26 18:13:56,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2067 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:13:56,084 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2067 finished: foreachPartition at streamingProcessTest.scala:48, took 0.031821 s
2017-07-26 18:13:56,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064036000 ms.0 from job set of time 1501064036000 ms
2017-07-26 18:13:56,085 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2066 from persistence list
2017-07-26 18:13:56,085 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1501064036000 ms (execution: 0.065 s)
2017-07-26 18:13:56,085 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2066
2017-07-26 18:13:56,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:56,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064032000 ms
2017-07-26 18:13:58,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064038000 ms
2017-07-26 18:13:58,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064038000 ms.0 from job set of time 1501064038000 ms
2017-07-26 18:13:58,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:13:58,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2068 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:13:58,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2068 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:13:58,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:13:58,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:13:58,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2068 (KafkaRDD[2068] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:13:58,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2068 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:13:58,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2068_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:13:58,036 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2068_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:13:58,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2068 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:13:58,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2068 (KafkaRDD[2068] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:13:58,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2068.0 with 2 tasks
2017-07-26 18:13:58,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2068.0 (TID 4136, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:13:58,038 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2068.0 (TID 4137, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:13:58,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2068.0 (TID 4136)
2017-07-26 18:13:58,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2068.0 (TID 4137)
2017-07-26 18:13:58,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:13:58,040 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:13:58,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2068.0 (TID 4137). 635 bytes result sent to driver
2017-07-26 18:13:58,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2068.0 (TID 4136). 635 bytes result sent to driver
2017-07-26 18:13:58,043 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2068.0 (TID 4136) in 6 ms on localhost (1/2)
2017-07-26 18:13:58,043 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2068.0 (TID 4137) in 5 ms on localhost (2/2)
2017-07-26 18:13:58,043 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2068.0, whose tasks have all completed, from pool 
2017-07-26 18:13:58,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2068 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:13:58,044 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2068 finished: foreachPartition at streamingProcessTest.scala:48, took 0.016718 s
2017-07-26 18:13:58,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064038000 ms.0 from job set of time 1501064038000 ms
2017-07-26 18:13:58,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.044 s for time 1501064038000 ms (execution: 0.028 s)
2017-07-26 18:13:58,044 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2067 from persistence list
2017-07-26 18:13:58,044 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2067
2017-07-26 18:13:58,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:13:58,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064034000 ms
2017-07-26 18:14:00,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064040000 ms
2017-07-26 18:14:00,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064040000 ms.0 from job set of time 1501064040000 ms
2017-07-26 18:14:00,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2069 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2069 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2069 (KafkaRDD[2069] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2069 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:14:00,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2069_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.3 MB)
2017-07-26 18:14:00,087 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2069_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:00,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2069 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:00,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2069 (KafkaRDD[2069] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:00,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2069.0 with 2 tasks
2017-07-26 18:14:00,089 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2055_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:00,092 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2069.0 (TID 4138, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:00,093 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2069.0 (TID 4139, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:00,094 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2069.0 (TID 4138)
2017-07-26 18:14:00,094 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2069.0 (TID 4139)
2017-07-26 18:14:00,096 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2056_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:00,099 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:00,100 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:00,102 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2057_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:00,103 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2069.0 (TID 4138). 635 bytes result sent to driver
2017-07-26 18:14:00,104 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2069.0 (TID 4139). 635 bytes result sent to driver
2017-07-26 18:14:00,107 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2058_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:00,109 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2069.0 (TID 4138) in 19 ms on localhost (1/2)
2017-07-26 18:14:00,110 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2069.0 (TID 4139) in 18 ms on localhost (2/2)
2017-07-26 18:14:00,110 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2069.0, whose tasks have all completed, from pool 
2017-07-26 18:14:00,111 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2069 (foreachPartition at streamingProcessTest.scala:48) finished in 0.022 s
2017-07-26 18:14:00,111 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2059_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:00,112 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2069 finished: foreachPartition at streamingProcessTest.scala:48, took 0.063576 s
2017-07-26 18:14:00,113 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064040000 ms.0 from job set of time 1501064040000 ms
2017-07-26 18:14:00,113 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2068 from persistence list
2017-07-26 18:14:00,113 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.113 s for time 1501064040000 ms (execution: 0.097 s)
2017-07-26 18:14:00,114 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2068
2017-07-26 18:14:00,114 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2060_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:00,114 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:00,115 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064036000 ms
2017-07-26 18:14:00,117 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2061_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:00,119 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2062_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:00,121 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2063_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:00,123 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2064_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:00,124 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2065_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:00,127 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2066_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:00,128 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2067_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:00,130 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2068_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:02,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064042000 ms
2017-07-26 18:14:02,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064042000 ms.0 from job set of time 1501064042000 ms
2017-07-26 18:14:02,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:02,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2070 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:02,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2070 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:02,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:02,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:02,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2070 (KafkaRDD[2070] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:02,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2070 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:14:02,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2070_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:14:02,034 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2070_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:02,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2070 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:02,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2070 (KafkaRDD[2070] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:02,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2070.0 with 2 tasks
2017-07-26 18:14:02,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2070.0 (TID 4140, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:02,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2070.0 (TID 4141, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:02,037 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2070.0 (TID 4141)
2017-07-26 18:14:02,037 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2070.0 (TID 4140)
2017-07-26 18:14:02,039 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:02,039 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:02,040 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2070.0 (TID 4140). 635 bytes result sent to driver
2017-07-26 18:14:02,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2070.0 (TID 4141). 635 bytes result sent to driver
2017-07-26 18:14:02,042 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2070.0 (TID 4140) in 6 ms on localhost (1/2)
2017-07-26 18:14:02,042 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2070.0 (TID 4141) in 6 ms on localhost (2/2)
2017-07-26 18:14:02,042 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2070.0, whose tasks have all completed, from pool 
2017-07-26 18:14:02,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2070 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:14:02,043 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2070 finished: foreachPartition at streamingProcessTest.scala:48, took 0.017834 s
2017-07-26 18:14:02,043 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064042000 ms.0 from job set of time 1501064042000 ms
2017-07-26 18:14:02,043 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2069 from persistence list
2017-07-26 18:14:02,043 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.043 s for time 1501064042000 ms (execution: 0.029 s)
2017-07-26 18:14:02,043 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2069
2017-07-26 18:14:02,044 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:02,044 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064038000 ms
2017-07-26 18:14:04,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064044000 ms
2017-07-26 18:14:04,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064044000 ms.0 from job set of time 1501064044000 ms
2017-07-26 18:14:04,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:04,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2071 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:04,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2071 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:04,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:04,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:04,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2071 (KafkaRDD[2071] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:04,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2071 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:14:04,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2071_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:14:04,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2071_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:04,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2071 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:04,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2071 (KafkaRDD[2071] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:04,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2071.0 with 2 tasks
2017-07-26 18:14:04,039 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2071.0 (TID 4142, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:04,040 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2071.0 (TID 4143, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:04,040 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2071.0 (TID 4143)
2017-07-26 18:14:04,040 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2071.0 (TID 4142)
2017-07-26 18:14:04,042 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:04,042 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:04,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2071.0 (TID 4142). 635 bytes result sent to driver
2017-07-26 18:14:04,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2071.0 (TID 4143). 635 bytes result sent to driver
2017-07-26 18:14:04,047 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2071.0 (TID 4142) in 7 ms on localhost (1/2)
2017-07-26 18:14:04,047 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2071.0 (TID 4143) in 8 ms on localhost (2/2)
2017-07-26 18:14:04,047 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2071.0, whose tasks have all completed, from pool 
2017-07-26 18:14:04,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2071 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:14:04,048 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2071 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021029 s
2017-07-26 18:14:04,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064044000 ms.0 from job set of time 1501064044000 ms
2017-07-26 18:14:04,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1501064044000 ms (execution: 0.035 s)
2017-07-26 18:14:04,048 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2070 from persistence list
2017-07-26 18:14:04,048 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2070
2017-07-26 18:14:04,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:04,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064040000 ms
2017-07-26 18:14:06,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064046000 ms
2017-07-26 18:14:06,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064046000 ms.0 from job set of time 1501064046000 ms
2017-07-26 18:14:06,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:06,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2072 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:06,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2072 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:06,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:06,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:06,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2072 (KafkaRDD[2072] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:06,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2072 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:14:06,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2072_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:14:06,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2072_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:06,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2072 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:06,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2072 (KafkaRDD[2072] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:06,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2072.0 with 2 tasks
2017-07-26 18:14:06,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2072.0 (TID 4144, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:06,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2072.0 (TID 4145, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:06,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2072.0 (TID 4145)
2017-07-26 18:14:06,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2072.0 (TID 4144)
2017-07-26 18:14:06,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:06,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:06,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2072.0 (TID 4144). 714 bytes result sent to driver
2017-07-26 18:14:06,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2072.0 (TID 4145). 714 bytes result sent to driver
2017-07-26 18:14:06,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2072.0 (TID 4145) in 6 ms on localhost (1/2)
2017-07-26 18:14:06,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2072.0 (TID 4144) in 9 ms on localhost (2/2)
2017-07-26 18:14:06,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2072.0, whose tasks have all completed, from pool 
2017-07-26 18:14:06,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2072 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:14:06,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2072 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022062 s
2017-07-26 18:14:06,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064046000 ms.0 from job set of time 1501064046000 ms
2017-07-26 18:14:06,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1501064046000 ms (execution: 0.048 s)
2017-07-26 18:14:06,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2071 from persistence list
2017-07-26 18:14:06,065 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2071
2017-07-26 18:14:06,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:06,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064042000 ms
2017-07-26 18:14:08,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064048000 ms
2017-07-26 18:14:08,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064048000 ms.0 from job set of time 1501064048000 ms
2017-07-26 18:14:08,022 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:08,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2073 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:08,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2073 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:08,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:08,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:08,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2073 (KafkaRDD[2073] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:08,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2073 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:14:08,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2073_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:14:08,028 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2073_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:08,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2073 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:08,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2073 (KafkaRDD[2073] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:08,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2073.0 with 2 tasks
2017-07-26 18:14:08,030 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2073.0 (TID 4146, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:08,030 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2073.0 (TID 4147, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:08,030 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2073.0 (TID 4146)
2017-07-26 18:14:08,030 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2073.0 (TID 4147)
2017-07-26 18:14:08,032 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:08,032 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:08,034 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2073.0 (TID 4146). 635 bytes result sent to driver
2017-07-26 18:14:08,034 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2073.0 (TID 4147). 635 bytes result sent to driver
2017-07-26 18:14:08,036 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2073.0 (TID 4146) in 7 ms on localhost (1/2)
2017-07-26 18:14:08,036 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2073.0 (TID 4147) in 6 ms on localhost (2/2)
2017-07-26 18:14:08,036 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2073.0, whose tasks have all completed, from pool 
2017-07-26 18:14:08,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2073 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:14:08,036 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2073 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014549 s
2017-07-26 18:14:08,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064048000 ms.0 from job set of time 1501064048000 ms
2017-07-26 18:14:08,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.037 s for time 1501064048000 ms (execution: 0.025 s)
2017-07-26 18:14:08,037 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2072 from persistence list
2017-07-26 18:14:08,038 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2072
2017-07-26 18:14:08,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:08,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064044000 ms
2017-07-26 18:14:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064050000 ms
2017-07-26 18:14:10,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064050000 ms.0 from job set of time 1501064050000 ms
2017-07-26 18:14:10,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2074 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2074 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:10,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2074 (KafkaRDD[2074] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2074 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:14:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2074_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:14:10,050 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2074_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2074 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2074 (KafkaRDD[2074] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2074.0 with 2 tasks
2017-07-26 18:14:10,052 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2074.0 (TID 4148, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:10,052 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2074.0 (TID 4149, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:10,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2074.0 (TID 4149)
2017-07-26 18:14:10,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2074.0 (TID 4148)
2017-07-26 18:14:10,054 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:10,055 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:10,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2074.0 (TID 4149). 635 bytes result sent to driver
2017-07-26 18:14:10,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2074.0 (TID 4148). 714 bytes result sent to driver
2017-07-26 18:14:10,059 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2074.0 (TID 4148) in 7 ms on localhost (1/2)
2017-07-26 18:14:10,059 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2074.0 (TID 4149) in 7 ms on localhost (2/2)
2017-07-26 18:14:10,059 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2074.0, whose tasks have all completed, from pool 
2017-07-26 18:14:10,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2074 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:14:10,059 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2074 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020141 s
2017-07-26 18:14:10,060 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064050000 ms.0 from job set of time 1501064050000 ms
2017-07-26 18:14:10,060 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2073 from persistence list
2017-07-26 18:14:10,061 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2073
2017-07-26 18:14:10,060 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.060 s for time 1501064050000 ms (execution: 0.045 s)
2017-07-26 18:14:10,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:10,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064046000 ms
2017-07-26 18:14:12,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064052000 ms
2017-07-26 18:14:12,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064052000 ms.0 from job set of time 1501064052000 ms
2017-07-26 18:14:12,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:12,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2075 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:12,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2075 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:12,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:12,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:12,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2075 (KafkaRDD[2075] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:12,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2075 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:14:12,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2075_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:14:12,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2075_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:12,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2075 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:12,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2075 (KafkaRDD[2075] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:12,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2075.0 with 2 tasks
2017-07-26 18:14:12,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2075.0 (TID 4150, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:12,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2075.0 (TID 4151, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:12,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2075.0 (TID 4151)
2017-07-26 18:14:12,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2075.0 (TID 4150)
2017-07-26 18:14:12,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:12,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:12,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2075.0 (TID 4151). 635 bytes result sent to driver
2017-07-26 18:14:12,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2075.0 (TID 4150). 635 bytes result sent to driver
2017-07-26 18:14:12,062 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2075.0 (TID 4150) in 7 ms on localhost (1/2)
2017-07-26 18:14:12,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2075.0 (TID 4151) in 6 ms on localhost (2/2)
2017-07-26 18:14:12,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2075.0, whose tasks have all completed, from pool 
2017-07-26 18:14:12,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2075 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:14:12,062 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2075 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021734 s
2017-07-26 18:14:12,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064052000 ms.0 from job set of time 1501064052000 ms
2017-07-26 18:14:12,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1501064052000 ms (execution: 0.046 s)
2017-07-26 18:14:12,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2074 from persistence list
2017-07-26 18:14:12,064 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2074
2017-07-26 18:14:12,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:12,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064048000 ms
2017-07-26 18:14:14,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064054000 ms
2017-07-26 18:14:14,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064054000 ms.0 from job set of time 1501064054000 ms
2017-07-26 18:14:14,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2076 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2076 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2076 (KafkaRDD[2076] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:14,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2076 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:14:14,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2076_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:14:14,031 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2076_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:14,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2076 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:14,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2076 (KafkaRDD[2076] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:14,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2076.0 with 2 tasks
2017-07-26 18:14:14,034 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2076.0 (TID 4152, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:14,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2076.0 (TID 4153, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:14,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2076.0 (TID 4152)
2017-07-26 18:14:14,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2076.0 (TID 4153)
2017-07-26 18:14:14,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:14,037 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:14,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2076.0 (TID 4153). 635 bytes result sent to driver
2017-07-26 18:14:14,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2076.0 (TID 4152). 635 bytes result sent to driver
2017-07-26 18:14:14,040 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2076.0 (TID 4152) in 7 ms on localhost (1/2)
2017-07-26 18:14:14,040 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2076.0 (TID 4153) in 6 ms on localhost (2/2)
2017-07-26 18:14:14,041 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2076.0, whose tasks have all completed, from pool 
2017-07-26 18:14:14,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2076 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:14:14,041 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2076 finished: foreachPartition at streamingProcessTest.scala:48, took 0.017227 s
2017-07-26 18:14:14,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064054000 ms.0 from job set of time 1501064054000 ms
2017-07-26 18:14:14,042 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1501064054000 ms (execution: 0.029 s)
2017-07-26 18:14:14,042 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2075 from persistence list
2017-07-26 18:14:14,042 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2075
2017-07-26 18:14:14,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:14,042 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064050000 ms
2017-07-26 18:14:16,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064056000 ms
2017-07-26 18:14:16,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064056000 ms.0 from job set of time 1501064056000 ms
2017-07-26 18:14:16,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2077 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2077 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2077 (KafkaRDD[2077] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:16,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2077 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:14:16,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2077_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:14:16,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2077_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2077 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2077 (KafkaRDD[2077] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2077.0 with 2 tasks
2017-07-26 18:14:16,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2077.0 (TID 4154, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:16,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2077.0 (TID 4155, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:16,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2077.0 (TID 4154)
2017-07-26 18:14:16,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2077.0 (TID 4155)
2017-07-26 18:14:16,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:16,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:16,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2077.0 (TID 4155). 714 bytes result sent to driver
2017-07-26 18:14:16,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2077.0 (TID 4154). 714 bytes result sent to driver
2017-07-26 18:14:16,070 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2077.0 (TID 4155) in 11 ms on localhost (1/2)
2017-07-26 18:14:16,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2077.0 (TID 4154) in 12 ms on localhost (2/2)
2017-07-26 18:14:16,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2077.0, whose tasks have all completed, from pool 
2017-07-26 18:14:16,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2077 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:14:16,071 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2077 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025624 s
2017-07-26 18:14:16,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064056000 ms.0 from job set of time 1501064056000 ms
2017-07-26 18:14:16,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501064056000 ms (execution: 0.056 s)
2017-07-26 18:14:16,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2076 from persistence list
2017-07-26 18:14:16,072 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2076
2017-07-26 18:14:16,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:16,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064052000 ms
2017-07-26 18:14:18,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064058000 ms
2017-07-26 18:14:18,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064058000 ms.0 from job set of time 1501064058000 ms
2017-07-26 18:14:18,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2078 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2078 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:18,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:18,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2078 (KafkaRDD[2078] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:18,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2078 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:14:18,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2078_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:14:18,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2078_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:18,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2078 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:18,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2078 (KafkaRDD[2078] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:18,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2078.0 with 2 tasks
2017-07-26 18:14:18,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2078.0 (TID 4156, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:18,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2078.0 (TID 4157, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:18,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2078.0 (TID 4157)
2017-07-26 18:14:18,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2078.0 (TID 4156)
2017-07-26 18:14:18,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:18,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:18,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2078.0 (TID 4157). 714 bytes result sent to driver
2017-07-26 18:14:18,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2078.0 (TID 4156). 635 bytes result sent to driver
2017-07-26 18:14:18,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2078.0 (TID 4157) in 14 ms on localhost (1/2)
2017-07-26 18:14:18,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2078.0 (TID 4156) in 17 ms on localhost (2/2)
2017-07-26 18:14:18,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2078.0, whose tasks have all completed, from pool 
2017-07-26 18:14:18,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2078 (foreachPartition at streamingProcessTest.scala:48) finished in 0.017 s
2017-07-26 18:14:18,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2078 finished: foreachPartition at streamingProcessTest.scala:48, took 0.039250 s
2017-07-26 18:14:18,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064058000 ms.0 from job set of time 1501064058000 ms
2017-07-26 18:14:18,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1501064058000 ms (execution: 0.072 s)
2017-07-26 18:14:18,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2077 from persistence list
2017-07-26 18:14:18,090 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2077
2017-07-26 18:14:18,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:18,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064054000 ms
2017-07-26 18:14:20,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064060000 ms
2017-07-26 18:14:20,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064060000 ms.0 from job set of time 1501064060000 ms
2017-07-26 18:14:20,035 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2079 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2079 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:20,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:20,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2079 (KafkaRDD[2079] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:20,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2079 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:14:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2079_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:14:20,047 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2079_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2079 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2079 (KafkaRDD[2079] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2079.0 with 2 tasks
2017-07-26 18:14:20,050 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2079.0 (TID 4158, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:20,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2079.0 (TID 4159, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:20,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2079.0 (TID 4158)
2017-07-26 18:14:20,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2079.0 (TID 4159)
2017-07-26 18:14:20,055 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:20,055 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:20,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2079.0 (TID 4159). 714 bytes result sent to driver
2017-07-26 18:14:20,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2079.0 (TID 4158). 714 bytes result sent to driver
2017-07-26 18:14:20,061 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2079.0 (TID 4159) in 10 ms on localhost (1/2)
2017-07-26 18:14:20,061 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2079.0 (TID 4158) in 12 ms on localhost (2/2)
2017-07-26 18:14:20,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2079.0, whose tasks have all completed, from pool 
2017-07-26 18:14:20,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2079 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:14:20,062 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2079 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026770 s
2017-07-26 18:14:20,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064060000 ms.0 from job set of time 1501064060000 ms
2017-07-26 18:14:20,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1501064060000 ms (execution: 0.048 s)
2017-07-26 18:14:20,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2078 from persistence list
2017-07-26 18:14:20,064 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2078
2017-07-26 18:14:20,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:20,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064056000 ms
2017-07-26 18:14:22,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064062000 ms
2017-07-26 18:14:22,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064062000 ms.0 from job set of time 1501064062000 ms
2017-07-26 18:14:22,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:22,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2080 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:22,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2080 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:22,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:22,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:22,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2080 (KafkaRDD[2080] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:22,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2080 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:14:22,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2080_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:14:22,043 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2080_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:22,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2080 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:22,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2080 (KafkaRDD[2080] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:22,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2080.0 with 2 tasks
2017-07-26 18:14:22,046 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2080.0 (TID 4160, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:22,047 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2080.0 (TID 4161, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:22,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2080.0 (TID 4160)
2017-07-26 18:14:22,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2080.0 (TID 4161)
2017-07-26 18:14:22,049 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:22,049 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:22,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2080.0 (TID 4160). 635 bytes result sent to driver
2017-07-26 18:14:22,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2080.0 (TID 4161). 635 bytes result sent to driver
2017-07-26 18:14:22,052 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2080.0 (TID 4160) in 7 ms on localhost (1/2)
2017-07-26 18:14:22,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2080.0 (TID 4161) in 6 ms on localhost (2/2)
2017-07-26 18:14:22,053 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2080.0, whose tasks have all completed, from pool 
2017-07-26 18:14:22,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2080 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:14:22,053 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2080 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019919 s
2017-07-26 18:14:22,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064062000 ms.0 from job set of time 1501064062000 ms
2017-07-26 18:14:22,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.053 s for time 1501064062000 ms (execution: 0.039 s)
2017-07-26 18:14:22,053 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2079 from persistence list
2017-07-26 18:14:22,054 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2079
2017-07-26 18:14:22,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:22,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064058000 ms
2017-07-26 18:14:24,010 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064064000 ms
2017-07-26 18:14:24,010 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064064000 ms.0 from job set of time 1501064064000 ms
2017-07-26 18:14:24,020 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:24,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2081 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:24,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2081 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:24,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:24,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:24,021 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2081 (KafkaRDD[2081] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:24,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2081 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:14:24,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2081_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:14:24,027 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2081_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:24,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2081 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:24,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2081 (KafkaRDD[2081] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:24,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2081.0 with 2 tasks
2017-07-26 18:14:24,029 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2081.0 (TID 4162, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:24,030 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2081.0 (TID 4163, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:24,030 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2081.0 (TID 4162)
2017-07-26 18:14:24,030 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2081.0 (TID 4163)
2017-07-26 18:14:24,031 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:24,032 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:24,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2081.0 (TID 4162). 635 bytes result sent to driver
2017-07-26 18:14:24,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2081.0 (TID 4163). 714 bytes result sent to driver
2017-07-26 18:14:24,036 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2081.0 (TID 4162) in 6 ms on localhost (1/2)
2017-07-26 18:14:24,036 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2081.0 (TID 4163) in 7 ms on localhost (2/2)
2017-07-26 18:14:24,036 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2081.0, whose tasks have all completed, from pool 
2017-07-26 18:14:24,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2081 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:14:24,036 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2081 finished: foreachPartition at streamingProcessTest.scala:48, took 0.016311 s
2017-07-26 18:14:24,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064064000 ms.0 from job set of time 1501064064000 ms
2017-07-26 18:14:24,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.037 s for time 1501064064000 ms (execution: 0.027 s)
2017-07-26 18:14:24,037 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2080 from persistence list
2017-07-26 18:14:24,037 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2080
2017-07-26 18:14:24,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:24,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064060000 ms
2017-07-26 18:14:26,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064066000 ms
2017-07-26 18:14:26,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064066000 ms.0 from job set of time 1501064066000 ms
2017-07-26 18:14:26,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:26,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2082 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:26,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2082 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:26,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:26,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:26,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2082 (KafkaRDD[2082] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:26,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2082 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:14:26,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2082_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:14:26,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2082_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:26,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2082 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:26,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2082 (KafkaRDD[2082] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:26,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2082.0 with 2 tasks
2017-07-26 18:14:26,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2082.0 (TID 4164, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:26,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2082.0 (TID 4165, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:26,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2082.0 (TID 4164)
2017-07-26 18:14:26,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2082.0 (TID 4165)
2017-07-26 18:14:26,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:26,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:26,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2082.0 (TID 4165). 714 bytes result sent to driver
2017-07-26 18:14:26,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2082.0 (TID 4164). 714 bytes result sent to driver
2017-07-26 18:14:26,078 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2082.0 (TID 4164) in 9 ms on localhost (1/2)
2017-07-26 18:14:26,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2082.0 (TID 4165) in 8 ms on localhost (2/2)
2017-07-26 18:14:26,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2082.0, whose tasks have all completed, from pool 
2017-07-26 18:14:26,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2082 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:14:26,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2082 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028162 s
2017-07-26 18:14:26,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064066000 ms.0 from job set of time 1501064066000 ms
2017-07-26 18:14:26,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1501064066000 ms (execution: 0.062 s)
2017-07-26 18:14:26,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2081 from persistence list
2017-07-26 18:14:26,080 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2081
2017-07-26 18:14:26,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:26,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064062000 ms
2017-07-26 18:14:28,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064068000 ms
2017-07-26 18:14:28,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064068000 ms.0 from job set of time 1501064068000 ms
2017-07-26 18:14:28,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:28,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2083 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:28,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2083 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:28,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:28,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:28,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2083 (KafkaRDD[2083] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:28,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2082_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:28,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2083 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:14:28,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2069_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:28,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2070_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:28,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2083_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:14:28,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2083_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:28,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2083 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:28,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2083 (KafkaRDD[2083] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:28,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2083.0 with 2 tasks
2017-07-26 18:14:28,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2071_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:28,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2083.0 (TID 4166, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:28,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2083.0 (TID 4167, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:28,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2083.0 (TID 4166)
2017-07-26 18:14:28,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2072_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:28,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2083.0 (TID 4167)
2017-07-26 18:14:28,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2073_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:28,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:28,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:28,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2074_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:28,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2083.0 (TID 4167). 635 bytes result sent to driver
2017-07-26 18:14:28,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2083.0 (TID 4166). 635 bytes result sent to driver
2017-07-26 18:14:28,076 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2075_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:28,077 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2083.0 (TID 4167) in 8 ms on localhost (1/2)
2017-07-26 18:14:28,077 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2083.0 (TID 4166) in 9 ms on localhost (2/2)
2017-07-26 18:14:28,077 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2083.0, whose tasks have all completed, from pool 
2017-07-26 18:14:28,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2083 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:14:28,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2076_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:28,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2083 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028465 s
2017-07-26 18:14:28,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064068000 ms.0 from job set of time 1501064068000 ms
2017-07-26 18:14:28,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1501064068000 ms (execution: 0.059 s)
2017-07-26 18:14:28,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2082 from persistence list
2017-07-26 18:14:28,079 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2082
2017-07-26 18:14:28,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:28,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064064000 ms
2017-07-26 18:14:28,079 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2077_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:28,080 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2078_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:28,082 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2079_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:28,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2080_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:28,085 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2081_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:30,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064070000 ms
2017-07-26 18:14:30,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064070000 ms.0 from job set of time 1501064070000 ms
2017-07-26 18:14:30,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2084 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2084 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2084 (KafkaRDD[2084] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2084 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:14:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2084_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:14:30,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2084_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2084 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2084 (KafkaRDD[2084] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2084.0 with 2 tasks
2017-07-26 18:14:30,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2084.0 (TID 4168, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:30,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2084.0 (TID 4169, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:30,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2084.0 (TID 4168)
2017-07-26 18:14:30,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2084.0 (TID 4169)
2017-07-26 18:14:30,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:30,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:30,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2084.0 (TID 4168). 635 bytes result sent to driver
2017-07-26 18:14:30,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2084.0 (TID 4169). 635 bytes result sent to driver
2017-07-26 18:14:30,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2084.0 (TID 4169) in 9 ms on localhost (1/2)
2017-07-26 18:14:30,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2084.0 (TID 4168) in 10 ms on localhost (2/2)
2017-07-26 18:14:30,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2084.0, whose tasks have all completed, from pool 
2017-07-26 18:14:30,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2084 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:14:30,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2084 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026283 s
2017-07-26 18:14:30,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064070000 ms.0 from job set of time 1501064070000 ms
2017-07-26 18:14:30,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501064070000 ms (execution: 0.060 s)
2017-07-26 18:14:30,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2083 from persistence list
2017-07-26 18:14:30,076 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2083
2017-07-26 18:14:30,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:30,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064066000 ms
2017-07-26 18:14:32,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064072000 ms
2017-07-26 18:14:32,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064072000 ms.0 from job set of time 1501064072000 ms
2017-07-26 18:14:32,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:32,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2085 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:32,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2085 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:32,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:32,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:32,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2085 (KafkaRDD[2085] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:32,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2085 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:14:32,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2085_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:14:32,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2085_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:32,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2085 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:32,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2085 (KafkaRDD[2085] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:32,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2085.0 with 2 tasks
2017-07-26 18:14:32,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2085.0 (TID 4170, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:32,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2085.0 (TID 4171, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:32,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2085.0 (TID 4171)
2017-07-26 18:14:32,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2085.0 (TID 4170)
2017-07-26 18:14:32,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:32,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:32,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2085.0 (TID 4171). 635 bytes result sent to driver
2017-07-26 18:14:32,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2085.0 (TID 4170). 714 bytes result sent to driver
2017-07-26 18:14:32,079 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2085.0 (TID 4171) in 10 ms on localhost (1/2)
2017-07-26 18:14:32,080 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2085.0 (TID 4170) in 11 ms on localhost (2/2)
2017-07-26 18:14:32,080 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2085.0, whose tasks have all completed, from pool 
2017-07-26 18:14:32,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2085 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:14:32,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2085 finished: foreachPartition at streamingProcessTest.scala:48, took 0.032467 s
2017-07-26 18:14:32,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064072000 ms.0 from job set of time 1501064072000 ms
2017-07-26 18:14:32,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501064072000 ms (execution: 0.065 s)
2017-07-26 18:14:32,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2084 from persistence list
2017-07-26 18:14:32,082 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2084
2017-07-26 18:14:32,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:32,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064068000 ms
2017-07-26 18:14:34,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064074000 ms
2017-07-26 18:14:34,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064074000 ms.0 from job set of time 1501064074000 ms
2017-07-26 18:14:34,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2086 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2086 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:34,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2086 (KafkaRDD[2086] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:34,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2086 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:14:34,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2086_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:14:34,041 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2086_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:34,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2086 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:34,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2086 (KafkaRDD[2086] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:34,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2086.0 with 2 tasks
2017-07-26 18:14:34,043 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2086.0 (TID 4172, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:34,044 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2086.0 (TID 4173, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:34,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2086.0 (TID 4172)
2017-07-26 18:14:34,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2086.0 (TID 4173)
2017-07-26 18:14:34,047 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:34,047 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:34,049 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2086.0 (TID 4172). 635 bytes result sent to driver
2017-07-26 18:14:34,049 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2086.0 (TID 4173). 635 bytes result sent to driver
2017-07-26 18:14:34,050 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2086.0 (TID 4173) in 6 ms on localhost (1/2)
2017-07-26 18:14:34,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2086.0 (TID 4172) in 9 ms on localhost (2/2)
2017-07-26 18:14:34,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2086.0, whose tasks have all completed, from pool 
2017-07-26 18:14:34,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2086 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:14:34,051 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2086 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020721 s
2017-07-26 18:14:34,051 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064074000 ms.0 from job set of time 1501064074000 ms
2017-07-26 18:14:34,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.051 s for time 1501064074000 ms (execution: 0.038 s)
2017-07-26 18:14:34,052 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2085 from persistence list
2017-07-26 18:14:34,052 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2085
2017-07-26 18:14:34,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:34,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064070000 ms
2017-07-26 18:14:36,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064076000 ms
2017-07-26 18:14:36,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064076000 ms.0 from job set of time 1501064076000 ms
2017-07-26 18:14:36,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:36,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2087 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:36,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2087 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:36,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:36,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:36,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2087 (KafkaRDD[2087] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:36,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2087 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:14:36,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2087_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:14:36,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2087_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:36,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2087 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:36,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2087 (KafkaRDD[2087] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:36,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2087.0 with 2 tasks
2017-07-26 18:14:36,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2087.0 (TID 4174, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:36,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2087.0 (TID 4175, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:36,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2087.0 (TID 4175)
2017-07-26 18:14:36,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2087.0 (TID 4174)
2017-07-26 18:14:36,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:36,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:36,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2087.0 (TID 4174). 635 bytes result sent to driver
2017-07-26 18:14:36,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2087.0 (TID 4175). 635 bytes result sent to driver
2017-07-26 18:14:36,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2087.0 (TID 4175) in 9 ms on localhost (1/2)
2017-07-26 18:14:36,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2087.0 (TID 4174) in 10 ms on localhost (2/2)
2017-07-26 18:14:36,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2087.0, whose tasks have all completed, from pool 
2017-07-26 18:14:36,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2087 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:14:36,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2087 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025005 s
2017-07-26 18:14:36,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064076000 ms.0 from job set of time 1501064076000 ms
2017-07-26 18:14:36,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501064076000 ms (execution: 0.060 s)
2017-07-26 18:14:36,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2086 from persistence list
2017-07-26 18:14:36,076 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2086
2017-07-26 18:14:36,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:36,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064072000 ms
2017-07-26 18:14:38,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064078000 ms
2017-07-26 18:14:38,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064078000 ms.0 from job set of time 1501064078000 ms
2017-07-26 18:14:38,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:38,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2088 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:38,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2088 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:38,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:38,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:38,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2088 (KafkaRDD[2088] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:38,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2088 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:14:38,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2088_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:14:38,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2088_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:38,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2088 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:38,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2088 (KafkaRDD[2088] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:38,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2088.0 with 2 tasks
2017-07-26 18:14:38,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2088.0 (TID 4176, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:38,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2088.0 (TID 4177, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:38,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2088.0 (TID 4177)
2017-07-26 18:14:38,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2088.0 (TID 4176)
2017-07-26 18:14:38,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:38,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:38,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2088.0 (TID 4177). 635 bytes result sent to driver
2017-07-26 18:14:38,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2088.0 (TID 4176). 635 bytes result sent to driver
2017-07-26 18:14:38,086 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2088.0 (TID 4177) in 16 ms on localhost (1/2)
2017-07-26 18:14:38,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2088.0 (TID 4176) in 19 ms on localhost (2/2)
2017-07-26 18:14:38,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2088.0, whose tasks have all completed, from pool 
2017-07-26 18:14:38,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2088 (foreachPartition at streamingProcessTest.scala:48) finished in 0.019 s
2017-07-26 18:14:38,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2088 finished: foreachPartition at streamingProcessTest.scala:48, took 0.041249 s
2017-07-26 18:14:38,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064078000 ms.0 from job set of time 1501064078000 ms
2017-07-26 18:14:38,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2087 from persistence list
2017-07-26 18:14:38,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1501064078000 ms (execution: 0.073 s)
2017-07-26 18:14:38,090 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2087
2017-07-26 18:14:38,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:38,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064074000 ms
2017-07-26 18:14:40,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064080000 ms
2017-07-26 18:14:40,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064080000 ms.0 from job set of time 1501064080000 ms
2017-07-26 18:14:40,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2089 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2089 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2089 (KafkaRDD[2089] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2089 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:14:40,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2089_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:14:40,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2089_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2089 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2089 (KafkaRDD[2089] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2089.0 with 2 tasks
2017-07-26 18:14:40,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2089.0 (TID 4178, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:40,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2089.0 (TID 4179, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:40,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2089.0 (TID 4179)
2017-07-26 18:14:40,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2089.0 (TID 4178)
2017-07-26 18:14:40,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:40,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:40,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2089.0 (TID 4179). 635 bytes result sent to driver
2017-07-26 18:14:40,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2089.0 (TID 4178). 801 bytes result sent to driver
2017-07-26 18:14:40,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2089.0 (TID 4179) in 8 ms on localhost (1/2)
2017-07-26 18:14:40,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2089.0 (TID 4178) in 9 ms on localhost (2/2)
2017-07-26 18:14:40,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2089 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:14:40,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2089.0, whose tasks have all completed, from pool 
2017-07-26 18:14:40,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2089 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021294 s
2017-07-26 18:14:40,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064080000 ms.0 from job set of time 1501064080000 ms
2017-07-26 18:14:40,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2088 from persistence list
2017-07-26 18:14:40,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1501064080000 ms (execution: 0.052 s)
2017-07-26 18:14:40,069 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2088
2017-07-26 18:14:40,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:40,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064076000 ms
2017-07-26 18:14:42,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064082000 ms
2017-07-26 18:14:42,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064082000 ms.0 from job set of time 1501064082000 ms
2017-07-26 18:14:42,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:42,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2090 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:42,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2090 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:42,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:42,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:42,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2090 (KafkaRDD[2090] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:42,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2090 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:14:42,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2090_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:14:42,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2090_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:42,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2090 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:42,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2090 (KafkaRDD[2090] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:42,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2090.0 with 2 tasks
2017-07-26 18:14:42,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2090.0 (TID 4180, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:42,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2090.0 (TID 4181, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:42,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2090.0 (TID 4181)
2017-07-26 18:14:42,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2090.0 (TID 4180)
2017-07-26 18:14:42,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:42,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:42,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2090.0 (TID 4180). 635 bytes result sent to driver
2017-07-26 18:14:42,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2090.0 (TID 4181). 635 bytes result sent to driver
2017-07-26 18:14:42,072 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2090.0 (TID 4180) in 8 ms on localhost (1/2)
2017-07-26 18:14:42,072 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2090.0 (TID 4181) in 8 ms on localhost (2/2)
2017-07-26 18:14:42,072 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2090.0, whose tasks have all completed, from pool 
2017-07-26 18:14:42,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2090 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:14:42,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2090 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025551 s
2017-07-26 18:14:42,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064082000 ms.0 from job set of time 1501064082000 ms
2017-07-26 18:14:42,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1501064082000 ms (execution: 0.057 s)
2017-07-26 18:14:42,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2089 from persistence list
2017-07-26 18:14:42,074 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2089
2017-07-26 18:14:42,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:42,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064078000 ms
2017-07-26 18:14:44,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064084000 ms
2017-07-26 18:14:44,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064084000 ms.0 from job set of time 1501064084000 ms
2017-07-26 18:14:44,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:44,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2091 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:44,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2091 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:44,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:44,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:44,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2091 (KafkaRDD[2091] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:44,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2091 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:14:44,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2091_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:14:44,038 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2091_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:44,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2091 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:44,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2091 (KafkaRDD[2091] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:44,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2091.0 with 2 tasks
2017-07-26 18:14:44,041 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2091.0 (TID 4182, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:44,041 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2091.0 (TID 4183, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:44,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2091.0 (TID 4183)
2017-07-26 18:14:44,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2091.0 (TID 4182)
2017-07-26 18:14:44,043 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:44,044 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:44,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2091.0 (TID 4182). 635 bytes result sent to driver
2017-07-26 18:14:44,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2091.0 (TID 4183). 714 bytes result sent to driver
2017-07-26 18:14:44,049 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2091.0 (TID 4183) in 8 ms on localhost (1/2)
2017-07-26 18:14:44,049 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2091.0 (TID 4182) in 9 ms on localhost (2/2)
2017-07-26 18:14:44,050 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2091.0, whose tasks have all completed, from pool 
2017-07-26 18:14:44,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2091 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:14:44,050 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2091 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022496 s
2017-07-26 18:14:44,051 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064084000 ms.0 from job set of time 1501064084000 ms
2017-07-26 18:14:44,051 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2090 from persistence list
2017-07-26 18:14:44,051 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.051 s for time 1501064084000 ms (execution: 0.039 s)
2017-07-26 18:14:44,051 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2090
2017-07-26 18:14:44,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:44,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064080000 ms
2017-07-26 18:14:46,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064086000 ms
2017-07-26 18:14:46,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064086000 ms.0 from job set of time 1501064086000 ms
2017-07-26 18:14:46,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:46,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2092 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:46,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2092 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:46,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:46,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:46,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2092 (KafkaRDD[2092] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:46,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2092 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:14:46,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2092_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:14:46,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2092_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:46,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2092 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:46,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2092 (KafkaRDD[2092] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:46,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2092.0 with 2 tasks
2017-07-26 18:14:46,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2092.0 (TID 4184, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:46,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2092.0 (TID 4185, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:46,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2092.0 (TID 4184)
2017-07-26 18:14:46,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2092.0 (TID 4185)
2017-07-26 18:14:46,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:46,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:46,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2092.0 (TID 4185). 714 bytes result sent to driver
2017-07-26 18:14:46,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2092.0 (TID 4184). 714 bytes result sent to driver
2017-07-26 18:14:46,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2092.0 (TID 4185) in 14 ms on localhost (1/2)
2017-07-26 18:14:46,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2092.0 (TID 4184) in 16 ms on localhost (2/2)
2017-07-26 18:14:46,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2092.0, whose tasks have all completed, from pool 
2017-07-26 18:14:46,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2092 (foreachPartition at streamingProcessTest.scala:48) finished in 0.016 s
2017-07-26 18:14:46,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2092 finished: foreachPartition at streamingProcessTest.scala:48, took 0.030148 s
2017-07-26 18:14:46,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064086000 ms.0 from job set of time 1501064086000 ms
2017-07-26 18:14:46,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1501064086000 ms (execution: 0.061 s)
2017-07-26 18:14:46,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2091 from persistence list
2017-07-26 18:14:46,079 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2091
2017-07-26 18:14:46,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:46,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064082000 ms
2017-07-26 18:14:48,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064088000 ms
2017-07-26 18:14:48,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064088000 ms.0 from job set of time 1501064088000 ms
2017-07-26 18:14:48,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2093 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2093 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2093 (KafkaRDD[2093] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:48,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2093 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:14:48,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2093_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:14:48,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2093_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:48,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2093 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:48,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2093 (KafkaRDD[2093] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:48,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2093.0 with 2 tasks
2017-07-26 18:14:48,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2093.0 (TID 4186, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:48,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2093.0 (TID 4187, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:48,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2093.0 (TID 4187)
2017-07-26 18:14:48,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2093.0 (TID 4186)
2017-07-26 18:14:48,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:48,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:48,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2093.0 (TID 4186). 714 bytes result sent to driver
2017-07-26 18:14:48,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2093.0 (TID 4187). 714 bytes result sent to driver
2017-07-26 18:14:48,089 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2093.0 (TID 4187) in 19 ms on localhost (1/2)
2017-07-26 18:14:48,089 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2093.0 (TID 4186) in 21 ms on localhost (2/2)
2017-07-26 18:14:48,089 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2093.0, whose tasks have all completed, from pool 
2017-07-26 18:14:48,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2093 (foreachPartition at streamingProcessTest.scala:48) finished in 0.021 s
2017-07-26 18:14:48,090 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2093 finished: foreachPartition at streamingProcessTest.scala:48, took 0.042537 s
2017-07-26 18:14:48,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064088000 ms.0 from job set of time 1501064088000 ms
2017-07-26 18:14:48,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1501064088000 ms (execution: 0.073 s)
2017-07-26 18:14:48,091 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2092 from persistence list
2017-07-26 18:14:48,091 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2092
2017-07-26 18:14:48,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:48,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064084000 ms
2017-07-26 18:14:50,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064090000 ms
2017-07-26 18:14:50,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064090000 ms.0 from job set of time 1501064090000 ms
2017-07-26 18:14:50,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:50,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2094 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:50,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2094 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:50,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:50,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2094 (KafkaRDD[2094] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2094 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:14:50,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2094_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:14:50,052 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2094_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:50,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2094 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2094 (KafkaRDD[2094] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2094.0 with 2 tasks
2017-07-26 18:14:50,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2094.0 (TID 4188, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:50,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2094.0 (TID 4189, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:50,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2094.0 (TID 4189)
2017-07-26 18:14:50,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2094.0 (TID 4188)
2017-07-26 18:14:50,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:50,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:50,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2094.0 (TID 4188). 635 bytes result sent to driver
2017-07-26 18:14:50,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2094.0 (TID 4189). 635 bytes result sent to driver
2017-07-26 18:14:50,064 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2094.0 (TID 4189) in 9 ms on localhost (1/2)
2017-07-26 18:14:50,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2094.0 (TID 4188) in 12 ms on localhost (2/2)
2017-07-26 18:14:50,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2094.0, whose tasks have all completed, from pool 
2017-07-26 18:14:50,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2094 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:14:50,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2094 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023878 s
2017-07-26 18:14:50,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064090000 ms.0 from job set of time 1501064090000 ms
2017-07-26 18:14:50,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2093 from persistence list
2017-07-26 18:14:50,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501064090000 ms (execution: 0.050 s)
2017-07-26 18:14:50,067 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2093
2017-07-26 18:14:50,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:50,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064086000 ms
2017-07-26 18:14:52,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064092000 ms
2017-07-26 18:14:52,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064092000 ms.0 from job set of time 1501064092000 ms
2017-07-26 18:14:52,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2095 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2095 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:52,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:52,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2095 (KafkaRDD[2095] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:52,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2095 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:14:52,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2095_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:14:52,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2095_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:52,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2095 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:52,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2095 (KafkaRDD[2095] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:52,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2095.0 with 2 tasks
2017-07-26 18:14:52,076 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2095.0 (TID 4190, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:52,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2095.0 (TID 4191, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:52,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2095.0 (TID 4190)
2017-07-26 18:14:52,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2095.0 (TID 4191)
2017-07-26 18:14:52,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:52,081 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:52,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2095.0 (TID 4191). 635 bytes result sent to driver
2017-07-26 18:14:52,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2095.0 (TID 4190). 635 bytes result sent to driver
2017-07-26 18:14:52,086 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2095.0 (TID 4191) in 9 ms on localhost (1/2)
2017-07-26 18:14:52,087 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2095.0 (TID 4190) in 12 ms on localhost (2/2)
2017-07-26 18:14:52,087 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2095.0, whose tasks have all completed, from pool 
2017-07-26 18:14:52,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2095 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:14:52,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2095 finished: foreachPartition at streamingProcessTest.scala:48, took 0.037731 s
2017-07-26 18:14:52,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064092000 ms.0 from job set of time 1501064092000 ms
2017-07-26 18:14:52,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1501064092000 ms (execution: 0.069 s)
2017-07-26 18:14:52,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2094 from persistence list
2017-07-26 18:14:52,089 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2094
2017-07-26 18:14:52,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:52,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064088000 ms
2017-07-26 18:14:54,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064094000 ms
2017-07-26 18:14:54,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064094000 ms.0 from job set of time 1501064094000 ms
2017-07-26 18:14:54,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:54,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2096 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:54,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2096 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:54,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:54,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:54,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2096 (KafkaRDD[2096] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:54,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2096 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:14:54,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2096_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:14:54,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2096_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:54,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2096 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:54,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2096 (KafkaRDD[2096] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:54,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2096.0 with 2 tasks
2017-07-26 18:14:54,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2096.0 (TID 4192, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:54,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2096.0 (TID 4193, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:54,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2096.0 (TID 4193)
2017-07-26 18:14:54,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2096.0 (TID 4192)
2017-07-26 18:14:54,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:54,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:54,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2096.0 (TID 4193). 714 bytes result sent to driver
2017-07-26 18:14:54,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2096.0 (TID 4192). 714 bytes result sent to driver
2017-07-26 18:14:54,061 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2096.0 (TID 4193) in 7 ms on localhost (1/2)
2017-07-26 18:14:54,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2096.0 (TID 4192) in 8 ms on localhost (2/2)
2017-07-26 18:14:54,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2096.0, whose tasks have all completed, from pool 
2017-07-26 18:14:54,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2096 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:14:54,062 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2096 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021801 s
2017-07-26 18:14:54,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064094000 ms.0 from job set of time 1501064094000 ms
2017-07-26 18:14:54,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.062 s for time 1501064094000 ms (execution: 0.042 s)
2017-07-26 18:14:54,062 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2095 from persistence list
2017-07-26 18:14:54,063 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2095
2017-07-26 18:14:54,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:54,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064090000 ms
2017-07-26 18:14:56,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064096000 ms
2017-07-26 18:14:56,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064096000 ms.0 from job set of time 1501064096000 ms
2017-07-26 18:14:56,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2096_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:56,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2083_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:56,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2084_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:14:56,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2085_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:56,065 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:56,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2097 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:56,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2097 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:56,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:56,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:56,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2097 (KafkaRDD[2097] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:56,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2086_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:56,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2087_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:56,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2097 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:14:56,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2088_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:56,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2089_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:56,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2097_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:14:56,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2097_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:56,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2090_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:56,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2097 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:56,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2097 (KafkaRDD[2097] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:56,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2097.0 with 2 tasks
2017-07-26 18:14:56,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2097.0 (TID 4194, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:56,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2091_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:14:56,078 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2097.0 (TID 4195, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:56,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2097.0 (TID 4195)
2017-07-26 18:14:56,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2097.0 (TID 4194)
2017-07-26 18:14:56,080 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2092_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:56,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:56,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:56,082 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2093_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:56,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2094_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:56,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2097.0 (TID 4195). 635 bytes result sent to driver
2017-07-26 18:14:56,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2097.0 (TID 4194). 714 bytes result sent to driver
2017-07-26 18:14:56,086 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2095_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:56,086 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2097.0 (TID 4194) in 10 ms on localhost (1/2)
2017-07-26 18:14:56,087 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2097.0 (TID 4195) in 9 ms on localhost (2/2)
2017-07-26 18:14:56,087 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2097.0, whose tasks have all completed, from pool 
2017-07-26 18:14:56,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2097 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:14:56,087 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2097 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022188 s
2017-07-26 18:14:56,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064096000 ms.0 from job set of time 1501064096000 ms
2017-07-26 18:14:56,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1501064096000 ms (execution: 0.073 s)
2017-07-26 18:14:56,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2096 from persistence list
2017-07-26 18:14:56,088 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2096
2017-07-26 18:14:56,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:56,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064092000 ms
2017-07-26 18:14:58,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064098000 ms
2017-07-26 18:14:58,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064098000 ms.0 from job set of time 1501064098000 ms
2017-07-26 18:14:58,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:14:58,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2098 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:14:58,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2098 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:14:58,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:14:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:14:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2098 (KafkaRDD[2098] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:14:58,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2098 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:14:58,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2098_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:14:58,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2098_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:14:58,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2098 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:14:58,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2098 (KafkaRDD[2098] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:14:58,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2098.0 with 2 tasks
2017-07-26 18:14:58,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2098.0 (TID 4196, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:14:58,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2098.0 (TID 4197, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:14:58,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2098.0 (TID 4196)
2017-07-26 18:14:58,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2098.0 (TID 4197)
2017-07-26 18:14:58,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:14:58,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:14:58,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2098.0 (TID 4196). 714 bytes result sent to driver
2017-07-26 18:14:58,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2098.0 (TID 4197). 635 bytes result sent to driver
2017-07-26 18:14:58,084 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2098.0 (TID 4197) in 14 ms on localhost (1/2)
2017-07-26 18:14:58,084 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2098.0 (TID 4196) in 16 ms on localhost (2/2)
2017-07-26 18:14:58,085 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2098.0, whose tasks have all completed, from pool 
2017-07-26 18:14:58,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2098 (foreachPartition at streamingProcessTest.scala:48) finished in 0.017 s
2017-07-26 18:14:58,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2098 finished: foreachPartition at streamingProcessTest.scala:48, took 0.038233 s
2017-07-26 18:14:58,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064098000 ms.0 from job set of time 1501064098000 ms
2017-07-26 18:14:58,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.086 s for time 1501064098000 ms (execution: 0.069 s)
2017-07-26 18:14:58,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2097 from persistence list
2017-07-26 18:14:58,087 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2097
2017-07-26 18:14:58,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:14:58,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064094000 ms
2017-07-26 18:15:00,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064100000 ms
2017-07-26 18:15:00,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064100000 ms.0 from job set of time 1501064100000 ms
2017-07-26 18:15:00,027 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:00,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2099 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:00,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2099 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:00,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:00,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:00,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2099 (KafkaRDD[2099] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:00,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2099 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:15:00,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2099_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:15:00,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2099_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:00,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2099 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:00,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2099 (KafkaRDD[2099] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:00,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2099.0 with 2 tasks
2017-07-26 18:15:00,035 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2099.0 (TID 4198, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:00,035 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2099.0 (TID 4199, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:00,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2099.0 (TID 4199)
2017-07-26 18:15:00,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2099.0 (TID 4198)
2017-07-26 18:15:00,038 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:00,038 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:00,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2099.0 (TID 4198). 714 bytes result sent to driver
2017-07-26 18:15:00,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2099.0 (TID 4199). 635 bytes result sent to driver
2017-07-26 18:15:00,041 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2099.0 (TID 4198) in 7 ms on localhost (1/2)
2017-07-26 18:15:00,041 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2099.0 (TID 4199) in 6 ms on localhost (2/2)
2017-07-26 18:15:00,042 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2099.0, whose tasks have all completed, from pool 
2017-07-26 18:15:00,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2099 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:15:00,042 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2099 finished: foreachPartition at streamingProcessTest.scala:48, took 0.015220 s
2017-07-26 18:15:00,042 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064100000 ms.0 from job set of time 1501064100000 ms
2017-07-26 18:15:00,042 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.042 s for time 1501064100000 ms (execution: 0.029 s)
2017-07-26 18:15:00,042 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2098 from persistence list
2017-07-26 18:15:00,043 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2098
2017-07-26 18:15:00,043 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:00,043 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064096000 ms
2017-07-26 18:15:02,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064102000 ms
2017-07-26 18:15:02,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064102000 ms.0 from job set of time 1501064102000 ms
2017-07-26 18:15:02,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:02,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2100 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:02,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2100 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:02,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:02,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:02,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2100 (KafkaRDD[2100] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2100 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:15:02,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2100_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:15:02,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2100_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:02,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2100 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:02,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2100 (KafkaRDD[2100] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:02,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2100.0 with 2 tasks
2017-07-26 18:15:02,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2100.0 (TID 4200, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:02,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2100.0 (TID 4201, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:02,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2100.0 (TID 4201)
2017-07-26 18:15:02,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2100.0 (TID 4200)
2017-07-26 18:15:02,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:02,063 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:02,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2100.0 (TID 4200). 722 bytes result sent to driver
2017-07-26 18:15:02,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2100.0 (TID 4201). 722 bytes result sent to driver
2017-07-26 18:15:02,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2100.0 (TID 4200) in 11 ms on localhost (1/2)
2017-07-26 18:15:02,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2100.0 (TID 4201) in 10 ms on localhost (2/2)
2017-07-26 18:15:02,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2100.0, whose tasks have all completed, from pool 
2017-07-26 18:15:02,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2100 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:15:02,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2100 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025807 s
2017-07-26 18:15:02,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064102000 ms.0 from job set of time 1501064102000 ms
2017-07-26 18:15:02,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501064102000 ms (execution: 0.054 s)
2017-07-26 18:15:02,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2099 from persistence list
2017-07-26 18:15:02,071 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2099
2017-07-26 18:15:02,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:02,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064098000 ms
2017-07-26 18:15:04,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064104000 ms
2017-07-26 18:15:04,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064104000 ms.0 from job set of time 1501064104000 ms
2017-07-26 18:15:04,035 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:04,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2101 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:04,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2101 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:04,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:04,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:04,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2101 (KafkaRDD[2101] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:04,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2101 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:15:04,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2101_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:15:04,048 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2101_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2101 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2101 (KafkaRDD[2101] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2101.0 with 2 tasks
2017-07-26 18:15:04,052 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2101.0 (TID 4202, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:04,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2101.0 (TID 4203, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:04,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2101.0 (TID 4203)
2017-07-26 18:15:04,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2101.0 (TID 4202)
2017-07-26 18:15:04,056 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:04,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:04,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2101.0 (TID 4203). 714 bytes result sent to driver
2017-07-26 18:15:04,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2101.0 (TID 4202). 714 bytes result sent to driver
2017-07-26 18:15:04,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2101.0 (TID 4203) in 9 ms on localhost (1/2)
2017-07-26 18:15:04,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2101.0 (TID 4202) in 12 ms on localhost (2/2)
2017-07-26 18:15:04,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2101.0, whose tasks have all completed, from pool 
2017-07-26 18:15:04,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2101 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:15:04,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2101 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027548 s
2017-07-26 18:15:04,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064104000 ms.0 from job set of time 1501064104000 ms
2017-07-26 18:15:04,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1501064104000 ms (execution: 0.048 s)
2017-07-26 18:15:04,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2100 from persistence list
2017-07-26 18:15:04,065 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2100
2017-07-26 18:15:04,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:04,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064100000 ms
2017-07-26 18:15:06,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064106000 ms
2017-07-26 18:15:06,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064106000 ms.0 from job set of time 1501064106000 ms
2017-07-26 18:15:06,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:06,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2102 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:06,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2102 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:06,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:06,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:06,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2102 (KafkaRDD[2102] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:06,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2102 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:15:06,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2102_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:15:06,064 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2102_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:06,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2102 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:06,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2102 (KafkaRDD[2102] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:06,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2102.0 with 2 tasks
2017-07-26 18:15:06,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2102.0 (TID 4204, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:06,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2102.0 (TID 4205, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:06,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2102.0 (TID 4205)
2017-07-26 18:15:06,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2102.0 (TID 4204)
2017-07-26 18:15:06,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:06,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:06,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2102.0 (TID 4204). 635 bytes result sent to driver
2017-07-26 18:15:06,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2102.0 (TID 4205). 635 bytes result sent to driver
2017-07-26 18:15:06,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2102.0 (TID 4204) in 12 ms on localhost (1/2)
2017-07-26 18:15:06,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2102.0 (TID 4205) in 12 ms on localhost (2/2)
2017-07-26 18:15:06,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2102.0, whose tasks have all completed, from pool 
2017-07-26 18:15:06,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2102 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:15:06,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2102 finished: foreachPartition at streamingProcessTest.scala:48, took 0.030449 s
2017-07-26 18:15:06,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064106000 ms.0 from job set of time 1501064106000 ms
2017-07-26 18:15:06,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501064106000 ms (execution: 0.065 s)
2017-07-26 18:15:06,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2101 from persistence list
2017-07-26 18:15:06,082 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2101
2017-07-26 18:15:06,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:06,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064102000 ms
2017-07-26 18:15:08,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064108000 ms
2017-07-26 18:15:08,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064108000 ms.0 from job set of time 1501064108000 ms
2017-07-26 18:15:08,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2103 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:08,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2103 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:08,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:08,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:08,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2103 (KafkaRDD[2103] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:08,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2103 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:15:08,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2103_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:15:08,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2103_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:08,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2103 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:08,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2103 (KafkaRDD[2103] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:08,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2103.0 with 2 tasks
2017-07-26 18:15:08,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2103.0 (TID 4206, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:08,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2103.0 (TID 4207, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:08,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2103.0 (TID 4207)
2017-07-26 18:15:08,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2103.0 (TID 4206)
2017-07-26 18:15:08,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:08,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:08,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2103.0 (TID 4207). 635 bytes result sent to driver
2017-07-26 18:15:08,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2103.0 (TID 4206). 714 bytes result sent to driver
2017-07-26 18:15:08,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2103.0 (TID 4207) in 12 ms on localhost (1/2)
2017-07-26 18:15:08,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2103.0 (TID 4206) in 14 ms on localhost (2/2)
2017-07-26 18:15:08,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2103.0, whose tasks have all completed, from pool 
2017-07-26 18:15:08,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2103 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:15:08,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2103 finished: foreachPartition at streamingProcessTest.scala:48, took 0.036310 s
2017-07-26 18:15:08,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064108000 ms.0 from job set of time 1501064108000 ms
2017-07-26 18:15:08,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.085 s for time 1501064108000 ms (execution: 0.067 s)
2017-07-26 18:15:08,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2102 from persistence list
2017-07-26 18:15:08,087 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2102
2017-07-26 18:15:08,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:08,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064104000 ms
2017-07-26 18:15:10,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064110000 ms
2017-07-26 18:15:10,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064110000 ms.0 from job set of time 1501064110000 ms
2017-07-26 18:15:10,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:10,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2104 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2104 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:10,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2104 (KafkaRDD[2104] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:10,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2104 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:15:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2104_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:15:10,049 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2104_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2104 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2104 (KafkaRDD[2104] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2104.0 with 2 tasks
2017-07-26 18:15:10,052 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2104.0 (TID 4208, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:10,052 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2104.0 (TID 4209, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:10,052 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2104.0 (TID 4208)
2017-07-26 18:15:10,052 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2104.0 (TID 4209)
2017-07-26 18:15:10,055 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:10,055 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:10,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2104.0 (TID 4208). 635 bytes result sent to driver
2017-07-26 18:15:10,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2104.0 (TID 4209). 635 bytes result sent to driver
2017-07-26 18:15:10,060 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2104.0 (TID 4208) in 9 ms on localhost (1/2)
2017-07-26 18:15:10,060 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2104.0 (TID 4209) in 8 ms on localhost (2/2)
2017-07-26 18:15:10,061 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2104.0, whose tasks have all completed, from pool 
2017-07-26 18:15:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2104 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:15:10,061 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2104 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023417 s
2017-07-26 18:15:10,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064110000 ms.0 from job set of time 1501064110000 ms
2017-07-26 18:15:10,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.062 s for time 1501064110000 ms (execution: 0.045 s)
2017-07-26 18:15:10,062 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2103 from persistence list
2017-07-26 18:15:10,062 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2103
2017-07-26 18:15:10,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:10,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064106000 ms
2017-07-26 18:15:12,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064112000 ms
2017-07-26 18:15:12,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064112000 ms.0 from job set of time 1501064112000 ms
2017-07-26 18:15:12,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:12,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2105 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:12,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2105 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:12,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:12,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:12,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2105 (KafkaRDD[2105] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:12,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2105 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:15:12,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2105_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:15:12,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2105_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:12,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2105 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:12,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2105 (KafkaRDD[2105] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:12,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2105.0 with 2 tasks
2017-07-26 18:15:12,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2105.0 (TID 4210, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:12,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2105.0 (TID 4211, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:12,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2105.0 (TID 4211)
2017-07-26 18:15:12,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2105.0 (TID 4210)
2017-07-26 18:15:12,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:12,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:12,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2105.0 (TID 4211). 714 bytes result sent to driver
2017-07-26 18:15:12,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2105.0 (TID 4210). 635 bytes result sent to driver
2017-07-26 18:15:12,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2105.0 (TID 4211) in 9 ms on localhost (1/2)
2017-07-26 18:15:12,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2105.0 (TID 4210) in 11 ms on localhost (2/2)
2017-07-26 18:15:12,076 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2105.0, whose tasks have all completed, from pool 
2017-07-26 18:15:12,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2105 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:15:12,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2105 finished: foreachPartition at streamingProcessTest.scala:48, took 0.029255 s
2017-07-26 18:15:12,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064112000 ms.0 from job set of time 1501064112000 ms
2017-07-26 18:15:12,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2104 from persistence list
2017-07-26 18:15:12,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1501064112000 ms (execution: 0.060 s)
2017-07-26 18:15:12,078 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2104
2017-07-26 18:15:12,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:12,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064108000 ms
2017-07-26 18:15:14,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064114000 ms
2017-07-26 18:15:14,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064114000 ms.0 from job set of time 1501064114000 ms
2017-07-26 18:15:14,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:14,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2106 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:14,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2106 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:14,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:14,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:14,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2106 (KafkaRDD[2106] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:14,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2106 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:15:14,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2106_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:15:14,042 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2106_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:14,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2106 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:14,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2106 (KafkaRDD[2106] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:14,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2106.0 with 2 tasks
2017-07-26 18:15:14,044 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2106.0 (TID 4212, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:14,044 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2106.0 (TID 4213, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:14,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2106.0 (TID 4213)
2017-07-26 18:15:14,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2106.0 (TID 4212)
2017-07-26 18:15:14,046 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:14,046 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:14,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2106.0 (TID 4212). 635 bytes result sent to driver
2017-07-26 18:15:14,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2106.0 (TID 4213). 635 bytes result sent to driver
2017-07-26 18:15:14,050 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2106.0 (TID 4212) in 7 ms on localhost (1/2)
2017-07-26 18:15:14,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2106.0 (TID 4213) in 7 ms on localhost (2/2)
2017-07-26 18:15:14,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2106.0, whose tasks have all completed, from pool 
2017-07-26 18:15:14,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2106 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:15:14,051 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2106 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014550 s
2017-07-26 18:15:14,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064114000 ms.0 from job set of time 1501064114000 ms
2017-07-26 18:15:14,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.052 s for time 1501064114000 ms (execution: 0.038 s)
2017-07-26 18:15:14,052 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2105 from persistence list
2017-07-26 18:15:14,052 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2105
2017-07-26 18:15:14,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:14,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064110000 ms
2017-07-26 18:15:16,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064116000 ms
2017-07-26 18:15:16,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064116000 ms.0 from job set of time 1501064116000 ms
2017-07-26 18:15:16,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:16,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2107 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:16,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2107 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:16,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:16,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:16,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2107 (KafkaRDD[2107] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:16,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2107 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:15:16,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2107_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:15:16,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2107_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:16,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2107 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:16,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2107 (KafkaRDD[2107] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:16,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2107.0 with 2 tasks
2017-07-26 18:15:16,037 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2107.0 (TID 4214, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:16,038 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2107.0 (TID 4215, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:16,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2107.0 (TID 4215)
2017-07-26 18:15:16,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2107.0 (TID 4214)
2017-07-26 18:15:16,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:16,040 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:16,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2107.0 (TID 4215). 635 bytes result sent to driver
2017-07-26 18:15:16,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2107.0 (TID 4214). 635 bytes result sent to driver
2017-07-26 18:15:16,044 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2107.0 (TID 4215) in 6 ms on localhost (1/2)
2017-07-26 18:15:16,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2107.0 (TID 4214) in 8 ms on localhost (2/2)
2017-07-26 18:15:16,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2107.0, whose tasks have all completed, from pool 
2017-07-26 18:15:16,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2107 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:15:16,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2107 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019310 s
2017-07-26 18:15:16,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064116000 ms.0 from job set of time 1501064116000 ms
2017-07-26 18:15:16,047 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2106 from persistence list
2017-07-26 18:15:16,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.046 s for time 1501064116000 ms (execution: 0.031 s)
2017-07-26 18:15:16,048 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2106
2017-07-26 18:15:16,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:16,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064112000 ms
2017-07-26 18:15:18,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064118000 ms
2017-07-26 18:15:18,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064118000 ms.0 from job set of time 1501064118000 ms
2017-07-26 18:15:18,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2108 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2108 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:18,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2108 (KafkaRDD[2108] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:18,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2108 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:15:18,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2108_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:15:18,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2108_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:18,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2108 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:18,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2108 (KafkaRDD[2108] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:18,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2108.0 with 2 tasks
2017-07-26 18:15:18,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2108.0 (TID 4216, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:18,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2108.0 (TID 4217, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:18,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2108.0 (TID 4216)
2017-07-26 18:15:18,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2108.0 (TID 4217)
2017-07-26 18:15:18,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:18,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:18,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2108.0 (TID 4217). 714 bytes result sent to driver
2017-07-26 18:15:18,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2108.0 (TID 4216). 714 bytes result sent to driver
2017-07-26 18:15:18,087 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2108.0 (TID 4217) in 15 ms on localhost (1/2)
2017-07-26 18:15:18,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2108.0 (TID 4216) in 18 ms on localhost (2/2)
2017-07-26 18:15:18,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2108.0, whose tasks have all completed, from pool 
2017-07-26 18:15:18,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2108 (foreachPartition at streamingProcessTest.scala:48) finished in 0.018 s
2017-07-26 18:15:18,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2108 finished: foreachPartition at streamingProcessTest.scala:48, took 0.039113 s
2017-07-26 18:15:18,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064118000 ms.0 from job set of time 1501064118000 ms
2017-07-26 18:15:18,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1501064118000 ms (execution: 0.070 s)
2017-07-26 18:15:18,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2107 from persistence list
2017-07-26 18:15:18,089 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2107
2017-07-26 18:15:18,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:18,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064114000 ms
2017-07-26 18:15:20,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064120000 ms
2017-07-26 18:15:20,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064120000 ms.0 from job set of time 1501064120000 ms
2017-07-26 18:15:20,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2109 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2109 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2109 (KafkaRDD[2109] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2109 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:15:20,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2109_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:15:20,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2109_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:20,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2109 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2109 (KafkaRDD[2109] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2109.0 with 2 tasks
2017-07-26 18:15:20,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2109.0 (TID 4218, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:20,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2109.0 (TID 4219, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:20,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2109.0 (TID 4219)
2017-07-26 18:15:20,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2109.0 (TID 4218)
2017-07-26 18:15:20,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:20,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:20,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2109.0 (TID 4218). 714 bytes result sent to driver
2017-07-26 18:15:20,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2109.0 (TID 4219). 635 bytes result sent to driver
2017-07-26 18:15:20,064 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2109.0 (TID 4219) in 8 ms on localhost (1/2)
2017-07-26 18:15:20,064 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2109.0 (TID 4218) in 9 ms on localhost (2/2)
2017-07-26 18:15:20,065 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2109.0, whose tasks have all completed, from pool 
2017-07-26 18:15:20,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2109 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:15:20,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2109 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022694 s
2017-07-26 18:15:20,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064120000 ms.0 from job set of time 1501064120000 ms
2017-07-26 18:15:20,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1501064120000 ms (execution: 0.048 s)
2017-07-26 18:15:20,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2108 from persistence list
2017-07-26 18:15:20,066 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2108
2017-07-26 18:15:20,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:20,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064116000 ms
2017-07-26 18:15:22,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064122000 ms
2017-07-26 18:15:22,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064122000 ms.0 from job set of time 1501064122000 ms
2017-07-26 18:15:22,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2110 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2110 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:22,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:22,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2110 (KafkaRDD[2110] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:22,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2110 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:15:22,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2110_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:15:22,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2110_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:22,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2110 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:22,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2110 (KafkaRDD[2110] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:22,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2110.0 with 2 tasks
2017-07-26 18:15:22,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2110.0 (TID 4220, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:22,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2110.0 (TID 4221, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:22,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2110.0 (TID 4221)
2017-07-26 18:15:22,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2110.0 (TID 4220)
2017-07-26 18:15:22,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:22,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:22,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2110.0 (TID 4220). 708 bytes result sent to driver
2017-07-26 18:15:22,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2110.0 (TID 4221). 787 bytes result sent to driver
2017-07-26 18:15:22,084 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2097_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:22,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2110.0 (TID 4221) in 24 ms on localhost (1/2)
2017-07-26 18:15:22,087 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2110.0 (TID 4220) in 25 ms on localhost (2/2)
2017-07-26 18:15:22,087 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2110.0, whose tasks have all completed, from pool 
2017-07-26 18:15:22,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2110 (foreachPartition at streamingProcessTest.scala:48) finished in 0.025 s
2017-07-26 18:15:22,088 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2098_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:22,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2110 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040497 s
2017-07-26 18:15:22,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064122000 ms.0 from job set of time 1501064122000 ms
2017-07-26 18:15:22,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1501064122000 ms (execution: 0.071 s)
2017-07-26 18:15:22,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2109 from persistence list
2017-07-26 18:15:22,089 [block-manager-slave-async-thread-pool-20] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2109
2017-07-26 18:15:22,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:22,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064118000 ms
2017-07-26 18:15:22,090 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2099_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:22,092 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2100_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:22,094 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2101_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:22,096 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2102_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:22,098 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2103_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:22,100 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2104_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:22,102 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2105_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:22,103 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2106_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:22,105 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2107_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:22,107 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2108_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:22,109 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2109_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:24,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064124000 ms
2017-07-26 18:15:24,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064124000 ms.0 from job set of time 1501064124000 ms
2017-07-26 18:15:24,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:24,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2111 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:24,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2111 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:24,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:24,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:24,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2111 (KafkaRDD[2111] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:24,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2111 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:15:24,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2111_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:15:24,058 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2111_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:24,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2111 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:24,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2111 (KafkaRDD[2111] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:24,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2111.0 with 2 tasks
2017-07-26 18:15:24,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2111.0 (TID 4222, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:24,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2111.0 (TID 4223, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:24,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2111.0 (TID 4223)
2017-07-26 18:15:24,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2111.0 (TID 4222)
2017-07-26 18:15:24,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:24,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:24,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2111.0 (TID 4222). 635 bytes result sent to driver
2017-07-26 18:15:24,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2111.0 (TID 4223). 714 bytes result sent to driver
2017-07-26 18:15:24,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2111.0 (TID 4222) in 9 ms on localhost (1/2)
2017-07-26 18:15:24,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2111.0 (TID 4223) in 8 ms on localhost (2/2)
2017-07-26 18:15:24,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2111.0, whose tasks have all completed, from pool 
2017-07-26 18:15:24,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2111 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:15:24,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2111 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024803 s
2017-07-26 18:15:24,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064124000 ms.0 from job set of time 1501064124000 ms
2017-07-26 18:15:24,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501064124000 ms (execution: 0.051 s)
2017-07-26 18:15:24,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2110 from persistence list
2017-07-26 18:15:24,071 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2110
2017-07-26 18:15:24,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:24,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064120000 ms
2017-07-26 18:15:26,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064126000 ms
2017-07-26 18:15:26,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064126000 ms.0 from job set of time 1501064126000 ms
2017-07-26 18:15:26,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:26,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2112 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:26,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2112 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:26,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:26,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:26,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2112 (KafkaRDD[2112] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:26,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2112 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:15:26,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2112_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:15:26,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2112_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:26,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2112 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:26,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2112 (KafkaRDD[2112] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:26,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2112.0 with 2 tasks
2017-07-26 18:15:26,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2112.0 (TID 4224, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:26,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2112.0 (TID 4225, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:26,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2112.0 (TID 4224)
2017-07-26 18:15:26,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2112.0 (TID 4225)
2017-07-26 18:15:26,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:26,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:26,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2112.0 (TID 4224). 635 bytes result sent to driver
2017-07-26 18:15:26,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2112.0 (TID 4225). 635 bytes result sent to driver
2017-07-26 18:15:26,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2112.0 (TID 4225) in 9 ms on localhost (1/2)
2017-07-26 18:15:26,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2112.0 (TID 4224) in 11 ms on localhost (2/2)
2017-07-26 18:15:26,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2112.0, whose tasks have all completed, from pool 
2017-07-26 18:15:26,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2112 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:15:26,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2112 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022591 s
2017-07-26 18:15:26,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064126000 ms.0 from job set of time 1501064126000 ms
2017-07-26 18:15:26,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1501064126000 ms (execution: 0.048 s)
2017-07-26 18:15:26,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2111 from persistence list
2017-07-26 18:15:26,068 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2111
2017-07-26 18:15:26,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:26,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064122000 ms
2017-07-26 18:15:28,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064128000 ms
2017-07-26 18:15:28,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064128000 ms.0 from job set of time 1501064128000 ms
2017-07-26 18:15:28,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:28,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2113 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:28,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2113 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:28,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:28,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:28,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2113 (KafkaRDD[2113] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:28,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2113 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:15:28,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2113_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:15:28,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2113_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:28,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2113 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:28,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2113 (KafkaRDD[2113] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:28,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2113.0 with 2 tasks
2017-07-26 18:15:28,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2113.0 (TID 4226, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:28,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2113.0 (TID 4227, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:28,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2113.0 (TID 4227)
2017-07-26 18:15:28,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2113.0 (TID 4226)
2017-07-26 18:15:28,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:28,066 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:28,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2113.0 (TID 4226). 635 bytes result sent to driver
2017-07-26 18:15:28,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2113.0 (TID 4227). 714 bytes result sent to driver
2017-07-26 18:15:28,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2113.0 (TID 4226) in 9 ms on localhost (1/2)
2017-07-26 18:15:28,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2113.0 (TID 4227) in 9 ms on localhost (2/2)
2017-07-26 18:15:28,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2113.0, whose tasks have all completed, from pool 
2017-07-26 18:15:28,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2113 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:15:28,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2113 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022578 s
2017-07-26 18:15:28,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064128000 ms.0 from job set of time 1501064128000 ms
2017-07-26 18:15:28,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501064128000 ms (execution: 0.052 s)
2017-07-26 18:15:28,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2112 from persistence list
2017-07-26 18:15:28,073 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2112
2017-07-26 18:15:28,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:28,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064124000 ms
2017-07-26 18:15:30,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064130000 ms
2017-07-26 18:15:30,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064130000 ms.0 from job set of time 1501064130000 ms
2017-07-26 18:15:30,032 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:30,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2114 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2114 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:30,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2114 (KafkaRDD[2114] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:30,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2114 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:15:30,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2114_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:15:30,040 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2114_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2114 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2114 (KafkaRDD[2114] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:30,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2114.0 with 2 tasks
2017-07-26 18:15:30,043 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2114.0 (TID 4228, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:30,043 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2114.0 (TID 4229, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:30,043 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2114.0 (TID 4229)
2017-07-26 18:15:30,043 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2114.0 (TID 4228)
2017-07-26 18:15:30,046 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:30,046 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:30,049 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2114.0 (TID 4228). 714 bytes result sent to driver
2017-07-26 18:15:30,049 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2114.0 (TID 4229). 714 bytes result sent to driver
2017-07-26 18:15:30,051 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2114.0 (TID 4228) in 9 ms on localhost (1/2)
2017-07-26 18:15:30,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2114.0 (TID 4229) in 8 ms on localhost (2/2)
2017-07-26 18:15:30,051 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2114.0, whose tasks have all completed, from pool 
2017-07-26 18:15:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2114 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:15:30,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2114 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018928 s
2017-07-26 18:15:30,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064130000 ms.0 from job set of time 1501064130000 ms
2017-07-26 18:15:30,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.052 s for time 1501064130000 ms (execution: 0.033 s)
2017-07-26 18:15:30,052 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2113 from persistence list
2017-07-26 18:15:30,053 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2113
2017-07-26 18:15:30,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:30,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064126000 ms
2017-07-26 18:15:32,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064132000 ms
2017-07-26 18:15:32,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064132000 ms.0 from job set of time 1501064132000 ms
2017-07-26 18:15:32,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:32,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2115 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:32,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2115 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:32,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:32,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:32,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2115 (KafkaRDD[2115] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:32,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2115 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:15:32,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2115_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:15:32,053 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2115_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:32,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2115 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:32,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2115 (KafkaRDD[2115] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:32,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2115.0 with 2 tasks
2017-07-26 18:15:32,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2115.0 (TID 4230, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:32,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2115.0 (TID 4231, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:32,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2115.0 (TID 4230)
2017-07-26 18:15:32,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2115.0 (TID 4231)
2017-07-26 18:15:32,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:32,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:32,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2115.0 (TID 4231). 635 bytes result sent to driver
2017-07-26 18:15:32,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2115.0 (TID 4230). 635 bytes result sent to driver
2017-07-26 18:15:32,065 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2115.0 (TID 4231) in 9 ms on localhost (1/2)
2017-07-26 18:15:32,065 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2115.0 (TID 4230) in 10 ms on localhost (2/2)
2017-07-26 18:15:32,065 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2115.0, whose tasks have all completed, from pool 
2017-07-26 18:15:32,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2115 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:15:32,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2115 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022931 s
2017-07-26 18:15:32,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064132000 ms.0 from job set of time 1501064132000 ms
2017-07-26 18:15:32,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1501064132000 ms (execution: 0.050 s)
2017-07-26 18:15:32,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2114 from persistence list
2017-07-26 18:15:32,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:32,067 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2114
2017-07-26 18:15:32,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064128000 ms
2017-07-26 18:15:34,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064134000 ms
2017-07-26 18:15:34,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064134000 ms.0 from job set of time 1501064134000 ms
2017-07-26 18:15:34,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:34,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2116 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:34,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2116 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:34,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:34,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2116 (KafkaRDD[2116] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:34,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2116 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:15:34,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2116_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:15:34,039 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2116_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:34,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2116 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:34,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2116 (KafkaRDD[2116] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:34,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2116.0 with 2 tasks
2017-07-26 18:15:34,041 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2116.0 (TID 4232, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:34,042 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2116.0 (TID 4233, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:34,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2116.0 (TID 4232)
2017-07-26 18:15:34,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2116.0 (TID 4233)
2017-07-26 18:15:34,044 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:34,045 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:34,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2116.0 (TID 4233). 714 bytes result sent to driver
2017-07-26 18:15:34,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2116.0 (TID 4232). 635 bytes result sent to driver
2017-07-26 18:15:34,048 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2116.0 (TID 4233) in 7 ms on localhost (1/2)
2017-07-26 18:15:34,049 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2116.0 (TID 4232) in 9 ms on localhost (2/2)
2017-07-26 18:15:34,049 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2116.0, whose tasks have all completed, from pool 
2017-07-26 18:15:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2116 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:15:34,049 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2116 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019846 s
2017-07-26 18:15:34,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064134000 ms.0 from job set of time 1501064134000 ms
2017-07-26 18:15:34,051 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.050 s for time 1501064134000 ms (execution: 0.036 s)
2017-07-26 18:15:34,051 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2115 from persistence list
2017-07-26 18:15:34,051 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2115
2017-07-26 18:15:34,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:34,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064130000 ms
2017-07-26 18:15:36,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064136000 ms
2017-07-26 18:15:36,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064136000 ms.0 from job set of time 1501064136000 ms
2017-07-26 18:15:36,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:36,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2117 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:36,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2117 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:36,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:36,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:36,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2117 (KafkaRDD[2117] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:36,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2117 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:15:36,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2117_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:15:36,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2117_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:36,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2117 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:36,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2117 (KafkaRDD[2117] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:36,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2117.0 with 2 tasks
2017-07-26 18:15:36,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2117.0 (TID 4234, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:36,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2117.0 (TID 4235, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:36,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2117.0 (TID 4234)
2017-07-26 18:15:36,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2117.0 (TID 4235)
2017-07-26 18:15:36,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:36,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:36,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2117.0 (TID 4235). 635 bytes result sent to driver
2017-07-26 18:15:36,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2117.0 (TID 4234). 635 bytes result sent to driver
2017-07-26 18:15:36,071 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2117.0 (TID 4235) in 8 ms on localhost (1/2)
2017-07-26 18:15:36,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2117.0 (TID 4234) in 9 ms on localhost (2/2)
2017-07-26 18:15:36,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2117.0, whose tasks have all completed, from pool 
2017-07-26 18:15:36,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2117 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:15:36,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2117 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025079 s
2017-07-26 18:15:36,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064136000 ms.0 from job set of time 1501064136000 ms
2017-07-26 18:15:36,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501064136000 ms (execution: 0.057 s)
2017-07-26 18:15:36,073 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2116 from persistence list
2017-07-26 18:15:36,073 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2116
2017-07-26 18:15:36,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:36,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064132000 ms
2017-07-26 18:15:38,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064138000 ms
2017-07-26 18:15:38,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064138000 ms.0 from job set of time 1501064138000 ms
2017-07-26 18:15:38,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:38,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2118 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:38,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2118 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2118 (KafkaRDD[2118] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:38,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2118 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:15:38,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2118_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:15:38,052 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2118_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:38,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2118 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:38,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2118 (KafkaRDD[2118] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:38,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2118.0 with 2 tasks
2017-07-26 18:15:38,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2118.0 (TID 4236, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:38,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2118.0 (TID 4237, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:38,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2118.0 (TID 4237)
2017-07-26 18:15:38,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2118.0 (TID 4236)
2017-07-26 18:15:38,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:38,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:38,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2118.0 (TID 4237). 635 bytes result sent to driver
2017-07-26 18:15:38,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2118.0 (TID 4236). 635 bytes result sent to driver
2017-07-26 18:15:38,064 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2118.0 (TID 4236) in 10 ms on localhost (1/2)
2017-07-26 18:15:38,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2118.0 (TID 4237) in 9 ms on localhost (2/2)
2017-07-26 18:15:38,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2118 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:15:38,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2118.0, whose tasks have all completed, from pool 
2017-07-26 18:15:38,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2118 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023077 s
2017-07-26 18:15:38,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064138000 ms.0 from job set of time 1501064138000 ms
2017-07-26 18:15:38,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2117 from persistence list
2017-07-26 18:15:38,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1501064138000 ms (execution: 0.048 s)
2017-07-26 18:15:38,066 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2117
2017-07-26 18:15:38,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:38,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064134000 ms
2017-07-26 18:15:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064140000 ms
2017-07-26 18:15:40,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064140000 ms.0 from job set of time 1501064140000 ms
2017-07-26 18:15:40,022 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:40,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2119 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:40,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2119 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:40,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:40,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:40,023 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2119 (KafkaRDD[2119] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:40,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2119 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:15:40,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2119_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:15:40,029 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2119_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:40,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2119 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:40,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2119 (KafkaRDD[2119] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:40,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2119.0 with 2 tasks
2017-07-26 18:15:40,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2119.0 (TID 4238, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:40,031 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2119.0 (TID 4239, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:40,031 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2119.0 (TID 4239)
2017-07-26 18:15:40,031 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2119.0 (TID 4238)
2017-07-26 18:15:40,032 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:40,033 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:40,034 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2119.0 (TID 4238). 635 bytes result sent to driver
2017-07-26 18:15:40,034 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2119.0 (TID 4239). 714 bytes result sent to driver
2017-07-26 18:15:40,035 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2119.0 (TID 4238) in 5 ms on localhost (1/2)
2017-07-26 18:15:40,036 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2119.0 (TID 4239) in 5 ms on localhost (2/2)
2017-07-26 18:15:40,036 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2119.0, whose tasks have all completed, from pool 
2017-07-26 18:15:40,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2119 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:15:40,036 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2119 finished: foreachPartition at streamingProcessTest.scala:48, took 0.013691 s
2017-07-26 18:15:40,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064140000 ms.0 from job set of time 1501064140000 ms
2017-07-26 18:15:40,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.036 s for time 1501064140000 ms (execution: 0.022 s)
2017-07-26 18:15:40,036 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2118 from persistence list
2017-07-26 18:15:40,037 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2118
2017-07-26 18:15:40,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:40,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064136000 ms
2017-07-26 18:15:42,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064142000 ms
2017-07-26 18:15:42,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064142000 ms.0 from job set of time 1501064142000 ms
2017-07-26 18:15:42,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:42,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2120 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:42,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2120 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:42,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:42,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:42,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2120 (KafkaRDD[2120] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:42,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2120 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:15:42,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2120_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:15:42,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2120_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:42,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2120 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:42,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2120 (KafkaRDD[2120] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:42,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2120.0 with 2 tasks
2017-07-26 18:15:42,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2120.0 (TID 4240, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:42,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2120.0 (TID 4241, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:42,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2120.0 (TID 4241)
2017-07-26 18:15:42,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2120.0 (TID 4240)
2017-07-26 18:15:42,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:42,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:42,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2120.0 (TID 4240). 714 bytes result sent to driver
2017-07-26 18:15:42,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2120.0 (TID 4241). 714 bytes result sent to driver
2017-07-26 18:15:42,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2120.0 (TID 4240) in 11 ms on localhost (1/2)
2017-07-26 18:15:42,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2120.0 (TID 4241) in 11 ms on localhost (2/2)
2017-07-26 18:15:42,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2120.0, whose tasks have all completed, from pool 
2017-07-26 18:15:42,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2120 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:15:42,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2120 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028361 s
2017-07-26 18:15:42,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064142000 ms.0 from job set of time 1501064142000 ms
2017-07-26 18:15:42,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501064142000 ms (execution: 0.059 s)
2017-07-26 18:15:42,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2119 from persistence list
2017-07-26 18:15:42,077 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2119
2017-07-26 18:15:42,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:42,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064138000 ms
2017-07-26 18:15:44,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064144000 ms
2017-07-26 18:15:44,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064144000 ms.0 from job set of time 1501064144000 ms
2017-07-26 18:15:44,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:44,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2121 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:44,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2121 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:44,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:44,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:44,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2121 (KafkaRDD[2121] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:44,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2121 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:15:44,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2121_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:15:44,048 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2121_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:44,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2121 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:44,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2121 (KafkaRDD[2121] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:44,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2121.0 with 2 tasks
2017-07-26 18:15:44,050 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2121.0 (TID 4242, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:44,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2121.0 (TID 4243, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:44,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2121.0 (TID 4243)
2017-07-26 18:15:44,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2121.0 (TID 4242)
2017-07-26 18:15:44,053 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:44,054 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:44,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2121.0 (TID 4242). 635 bytes result sent to driver
2017-07-26 18:15:44,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2121.0 (TID 4243). 714 bytes result sent to driver
2017-07-26 18:15:44,058 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2121.0 (TID 4242) in 9 ms on localhost (1/2)
2017-07-26 18:15:44,058 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2121.0 (TID 4243) in 7 ms on localhost (2/2)
2017-07-26 18:15:44,058 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2121.0, whose tasks have all completed, from pool 
2017-07-26 18:15:44,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2121 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:15:44,058 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2121 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018560 s
2017-07-26 18:15:44,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064144000 ms.0 from job set of time 1501064144000 ms
2017-07-26 18:15:44,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.059 s for time 1501064144000 ms (execution: 0.044 s)
2017-07-26 18:15:44,059 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2120 from persistence list
2017-07-26 18:15:44,060 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2120
2017-07-26 18:15:44,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:44,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064140000 ms
2017-07-26 18:15:46,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064146000 ms
2017-07-26 18:15:46,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064146000 ms.0 from job set of time 1501064146000 ms
2017-07-26 18:15:46,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:46,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2122 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:46,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2122 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:46,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:46,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:46,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2122 (KafkaRDD[2122] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:46,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2122 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:15:46,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2122_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:15:46,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2122_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:46,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2122 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:46,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2122 (KafkaRDD[2122] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:46,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2122.0 with 2 tasks
2017-07-26 18:15:46,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2122.0 (TID 4244, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:46,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2122.0 (TID 4245, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:46,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2122.0 (TID 4244)
2017-07-26 18:15:46,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2122.0 (TID 4245)
2017-07-26 18:15:46,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:46,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:46,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2122.0 (TID 4244). 635 bytes result sent to driver
2017-07-26 18:15:46,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2122.0 (TID 4245). 635 bytes result sent to driver
2017-07-26 18:15:46,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2122.0 (TID 4244) in 8 ms on localhost (1/2)
2017-07-26 18:15:46,066 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2122.0 (TID 4245) in 7 ms on localhost (2/2)
2017-07-26 18:15:46,066 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2122.0, whose tasks have all completed, from pool 
2017-07-26 18:15:46,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2122 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:15:46,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2122 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020742 s
2017-07-26 18:15:46,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064146000 ms.0 from job set of time 1501064146000 ms
2017-07-26 18:15:46,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501064146000 ms (execution: 0.050 s)
2017-07-26 18:15:46,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2121 from persistence list
2017-07-26 18:15:46,067 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2121
2017-07-26 18:15:46,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:46,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064142000 ms
2017-07-26 18:15:48,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064148000 ms
2017-07-26 18:15:48,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064148000 ms.0 from job set of time 1501064148000 ms
2017-07-26 18:15:48,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2123 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2123 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:48,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2123 (KafkaRDD[2123] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:48,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2123 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:15:48,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2123_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:15:48,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2123_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:48,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2123 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:48,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2123 (KafkaRDD[2123] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:48,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2123.0 with 2 tasks
2017-07-26 18:15:48,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2123.0 (TID 4246, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:48,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2123.0 (TID 4247, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:48,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2123.0 (TID 4246)
2017-07-26 18:15:48,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2123.0 (TID 4247)
2017-07-26 18:15:48,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:48,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:48,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2123.0 (TID 4246). 635 bytes result sent to driver
2017-07-26 18:15:48,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2123.0 (TID 4247). 635 bytes result sent to driver
2017-07-26 18:15:48,073 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2123.0 (TID 4246) in 10 ms on localhost (1/2)
2017-07-26 18:15:48,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2123.0 (TID 4247) in 9 ms on localhost (2/2)
2017-07-26 18:15:48,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2123.0, whose tasks have all completed, from pool 
2017-07-26 18:15:48,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2123 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:15:48,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2123 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026868 s
2017-07-26 18:15:48,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064148000 ms.0 from job set of time 1501064148000 ms
2017-07-26 18:15:48,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1501064148000 ms (execution: 0.057 s)
2017-07-26 18:15:48,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2122 from persistence list
2017-07-26 18:15:48,075 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2122
2017-07-26 18:15:48,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:48,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064144000 ms
2017-07-26 18:15:50,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064150000 ms
2017-07-26 18:15:50,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064150000 ms.0 from job set of time 1501064150000 ms
2017-07-26 18:15:50,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2124 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2124 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2124 (KafkaRDD[2124] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2124 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:15:50,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2124_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.3 MB)
2017-07-26 18:15:50,080 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2124_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:50,080 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2110_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:50,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2124 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:50,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2124 (KafkaRDD[2124] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:50,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2124.0 with 2 tasks
2017-07-26 18:15:50,083 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2124.0 (TID 4248, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:50,083 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2111_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:50,083 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2124.0 (TID 4249, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:50,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2124.0 (TID 4249)
2017-07-26 18:15:50,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2124.0 (TID 4248)
2017-07-26 18:15:50,085 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2112_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:50,087 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:50,087 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:50,088 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2113_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:15:50,090 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2124.0 (TID 4248). 635 bytes result sent to driver
2017-07-26 18:15:50,090 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2124.0 (TID 4249). 635 bytes result sent to driver
2017-07-26 18:15:50,091 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2114_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:50,093 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2124.0 (TID 4248) in 12 ms on localhost (1/2)
2017-07-26 18:15:50,094 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2115_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:50,094 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2124.0 (TID 4249) in 11 ms on localhost (2/2)
2017-07-26 18:15:50,094 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2124.0, whose tasks have all completed, from pool 
2017-07-26 18:15:50,094 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2124 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:15:50,095 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2124 finished: foreachPartition at streamingProcessTest.scala:48, took 0.047823 s
2017-07-26 18:15:50,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064150000 ms.0 from job set of time 1501064150000 ms
2017-07-26 18:15:50,096 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2123 from persistence list
2017-07-26 18:15:50,096 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.095 s for time 1501064150000 ms (execution: 0.078 s)
2017-07-26 18:15:50,096 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2116_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:50,096 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2123
2017-07-26 18:15:50,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:50,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064146000 ms
2017-07-26 18:15:50,098 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2117_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:50,099 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2118_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:50,100 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2119_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:50,102 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2120_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:50,103 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2121_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:50,105 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2122_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:50,106 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2123_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:52,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064152000 ms
2017-07-26 18:15:52,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064152000 ms.0 from job set of time 1501064152000 ms
2017-07-26 18:15:52,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:52,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2125 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:52,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2125 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:52,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:52,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:52,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2125 (KafkaRDD[2125] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:52,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2125 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:15:52,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2125_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:15:52,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2125_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:52,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2125 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:52,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2125 (KafkaRDD[2125] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:52,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2125.0 with 2 tasks
2017-07-26 18:15:52,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2125.0 (TID 4250, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:52,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2125.0 (TID 4251, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:52,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2125.0 (TID 4250)
2017-07-26 18:15:52,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2125.0 (TID 4251)
2017-07-26 18:15:52,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:52,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:52,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2125.0 (TID 4250). 714 bytes result sent to driver
2017-07-26 18:15:52,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2125.0 (TID 4251). 714 bytes result sent to driver
2017-07-26 18:15:52,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2125.0 (TID 4250) in 10 ms on localhost (1/2)
2017-07-26 18:15:52,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2125.0 (TID 4251) in 9 ms on localhost (2/2)
2017-07-26 18:15:52,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2125.0, whose tasks have all completed, from pool 
2017-07-26 18:15:52,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2125 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:15:52,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2125 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022883 s
2017-07-26 18:15:52,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064152000 ms.0 from job set of time 1501064152000 ms
2017-07-26 18:15:52,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501064152000 ms (execution: 0.058 s)
2017-07-26 18:15:52,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2124 from persistence list
2017-07-26 18:15:52,077 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2124
2017-07-26 18:15:52,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:52,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064148000 ms
2017-07-26 18:15:54,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064154000 ms
2017-07-26 18:15:54,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064154000 ms.0 from job set of time 1501064154000 ms
2017-07-26 18:15:54,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:54,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2126 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:54,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2126 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:54,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:54,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:54,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2126 (KafkaRDD[2126] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:54,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2126 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:15:54,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2126_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:15:54,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2126_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:54,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2126 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:54,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2126 (KafkaRDD[2126] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:54,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2126.0 with 2 tasks
2017-07-26 18:15:54,048 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2126.0 (TID 4252, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:54,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2126.0 (TID 4253, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:54,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2126.0 (TID 4253)
2017-07-26 18:15:54,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2126.0 (TID 4252)
2017-07-26 18:15:54,053 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:54,054 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:54,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2126.0 (TID 4253). 635 bytes result sent to driver
2017-07-26 18:15:54,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2126.0 (TID 4252). 714 bytes result sent to driver
2017-07-26 18:15:54,058 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2126.0 (TID 4253) in 9 ms on localhost (1/2)
2017-07-26 18:15:54,058 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2126.0 (TID 4252) in 11 ms on localhost (2/2)
2017-07-26 18:15:54,058 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2126.0, whose tasks have all completed, from pool 
2017-07-26 18:15:54,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2126 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:15:54,059 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2126 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025321 s
2017-07-26 18:15:54,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064154000 ms.0 from job set of time 1501064154000 ms
2017-07-26 18:15:54,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.059 s for time 1501064154000 ms (execution: 0.045 s)
2017-07-26 18:15:54,059 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2125 from persistence list
2017-07-26 18:15:54,060 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2125
2017-07-26 18:15:54,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:54,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064150000 ms
2017-07-26 18:15:56,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064156000 ms
2017-07-26 18:15:56,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064156000 ms.0 from job set of time 1501064156000 ms
2017-07-26 18:15:56,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:56,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2127 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:56,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2127 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:56,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:56,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:56,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2127 (KafkaRDD[2127] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:56,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2127 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:15:56,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2127_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:15:56,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2127_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:15:56,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2127 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:56,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2127 (KafkaRDD[2127] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:56,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2127.0 with 2 tasks
2017-07-26 18:15:56,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2127.0 (TID 4254, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:56,038 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2127.0 (TID 4255, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:56,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2127.0 (TID 4254)
2017-07-26 18:15:56,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2127.0 (TID 4255)
2017-07-26 18:15:56,040 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:56,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:56,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2127.0 (TID 4255). 635 bytes result sent to driver
2017-07-26 18:15:56,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2127.0 (TID 4254). 635 bytes result sent to driver
2017-07-26 18:15:56,044 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2127.0 (TID 4254) in 8 ms on localhost (1/2)
2017-07-26 18:15:56,045 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2127.0 (TID 4255) in 7 ms on localhost (2/2)
2017-07-26 18:15:56,045 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2127.0, whose tasks have all completed, from pool 
2017-07-26 18:15:56,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2127 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:15:56,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2127 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018767 s
2017-07-26 18:15:56,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064156000 ms.0 from job set of time 1501064156000 ms
2017-07-26 18:15:56,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.047 s for time 1501064156000 ms (execution: 0.031 s)
2017-07-26 18:15:56,048 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2126 from persistence list
2017-07-26 18:15:56,048 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2126
2017-07-26 18:15:56,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:56,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064152000 ms
2017-07-26 18:15:58,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064158000 ms
2017-07-26 18:15:58,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064158000 ms.0 from job set of time 1501064158000 ms
2017-07-26 18:15:58,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:15:58,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2128 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:15:58,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2128 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:15:58,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:15:58,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:15:58,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2128 (KafkaRDD[2128] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:15:58,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2128 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:15:58,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2128_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:15:58,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2128_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:15:58,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2128 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:15:58,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2128 (KafkaRDD[2128] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:15:58,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2128.0 with 2 tasks
2017-07-26 18:15:58,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2128.0 (TID 4256, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:15:58,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2128.0 (TID 4257, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:15:58,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2128.0 (TID 4257)
2017-07-26 18:15:58,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2128.0 (TID 4256)
2017-07-26 18:15:58,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:15:58,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:15:58,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2128.0 (TID 4256). 714 bytes result sent to driver
2017-07-26 18:15:58,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2128.0 (TID 4257). 714 bytes result sent to driver
2017-07-26 18:15:58,079 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2128.0 (TID 4256) in 14 ms on localhost (1/2)
2017-07-26 18:15:58,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2128.0 (TID 4257) in 14 ms on localhost (2/2)
2017-07-26 18:15:58,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2128.0, whose tasks have all completed, from pool 
2017-07-26 18:15:58,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2128 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:15:58,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2128 finished: foreachPartition at streamingProcessTest.scala:48, took 0.034489 s
2017-07-26 18:15:58,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064158000 ms.0 from job set of time 1501064158000 ms
2017-07-26 18:15:58,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1501064158000 ms (execution: 0.063 s)
2017-07-26 18:15:58,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2127 from persistence list
2017-07-26 18:15:58,081 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2127
2017-07-26 18:15:58,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:15:58,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064154000 ms
2017-07-26 18:16:00,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064160000 ms
2017-07-26 18:16:00,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064160000 ms.0 from job set of time 1501064160000 ms
2017-07-26 18:16:00,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2129 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2129 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2129 (KafkaRDD[2129] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:00,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2129 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:16:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2129_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:16:00,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2129_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2129 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2129 (KafkaRDD[2129] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2129.0 with 2 tasks
2017-07-26 18:16:00,064 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2129.0 (TID 4258, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:00,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2129.0 (TID 4259, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:00,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2129.0 (TID 4258)
2017-07-26 18:16:00,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2129.0 (TID 4259)
2017-07-26 18:16:00,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:00,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:00,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2129.0 (TID 4259). 714 bytes result sent to driver
2017-07-26 18:16:00,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2129.0 (TID 4258). 714 bytes result sent to driver
2017-07-26 18:16:00,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2129.0 (TID 4259) in 6 ms on localhost (1/2)
2017-07-26 18:16:00,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2129.0 (TID 4258) in 9 ms on localhost (2/2)
2017-07-26 18:16:00,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2129.0, whose tasks have all completed, from pool 
2017-07-26 18:16:00,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2129 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:16:00,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2129 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022848 s
2017-07-26 18:16:00,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064160000 ms.0 from job set of time 1501064160000 ms
2017-07-26 18:16:00,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1501064160000 ms (execution: 0.057 s)
2017-07-26 18:16:00,073 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2128 from persistence list
2017-07-26 18:16:00,074 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2128
2017-07-26 18:16:00,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:00,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064156000 ms
2017-07-26 18:16:02,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064162000 ms
2017-07-26 18:16:02,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064162000 ms.0 from job set of time 1501064162000 ms
2017-07-26 18:16:02,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2130 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2130 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:02,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2130 (KafkaRDD[2130] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:02,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2130 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:16:02,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2130_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:16:02,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2130_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:02,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2130 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:02,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2130 (KafkaRDD[2130] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:02,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2130.0 with 2 tasks
2017-07-26 18:16:02,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2130.0 (TID 4260, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:02,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2130.0 (TID 4261, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:02,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2130.0 (TID 4260)
2017-07-26 18:16:02,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2130.0 (TID 4261)
2017-07-26 18:16:02,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:02,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:02,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2130.0 (TID 4260). 714 bytes result sent to driver
2017-07-26 18:16:02,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2130.0 (TID 4261). 714 bytes result sent to driver
2017-07-26 18:16:02,079 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2130.0 (TID 4260) in 10 ms on localhost (1/2)
2017-07-26 18:16:02,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2130.0 (TID 4261) in 10 ms on localhost (2/2)
2017-07-26 18:16:02,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2130.0, whose tasks have all completed, from pool 
2017-07-26 18:16:02,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2130 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:16:02,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2130 finished: foreachPartition at streamingProcessTest.scala:48, took 0.031776 s
2017-07-26 18:16:02,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064162000 ms.0 from job set of time 1501064162000 ms
2017-07-26 18:16:02,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501064162000 ms (execution: 0.065 s)
2017-07-26 18:16:02,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2129 from persistence list
2017-07-26 18:16:02,082 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2129
2017-07-26 18:16:02,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:02,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064158000 ms
2017-07-26 18:16:04,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064164000 ms
2017-07-26 18:16:04,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064164000 ms.0 from job set of time 1501064164000 ms
2017-07-26 18:16:04,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:04,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2131 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:04,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2131 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:04,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:04,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:04,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2131 (KafkaRDD[2131] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:04,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2131 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:16:04,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2131_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:16:04,040 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2131_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:04,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2131 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:04,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2131 (KafkaRDD[2131] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:04,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2131.0 with 2 tasks
2017-07-26 18:16:04,042 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2131.0 (TID 4262, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:04,042 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2131.0 (TID 4263, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:04,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2131.0 (TID 4262)
2017-07-26 18:16:04,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2131.0 (TID 4263)
2017-07-26 18:16:04,046 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:04,047 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:04,049 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2131.0 (TID 4263). 635 bytes result sent to driver
2017-07-26 18:16:04,049 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2131.0 (TID 4262). 714 bytes result sent to driver
2017-07-26 18:16:04,050 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2131.0 (TID 4262) in 9 ms on localhost (1/2)
2017-07-26 18:16:04,051 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2131.0 (TID 4263) in 9 ms on localhost (2/2)
2017-07-26 18:16:04,051 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2131.0, whose tasks have all completed, from pool 
2017-07-26 18:16:04,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2131 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:16:04,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2131 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022371 s
2017-07-26 18:16:04,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064164000 ms.0 from job set of time 1501064164000 ms
2017-07-26 18:16:04,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.053 s for time 1501064164000 ms (execution: 0.033 s)
2017-07-26 18:16:04,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2130 from persistence list
2017-07-26 18:16:04,054 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2130
2017-07-26 18:16:04,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:04,055 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064160000 ms
2017-07-26 18:16:06,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064166000 ms
2017-07-26 18:16:06,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064166000 ms.0 from job set of time 1501064166000 ms
2017-07-26 18:16:06,053 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:06,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2132 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:06,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2132 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:06,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:06,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:06,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2132 (KafkaRDD[2132] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:06,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2132 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:16:06,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2132_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:16:06,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2132_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:06,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2132 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:06,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2132 (KafkaRDD[2132] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:06,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2132.0 with 2 tasks
2017-07-26 18:16:06,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2132.0 (TID 4264, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:06,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2132.0 (TID 4265, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:06,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2132.0 (TID 4264)
2017-07-26 18:16:06,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2132.0 (TID 4265)
2017-07-26 18:16:06,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:06,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:06,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2132.0 (TID 4264). 635 bytes result sent to driver
2017-07-26 18:16:06,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2132.0 (TID 4265). 635 bytes result sent to driver
2017-07-26 18:16:06,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2132.0 (TID 4264) in 9 ms on localhost (1/2)
2017-07-26 18:16:06,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2132.0 (TID 4265) in 8 ms on localhost (2/2)
2017-07-26 18:16:06,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2132.0, whose tasks have all completed, from pool 
2017-07-26 18:16:06,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2132 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:16:06,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2132 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023265 s
2017-07-26 18:16:06,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064166000 ms.0 from job set of time 1501064166000 ms
2017-07-26 18:16:06,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1501064166000 ms (execution: 0.058 s)
2017-07-26 18:16:06,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2131 from persistence list
2017-07-26 18:16:06,078 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2131
2017-07-26 18:16:06,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:06,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064162000 ms
2017-07-26 18:16:08,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064168000 ms
2017-07-26 18:16:08,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064168000 ms.0 from job set of time 1501064168000 ms
2017-07-26 18:16:08,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:08,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2133 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:08,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2133 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:08,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:08,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:08,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2133 (KafkaRDD[2133] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:08,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2133 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:16:08,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2133_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:16:08,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2133_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:08,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2133 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:08,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2133 (KafkaRDD[2133] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:08,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2133.0 with 2 tasks
2017-07-26 18:16:08,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2133.0 (TID 4266, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:08,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2133.0 (TID 4267, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:08,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2133.0 (TID 4267)
2017-07-26 18:16:08,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2133.0 (TID 4266)
2017-07-26 18:16:08,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:08,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:08,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2133.0 (TID 4266). 635 bytes result sent to driver
2017-07-26 18:16:08,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2133.0 (TID 4267). 722 bytes result sent to driver
2017-07-26 18:16:08,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2133.0 (TID 4266) in 10 ms on localhost (1/2)
2017-07-26 18:16:08,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2133.0 (TID 4267) in 9 ms on localhost (2/2)
2017-07-26 18:16:08,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2133.0, whose tasks have all completed, from pool 
2017-07-26 18:16:08,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2133 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:16:08,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2133 finished: foreachPartition at streamingProcessTest.scala:48, took 0.033408 s
2017-07-26 18:16:08,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064168000 ms.0 from job set of time 1501064168000 ms
2017-07-26 18:16:08,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1501064168000 ms (execution: 0.068 s)
2017-07-26 18:16:08,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2132 from persistence list
2017-07-26 18:16:08,084 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2132
2017-07-26 18:16:08,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:08,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064164000 ms
2017-07-26 18:16:10,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064170000 ms
2017-07-26 18:16:10,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064170000 ms.0 from job set of time 1501064170000 ms
2017-07-26 18:16:10,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2134 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2134 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:10,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:10,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2134 (KafkaRDD[2134] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2134 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:16:10,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2134_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:16:10,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2134_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:10,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2134 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:10,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2134 (KafkaRDD[2134] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:10,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2134.0 with 2 tasks
2017-07-26 18:16:10,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2134.0 (TID 4268, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:10,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2134.0 (TID 4269, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:10,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2134.0 (TID 4269)
2017-07-26 18:16:10,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2134.0 (TID 4268)
2017-07-26 18:16:10,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:10,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:10,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2134.0 (TID 4269). 635 bytes result sent to driver
2017-07-26 18:16:10,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2134.0 (TID 4268). 714 bytes result sent to driver
2017-07-26 18:16:10,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2134.0 (TID 4269) in 11 ms on localhost (1/2)
2017-07-26 18:16:10,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2134.0 (TID 4268) in 13 ms on localhost (2/2)
2017-07-26 18:16:10,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2134.0, whose tasks have all completed, from pool 
2017-07-26 18:16:10,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2134 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:16:10,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2134 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028732 s
2017-07-26 18:16:10,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064170000 ms.0 from job set of time 1501064170000 ms
2017-07-26 18:16:10,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2133 from persistence list
2017-07-26 18:16:10,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1501064170000 ms (execution: 0.051 s)
2017-07-26 18:16:10,069 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2133
2017-07-26 18:16:10,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:10,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064166000 ms
2017-07-26 18:16:12,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064172000 ms
2017-07-26 18:16:12,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064172000 ms.0 from job set of time 1501064172000 ms
2017-07-26 18:16:12,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:12,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2135 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:12,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2135 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:12,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:12,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:12,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2135 (KafkaRDD[2135] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:12,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2135 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:16:12,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2135_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:16:12,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2135_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:12,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2135 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:12,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2135 (KafkaRDD[2135] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:12,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2135.0 with 2 tasks
2017-07-26 18:16:12,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2135.0 (TID 4270, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:12,070 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2135.0 (TID 4271, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:12,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2135.0 (TID 4271)
2017-07-26 18:16:12,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2135.0 (TID 4270)
2017-07-26 18:16:12,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:12,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:12,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2135.0 (TID 4270). 722 bytes result sent to driver
2017-07-26 18:16:12,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2135.0 (TID 4271). 714 bytes result sent to driver
2017-07-26 18:16:12,084 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2135.0 (TID 4271) in 14 ms on localhost (1/2)
2017-07-26 18:16:12,085 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2135.0 (TID 4270) in 17 ms on localhost (2/2)
2017-07-26 18:16:12,085 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2135.0, whose tasks have all completed, from pool 
2017-07-26 18:16:12,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2135 (foreachPartition at streamingProcessTest.scala:48) finished in 0.018 s
2017-07-26 18:16:12,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2135 finished: foreachPartition at streamingProcessTest.scala:48, took 0.038432 s
2017-07-26 18:16:12,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064172000 ms.0 from job set of time 1501064172000 ms
2017-07-26 18:16:12,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1501064172000 ms (execution: 0.071 s)
2017-07-26 18:16:12,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2134 from persistence list
2017-07-26 18:16:12,087 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2134
2017-07-26 18:16:12,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:12,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064168000 ms
2017-07-26 18:16:14,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064174000 ms
2017-07-26 18:16:14,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064174000 ms.0 from job set of time 1501064174000 ms
2017-07-26 18:16:14,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:14,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2136 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:14,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2136 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:14,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:14,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:14,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2136 (KafkaRDD[2136] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:14,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2136 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:16:14,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2136_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:16:14,034 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2136_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:14,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2136 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:14,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2136 (KafkaRDD[2136] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:14,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2136.0 with 2 tasks
2017-07-26 18:16:14,037 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2136.0 (TID 4272, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:14,037 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2136.0 (TID 4273, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:14,037 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2136.0 (TID 4272)
2017-07-26 18:16:14,037 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2136.0 (TID 4273)
2017-07-26 18:16:14,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:14,040 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:14,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2136.0 (TID 4273). 635 bytes result sent to driver
2017-07-26 18:16:14,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2136.0 (TID 4272). 635 bytes result sent to driver
2017-07-26 18:16:14,044 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2136.0 (TID 4273) in 7 ms on localhost (1/2)
2017-07-26 18:16:14,044 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2136.0 (TID 4272) in 8 ms on localhost (2/2)
2017-07-26 18:16:14,044 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2136.0, whose tasks have all completed, from pool 
2017-07-26 18:16:14,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2136 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:16:14,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2136 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019100 s
2017-07-26 18:16:14,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064174000 ms.0 from job set of time 1501064174000 ms
2017-07-26 18:16:14,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.045 s for time 1501064174000 ms (execution: 0.033 s)
2017-07-26 18:16:14,045 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2135 from persistence list
2017-07-26 18:16:14,046 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2135
2017-07-26 18:16:14,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:14,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064170000 ms
2017-07-26 18:16:16,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064176000 ms
2017-07-26 18:16:16,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064176000 ms.0 from job set of time 1501064176000 ms
2017-07-26 18:16:16,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2137 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2137 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:16,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2137 (KafkaRDD[2137] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:16,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2137 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:16:16,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2137_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:16:16,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2137_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:16,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2137 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:16,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2137 (KafkaRDD[2137] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:16,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2137.0 with 2 tasks
2017-07-26 18:16:16,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2137.0 (TID 4274, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:16,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2137.0 (TID 4275, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:16,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2137.0 (TID 4274)
2017-07-26 18:16:16,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2137.0 (TID 4275)
2017-07-26 18:16:16,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:16,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:16,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2137.0 (TID 4275). 635 bytes result sent to driver
2017-07-26 18:16:16,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2137.0 (TID 4274). 714 bytes result sent to driver
2017-07-26 18:16:16,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2137.0 (TID 4275) in 12 ms on localhost (1/2)
2017-07-26 18:16:16,085 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2137.0 (TID 4274) in 14 ms on localhost (2/2)
2017-07-26 18:16:16,085 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2137.0, whose tasks have all completed, from pool 
2017-07-26 18:16:16,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2137 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:16:16,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2137 finished: foreachPartition at streamingProcessTest.scala:48, took 0.038476 s
2017-07-26 18:16:16,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064176000 ms.0 from job set of time 1501064176000 ms
2017-07-26 18:16:16,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1501064176000 ms (execution: 0.071 s)
2017-07-26 18:16:16,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2136 from persistence list
2017-07-26 18:16:16,087 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2136
2017-07-26 18:16:16,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:16,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064172000 ms
2017-07-26 18:16:18,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064178000 ms
2017-07-26 18:16:18,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064178000 ms.0 from job set of time 1501064178000 ms
2017-07-26 18:16:18,062 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:18,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2138 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:18,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2137_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:18,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2138 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:18,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:18,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:18,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2138 (KafkaRDD[2138] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:18,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2124_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:18,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2125_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:18,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2138 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:16:18,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2126_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:18,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2127_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:18,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2138_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:16:18,070 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2138_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:18,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2138 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:18,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2138 (KafkaRDD[2138] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:18,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2138.0 with 2 tasks
2017-07-26 18:16:18,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2128_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:18,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2138.0 (TID 4276, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:18,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2138.0 (TID 4277, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:18,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2138.0 (TID 4277)
2017-07-26 18:16:18,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2138.0 (TID 4276)
2017-07-26 18:16:18,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2129_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:18,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:18,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2130_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:18,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:18,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2138.0 (TID 4277). 714 bytes result sent to driver
2017-07-26 18:16:18,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2138.0 (TID 4276). 714 bytes result sent to driver
2017-07-26 18:16:18,076 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2131_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:18,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2132_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:18,079 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2138.0 (TID 4277) in 7 ms on localhost (1/2)
2017-07-26 18:16:18,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2138.0 (TID 4276) in 8 ms on localhost (2/2)
2017-07-26 18:16:18,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2138.0, whose tasks have all completed, from pool 
2017-07-26 18:16:18,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2138 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:16:18,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2138 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018169 s
2017-07-26 18:16:18,080 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2133_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:18,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064178000 ms.0 from job set of time 1501064178000 ms
2017-07-26 18:16:18,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2137 from persistence list
2017-07-26 18:16:18,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1501064178000 ms (execution: 0.063 s)
2017-07-26 18:16:18,081 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2137
2017-07-26 18:16:18,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:18,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064174000 ms
2017-07-26 18:16:18,082 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2134_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:18,084 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2135_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:18,086 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2136_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:20,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064180000 ms
2017-07-26 18:16:20,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064180000 ms.0 from job set of time 1501064180000 ms
2017-07-26 18:16:20,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2139 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2139 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:20,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2139 (KafkaRDD[2139] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2139 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:16:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2139_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:16:20,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2139_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:20,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2139 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:20,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2139 (KafkaRDD[2139] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:20,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2139.0 with 2 tasks
2017-07-26 18:16:20,058 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2139.0 (TID 4278, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:20,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2139.0 (TID 4279, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:20,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2139.0 (TID 4278)
2017-07-26 18:16:20,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2139.0 (TID 4279)
2017-07-26 18:16:20,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:20,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:20,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2139.0 (TID 4278). 635 bytes result sent to driver
2017-07-26 18:16:20,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2139.0 (TID 4279). 635 bytes result sent to driver
2017-07-26 18:16:20,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2139.0 (TID 4278) in 11 ms on localhost (1/2)
2017-07-26 18:16:20,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2139.0 (TID 4279) in 10 ms on localhost (2/2)
2017-07-26 18:16:20,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2139.0, whose tasks have all completed, from pool 
2017-07-26 18:16:20,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2139 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:16:20,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2139 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024760 s
2017-07-26 18:16:20,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064180000 ms.0 from job set of time 1501064180000 ms
2017-07-26 18:16:20,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1501064180000 ms (execution: 0.051 s)
2017-07-26 18:16:20,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2138 from persistence list
2017-07-26 18:16:20,069 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2138
2017-07-26 18:16:20,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:20,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064176000 ms
2017-07-26 18:16:22,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064182000 ms
2017-07-26 18:16:22,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064182000 ms.0 from job set of time 1501064182000 ms
2017-07-26 18:16:22,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:22,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2140 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:22,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2140 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:22,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2140 (KafkaRDD[2140] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:22,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2140 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:16:22,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2140_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:16:22,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2140_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:22,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2140 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:22,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2140 (KafkaRDD[2140] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:22,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2140.0 with 2 tasks
2017-07-26 18:16:22,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2140.0 (TID 4280, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:22,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2140.0 (TID 4281, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:22,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2140.0 (TID 4281)
2017-07-26 18:16:22,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2140.0 (TID 4280)
2017-07-26 18:16:22,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:22,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:22,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2140.0 (TID 4280). 635 bytes result sent to driver
2017-07-26 18:16:22,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2140.0 (TID 4281). 635 bytes result sent to driver
2017-07-26 18:16:22,070 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2140.0 (TID 4280) in 11 ms on localhost (1/2)
2017-07-26 18:16:22,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2140.0 (TID 4281) in 10 ms on localhost (2/2)
2017-07-26 18:16:22,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2140.0, whose tasks have all completed, from pool 
2017-07-26 18:16:22,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2140 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:16:22,071 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2140 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025923 s
2017-07-26 18:16:22,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064182000 ms.0 from job set of time 1501064182000 ms
2017-07-26 18:16:22,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501064182000 ms (execution: 0.056 s)
2017-07-26 18:16:22,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2139 from persistence list
2017-07-26 18:16:22,073 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2139
2017-07-26 18:16:22,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:22,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064178000 ms
2017-07-26 18:16:24,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064184000 ms
2017-07-26 18:16:24,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064184000 ms.0 from job set of time 1501064184000 ms
2017-07-26 18:16:24,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:24,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2141 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:24,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2141 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:24,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:24,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:24,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2141 (KafkaRDD[2141] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:24,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2141 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:16:24,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2141_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:16:24,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2141_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:24,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2141 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:24,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2141 (KafkaRDD[2141] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:24,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2141.0 with 2 tasks
2017-07-26 18:16:24,038 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2141.0 (TID 4282, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:24,038 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2141.0 (TID 4283, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:24,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2141.0 (TID 4283)
2017-07-26 18:16:24,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2141.0 (TID 4282)
2017-07-26 18:16:24,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:24,040 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:24,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2141.0 (TID 4282). 714 bytes result sent to driver
2017-07-26 18:16:24,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2141.0 (TID 4283). 714 bytes result sent to driver
2017-07-26 18:16:24,045 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2141.0 (TID 4282) in 7 ms on localhost (1/2)
2017-07-26 18:16:24,045 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2141.0 (TID 4283) in 7 ms on localhost (2/2)
2017-07-26 18:16:24,045 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2141.0, whose tasks have all completed, from pool 
2017-07-26 18:16:24,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2141 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:16:24,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2141 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019110 s
2017-07-26 18:16:24,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064184000 ms.0 from job set of time 1501064184000 ms
2017-07-26 18:16:24,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.046 s for time 1501064184000 ms (execution: 0.033 s)
2017-07-26 18:16:24,046 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2140 from persistence list
2017-07-26 18:16:24,046 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2140
2017-07-26 18:16:24,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:24,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064180000 ms
2017-07-26 18:16:26,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064186000 ms
2017-07-26 18:16:26,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064186000 ms.0 from job set of time 1501064186000 ms
2017-07-26 18:16:26,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:26,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2142 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:26,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2142 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:26,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:26,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:26,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2142 (KafkaRDD[2142] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:26,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2142 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:16:26,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2142_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:16:26,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2142_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:26,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2142 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:26,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2142 (KafkaRDD[2142] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:26,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2142.0 with 2 tasks
2017-07-26 18:16:26,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2142.0 (TID 4284, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:26,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2142.0 (TID 4285, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:26,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2142.0 (TID 4285)
2017-07-26 18:16:26,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2142.0 (TID 4284)
2017-07-26 18:16:26,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:26,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:26,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2142.0 (TID 4284). 714 bytes result sent to driver
2017-07-26 18:16:26,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2142.0 (TID 4285). 635 bytes result sent to driver
2017-07-26 18:16:26,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2142.0 (TID 4285) in 7 ms on localhost (1/2)
2017-07-26 18:16:26,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2142.0 (TID 4284) in 8 ms on localhost (2/2)
2017-07-26 18:16:26,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2142.0, whose tasks have all completed, from pool 
2017-07-26 18:16:26,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2142 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:16:26,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2142 finished: foreachPartition at streamingProcessTest.scala:48, took 0.029076 s
2017-07-26 18:16:26,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064186000 ms.0 from job set of time 1501064186000 ms
2017-07-26 18:16:26,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501064186000 ms (execution: 0.059 s)
2017-07-26 18:16:26,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2141 from persistence list
2017-07-26 18:16:26,077 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2141
2017-07-26 18:16:26,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:26,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064182000 ms
2017-07-26 18:16:28,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064188000 ms
2017-07-26 18:16:28,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064188000 ms.0 from job set of time 1501064188000 ms
2017-07-26 18:16:28,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:28,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2143 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:28,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2143 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:28,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2143 (KafkaRDD[2143] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:28,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2143 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:16:28,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2143_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:16:28,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2143_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:28,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2143 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:28,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2143 (KafkaRDD[2143] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:28,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2143.0 with 2 tasks
2017-07-26 18:16:28,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2143.0 (TID 4286, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:28,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2143.0 (TID 4287, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:28,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2143.0 (TID 4286)
2017-07-26 18:16:28,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2143.0 (TID 4287)
2017-07-26 18:16:28,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:28,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:28,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2143.0 (TID 4287). 714 bytes result sent to driver
2017-07-26 18:16:28,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2143.0 (TID 4286). 714 bytes result sent to driver
2017-07-26 18:16:28,079 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2143.0 (TID 4286) in 13 ms on localhost (1/2)
2017-07-26 18:16:28,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2143.0 (TID 4287) in 12 ms on localhost (2/2)
2017-07-26 18:16:28,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2143.0, whose tasks have all completed, from pool 
2017-07-26 18:16:28,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2143 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:16:28,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2143 finished: foreachPartition at streamingProcessTest.scala:48, took 0.034307 s
2017-07-26 18:16:28,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064188000 ms.0 from job set of time 1501064188000 ms
2017-07-26 18:16:28,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501064188000 ms (execution: 0.065 s)
2017-07-26 18:16:28,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2142 from persistence list
2017-07-26 18:16:28,081 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2142
2017-07-26 18:16:28,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:28,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064184000 ms
2017-07-26 18:16:30,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064190000 ms
2017-07-26 18:16:30,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064190000 ms.0 from job set of time 1501064190000 ms
2017-07-26 18:16:30,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2144 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2144 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2144 (KafkaRDD[2144] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:30,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2144 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:16:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2144_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:16:30,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2144_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2144 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2144 (KafkaRDD[2144] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2144.0 with 2 tasks
2017-07-26 18:16:30,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2144.0 (TID 4288, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:30,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2144.0 (TID 4289, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:30,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2144.0 (TID 4289)
2017-07-26 18:16:30,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2144.0 (TID 4288)
2017-07-26 18:16:30,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:30,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:30,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2144.0 (TID 4289). 635 bytes result sent to driver
2017-07-26 18:16:30,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2144.0 (TID 4288). 635 bytes result sent to driver
2017-07-26 18:16:30,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2144.0 (TID 4289) in 7 ms on localhost (1/2)
2017-07-26 18:16:30,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2144.0 (TID 4288) in 9 ms on localhost (2/2)
2017-07-26 18:16:30,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2144.0, whose tasks have all completed, from pool 
2017-07-26 18:16:30,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2144 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:16:30,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2144 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024681 s
2017-07-26 18:16:30,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064190000 ms.0 from job set of time 1501064190000 ms
2017-07-26 18:16:30,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1501064190000 ms (execution: 0.056 s)
2017-07-26 18:16:30,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2143 from persistence list
2017-07-26 18:16:30,075 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2143
2017-07-26 18:16:30,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:30,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064186000 ms
2017-07-26 18:16:32,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064192000 ms
2017-07-26 18:16:32,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064192000 ms.0 from job set of time 1501064192000 ms
2017-07-26 18:16:32,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:32,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2145 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:32,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2145 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:32,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:32,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:32,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2145 (KafkaRDD[2145] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:32,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2145 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:16:32,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2145_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:16:32,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2145_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:32,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2145 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:32,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2145 (KafkaRDD[2145] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:32,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2145.0 with 2 tasks
2017-07-26 18:16:32,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2145.0 (TID 4290, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:32,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2145.0 (TID 4291, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:32,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2145.0 (TID 4291)
2017-07-26 18:16:32,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2145.0 (TID 4290)
2017-07-26 18:16:32,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:32,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:32,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2145.0 (TID 4291). 635 bytes result sent to driver
2017-07-26 18:16:32,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2145.0 (TID 4290). 635 bytes result sent to driver
2017-07-26 18:16:32,084 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2145.0 (TID 4290) in 17 ms on localhost (1/2)
2017-07-26 18:16:32,085 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2145.0 (TID 4291) in 16 ms on localhost (2/2)
2017-07-26 18:16:32,085 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2145.0, whose tasks have all completed, from pool 
2017-07-26 18:16:32,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2145 (foreachPartition at streamingProcessTest.scala:48) finished in 0.018 s
2017-07-26 18:16:32,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2145 finished: foreachPartition at streamingProcessTest.scala:48, took 0.039387 s
2017-07-26 18:16:32,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064192000 ms.0 from job set of time 1501064192000 ms
2017-07-26 18:16:32,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1501064192000 ms (execution: 0.071 s)
2017-07-26 18:16:32,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2144 from persistence list
2017-07-26 18:16:32,088 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2144
2017-07-26 18:16:32,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:32,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064188000 ms
2017-07-26 18:16:34,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064194000 ms
2017-07-26 18:16:34,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064194000 ms.0 from job set of time 1501064194000 ms
2017-07-26 18:16:34,036 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:34,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2146 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:34,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2146 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:34,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:34,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:34,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2146 (KafkaRDD[2146] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:34,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2146 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:16:34,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2146_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:16:34,048 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2146_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2146 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2146 (KafkaRDD[2146] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2146.0 with 2 tasks
2017-07-26 18:16:34,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2146.0 (TID 4292, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:34,052 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2146.0 (TID 4293, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:34,052 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2146.0 (TID 4292)
2017-07-26 18:16:34,052 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2146.0 (TID 4293)
2017-07-26 18:16:34,055 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:34,055 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:34,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2146.0 (TID 4292). 635 bytes result sent to driver
2017-07-26 18:16:34,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2146.0 (TID 4293). 635 bytes result sent to driver
2017-07-26 18:16:34,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2146.0 (TID 4292) in 11 ms on localhost (1/2)
2017-07-26 18:16:34,061 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2146.0 (TID 4293) in 10 ms on localhost (2/2)
2017-07-26 18:16:34,061 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2146.0, whose tasks have all completed, from pool 
2017-07-26 18:16:34,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2146 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:16:34,062 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2146 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025829 s
2017-07-26 18:16:34,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064194000 ms.0 from job set of time 1501064194000 ms
2017-07-26 18:16:34,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1501064194000 ms (execution: 0.046 s)
2017-07-26 18:16:34,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2145 from persistence list
2017-07-26 18:16:34,063 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2145
2017-07-26 18:16:34,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:34,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064190000 ms
2017-07-26 18:16:36,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064196000 ms
2017-07-26 18:16:36,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064196000 ms.0 from job set of time 1501064196000 ms
2017-07-26 18:16:36,102 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:36,103 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2147 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:36,103 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2147 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:36,103 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:36,104 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:36,104 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2147 (KafkaRDD[2147] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:36,111 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2147 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:16:36,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2147_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:16:36,119 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2147_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:36,119 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2147 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:36,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2147 (KafkaRDD[2147] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:36,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2147.0 with 2 tasks
2017-07-26 18:16:36,122 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2147.0 (TID 4294, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:36,123 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2147.0 (TID 4295, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:36,123 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2147.0 (TID 4295)
2017-07-26 18:16:36,123 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2147.0 (TID 4294)
2017-07-26 18:16:36,129 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:36,129 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:36,132 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2147.0 (TID 4295). 714 bytes result sent to driver
2017-07-26 18:16:36,132 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2147.0 (TID 4294). 714 bytes result sent to driver
2017-07-26 18:16:36,134 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2147.0 (TID 4295) in 12 ms on localhost (1/2)
2017-07-26 18:16:36,135 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2147.0 (TID 4294) in 14 ms on localhost (2/2)
2017-07-26 18:16:36,135 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2147.0, whose tasks have all completed, from pool 
2017-07-26 18:16:36,135 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2147 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:16:36,135 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2147 finished: foreachPartition at streamingProcessTest.scala:48, took 0.033272 s
2017-07-26 18:16:36,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064196000 ms.0 from job set of time 1501064196000 ms
2017-07-26 18:16:36,136 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.136 s for time 1501064196000 ms (execution: 0.066 s)
2017-07-26 18:16:36,136 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2146 from persistence list
2017-07-26 18:16:36,137 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2146
2017-07-26 18:16:36,137 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:36,137 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064192000 ms
2017-07-26 18:16:38,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064198000 ms
2017-07-26 18:16:38,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064198000 ms.0 from job set of time 1501064198000 ms
2017-07-26 18:16:38,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:38,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2148 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:38,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2148 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:38,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:38,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:38,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2148 (KafkaRDD[2148] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:38,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2148 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:16:38,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2148_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:16:38,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2148_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:38,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2148 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:38,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2148 (KafkaRDD[2148] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:38,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2148.0 with 2 tasks
2017-07-26 18:16:38,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2148.0 (TID 4296, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:38,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2148.0 (TID 4297, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:38,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2148.0 (TID 4297)
2017-07-26 18:16:38,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2148.0 (TID 4296)
2017-07-26 18:16:38,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:38,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:38,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2148.0 (TID 4296). 635 bytes result sent to driver
2017-07-26 18:16:38,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2148.0 (TID 4297). 635 bytes result sent to driver
2017-07-26 18:16:38,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2148.0 (TID 4296) in 8 ms on localhost (1/2)
2017-07-26 18:16:38,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2148.0 (TID 4297) in 7 ms on localhost (2/2)
2017-07-26 18:16:38,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2148.0, whose tasks have all completed, from pool 
2017-07-26 18:16:38,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2148 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:16:38,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2148 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028493 s
2017-07-26 18:16:38,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064198000 ms.0 from job set of time 1501064198000 ms
2017-07-26 18:16:38,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501064198000 ms (execution: 0.059 s)
2017-07-26 18:16:38,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2147 from persistence list
2017-07-26 18:16:38,075 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2147
2017-07-26 18:16:38,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:38,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064194000 ms
2017-07-26 18:16:40,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064200000 ms
2017-07-26 18:16:40,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064200000 ms.0 from job set of time 1501064200000 ms
2017-07-26 18:16:40,035 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:40,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2149 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:40,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2149 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:40,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:40,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:40,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2149 (KafkaRDD[2149] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:40,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2149 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:16:40,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2149_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:16:40,047 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2149_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2149 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2149 (KafkaRDD[2149] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2149.0 with 2 tasks
2017-07-26 18:16:40,049 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2149.0 (TID 4298, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:40,049 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2149.0 (TID 4299, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:40,049 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2149.0 (TID 4299)
2017-07-26 18:16:40,049 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2149.0 (TID 4298)
2017-07-26 18:16:40,052 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:40,052 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:40,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2149.0 (TID 4298). 635 bytes result sent to driver
2017-07-26 18:16:40,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2149.0 (TID 4299). 635 bytes result sent to driver
2017-07-26 18:16:40,055 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2149.0 (TID 4298) in 7 ms on localhost (1/2)
2017-07-26 18:16:40,056 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2149.0 (TID 4299) in 7 ms on localhost (2/2)
2017-07-26 18:16:40,056 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2149.0, whose tasks have all completed, from pool 
2017-07-26 18:16:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2149 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:16:40,056 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2149 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020570 s
2017-07-26 18:16:40,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064200000 ms.0 from job set of time 1501064200000 ms
2017-07-26 18:16:40,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.057 s for time 1501064200000 ms (execution: 0.041 s)
2017-07-26 18:16:40,057 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2148 from persistence list
2017-07-26 18:16:40,057 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2148
2017-07-26 18:16:40,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:40,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064196000 ms
2017-07-26 18:16:42,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064202000 ms
2017-07-26 18:16:42,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064202000 ms.0 from job set of time 1501064202000 ms
2017-07-26 18:16:42,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:42,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2150 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:42,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2150 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:42,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:42,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:42,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2150 (KafkaRDD[2150] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:42,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2150 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:16:42,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2150_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:16:42,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2150_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:42,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2150 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:42,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2150 (KafkaRDD[2150] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:42,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2150.0 with 2 tasks
2017-07-26 18:16:42,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2150.0 (TID 4300, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:42,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2150.0 (TID 4301, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:42,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2150.0 (TID 4300)
2017-07-26 18:16:42,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2150.0 (TID 4301)
2017-07-26 18:16:42,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:42,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:42,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2150.0 (TID 4301). 714 bytes result sent to driver
2017-07-26 18:16:42,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2150.0 (TID 4300). 714 bytes result sent to driver
2017-07-26 18:16:42,085 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2150.0 (TID 4301) in 14 ms on localhost (1/2)
2017-07-26 18:16:42,086 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2150.0 (TID 4300) in 17 ms on localhost (2/2)
2017-07-26 18:16:42,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2150 (foreachPartition at streamingProcessTest.scala:48) finished in 0.018 s
2017-07-26 18:16:42,086 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2150.0, whose tasks have all completed, from pool 
2017-07-26 18:16:42,087 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2150 finished: foreachPartition at streamingProcessTest.scala:48, took 0.039700 s
2017-07-26 18:16:42,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064202000 ms.0 from job set of time 1501064202000 ms
2017-07-26 18:16:42,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2149 from persistence list
2017-07-26 18:16:42,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1501064202000 ms (execution: 0.070 s)
2017-07-26 18:16:42,088 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2149
2017-07-26 18:16:42,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:42,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064198000 ms
2017-07-26 18:16:44,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064204000 ms
2017-07-26 18:16:44,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064204000 ms.0 from job set of time 1501064204000 ms
2017-07-26 18:16:44,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:44,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2151 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:44,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2151 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:44,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:44,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:44,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2151 (KafkaRDD[2151] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:44,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2151 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:16:44,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2151_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:16:44,052 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2151_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:44,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2151 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:44,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2151 (KafkaRDD[2151] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:44,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2151.0 with 2 tasks
2017-07-26 18:16:44,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2151.0 (TID 4302, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:44,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2151.0 (TID 4303, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:44,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2151.0 (TID 4303)
2017-07-26 18:16:44,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2151.0 (TID 4302)
2017-07-26 18:16:44,058 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:44,058 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:44,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2151.0 (TID 4303). 635 bytes result sent to driver
2017-07-26 18:16:44,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2151.0 (TID 4302). 635 bytes result sent to driver
2017-07-26 18:16:44,063 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2151.0 (TID 4303) in 8 ms on localhost (1/2)
2017-07-26 18:16:44,064 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2151.0 (TID 4302) in 10 ms on localhost (2/2)
2017-07-26 18:16:44,064 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2151.0, whose tasks have all completed, from pool 
2017-07-26 18:16:44,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2151 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:16:44,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2151 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022886 s
2017-07-26 18:16:44,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064204000 ms.0 from job set of time 1501064204000 ms
2017-07-26 18:16:44,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1501064204000 ms (execution: 0.049 s)
2017-07-26 18:16:44,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2150 from persistence list
2017-07-26 18:16:44,066 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2150
2017-07-26 18:16:44,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:44,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064200000 ms
2017-07-26 18:16:46,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064206000 ms
2017-07-26 18:16:46,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064206000 ms.0 from job set of time 1501064206000 ms
2017-07-26 18:16:46,047 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2151_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:46,050 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2138_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:46,052 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2139_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:16:46,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2140_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:46,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2141_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:46,059 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:46,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2142_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:46,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2152 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:46,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2152 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:46,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:46,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:46,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2152 (KafkaRDD[2152] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:46,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2143_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:46,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2144_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:46,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2152 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:16:46,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2145_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:46,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2146_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:46,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2152_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:16:46,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2152_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:46,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2152 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:46,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2147_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:46,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2152 (KafkaRDD[2152] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:46,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2152.0 with 2 tasks
2017-07-26 18:16:46,073 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2152.0 (TID 4304, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:46,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2148_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:46,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2152.0 (TID 4305, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:46,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2152.0 (TID 4304)
2017-07-26 18:16:46,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2152.0 (TID 4305)
2017-07-26 18:16:46,077 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2149_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:46,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:46,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:46,078 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2150_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:46,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2152.0 (TID 4304). 635 bytes result sent to driver
2017-07-26 18:16:46,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2152.0 (TID 4305). 714 bytes result sent to driver
2017-07-26 18:16:46,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2152.0 (TID 4304) in 8 ms on localhost (1/2)
2017-07-26 18:16:46,081 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2152.0 (TID 4305) in 7 ms on localhost (2/2)
2017-07-26 18:16:46,081 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2152.0, whose tasks have all completed, from pool 
2017-07-26 18:16:46,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2152 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:16:46,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2152 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021812 s
2017-07-26 18:16:46,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064206000 ms.0 from job set of time 1501064206000 ms
2017-07-26 18:16:46,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501064206000 ms (execution: 0.065 s)
2017-07-26 18:16:46,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2151 from persistence list
2017-07-26 18:16:46,082 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2151
2017-07-26 18:16:46,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:46,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064202000 ms
2017-07-26 18:16:48,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064208000 ms
2017-07-26 18:16:48,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064208000 ms.0 from job set of time 1501064208000 ms
2017-07-26 18:16:48,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2153 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2153 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2153 (KafkaRDD[2153] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:48,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2153 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:16:48,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2153_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:16:48,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2153_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:48,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2153 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:48,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2153 (KafkaRDD[2153] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:48,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2153.0 with 2 tasks
2017-07-26 18:16:48,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2153.0 (TID 4306, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:48,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2153.0 (TID 4307, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:48,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2153.0 (TID 4306)
2017-07-26 18:16:48,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2153.0 (TID 4307)
2017-07-26 18:16:48,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:48,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:48,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2153.0 (TID 4306). 635 bytes result sent to driver
2017-07-26 18:16:48,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2153.0 (TID 4307). 635 bytes result sent to driver
2017-07-26 18:16:48,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2153.0 (TID 4307) in 7 ms on localhost (1/2)
2017-07-26 18:16:48,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2153.0 (TID 4306) in 8 ms on localhost (2/2)
2017-07-26 18:16:48,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2153.0, whose tasks have all completed, from pool 
2017-07-26 18:16:48,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2153 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:16:48,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2153 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022457 s
2017-07-26 18:16:48,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064208000 ms.0 from job set of time 1501064208000 ms
2017-07-26 18:16:48,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2152 from persistence list
2017-07-26 18:16:48,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501064208000 ms (execution: 0.053 s)
2017-07-26 18:16:48,071 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2152
2017-07-26 18:16:48,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:48,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064204000 ms
2017-07-26 18:16:50,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064210000 ms
2017-07-26 18:16:50,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064210000 ms.0 from job set of time 1501064210000 ms
2017-07-26 18:16:50,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2154 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2154 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:50,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2154 (KafkaRDD[2154] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:50,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2154 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:16:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2154_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:16:50,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2154_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2154 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2154 (KafkaRDD[2154] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2154.0 with 2 tasks
2017-07-26 18:16:50,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2154.0 (TID 4308, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:50,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2154.0 (TID 4309, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:50,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2154.0 (TID 4308)
2017-07-26 18:16:50,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2154.0 (TID 4309)
2017-07-26 18:16:50,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:50,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:50,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2154.0 (TID 4309). 635 bytes result sent to driver
2017-07-26 18:16:50,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2154.0 (TID 4308). 635 bytes result sent to driver
2017-07-26 18:16:50,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2154.0 (TID 4309) in 10 ms on localhost (1/2)
2017-07-26 18:16:50,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2154.0 (TID 4308) in 11 ms on localhost (2/2)
2017-07-26 18:16:50,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2154.0, whose tasks have all completed, from pool 
2017-07-26 18:16:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2154 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:16:50,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2154 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026121 s
2017-07-26 18:16:50,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064210000 ms.0 from job set of time 1501064210000 ms
2017-07-26 18:16:50,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501064210000 ms (execution: 0.054 s)
2017-07-26 18:16:50,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2153 from persistence list
2017-07-26 18:16:50,070 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2153
2017-07-26 18:16:50,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:50,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064206000 ms
2017-07-26 18:16:52,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064212000 ms
2017-07-26 18:16:52,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064212000 ms.0 from job set of time 1501064212000 ms
2017-07-26 18:16:52,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:52,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2155 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:52,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2155 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:52,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2155 (KafkaRDD[2155] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:52,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2155 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:16:52,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2155_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:16:52,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2155_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:16:52,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2155 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:52,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2155 (KafkaRDD[2155] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:52,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2155.0 with 2 tasks
2017-07-26 18:16:52,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2155.0 (TID 4310, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:52,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2155.0 (TID 4311, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:52,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2155.0 (TID 4311)
2017-07-26 18:16:52,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2155.0 (TID 4310)
2017-07-26 18:16:52,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:52,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:52,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2155.0 (TID 4311). 714 bytes result sent to driver
2017-07-26 18:16:52,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2155.0 (TID 4310). 714 bytes result sent to driver
2017-07-26 18:16:52,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2155.0 (TID 4311) in 9 ms on localhost (1/2)
2017-07-26 18:16:52,070 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2155.0 (TID 4310) in 10 ms on localhost (2/2)
2017-07-26 18:16:52,070 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2155.0, whose tasks have all completed, from pool 
2017-07-26 18:16:52,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2155 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:16:52,071 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2155 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024217 s
2017-07-26 18:16:52,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064212000 ms.0 from job set of time 1501064212000 ms
2017-07-26 18:16:52,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501064212000 ms (execution: 0.055 s)
2017-07-26 18:16:52,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2154 from persistence list
2017-07-26 18:16:52,073 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2154
2017-07-26 18:16:52,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:52,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064208000 ms
2017-07-26 18:16:54,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064214000 ms
2017-07-26 18:16:54,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064214000 ms.0 from job set of time 1501064214000 ms
2017-07-26 18:16:54,022 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:54,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2156 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:54,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2156 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:54,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:54,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:54,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2156 (KafkaRDD[2156] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:54,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2156 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:16:54,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2156_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:16:54,028 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2156_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:54,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2156 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:54,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2156 (KafkaRDD[2156] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:54,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2156.0 with 2 tasks
2017-07-26 18:16:54,029 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2156.0 (TID 4312, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:54,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2156.0 (TID 4313, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:54,030 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2156.0 (TID 4312)
2017-07-26 18:16:54,030 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2156.0 (TID 4313)
2017-07-26 18:16:54,032 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:54,032 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:54,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2156.0 (TID 4313). 635 bytes result sent to driver
2017-07-26 18:16:54,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2156.0 (TID 4312). 635 bytes result sent to driver
2017-07-26 18:16:54,034 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2156.0 (TID 4313) in 5 ms on localhost (1/2)
2017-07-26 18:16:54,035 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2156.0 (TID 4312) in 6 ms on localhost (2/2)
2017-07-26 18:16:54,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2156 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:16:54,035 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2156.0, whose tasks have all completed, from pool 
2017-07-26 18:16:54,035 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2156 finished: foreachPartition at streamingProcessTest.scala:48, took 0.013050 s
2017-07-26 18:16:54,035 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064214000 ms.0 from job set of time 1501064214000 ms
2017-07-26 18:16:54,036 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2155 from persistence list
2017-07-26 18:16:54,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.035 s for time 1501064214000 ms (execution: 0.023 s)
2017-07-26 18:16:54,036 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2155
2017-07-26 18:16:54,036 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:54,036 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064210000 ms
2017-07-26 18:16:56,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064216000 ms
2017-07-26 18:16:56,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064216000 ms.0 from job set of time 1501064216000 ms
2017-07-26 18:16:56,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:56,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2157 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:56,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2157 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:56,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:56,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:56,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2157 (KafkaRDD[2157] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:56,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2157 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:16:56,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2157_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:16:56,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2157_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:56,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2157 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:56,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2157 (KafkaRDD[2157] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:56,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2157.0 with 2 tasks
2017-07-26 18:16:56,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2157.0 (TID 4314, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:56,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2157.0 (TID 4315, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:56,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2157.0 (TID 4314)
2017-07-26 18:16:56,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2157.0 (TID 4315)
2017-07-26 18:16:56,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:56,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:56,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2157.0 (TID 4315). 714 bytes result sent to driver
2017-07-26 18:16:56,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2157.0 (TID 4314). 714 bytes result sent to driver
2017-07-26 18:16:56,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2157.0 (TID 4315) in 13 ms on localhost (1/2)
2017-07-26 18:16:56,079 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2157.0 (TID 4314) in 16 ms on localhost (2/2)
2017-07-26 18:16:56,079 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2157.0, whose tasks have all completed, from pool 
2017-07-26 18:16:56,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2157 (foreachPartition at streamingProcessTest.scala:48) finished in 0.017 s
2017-07-26 18:16:56,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2157 finished: foreachPartition at streamingProcessTest.scala:48, took 0.034747 s
2017-07-26 18:16:56,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064216000 ms.0 from job set of time 1501064216000 ms
2017-07-26 18:16:56,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501064216000 ms (execution: 0.065 s)
2017-07-26 18:16:56,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2156 from persistence list
2017-07-26 18:16:56,082 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2156
2017-07-26 18:16:56,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:56,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064212000 ms
2017-07-26 18:16:58,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064218000 ms
2017-07-26 18:16:58,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064218000 ms.0 from job set of time 1501064218000 ms
2017-07-26 18:16:58,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:16:58,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2158 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:16:58,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2158 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:16:58,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:16:58,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:16:58,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2158 (KafkaRDD[2158] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:16:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2158 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:16:58,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2158_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:16:58,052 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2158_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:16:58,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2158 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:16:58,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2158 (KafkaRDD[2158] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:16:58,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2158.0 with 2 tasks
2017-07-26 18:16:58,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2158.0 (TID 4316, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:16:58,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2158.0 (TID 4317, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:16:58,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2158.0 (TID 4317)
2017-07-26 18:16:58,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2158.0 (TID 4316)
2017-07-26 18:16:58,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:16:58,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:16:58,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2158.0 (TID 4316). 635 bytes result sent to driver
2017-07-26 18:16:58,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2158.0 (TID 4317). 635 bytes result sent to driver
2017-07-26 18:16:58,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2158.0 (TID 4316) in 7 ms on localhost (1/2)
2017-07-26 18:16:58,061 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2158.0 (TID 4317) in 7 ms on localhost (2/2)
2017-07-26 18:16:58,061 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2158.0, whose tasks have all completed, from pool 
2017-07-26 18:16:58,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2158 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:16:58,062 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2158 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018670 s
2017-07-26 18:16:58,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064218000 ms.0 from job set of time 1501064218000 ms
2017-07-26 18:16:58,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.062 s for time 1501064218000 ms (execution: 0.046 s)
2017-07-26 18:16:58,062 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2157 from persistence list
2017-07-26 18:16:58,062 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2157
2017-07-26 18:16:58,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:16:58,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064214000 ms
2017-07-26 18:17:00,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064220000 ms
2017-07-26 18:17:00,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064220000 ms.0 from job set of time 1501064220000 ms
2017-07-26 18:17:00,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2159 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2159 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2159 (KafkaRDD[2159] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2159 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:17:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2159_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:17:00,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2159_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2159 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2159 (KafkaRDD[2159] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2159.0 with 2 tasks
2017-07-26 18:17:00,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2159.0 (TID 4318, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:00,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2159.0 (TID 4319, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:00,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2159.0 (TID 4319)
2017-07-26 18:17:00,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2159.0 (TID 4318)
2017-07-26 18:17:00,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:00,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:00,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2159.0 (TID 4319). 635 bytes result sent to driver
2017-07-26 18:17:00,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2159.0 (TID 4318). 635 bytes result sent to driver
2017-07-26 18:17:00,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2159.0 (TID 4319) in 8 ms on localhost (1/2)
2017-07-26 18:17:00,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2159.0 (TID 4318) in 9 ms on localhost (2/2)
2017-07-26 18:17:00,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2159.0, whose tasks have all completed, from pool 
2017-07-26 18:17:00,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2159 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:17:00,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2159 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025881 s
2017-07-26 18:17:00,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064220000 ms.0 from job set of time 1501064220000 ms
2017-07-26 18:17:00,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2158 from persistence list
2017-07-26 18:17:00,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1501064220000 ms (execution: 0.057 s)
2017-07-26 18:17:00,074 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2158
2017-07-26 18:17:00,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:00,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064216000 ms
2017-07-26 18:17:02,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064222000 ms
2017-07-26 18:17:02,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064222000 ms.0 from job set of time 1501064222000 ms
2017-07-26 18:17:02,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:02,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2160 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:02,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2160 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:02,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:02,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:02,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2160 (KafkaRDD[2160] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:02,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2160 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:17:02,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2160_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:17:02,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2160_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:02,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2160 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:02,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2160 (KafkaRDD[2160] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:02,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2160.0 with 2 tasks
2017-07-26 18:17:02,068 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2160.0 (TID 4320, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:02,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2160.0 (TID 4321, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:02,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2160.0 (TID 4320)
2017-07-26 18:17:02,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2160.0 (TID 4321)
2017-07-26 18:17:02,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:02,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:02,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2160.0 (TID 4320). 635 bytes result sent to driver
2017-07-26 18:17:02,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2160.0 (TID 4321). 635 bytes result sent to driver
2017-07-26 18:17:02,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2160.0 (TID 4320) in 10 ms on localhost (1/2)
2017-07-26 18:17:02,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2160.0 (TID 4321) in 10 ms on localhost (2/2)
2017-07-26 18:17:02,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2160.0, whose tasks have all completed, from pool 
2017-07-26 18:17:02,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2160 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:17:02,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2160 finished: foreachPartition at streamingProcessTest.scala:48, took 0.031400 s
2017-07-26 18:17:02,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064222000 ms.0 from job set of time 1501064222000 ms
2017-07-26 18:17:02,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2159 from persistence list
2017-07-26 18:17:02,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1501064222000 ms (execution: 0.063 s)
2017-07-26 18:17:02,080 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2159
2017-07-26 18:17:02,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:02,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064218000 ms
2017-07-26 18:17:04,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064224000 ms
2017-07-26 18:17:04,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064224000 ms.0 from job set of time 1501064224000 ms
2017-07-26 18:17:04,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:04,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2161 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:04,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2161 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:04,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:04,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:04,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2161 (KafkaRDD[2161] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:04,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2161 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:17:04,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2161_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:17:04,035 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2161_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:04,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2161 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:04,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2161 (KafkaRDD[2161] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:04,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2161.0 with 2 tasks
2017-07-26 18:17:04,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2161.0 (TID 4322, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:04,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2161.0 (TID 4323, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:04,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2161.0 (TID 4322)
2017-07-26 18:17:04,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2161.0 (TID 4323)
2017-07-26 18:17:04,041 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:04,041 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:04,043 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2161.0 (TID 4322). 635 bytes result sent to driver
2017-07-26 18:17:04,043 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2161.0 (TID 4323). 635 bytes result sent to driver
2017-07-26 18:17:04,045 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2161.0 (TID 4322) in 8 ms on localhost (1/2)
2017-07-26 18:17:04,046 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2161.0 (TID 4323) in 8 ms on localhost (2/2)
2017-07-26 18:17:04,046 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2161.0, whose tasks have all completed, from pool 
2017-07-26 18:17:04,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2161 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:17:04,046 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2161 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020542 s
2017-07-26 18:17:04,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064224000 ms.0 from job set of time 1501064224000 ms
2017-07-26 18:17:04,047 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.047 s for time 1501064224000 ms (execution: 0.035 s)
2017-07-26 18:17:04,047 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2160 from persistence list
2017-07-26 18:17:04,047 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2160
2017-07-26 18:17:04,047 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:04,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064220000 ms
2017-07-26 18:17:06,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064226000 ms
2017-07-26 18:17:06,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064226000 ms.0 from job set of time 1501064226000 ms
2017-07-26 18:17:06,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:06,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2162 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:06,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2162 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:06,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:06,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:06,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2162 (KafkaRDD[2162] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:06,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2162 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:17:06,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2162_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:17:06,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2162_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:06,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2162 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:06,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2162 (KafkaRDD[2162] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:06,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2162.0 with 2 tasks
2017-07-26 18:17:06,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2162.0 (TID 4324, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:06,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2162.0 (TID 4325, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:06,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2162.0 (TID 4325)
2017-07-26 18:17:06,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2162.0 (TID 4324)
2017-07-26 18:17:06,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:06,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:06,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2162.0 (TID 4324). 635 bytes result sent to driver
2017-07-26 18:17:06,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2162.0 (TID 4325). 635 bytes result sent to driver
2017-07-26 18:17:06,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2162.0 (TID 4325) in 7 ms on localhost (1/2)
2017-07-26 18:17:06,066 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2162.0 (TID 4324) in 9 ms on localhost (2/2)
2017-07-26 18:17:06,066 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2162.0, whose tasks have all completed, from pool 
2017-07-26 18:17:06,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2162 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:17:06,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2162 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023940 s
2017-07-26 18:17:06,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064226000 ms.0 from job set of time 1501064226000 ms
2017-07-26 18:17:06,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1501064226000 ms (execution: 0.047 s)
2017-07-26 18:17:06,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2161 from persistence list
2017-07-26 18:17:06,067 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2161
2017-07-26 18:17:06,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:06,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064222000 ms
2017-07-26 18:17:08,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064228000 ms
2017-07-26 18:17:08,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064228000 ms.0 from job set of time 1501064228000 ms
2017-07-26 18:17:08,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:08,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2163 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:08,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2163 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:08,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:08,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:08,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2163 (KafkaRDD[2163] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:08,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2163 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:17:08,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2163_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:17:08,052 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2163_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:08,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2163 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:08,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2163 (KafkaRDD[2163] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:08,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2163.0 with 2 tasks
2017-07-26 18:17:08,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2163.0 (TID 4326, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:08,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2163.0 (TID 4327, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:08,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2163.0 (TID 4327)
2017-07-26 18:17:08,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2163.0 (TID 4326)
2017-07-26 18:17:08,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:08,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:08,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2163.0 (TID 4326). 714 bytes result sent to driver
2017-07-26 18:17:08,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2163.0 (TID 4327). 714 bytes result sent to driver
2017-07-26 18:17:08,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2163.0 (TID 4327) in 13 ms on localhost (1/2)
2017-07-26 18:17:08,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2163.0 (TID 4326) in 14 ms on localhost (2/2)
2017-07-26 18:17:08,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2163.0, whose tasks have all completed, from pool 
2017-07-26 18:17:08,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2163 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:17:08,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2163 finished: foreachPartition at streamingProcessTest.scala:48, took 0.031172 s
2017-07-26 18:17:08,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064228000 ms.0 from job set of time 1501064228000 ms
2017-07-26 18:17:08,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501064228000 ms (execution: 0.054 s)
2017-07-26 18:17:08,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2162 from persistence list
2017-07-26 18:17:08,071 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2162
2017-07-26 18:17:08,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:08,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064224000 ms
2017-07-26 18:17:10,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064230000 ms
2017-07-26 18:17:10,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064230000 ms.0 from job set of time 1501064230000 ms
2017-07-26 18:17:10,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2164 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2164 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2164 (KafkaRDD[2164] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:10,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2164 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:17:10,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2164_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:17:10,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2164_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2164 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2164 (KafkaRDD[2164] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2164.0 with 2 tasks
2017-07-26 18:17:10,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2164.0 (TID 4328, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:10,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2164.0 (TID 4329, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:10,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2164.0 (TID 4328)
2017-07-26 18:17:10,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2164.0 (TID 4329)
2017-07-26 18:17:10,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:10,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:10,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2164.0 (TID 4329). 714 bytes result sent to driver
2017-07-26 18:17:10,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2164.0 (TID 4328). 714 bytes result sent to driver
2017-07-26 18:17:10,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2164.0 (TID 4328) in 13 ms on localhost (1/2)
2017-07-26 18:17:10,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2164.0 (TID 4329) in 11 ms on localhost (2/2)
2017-07-26 18:17:10,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2164.0, whose tasks have all completed, from pool 
2017-07-26 18:17:10,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2164 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:17:10,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2164 finished: foreachPartition at streamingProcessTest.scala:48, took 0.031524 s
2017-07-26 18:17:10,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064230000 ms.0 from job set of time 1501064230000 ms
2017-07-26 18:17:10,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1501064230000 ms (execution: 0.063 s)
2017-07-26 18:17:10,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2163 from persistence list
2017-07-26 18:17:10,081 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2163
2017-07-26 18:17:10,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:10,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064226000 ms
2017-07-26 18:17:12,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064232000 ms
2017-07-26 18:17:12,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064232000 ms.0 from job set of time 1501064232000 ms
2017-07-26 18:17:12,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:12,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2165 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:12,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2165 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:12,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:12,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:12,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2165 (KafkaRDD[2165] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:12,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2165 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:17:12,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2165_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:17:12,032 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2165_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:12,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2165 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:12,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2165 (KafkaRDD[2165] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:12,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2165.0 with 2 tasks
2017-07-26 18:17:12,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2165.0 (TID 4330, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:12,036 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2165.0 (TID 4331, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:12,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2165.0 (TID 4330)
2017-07-26 18:17:12,037 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2165.0 (TID 4331)
2017-07-26 18:17:12,056 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:12,056 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:12,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2165.0 (TID 4331). 708 bytes result sent to driver
2017-07-26 18:17:12,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2165.0 (TID 4330). 787 bytes result sent to driver
2017-07-26 18:17:12,060 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2165.0 (TID 4331) in 25 ms on localhost (1/2)
2017-07-26 18:17:12,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2165.0 (TID 4330) in 28 ms on localhost (2/2)
2017-07-26 18:17:12,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2152_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:12,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2165.0, whose tasks have all completed, from pool 
2017-07-26 18:17:12,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2165 (foreachPartition at streamingProcessTest.scala:48) finished in 0.029 s
2017-07-26 18:17:12,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2165 finished: foreachPartition at streamingProcessTest.scala:48, took 0.039427 s
2017-07-26 18:17:12,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064232000 ms.0 from job set of time 1501064232000 ms
2017-07-26 18:17:12,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1501064232000 ms (execution: 0.050 s)
2017-07-26 18:17:12,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2164 from persistence list
2017-07-26 18:17:12,066 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2164
2017-07-26 18:17:12,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:12,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064228000 ms
2017-07-26 18:17:12,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2153_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:12,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2154_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:12,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2155_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:12,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2156_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:12,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2157_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:12,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2158_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:12,075 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2159_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:12,076 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2160_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:12,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2161_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:12,081 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2162_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:12,083 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2163_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:12,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2164_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:14,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064234000 ms
2017-07-26 18:17:14,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064234000 ms.0 from job set of time 1501064234000 ms
2017-07-26 18:17:14,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:14,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2166 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:14,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2166 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:14,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:14,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:14,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2166 (KafkaRDD[2166] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:14,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2166 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:17:14,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2166_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:17:14,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2166_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:14,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2166 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:14,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2166 (KafkaRDD[2166] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:14,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2166.0 with 2 tasks
2017-07-26 18:17:14,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2166.0 (TID 4332, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:14,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2166.0 (TID 4333, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:14,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2166.0 (TID 4333)
2017-07-26 18:17:14,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2166.0 (TID 4332)
2017-07-26 18:17:14,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:14,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:14,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2166.0 (TID 4333). 714 bytes result sent to driver
2017-07-26 18:17:14,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2166.0 (TID 4332). 714 bytes result sent to driver
2017-07-26 18:17:14,062 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2166.0 (TID 4333) in 8 ms on localhost (1/2)
2017-07-26 18:17:14,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2166.0 (TID 4332) in 10 ms on localhost (2/2)
2017-07-26 18:17:14,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2166.0, whose tasks have all completed, from pool 
2017-07-26 18:17:14,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2166 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:17:14,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2166 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022092 s
2017-07-26 18:17:14,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064234000 ms.0 from job set of time 1501064234000 ms
2017-07-26 18:17:14,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2165 from persistence list
2017-07-26 18:17:14,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1501064234000 ms (execution: 0.045 s)
2017-07-26 18:17:14,065 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2165
2017-07-26 18:17:14,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:14,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064230000 ms
2017-07-26 18:17:16,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064236000 ms
2017-07-26 18:17:16,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064236000 ms.0 from job set of time 1501064236000 ms
2017-07-26 18:17:16,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:16,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2167 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:16,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2167 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:16,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:16,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:16,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2167 (KafkaRDD[2167] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:16,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2167 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:17:16,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2167_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:17:16,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2167_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:16,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2167 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:16,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2167 (KafkaRDD[2167] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:16,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2167.0 with 2 tasks
2017-07-26 18:17:16,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2167.0 (TID 4334, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:16,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2167.0 (TID 4335, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:16,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2167.0 (TID 4335)
2017-07-26 18:17:16,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2167.0 (TID 4334)
2017-07-26 18:17:16,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:16,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:16,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2167.0 (TID 4335). 714 bytes result sent to driver
2017-07-26 18:17:16,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2167.0 (TID 4334). 635 bytes result sent to driver
2017-07-26 18:17:16,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2167.0 (TID 4335) in 11 ms on localhost (1/2)
2017-07-26 18:17:16,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2167.0 (TID 4334) in 13 ms on localhost (2/2)
2017-07-26 18:17:16,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2167 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:17:16,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2167.0, whose tasks have all completed, from pool 
2017-07-26 18:17:16,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2167 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026555 s
2017-07-26 18:17:16,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064236000 ms.0 from job set of time 1501064236000 ms
2017-07-26 18:17:16,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501064236000 ms (execution: 0.053 s)
2017-07-26 18:17:16,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2166 from persistence list
2017-07-26 18:17:16,070 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2166
2017-07-26 18:17:16,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:16,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064232000 ms
2017-07-26 18:17:18,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064238000 ms
2017-07-26 18:17:18,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064238000 ms.0 from job set of time 1501064238000 ms
2017-07-26 18:17:18,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:18,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2168 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:18,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2168 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:18,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2168 (KafkaRDD[2168] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:18,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2168 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:17:18,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2168_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:17:18,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2168_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:18,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2168 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:18,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2168 (KafkaRDD[2168] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:18,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2168.0 with 2 tasks
2017-07-26 18:17:18,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2168.0 (TID 4336, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:18,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2168.0 (TID 4337, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:18,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2168.0 (TID 4336)
2017-07-26 18:17:18,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2168.0 (TID 4337)
2017-07-26 18:17:18,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:18,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:18,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2168.0 (TID 4336). 714 bytes result sent to driver
2017-07-26 18:17:18,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2168.0 (TID 4337). 714 bytes result sent to driver
2017-07-26 18:17:18,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2168.0 (TID 4336) in 11 ms on localhost (1/2)
2017-07-26 18:17:18,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2168.0 (TID 4337) in 10 ms on localhost (2/2)
2017-07-26 18:17:18,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2168.0, whose tasks have all completed, from pool 
2017-07-26 18:17:18,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2168 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:17:18,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2168 finished: foreachPartition at streamingProcessTest.scala:48, took 0.030094 s
2017-07-26 18:17:18,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064238000 ms.0 from job set of time 1501064238000 ms
2017-07-26 18:17:18,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1501064238000 ms (execution: 0.061 s)
2017-07-26 18:17:18,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2167 from persistence list
2017-07-26 18:17:18,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:18,078 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2167
2017-07-26 18:17:18,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064234000 ms
2017-07-26 18:17:20,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064240000 ms
2017-07-26 18:17:20,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064240000 ms.0 from job set of time 1501064240000 ms
2017-07-26 18:17:20,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2169 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2169 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:20,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2169 (KafkaRDD[2169] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2169 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:17:20,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2169_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:17:20,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2169_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2169 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2169 (KafkaRDD[2169] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:20,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2169.0 with 2 tasks
2017-07-26 18:17:20,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2169.0 (TID 4338, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:20,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2169.0 (TID 4339, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:20,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2169.0 (TID 4339)
2017-07-26 18:17:20,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2169.0 (TID 4338)
2017-07-26 18:17:20,058 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:20,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:20,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2169.0 (TID 4339). 714 bytes result sent to driver
2017-07-26 18:17:20,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2169.0 (TID 4338). 635 bytes result sent to driver
2017-07-26 18:17:20,064 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2169.0 (TID 4339) in 8 ms on localhost (1/2)
2017-07-26 18:17:20,065 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2169.0 (TID 4338) in 9 ms on localhost (2/2)
2017-07-26 18:17:20,065 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2169.0, whose tasks have all completed, from pool 
2017-07-26 18:17:20,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2169 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:17:20,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2169 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021158 s
2017-07-26 18:17:20,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064240000 ms.0 from job set of time 1501064240000 ms
2017-07-26 18:17:20,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501064240000 ms (execution: 0.048 s)
2017-07-26 18:17:20,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2168 from persistence list
2017-07-26 18:17:20,067 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2168
2017-07-26 18:17:20,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:20,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064236000 ms
2017-07-26 18:17:22,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064242000 ms
2017-07-26 18:17:22,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064242000 ms.0 from job set of time 1501064242000 ms
2017-07-26 18:17:22,036 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:22,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2170 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:22,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2170 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:22,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:22,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:22,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2170 (KafkaRDD[2170] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:22,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2170 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:17:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2170_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:17:22,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2170_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:22,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2170 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:22,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2170 (KafkaRDD[2170] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:22,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2170.0 with 2 tasks
2017-07-26 18:17:22,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2170.0 (TID 4340, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:22,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2170.0 (TID 4341, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:22,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2170.0 (TID 4341)
2017-07-26 18:17:22,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2170.0 (TID 4340)
2017-07-26 18:17:22,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:22,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:22,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2170.0 (TID 4340). 714 bytes result sent to driver
2017-07-26 18:17:22,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2170.0 (TID 4341). 635 bytes result sent to driver
2017-07-26 18:17:22,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2170.0 (TID 4340) in 13 ms on localhost (1/2)
2017-07-26 18:17:22,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2170.0 (TID 4341) in 11 ms on localhost (2/2)
2017-07-26 18:17:22,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2170.0, whose tasks have all completed, from pool 
2017-07-26 18:17:22,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2170 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:17:22,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2170 finished: foreachPartition at streamingProcessTest.scala:48, took 0.029215 s
2017-07-26 18:17:22,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064242000 ms.0 from job set of time 1501064242000 ms
2017-07-26 18:17:22,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501064242000 ms (execution: 0.051 s)
2017-07-26 18:17:22,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2169 from persistence list
2017-07-26 18:17:22,067 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2169
2017-07-26 18:17:22,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:22,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064238000 ms
2017-07-26 18:17:24,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064244000 ms
2017-07-26 18:17:24,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064244000 ms.0 from job set of time 1501064244000 ms
2017-07-26 18:17:24,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:24,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2171 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:24,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2171 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:24,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:24,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:24,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2171 (KafkaRDD[2171] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:24,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2171 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:17:24,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2171_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:17:24,064 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2171_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:24,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2171 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:24,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2171 (KafkaRDD[2171] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:24,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2171.0 with 2 tasks
2017-07-26 18:17:24,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2171.0 (TID 4342, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:24,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2171.0 (TID 4343, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:24,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2171.0 (TID 4343)
2017-07-26 18:17:24,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2171.0 (TID 4342)
2017-07-26 18:17:24,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:24,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:24,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2171.0 (TID 4343). 635 bytes result sent to driver
2017-07-26 18:17:24,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2171.0 (TID 4342). 714 bytes result sent to driver
2017-07-26 18:17:24,080 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2171.0 (TID 4343) in 12 ms on localhost (1/2)
2017-07-26 18:17:24,081 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2171.0 (TID 4342) in 13 ms on localhost (2/2)
2017-07-26 18:17:24,081 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2171.0, whose tasks have all completed, from pool 
2017-07-26 18:17:24,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2171 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:17:24,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2171 finished: foreachPartition at streamingProcessTest.scala:48, took 0.032913 s
2017-07-26 18:17:24,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064244000 ms.0 from job set of time 1501064244000 ms
2017-07-26 18:17:24,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2170 from persistence list
2017-07-26 18:17:24,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1501064244000 ms (execution: 0.066 s)
2017-07-26 18:17:24,083 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2170
2017-07-26 18:17:24,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:24,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064240000 ms
2017-07-26 18:17:26,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064246000 ms
2017-07-26 18:17:26,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064246000 ms.0 from job set of time 1501064246000 ms
2017-07-26 18:17:26,036 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:26,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2172 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:26,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2172 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:26,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:26,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:26,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2172 (KafkaRDD[2172] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:26,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2172 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:17:26,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2172_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:17:26,043 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2172_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:26,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2172 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:26,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2172 (KafkaRDD[2172] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:26,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2172.0 with 2 tasks
2017-07-26 18:17:26,045 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2172.0 (TID 4344, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:26,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2172.0 (TID 4345, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:26,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2172.0 (TID 4344)
2017-07-26 18:17:26,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2172.0 (TID 4345)
2017-07-26 18:17:26,048 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:26,048 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:26,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2172.0 (TID 4345). 635 bytes result sent to driver
2017-07-26 18:17:26,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2172.0 (TID 4344). 635 bytes result sent to driver
2017-07-26 18:17:26,052 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2172.0 (TID 4344) in 8 ms on localhost (1/2)
2017-07-26 18:17:26,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2172.0 (TID 4345) in 6 ms on localhost (2/2)
2017-07-26 18:17:26,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2172.0, whose tasks have all completed, from pool 
2017-07-26 18:17:26,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2172 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:17:26,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2172 finished: foreachPartition at streamingProcessTest.scala:48, took 0.016040 s
2017-07-26 18:17:26,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064246000 ms.0 from job set of time 1501064246000 ms
2017-07-26 18:17:26,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.053 s for time 1501064246000 ms (execution: 0.038 s)
2017-07-26 18:17:26,053 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2171 from persistence list
2017-07-26 18:17:26,054 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2171
2017-07-26 18:17:26,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:26,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064242000 ms
2017-07-26 18:17:28,010 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064248000 ms
2017-07-26 18:17:28,010 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064248000 ms.0 from job set of time 1501064248000 ms
2017-07-26 18:17:28,019 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:28,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2173 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:28,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2173 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:28,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:28,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:28,020 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2173 (KafkaRDD[2173] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:28,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2173 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:17:28,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2173_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:17:28,026 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2173_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:28,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2173 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:28,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2173 (KafkaRDD[2173] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:28,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2173.0 with 2 tasks
2017-07-26 18:17:28,028 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2173.0 (TID 4346, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:28,028 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2173.0 (TID 4347, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:28,028 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2173.0 (TID 4347)
2017-07-26 18:17:28,028 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2173.0 (TID 4346)
2017-07-26 18:17:28,030 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:28,030 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:28,031 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2173.0 (TID 4346). 635 bytes result sent to driver
2017-07-26 18:17:28,031 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2173.0 (TID 4347). 635 bytes result sent to driver
2017-07-26 18:17:28,033 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2173.0 (TID 4346) in 6 ms on localhost (1/2)
2017-07-26 18:17:28,033 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2173.0 (TID 4347) in 5 ms on localhost (2/2)
2017-07-26 18:17:28,033 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2173.0, whose tasks have all completed, from pool 
2017-07-26 18:17:28,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2173 (foreachPartition at streamingProcessTest.scala:48) finished in 0.006 s
2017-07-26 18:17:28,034 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2173 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014167 s
2017-07-26 18:17:28,034 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064248000 ms.0 from job set of time 1501064248000 ms
2017-07-26 18:17:28,034 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.034 s for time 1501064248000 ms (execution: 0.024 s)
2017-07-26 18:17:28,034 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2172 from persistence list
2017-07-26 18:17:28,034 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2172
2017-07-26 18:17:28,034 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:28,034 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064244000 ms
2017-07-26 18:17:30,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064250000 ms
2017-07-26 18:17:30,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064250000 ms.0 from job set of time 1501064250000 ms
2017-07-26 18:17:30,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2174 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2174 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2174 (KafkaRDD[2174] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:30,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2174 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:17:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2174_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:17:30,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2174_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2174 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2174 (KafkaRDD[2174] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2174.0 with 2 tasks
2017-07-26 18:17:30,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2174.0 (TID 4348, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:30,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2174.0 (TID 4349, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:30,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2174.0 (TID 4349)
2017-07-26 18:17:30,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2174.0 (TID 4348)
2017-07-26 18:17:30,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:30,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:30,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2174.0 (TID 4349). 722 bytes result sent to driver
2017-07-26 18:17:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2174.0 (TID 4348). 801 bytes result sent to driver
2017-07-26 18:17:30,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2174.0 (TID 4349) in 10 ms on localhost (1/2)
2017-07-26 18:17:30,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2174.0 (TID 4348) in 11 ms on localhost (2/2)
2017-07-26 18:17:30,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2174.0, whose tasks have all completed, from pool 
2017-07-26 18:17:30,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2174 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:17:30,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2174 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028955 s
2017-07-26 18:17:30,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064250000 ms.0 from job set of time 1501064250000 ms
2017-07-26 18:17:30,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1501064250000 ms (execution: 0.062 s)
2017-07-26 18:17:30,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2173 from persistence list
2017-07-26 18:17:30,079 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2173
2017-07-26 18:17:30,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:30,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064246000 ms
2017-07-26 18:17:32,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064252000 ms
2017-07-26 18:17:32,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064252000 ms.0 from job set of time 1501064252000 ms
2017-07-26 18:17:32,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:32,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2175 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:32,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2175 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:32,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:32,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:32,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2175 (KafkaRDD[2175] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:32,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2175 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:17:32,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2175_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:17:32,069 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2175_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:32,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2175 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:32,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2175 (KafkaRDD[2175] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:32,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2175.0 with 2 tasks
2017-07-26 18:17:32,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2175.0 (TID 4350, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:32,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2175.0 (TID 4351, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:32,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2175.0 (TID 4351)
2017-07-26 18:17:32,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2175.0 (TID 4350)
2017-07-26 18:17:32,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:32,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:32,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2175.0 (TID 4350). 714 bytes result sent to driver
2017-07-26 18:17:32,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2175.0 (TID 4351). 714 bytes result sent to driver
2017-07-26 18:17:32,084 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2175.0 (TID 4350) in 13 ms on localhost (1/2)
2017-07-26 18:17:32,085 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2175.0 (TID 4351) in 12 ms on localhost (2/2)
2017-07-26 18:17:32,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2175 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:17:32,085 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2175.0, whose tasks have all completed, from pool 
2017-07-26 18:17:32,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2175 finished: foreachPartition at streamingProcessTest.scala:48, took 0.033796 s
2017-07-26 18:17:32,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064252000 ms.0 from job set of time 1501064252000 ms
2017-07-26 18:17:32,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.086 s for time 1501064252000 ms (execution: 0.067 s)
2017-07-26 18:17:32,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2174 from persistence list
2017-07-26 18:17:32,087 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2174
2017-07-26 18:17:32,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:32,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064248000 ms
2017-07-26 18:17:34,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064254000 ms
2017-07-26 18:17:34,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064254000 ms.0 from job set of time 1501064254000 ms
2017-07-26 18:17:34,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2176 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2176 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2176 (KafkaRDD[2176] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:34,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2176 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:17:34,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2176_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:17:34,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2176_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:34,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2176 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:34,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2176 (KafkaRDD[2176] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:34,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2176.0 with 2 tasks
2017-07-26 18:17:34,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2176.0 (TID 4352, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:34,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2176.0 (TID 4353, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:34,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2176.0 (TID 4353)
2017-07-26 18:17:34,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2176.0 (TID 4352)
2017-07-26 18:17:34,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:34,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:34,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2176.0 (TID 4353). 635 bytes result sent to driver
2017-07-26 18:17:34,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2176.0 (TID 4352). 714 bytes result sent to driver
2017-07-26 18:17:34,072 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2176.0 (TID 4353) in 9 ms on localhost (1/2)
2017-07-26 18:17:34,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2176.0 (TID 4352) in 12 ms on localhost (2/2)
2017-07-26 18:17:34,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2176.0, whose tasks have all completed, from pool 
2017-07-26 18:17:34,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2176 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:17:34,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2176 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025291 s
2017-07-26 18:17:34,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064254000 ms.0 from job set of time 1501064254000 ms
2017-07-26 18:17:34,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1501064254000 ms (execution: 0.057 s)
2017-07-26 18:17:34,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2175 from persistence list
2017-07-26 18:17:34,075 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2175
2017-07-26 18:17:34,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:34,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064250000 ms
2017-07-26 18:17:36,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064256000 ms
2017-07-26 18:17:36,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064256000 ms.0 from job set of time 1501064256000 ms
2017-07-26 18:17:36,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:36,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2177 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:36,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2177 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:36,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:36,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:36,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2177 (KafkaRDD[2177] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:36,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2177 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:17:36,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2177_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:17:36,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2177_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:36,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2177 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:36,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2177 (KafkaRDD[2177] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:36,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2177.0 with 2 tasks
2017-07-26 18:17:36,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2177.0 (TID 4354, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:36,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2177.0 (TID 4355, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:36,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2177.0 (TID 4355)
2017-07-26 18:17:36,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2177.0 (TID 4354)
2017-07-26 18:17:36,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:36,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:36,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2177.0 (TID 4354). 635 bytes result sent to driver
2017-07-26 18:17:36,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2177.0 (TID 4355). 635 bytes result sent to driver
2017-07-26 18:17:36,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2177.0 (TID 4354) in 8 ms on localhost (1/2)
2017-07-26 18:17:36,073 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2177.0 (TID 4355) in 7 ms on localhost (2/2)
2017-07-26 18:17:36,073 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2177.0, whose tasks have all completed, from pool 
2017-07-26 18:17:36,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2177 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:17:36,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2177 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024409 s
2017-07-26 18:17:36,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064256000 ms.0 from job set of time 1501064256000 ms
2017-07-26 18:17:36,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1501064256000 ms (execution: 0.056 s)
2017-07-26 18:17:36,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2176 from persistence list
2017-07-26 18:17:36,074 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2176
2017-07-26 18:17:36,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:36,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064252000 ms
2017-07-26 18:17:38,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064258000 ms
2017-07-26 18:17:38,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064258000 ms.0 from job set of time 1501064258000 ms
2017-07-26 18:17:38,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:38,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2178 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:38,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2178 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:38,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:38,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:38,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2178 (KafkaRDD[2178] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:38,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2178 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:17:38,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2178_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:17:38,031 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2178_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:38,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2178 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:38,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2178 (KafkaRDD[2178] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:38,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2178.0 with 2 tasks
2017-07-26 18:17:38,033 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2178.0 (TID 4356, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:38,033 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2178.0 (TID 4357, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:38,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2178.0 (TID 4357)
2017-07-26 18:17:38,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2178.0 (TID 4356)
2017-07-26 18:17:38,035 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:38,035 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:38,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2178.0 (TID 4356). 635 bytes result sent to driver
2017-07-26 18:17:38,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2178.0 (TID 4357). 635 bytes result sent to driver
2017-07-26 18:17:38,038 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2178.0 (TID 4356) in 6 ms on localhost (1/2)
2017-07-26 18:17:38,038 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2178.0 (TID 4357) in 5 ms on localhost (2/2)
2017-07-26 18:17:38,038 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2178.0, whose tasks have all completed, from pool 
2017-07-26 18:17:38,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2178 (foreachPartition at streamingProcessTest.scala:48) finished in 0.006 s
2017-07-26 18:17:38,038 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2178 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014834 s
2017-07-26 18:17:38,039 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064258000 ms.0 from job set of time 1501064258000 ms
2017-07-26 18:17:38,039 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.039 s for time 1501064258000 ms (execution: 0.024 s)
2017-07-26 18:17:38,039 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2177 from persistence list
2017-07-26 18:17:38,039 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2177
2017-07-26 18:17:38,039 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:38,039 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064254000 ms
2017-07-26 18:17:40,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064260000 ms
2017-07-26 18:17:40,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064260000 ms.0 from job set of time 1501064260000 ms
2017-07-26 18:17:40,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2179 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2179 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:40,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2179 (KafkaRDD[2179] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:40,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2179 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:17:40,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2179_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.3 MB)
2017-07-26 18:17:40,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2179_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:40,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2165_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:40,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2179 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:40,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2179 (KafkaRDD[2179] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:40,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2179.0 with 2 tasks
2017-07-26 18:17:40,076 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2179.0 (TID 4358, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:40,076 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2166_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:40,077 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2179.0 (TID 4359, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:40,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2179.0 (TID 4359)
2017-07-26 18:17:40,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2179.0 (TID 4358)
2017-07-26 18:17:40,078 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2167_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:40,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:40,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:40,080 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2168_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:17:40,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2179.0 (TID 4359). 714 bytes result sent to driver
2017-07-26 18:17:40,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2179.0 (TID 4358). 714 bytes result sent to driver
2017-07-26 18:17:40,081 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2169_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:40,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2179.0 (TID 4359) in 6 ms on localhost (1/2)
2017-07-26 18:17:40,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2179.0 (TID 4358) in 7 ms on localhost (2/2)
2017-07-26 18:17:40,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2179.0, whose tasks have all completed, from pool 
2017-07-26 18:17:40,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2179 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:17:40,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2179 finished: foreachPartition at streamingProcessTest.scala:48, took 0.031441 s
2017-07-26 18:17:40,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064260000 ms.0 from job set of time 1501064260000 ms
2017-07-26 18:17:40,083 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2170_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:40,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1501064260000 ms (execution: 0.065 s)
2017-07-26 18:17:40,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2178 from persistence list
2017-07-26 18:17:40,083 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2178
2017-07-26 18:17:40,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:40,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064256000 ms
2017-07-26 18:17:40,085 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2171_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:40,086 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2172_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:40,088 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2173_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:40,089 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2174_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:40,091 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2175_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:40,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2176_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:40,093 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2177_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:40,094 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2178_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:42,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064262000 ms
2017-07-26 18:17:42,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064262000 ms.0 from job set of time 1501064262000 ms
2017-07-26 18:17:42,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2180 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2180 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:42,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2180 (KafkaRDD[2180] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:42,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2180 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:17:42,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2180_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:17:42,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2180_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:42,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2180 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:42,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2180 (KafkaRDD[2180] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:42,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2180.0 with 2 tasks
2017-07-26 18:17:42,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2180.0 (TID 4360, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:42,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2180.0 (TID 4361, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:42,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2180.0 (TID 4361)
2017-07-26 18:17:42,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2180.0 (TID 4360)
2017-07-26 18:17:42,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:42,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:42,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2180.0 (TID 4361). 722 bytes result sent to driver
2017-07-26 18:17:42,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2180.0 (TID 4360). 635 bytes result sent to driver
2017-07-26 18:17:42,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2180.0 (TID 4360) in 18 ms on localhost (1/2)
2017-07-26 18:17:42,088 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2180.0 (TID 4361) in 16 ms on localhost (2/2)
2017-07-26 18:17:42,088 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2180.0, whose tasks have all completed, from pool 
2017-07-26 18:17:42,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2180 (foreachPartition at streamingProcessTest.scala:48) finished in 0.018 s
2017-07-26 18:17:42,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2180 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040287 s
2017-07-26 18:17:42,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064262000 ms.0 from job set of time 1501064262000 ms
2017-07-26 18:17:42,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1501064262000 ms (execution: 0.071 s)
2017-07-26 18:17:42,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2179 from persistence list
2017-07-26 18:17:42,090 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2179
2017-07-26 18:17:42,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:42,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064258000 ms
2017-07-26 18:17:44,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064264000 ms
2017-07-26 18:17:44,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064264000 ms.0 from job set of time 1501064264000 ms
2017-07-26 18:17:44,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:44,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2181 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:44,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2181 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:44,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:44,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:44,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2181 (KafkaRDD[2181] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:44,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2181 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:17:44,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2181_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:17:44,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2181_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:44,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2181 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:44,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2181 (KafkaRDD[2181] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:44,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2181.0 with 2 tasks
2017-07-26 18:17:44,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2181.0 (TID 4362, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:44,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2181.0 (TID 4363, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:44,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2181.0 (TID 4362)
2017-07-26 18:17:44,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2181.0 (TID 4363)
2017-07-26 18:17:44,056 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:44,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:44,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2181.0 (TID 4363). 714 bytes result sent to driver
2017-07-26 18:17:44,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2181.0 (TID 4362). 635 bytes result sent to driver
2017-07-26 18:17:44,061 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2181.0 (TID 4363) in 8 ms on localhost (1/2)
2017-07-26 18:17:44,062 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2181.0 (TID 4362) in 9 ms on localhost (2/2)
2017-07-26 18:17:44,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2181 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:17:44,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2181.0, whose tasks have all completed, from pool 
2017-07-26 18:17:44,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2181 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021224 s
2017-07-26 18:17:44,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064264000 ms.0 from job set of time 1501064264000 ms
2017-07-26 18:17:44,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1501064264000 ms (execution: 0.046 s)
2017-07-26 18:17:44,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2180 from persistence list
2017-07-26 18:17:44,064 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2180
2017-07-26 18:17:44,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:44,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064260000 ms
2017-07-26 18:17:46,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064266000 ms
2017-07-26 18:17:46,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064266000 ms.0 from job set of time 1501064266000 ms
2017-07-26 18:17:46,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2182 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2182 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:46,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:46,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:46,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2182 (KafkaRDD[2182] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:46,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2182 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:17:46,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2182_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:17:46,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2182_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:17:46,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2182 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:46,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2182 (KafkaRDD[2182] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:46,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2182.0 with 2 tasks
2017-07-26 18:17:46,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2182.0 (TID 4364, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:46,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2182.0 (TID 4365, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:46,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2182.0 (TID 4364)
2017-07-26 18:17:46,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2182.0 (TID 4365)
2017-07-26 18:17:46,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:46,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:46,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2182.0 (TID 4365). 635 bytes result sent to driver
2017-07-26 18:17:46,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2182.0 (TID 4364). 635 bytes result sent to driver
2017-07-26 18:17:46,072 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2182.0 (TID 4365) in 9 ms on localhost (1/2)
2017-07-26 18:17:46,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2182.0 (TID 4364) in 10 ms on localhost (2/2)
2017-07-26 18:17:46,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2182.0, whose tasks have all completed, from pool 
2017-07-26 18:17:46,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2182 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:17:46,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2182 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023404 s
2017-07-26 18:17:46,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064266000 ms.0 from job set of time 1501064266000 ms
2017-07-26 18:17:46,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1501064266000 ms (execution: 0.056 s)
2017-07-26 18:17:46,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2181 from persistence list
2017-07-26 18:17:46,074 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2181
2017-07-26 18:17:46,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:46,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064262000 ms
2017-07-26 18:17:48,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064268000 ms
2017-07-26 18:17:48,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064268000 ms.0 from job set of time 1501064268000 ms
2017-07-26 18:17:48,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:48,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2183 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:48,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2183 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:48,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:48,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:48,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2183 (KafkaRDD[2183] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:48,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2183 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:17:48,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2183_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:17:48,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2183_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:48,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2183 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:48,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2183 (KafkaRDD[2183] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:48,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2183.0 with 2 tasks
2017-07-26 18:17:48,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2183.0 (TID 4366, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:48,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2183.0 (TID 4367, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:48,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2183.0 (TID 4366)
2017-07-26 18:17:48,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2183.0 (TID 4367)
2017-07-26 18:17:48,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:48,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:48,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2183.0 (TID 4366). 714 bytes result sent to driver
2017-07-26 18:17:48,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2183.0 (TID 4367). 635 bytes result sent to driver
2017-07-26 18:17:48,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2183.0 (TID 4367) in 9 ms on localhost (1/2)
2017-07-26 18:17:48,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2183.0 (TID 4366) in 10 ms on localhost (2/2)
2017-07-26 18:17:48,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2183.0, whose tasks have all completed, from pool 
2017-07-26 18:17:48,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2183 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:17:48,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2183 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024400 s
2017-07-26 18:17:48,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064268000 ms.0 from job set of time 1501064268000 ms
2017-07-26 18:17:48,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501064268000 ms (execution: 0.057 s)
2017-07-26 18:17:48,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2182 from persistence list
2017-07-26 18:17:48,077 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2182
2017-07-26 18:17:48,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:48,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064264000 ms
2017-07-26 18:17:50,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064270000 ms
2017-07-26 18:17:50,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064270000 ms.0 from job set of time 1501064270000 ms
2017-07-26 18:17:50,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2184 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2184 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:50,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:50,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:50,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2184 (KafkaRDD[2184] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:50,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2184 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:17:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2184_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:17:50,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2184_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2184 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:50,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2184 (KafkaRDD[2184] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2184.0 with 2 tasks
2017-07-26 18:17:50,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2184.0 (TID 4368, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:50,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2184.0 (TID 4369, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:50,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2184.0 (TID 4369)
2017-07-26 18:17:50,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2184.0 (TID 4368)
2017-07-26 18:17:50,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:50,063 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:50,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2184.0 (TID 4369). 714 bytes result sent to driver
2017-07-26 18:17:50,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2184.0 (TID 4368). 635 bytes result sent to driver
2017-07-26 18:17:50,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2184.0 (TID 4369) in 10 ms on localhost (1/2)
2017-07-26 18:17:50,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2184.0 (TID 4368) in 11 ms on localhost (2/2)
2017-07-26 18:17:50,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2184.0, whose tasks have all completed, from pool 
2017-07-26 18:17:50,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2184 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:17:50,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2184 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025081 s
2017-07-26 18:17:50,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064270000 ms.0 from job set of time 1501064270000 ms
2017-07-26 18:17:50,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501064270000 ms (execution: 0.052 s)
2017-07-26 18:17:50,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2183 from persistence list
2017-07-26 18:17:50,069 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2183
2017-07-26 18:17:50,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:50,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064266000 ms
2017-07-26 18:17:52,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064272000 ms
2017-07-26 18:17:52,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064272000 ms.0 from job set of time 1501064272000 ms
2017-07-26 18:17:52,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2185 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2185 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2185 (KafkaRDD[2185] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:52,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2185 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:17:52,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2185_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:17:52,060 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2185_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:52,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2185 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:52,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2185 (KafkaRDD[2185] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:52,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2185.0 with 2 tasks
2017-07-26 18:17:52,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2185.0 (TID 4370, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:52,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2185.0 (TID 4371, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:52,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2185.0 (TID 4371)
2017-07-26 18:17:52,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2185.0 (TID 4370)
2017-07-26 18:17:52,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:52,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:52,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2185.0 (TID 4370). 635 bytes result sent to driver
2017-07-26 18:17:52,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2185.0 (TID 4371). 714 bytes result sent to driver
2017-07-26 18:17:52,073 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2185.0 (TID 4370) in 10 ms on localhost (1/2)
2017-07-26 18:17:52,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2185.0 (TID 4371) in 10 ms on localhost (2/2)
2017-07-26 18:17:52,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2185 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:17:52,073 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2185.0, whose tasks have all completed, from pool 
2017-07-26 18:17:52,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2185 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023251 s
2017-07-26 18:17:52,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064272000 ms.0 from job set of time 1501064272000 ms
2017-07-26 18:17:52,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1501064272000 ms (execution: 0.053 s)
2017-07-26 18:17:52,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2184 from persistence list
2017-07-26 18:17:52,075 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2184
2017-07-26 18:17:52,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:52,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064268000 ms
2017-07-26 18:17:54,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064274000 ms
2017-07-26 18:17:54,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064274000 ms.0 from job set of time 1501064274000 ms
2017-07-26 18:17:54,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:54,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2186 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:54,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2186 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:54,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:54,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:54,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2186 (KafkaRDD[2186] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:54,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2186 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:17:54,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2186_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:17:54,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2186_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:54,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2186 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:54,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2186 (KafkaRDD[2186] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:54,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2186.0 with 2 tasks
2017-07-26 18:17:54,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2186.0 (TID 4372, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:54,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2186.0 (TID 4373, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:54,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2186.0 (TID 4373)
2017-07-26 18:17:54,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2186.0 (TID 4372)
2017-07-26 18:17:54,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:54,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:54,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2186.0 (TID 4373). 714 bytes result sent to driver
2017-07-26 18:17:54,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2186.0 (TID 4372). 635 bytes result sent to driver
2017-07-26 18:17:54,071 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2186.0 (TID 4372) in 8 ms on localhost (1/2)
2017-07-26 18:17:54,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2186.0 (TID 4373) in 7 ms on localhost (2/2)
2017-07-26 18:17:54,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2186.0, whose tasks have all completed, from pool 
2017-07-26 18:17:54,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2186 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:17:54,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2186 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021726 s
2017-07-26 18:17:54,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064274000 ms.0 from job set of time 1501064274000 ms
2017-07-26 18:17:54,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501064274000 ms (execution: 0.053 s)
2017-07-26 18:17:54,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2185 from persistence list
2017-07-26 18:17:54,073 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2185
2017-07-26 18:17:54,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:54,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064270000 ms
2017-07-26 18:17:56,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064276000 ms
2017-07-26 18:17:56,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064276000 ms.0 from job set of time 1501064276000 ms
2017-07-26 18:17:56,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:56,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2187 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:56,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2187 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:56,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:56,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:56,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2187 (KafkaRDD[2187] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:56,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2187 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:17:56,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2187_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:17:56,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2187_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:56,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2187 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:56,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2187 (KafkaRDD[2187] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:56,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2187.0 with 2 tasks
2017-07-26 18:17:56,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2187.0 (TID 4374, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:56,072 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2187.0 (TID 4375, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:56,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2187.0 (TID 4374)
2017-07-26 18:17:56,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2187.0 (TID 4375)
2017-07-26 18:17:56,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:56,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:56,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2187.0 (TID 4374). 635 bytes result sent to driver
2017-07-26 18:17:56,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2187.0 (TID 4375). 635 bytes result sent to driver
2017-07-26 18:17:56,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2187.0 (TID 4374) in 13 ms on localhost (1/2)
2017-07-26 18:17:56,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2187.0 (TID 4375) in 12 ms on localhost (2/2)
2017-07-26 18:17:56,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2187.0, whose tasks have all completed, from pool 
2017-07-26 18:17:56,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2187 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:17:56,085 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2187 finished: foreachPartition at streamingProcessTest.scala:48, took 0.034161 s
2017-07-26 18:17:56,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064276000 ms.0 from job set of time 1501064276000 ms
2017-07-26 18:17:56,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2186 from persistence list
2017-07-26 18:17:56,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.086 s for time 1501064276000 ms (execution: 0.069 s)
2017-07-26 18:17:56,087 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2186
2017-07-26 18:17:56,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:56,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064272000 ms
2017-07-26 18:17:58,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064278000 ms
2017-07-26 18:17:58,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064278000 ms.0 from job set of time 1501064278000 ms
2017-07-26 18:17:58,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:17:58,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2188 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:17:58,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2188 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:17:58,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:17:58,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:17:58,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2188 (KafkaRDD[2188] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:17:58,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2188 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:17:58,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2188_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:17:58,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2188_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:17:58,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2188 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:17:58,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2188 (KafkaRDD[2188] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:17:58,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2188.0 with 2 tasks
2017-07-26 18:17:58,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2188.0 (TID 4376, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:17:58,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2188.0 (TID 4377, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:17:58,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2188.0 (TID 4377)
2017-07-26 18:17:58,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2188.0 (TID 4376)
2017-07-26 18:17:58,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:17:58,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:17:58,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2188.0 (TID 4377). 635 bytes result sent to driver
2017-07-26 18:17:58,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2188.0 (TID 4376). 635 bytes result sent to driver
2017-07-26 18:17:58,075 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2188.0 (TID 4377) in 12 ms on localhost (1/2)
2017-07-26 18:17:58,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2188.0 (TID 4376) in 13 ms on localhost (2/2)
2017-07-26 18:17:58,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2188.0, whose tasks have all completed, from pool 
2017-07-26 18:17:58,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2188 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:17:58,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2188 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026209 s
2017-07-26 18:17:58,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064278000 ms.0 from job set of time 1501064278000 ms
2017-07-26 18:17:58,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501064278000 ms (execution: 0.058 s)
2017-07-26 18:17:58,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2187 from persistence list
2017-07-26 18:17:58,077 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2187
2017-07-26 18:17:58,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:17:58,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064274000 ms
2017-07-26 18:18:00,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064280000 ms
2017-07-26 18:18:00,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064280000 ms.0 from job set of time 1501064280000 ms
2017-07-26 18:18:00,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2189 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2189 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2189 (KafkaRDD[2189] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:00,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2189 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:18:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2189_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:18:00,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2189_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2189 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2189 (KafkaRDD[2189] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:00,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2189.0 with 2 tasks
2017-07-26 18:18:00,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2189.0 (TID 4378, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:00,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2189.0 (TID 4379, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:00,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2189.0 (TID 4379)
2017-07-26 18:18:00,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2189.0 (TID 4378)
2017-07-26 18:18:00,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:00,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:00,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2189.0 (TID 4378). 714 bytes result sent to driver
2017-07-26 18:18:00,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2189.0 (TID 4379). 714 bytes result sent to driver
2017-07-26 18:18:00,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2189.0 (TID 4378) in 17 ms on localhost (1/2)
2017-07-26 18:18:00,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2189.0 (TID 4379) in 17 ms on localhost (2/2)
2017-07-26 18:18:00,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2189.0, whose tasks have all completed, from pool 
2017-07-26 18:18:00,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2189 (foreachPartition at streamingProcessTest.scala:48) finished in 0.019 s
2017-07-26 18:18:00,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2189 finished: foreachPartition at streamingProcessTest.scala:48, took 0.039150 s
2017-07-26 18:18:00,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064280000 ms.0 from job set of time 1501064280000 ms
2017-07-26 18:18:00,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1501064280000 ms (execution: 0.070 s)
2017-07-26 18:18:00,087 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2188 from persistence list
2017-07-26 18:18:00,088 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2188
2017-07-26 18:18:00,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:00,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064276000 ms
2017-07-26 18:18:02,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064282000 ms
2017-07-26 18:18:02,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064282000 ms.0 from job set of time 1501064282000 ms
2017-07-26 18:18:02,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:02,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2190 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:02,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2190 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:02,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:02,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:02,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2190 (KafkaRDD[2190] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:02,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2190 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:18:02,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2190_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:18:02,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2190_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:02,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2190 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:02,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2190 (KafkaRDD[2190] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:02,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2190.0 with 2 tasks
2017-07-26 18:18:02,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2190.0 (TID 4380, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:02,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2190.0 (TID 4381, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:02,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2190.0 (TID 4380)
2017-07-26 18:18:02,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2190.0 (TID 4381)
2017-07-26 18:18:02,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:02,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:02,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2190.0 (TID 4381). 635 bytes result sent to driver
2017-07-26 18:18:02,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2190.0 (TID 4380). 635 bytes result sent to driver
2017-07-26 18:18:02,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2190.0 (TID 4380) in 13 ms on localhost (1/2)
2017-07-26 18:18:02,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2190.0 (TID 4381) in 10 ms on localhost (2/2)
2017-07-26 18:18:02,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2190.0, whose tasks have all completed, from pool 
2017-07-26 18:18:02,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2190 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:18:02,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2190 finished: foreachPartition at streamingProcessTest.scala:48, took 0.033795 s
2017-07-26 18:18:02,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064282000 ms.0 from job set of time 1501064282000 ms
2017-07-26 18:18:02,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1501064282000 ms (execution: 0.058 s)
2017-07-26 18:18:02,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2189 from persistence list
2017-07-26 18:18:02,079 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2189
2017-07-26 18:18:02,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:02,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064278000 ms
2017-07-26 18:18:04,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064284000 ms
2017-07-26 18:18:04,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064284000 ms.0 from job set of time 1501064284000 ms
2017-07-26 18:18:04,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2191 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2191 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2191 (KafkaRDD[2191] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:04,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2191 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:18:04,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2191_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:18:04,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2191_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:04,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2191 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:04,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2191 (KafkaRDD[2191] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:04,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2191.0 with 2 tasks
2017-07-26 18:18:04,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2191.0 (TID 4382, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:04,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2191.0 (TID 4383, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:04,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2191.0 (TID 4383)
2017-07-26 18:18:04,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2191.0 (TID 4382)
2017-07-26 18:18:04,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:04,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:04,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2191.0 (TID 4382). 635 bytes result sent to driver
2017-07-26 18:18:04,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2191.0 (TID 4383). 635 bytes result sent to driver
2017-07-26 18:18:04,073 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2191.0 (TID 4382) in 9 ms on localhost (1/2)
2017-07-26 18:18:04,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2191.0 (TID 4383) in 9 ms on localhost (2/2)
2017-07-26 18:18:04,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2191.0, whose tasks have all completed, from pool 
2017-07-26 18:18:04,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2191 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:18:04,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2191 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026479 s
2017-07-26 18:18:04,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064284000 ms.0 from job set of time 1501064284000 ms
2017-07-26 18:18:04,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501064284000 ms (execution: 0.058 s)
2017-07-26 18:18:04,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2190 from persistence list
2017-07-26 18:18:04,076 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2190
2017-07-26 18:18:04,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:04,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064280000 ms
2017-07-26 18:18:06,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064286000 ms
2017-07-26 18:18:06,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064286000 ms.0 from job set of time 1501064286000 ms
2017-07-26 18:18:06,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:06,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2192 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:06,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2192 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:06,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:06,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:06,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2192 (KafkaRDD[2192] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:06,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2192 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:18:06,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2192_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:18:06,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2192_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:06,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2192 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:06,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2192 (KafkaRDD[2192] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:06,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2192.0 with 2 tasks
2017-07-26 18:18:06,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2192.0 (TID 4384, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:06,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2192.0 (TID 4385, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:06,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2192.0 (TID 4384)
2017-07-26 18:18:06,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2192.0 (TID 4385)
2017-07-26 18:18:06,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:06,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:06,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2192.0 (TID 4385). 714 bytes result sent to driver
2017-07-26 18:18:06,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2192.0 (TID 4384). 635 bytes result sent to driver
2017-07-26 18:18:06,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2192.0 (TID 4384) in 11 ms on localhost (1/2)
2017-07-26 18:18:06,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2192.0 (TID 4385) in 10 ms on localhost (2/2)
2017-07-26 18:18:06,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2192.0, whose tasks have all completed, from pool 
2017-07-26 18:18:06,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2192 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:18:06,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2192 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026828 s
2017-07-26 18:18:06,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064286000 ms.0 from job set of time 1501064286000 ms
2017-07-26 18:18:06,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501064286000 ms (execution: 0.057 s)
2017-07-26 18:18:06,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2191 from persistence list
2017-07-26 18:18:06,076 [block-manager-slave-async-thread-pool-22] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2191
2017-07-26 18:18:06,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:06,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064282000 ms
2017-07-26 18:18:08,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064288000 ms
2017-07-26 18:18:08,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064288000 ms.0 from job set of time 1501064288000 ms
2017-07-26 18:18:08,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2193 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2193 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:08,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2193 (KafkaRDD[2193] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:08,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2193 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:18:08,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2179_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:08,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2193_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:18:08,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2193_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:08,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2193 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:08,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2193 (KafkaRDD[2193] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:08,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2193.0 with 2 tasks
2017-07-26 18:18:08,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2180_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:08,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2193.0 (TID 4386, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:08,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2193.0 (TID 4387, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:08,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2193.0 (TID 4387)
2017-07-26 18:18:08,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2193.0 (TID 4386)
2017-07-26 18:18:08,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:08,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2181_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:08,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:08,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2193.0 (TID 4387). 714 bytes result sent to driver
2017-07-26 18:18:08,077 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2182_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:08,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2193.0 (TID 4386). 714 bytes result sent to driver
2017-07-26 18:18:08,080 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2193.0 (TID 4387) in 12 ms on localhost (1/2)
2017-07-26 18:18:08,081 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2183_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:08,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2193.0 (TID 4386) in 15 ms on localhost (2/2)
2017-07-26 18:18:08,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2193.0, whose tasks have all completed, from pool 
2017-07-26 18:18:08,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2193 (foreachPartition at streamingProcessTest.scala:48) finished in 0.016 s
2017-07-26 18:18:08,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2193 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040156 s
2017-07-26 18:18:08,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064288000 ms.0 from job set of time 1501064288000 ms
2017-07-26 18:18:08,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1501064288000 ms (execution: 0.066 s)
2017-07-26 18:18:08,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2192 from persistence list
2017-07-26 18:18:08,084 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2184_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:08,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:08,085 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2192
2017-07-26 18:18:08,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064284000 ms
2017-07-26 18:18:08,088 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2185_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:08,090 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2186_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:08,092 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2187_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:08,094 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2188_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:08,096 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2189_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:08,100 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2190_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:08,102 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2191_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:08,103 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2192_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:10,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064290000 ms
2017-07-26 18:18:10,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064290000 ms.0 from job set of time 1501064290000 ms
2017-07-26 18:18:10,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2194 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2194 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:10,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2194 (KafkaRDD[2194] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:10,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2194 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:18:10,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2194_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:18:10,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2194_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:10,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2194 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:10,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2194 (KafkaRDD[2194] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:10,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2194.0 with 2 tasks
2017-07-26 18:18:10,075 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2194.0 (TID 4388, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:10,076 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2194.0 (TID 4389, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:10,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2194.0 (TID 4389)
2017-07-26 18:18:10,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2194.0 (TID 4388)
2017-07-26 18:18:10,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:10,080 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:10,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2194.0 (TID 4388). 714 bytes result sent to driver
2017-07-26 18:18:10,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2194.0 (TID 4389). 714 bytes result sent to driver
2017-07-26 18:18:10,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2194.0 (TID 4388) in 13 ms on localhost (1/2)
2017-07-26 18:18:10,087 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2194.0 (TID 4389) in 12 ms on localhost (2/2)
2017-07-26 18:18:10,087 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2194.0, whose tasks have all completed, from pool 
2017-07-26 18:18:10,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2194 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:18:10,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2194 finished: foreachPartition at streamingProcessTest.scala:48, took 0.037219 s
2017-07-26 18:18:10,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064290000 ms.0 from job set of time 1501064290000 ms
2017-07-26 18:18:10,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1501064290000 ms (execution: 0.072 s)
2017-07-26 18:18:10,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2193 from persistence list
2017-07-26 18:18:10,090 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2193
2017-07-26 18:18:10,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:10,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064286000 ms
2017-07-26 18:18:12,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064292000 ms
2017-07-26 18:18:12,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064292000 ms.0 from job set of time 1501064292000 ms
2017-07-26 18:18:12,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:12,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2195 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:12,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2195 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:12,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:12,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:12,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2195 (KafkaRDD[2195] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:12,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2195 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:18:12,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2195_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:18:12,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2195_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:12,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2195 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:12,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2195 (KafkaRDD[2195] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:12,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2195.0 with 2 tasks
2017-07-26 18:18:12,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2195.0 (TID 4390, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:12,060 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2195.0 (TID 4391, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:12,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2195.0 (TID 4391)
2017-07-26 18:18:12,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2195.0 (TID 4390)
2017-07-26 18:18:12,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:12,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:12,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2195.0 (TID 4391). 714 bytes result sent to driver
2017-07-26 18:18:12,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2195.0 (TID 4390). 635 bytes result sent to driver
2017-07-26 18:18:12,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2195.0 (TID 4391) in 6 ms on localhost (1/2)
2017-07-26 18:18:12,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2195.0 (TID 4390) in 7 ms on localhost (2/2)
2017-07-26 18:18:12,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2195 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:18:12,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2195.0, whose tasks have all completed, from pool 
2017-07-26 18:18:12,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2195 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023970 s
2017-07-26 18:18:12,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064292000 ms.0 from job set of time 1501064292000 ms
2017-07-26 18:18:12,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1501064292000 ms (execution: 0.052 s)
2017-07-26 18:18:12,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2194 from persistence list
2017-07-26 18:18:12,069 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2194
2017-07-26 18:18:12,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:12,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064288000 ms
2017-07-26 18:18:14,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064294000 ms
2017-07-26 18:18:14,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064294000 ms.0 from job set of time 1501064294000 ms
2017-07-26 18:18:14,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:14,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2196 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:14,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2196 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:14,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:14,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:14,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2196 (KafkaRDD[2196] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:14,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2196 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:18:14,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2196_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:18:14,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2196_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:14,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2196 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:14,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2196 (KafkaRDD[2196] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:14,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2196.0 with 2 tasks
2017-07-26 18:18:14,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2196.0 (TID 4392, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:14,072 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2196.0 (TID 4393, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:14,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2196.0 (TID 4393)
2017-07-26 18:18:14,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2196.0 (TID 4392)
2017-07-26 18:18:14,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:14,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:14,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2196.0 (TID 4393). 714 bytes result sent to driver
2017-07-26 18:18:14,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2196.0 (TID 4392). 714 bytes result sent to driver
2017-07-26 18:18:14,089 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2196.0 (TID 4393) in 17 ms on localhost (1/2)
2017-07-26 18:18:14,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2196.0 (TID 4392) in 20 ms on localhost (2/2)
2017-07-26 18:18:14,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2196.0, whose tasks have all completed, from pool 
2017-07-26 18:18:14,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2196 (foreachPartition at streamingProcessTest.scala:48) finished in 0.021 s
2017-07-26 18:18:14,091 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2196 finished: foreachPartition at streamingProcessTest.scala:48, took 0.043258 s
2017-07-26 18:18:14,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064294000 ms.0 from job set of time 1501064294000 ms
2017-07-26 18:18:14,092 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2195 from persistence list
2017-07-26 18:18:14,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.091 s for time 1501064294000 ms (execution: 0.075 s)
2017-07-26 18:18:14,093 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2195
2017-07-26 18:18:14,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:14,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064290000 ms
2017-07-26 18:18:16,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064296000 ms
2017-07-26 18:18:16,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064296000 ms.0 from job set of time 1501064296000 ms
2017-07-26 18:18:16,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2197 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2197 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2197 (KafkaRDD[2197] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:16,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2197 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:18:16,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2197_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:18:16,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2197_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:16,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2197 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:16,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2197 (KafkaRDD[2197] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:16,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2197.0 with 2 tasks
2017-07-26 18:18:16,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2197.0 (TID 4394, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:16,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2197.0 (TID 4395, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:16,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2197.0 (TID 4395)
2017-07-26 18:18:16,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2197.0 (TID 4394)
2017-07-26 18:18:16,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:16,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:16,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2197.0 (TID 4394). 714 bytes result sent to driver
2017-07-26 18:18:16,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2197.0 (TID 4395). 635 bytes result sent to driver
2017-07-26 18:18:16,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2197.0 (TID 4394) in 12 ms on localhost (1/2)
2017-07-26 18:18:16,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2197.0 (TID 4395) in 12 ms on localhost (2/2)
2017-07-26 18:18:16,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2197.0, whose tasks have all completed, from pool 
2017-07-26 18:18:16,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2197 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:18:16,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2197 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028809 s
2017-07-26 18:18:16,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064296000 ms.0 from job set of time 1501064296000 ms
2017-07-26 18:18:16,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501064296000 ms (execution: 0.059 s)
2017-07-26 18:18:16,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2196 from persistence list
2017-07-26 18:18:16,076 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2196
2017-07-26 18:18:16,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:16,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064292000 ms
2017-07-26 18:18:18,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064298000 ms
2017-07-26 18:18:18,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064298000 ms.0 from job set of time 1501064298000 ms
2017-07-26 18:18:18,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2198 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2198 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:18,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2198 (KafkaRDD[2198] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:18,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2198 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:18:18,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2198_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:18:18,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2198_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:18,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2198 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:18,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2198 (KafkaRDD[2198] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:18,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2198.0 with 2 tasks
2017-07-26 18:18:18,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2198.0 (TID 4396, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:18,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2198.0 (TID 4397, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:18,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2198.0 (TID 4397)
2017-07-26 18:18:18,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2198.0 (TID 4396)
2017-07-26 18:18:18,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:18,081 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:18,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2198.0 (TID 4396). 635 bytes result sent to driver
2017-07-26 18:18:18,087 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2198.0 (TID 4397). 714 bytes result sent to driver
2017-07-26 18:18:18,090 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2198.0 (TID 4396) in 17 ms on localhost (1/2)
2017-07-26 18:18:18,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2198.0 (TID 4397) in 16 ms on localhost (2/2)
2017-07-26 18:18:18,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2198.0, whose tasks have all completed, from pool 
2017-07-26 18:18:18,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2198 (foreachPartition at streamingProcessTest.scala:48) finished in 0.018 s
2017-07-26 18:18:18,090 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2198 finished: foreachPartition at streamingProcessTest.scala:48, took 0.041520 s
2017-07-26 18:18:18,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064298000 ms.0 from job set of time 1501064298000 ms
2017-07-26 18:18:18,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.091 s for time 1501064298000 ms (execution: 0.074 s)
2017-07-26 18:18:18,091 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2197 from persistence list
2017-07-26 18:18:18,092 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2197
2017-07-26 18:18:18,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:18,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064294000 ms
2017-07-26 18:18:20,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064300000 ms
2017-07-26 18:18:20,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064300000 ms.0 from job set of time 1501064300000 ms
2017-07-26 18:18:20,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:20,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2199 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:20,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2199 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:20,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:20,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2199 (KafkaRDD[2199] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:20,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2199 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:18:20,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2199_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:18:20,034 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2199_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2199 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2199 (KafkaRDD[2199] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:20,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2199.0 with 2 tasks
2017-07-26 18:18:20,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2199.0 (TID 4398, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:20,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2199.0 (TID 4399, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:20,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2199.0 (TID 4398)
2017-07-26 18:18:20,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2199.0 (TID 4399)
2017-07-26 18:18:20,040 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:20,040 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:20,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2199.0 (TID 4399). 635 bytes result sent to driver
2017-07-26 18:18:20,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2199.0 (TID 4398). 635 bytes result sent to driver
2017-07-26 18:18:20,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2199.0 (TID 4399) in 7 ms on localhost (1/2)
2017-07-26 18:18:20,045 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2199.0 (TID 4398) in 8 ms on localhost (2/2)
2017-07-26 18:18:20,045 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2199.0, whose tasks have all completed, from pool 
2017-07-26 18:18:20,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2199 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:18:20,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2199 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019357 s
2017-07-26 18:18:20,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064300000 ms.0 from job set of time 1501064300000 ms
2017-07-26 18:18:20,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.045 s for time 1501064300000 ms (execution: 0.033 s)
2017-07-26 18:18:20,046 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2198 from persistence list
2017-07-26 18:18:20,046 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2198
2017-07-26 18:18:20,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:20,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064296000 ms
2017-07-26 18:18:22,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064302000 ms
2017-07-26 18:18:22,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064302000 ms.0 from job set of time 1501064302000 ms
2017-07-26 18:18:22,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:22,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2200 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:22,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2200 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:22,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:22,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:22,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2200 (KafkaRDD[2200] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2200 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:18:22,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2200_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:18:22,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2200_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:22,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2200 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:22,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2200 (KafkaRDD[2200] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:22,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2200.0 with 2 tasks
2017-07-26 18:18:22,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2200.0 (TID 4400, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:22,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2200.0 (TID 4401, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:22,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2200.0 (TID 4401)
2017-07-26 18:18:22,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2200.0 (TID 4400)
2017-07-26 18:18:22,063 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:22,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:22,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2200.0 (TID 4400). 714 bytes result sent to driver
2017-07-26 18:18:22,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2200.0 (TID 4401). 714 bytes result sent to driver
2017-07-26 18:18:22,070 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2200.0 (TID 4400) in 13 ms on localhost (1/2)
2017-07-26 18:18:22,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2200.0 (TID 4401) in 12 ms on localhost (2/2)
2017-07-26 18:18:22,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2200.0, whose tasks have all completed, from pool 
2017-07-26 18:18:22,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2200 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:18:22,071 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2200 finished: foreachPartition at streamingProcessTest.scala:48, took 0.029810 s
2017-07-26 18:18:22,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064302000 ms.0 from job set of time 1501064302000 ms
2017-07-26 18:18:22,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2199 from persistence list
2017-07-26 18:18:22,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501064302000 ms (execution: 0.055 s)
2017-07-26 18:18:22,073 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2199
2017-07-26 18:18:22,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:22,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064298000 ms
2017-07-26 18:18:24,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064304000 ms
2017-07-26 18:18:24,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064304000 ms.0 from job set of time 1501064304000 ms
2017-07-26 18:18:24,036 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:24,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2201 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:24,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2201 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:24,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:24,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:24,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2201 (KafkaRDD[2201] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:24,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2201 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:18:24,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2201_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:18:24,046 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2201_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:24,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2201 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:24,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2201 (KafkaRDD[2201] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:24,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2201.0 with 2 tasks
2017-07-26 18:18:24,049 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2201.0 (TID 4402, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:24,049 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2201.0 (TID 4403, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:24,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2201.0 (TID 4402)
2017-07-26 18:18:24,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2201.0 (TID 4403)
2017-07-26 18:18:24,052 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:24,052 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:24,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2201.0 (TID 4402). 635 bytes result sent to driver
2017-07-26 18:18:24,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2201.0 (TID 4403). 635 bytes result sent to driver
2017-07-26 18:18:24,056 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2201.0 (TID 4402) in 8 ms on localhost (1/2)
2017-07-26 18:18:24,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2201.0 (TID 4403) in 8 ms on localhost (2/2)
2017-07-26 18:18:24,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2201.0, whose tasks have all completed, from pool 
2017-07-26 18:18:24,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2201 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:18:24,057 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2201 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021080 s
2017-07-26 18:18:24,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064304000 ms.0 from job set of time 1501064304000 ms
2017-07-26 18:18:24,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.058 s for time 1501064304000 ms (execution: 0.040 s)
2017-07-26 18:18:24,058 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2200 from persistence list
2017-07-26 18:18:24,058 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2200
2017-07-26 18:18:24,058 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:24,058 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064300000 ms
2017-07-26 18:18:26,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064306000 ms
2017-07-26 18:18:26,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064306000 ms.0 from job set of time 1501064306000 ms
2017-07-26 18:18:26,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:26,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2202 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:26,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2202 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:26,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:26,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:26,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2202 (KafkaRDD[2202] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:26,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2202 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:18:26,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2202_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:18:26,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2202_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:26,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2202 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:26,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2202 (KafkaRDD[2202] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:26,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2202.0 with 2 tasks
2017-07-26 18:18:26,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2202.0 (TID 4404, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:26,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2202.0 (TID 4405, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:26,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2202.0 (TID 4405)
2017-07-26 18:18:26,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2202.0 (TID 4404)
2017-07-26 18:18:26,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:26,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:26,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2202.0 (TID 4405). 714 bytes result sent to driver
2017-07-26 18:18:26,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2202.0 (TID 4404). 714 bytes result sent to driver
2017-07-26 18:18:26,070 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2202.0 (TID 4405) in 11 ms on localhost (1/2)
2017-07-26 18:18:26,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2202.0 (TID 4404) in 13 ms on localhost (2/2)
2017-07-26 18:18:26,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2202.0, whose tasks have all completed, from pool 
2017-07-26 18:18:26,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2202 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:18:26,071 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2202 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028626 s
2017-07-26 18:18:26,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064306000 ms.0 from job set of time 1501064306000 ms
2017-07-26 18:18:26,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501064306000 ms (execution: 0.056 s)
2017-07-26 18:18:26,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2201 from persistence list
2017-07-26 18:18:26,073 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2201
2017-07-26 18:18:26,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:26,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064302000 ms
2017-07-26 18:18:28,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064308000 ms
2017-07-26 18:18:28,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064308000 ms.0 from job set of time 1501064308000 ms
2017-07-26 18:18:28,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2203 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2203 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:28,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2203 (KafkaRDD[2203] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:28,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2203 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:18:28,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2203_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:18:28,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2203_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:28,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2203 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:28,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2203 (KafkaRDD[2203] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:28,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2203.0 with 2 tasks
2017-07-26 18:18:28,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2203.0 (TID 4406, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:28,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2203.0 (TID 4407, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:28,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2203.0 (TID 4406)
2017-07-26 18:18:28,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2203.0 (TID 4407)
2017-07-26 18:18:28,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:28,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:28,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2203.0 (TID 4407). 635 bytes result sent to driver
2017-07-26 18:18:28,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2203.0 (TID 4406). 635 bytes result sent to driver
2017-07-26 18:18:28,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2203.0 (TID 4407) in 9 ms on localhost (1/2)
2017-07-26 18:18:28,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2203.0 (TID 4406) in 10 ms on localhost (2/2)
2017-07-26 18:18:28,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2203 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:18:28,070 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2203.0, whose tasks have all completed, from pool 
2017-07-26 18:18:28,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2203 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022625 s
2017-07-26 18:18:28,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064308000 ms.0 from job set of time 1501064308000 ms
2017-07-26 18:18:28,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1501064308000 ms (execution: 0.055 s)
2017-07-26 18:18:28,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2202 from persistence list
2017-07-26 18:18:28,072 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2202
2017-07-26 18:18:28,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:28,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064304000 ms
2017-07-26 18:18:30,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064310000 ms
2017-07-26 18:18:30,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064310000 ms.0 from job set of time 1501064310000 ms
2017-07-26 18:18:30,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2204 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2204 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2204 (KafkaRDD[2204] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:30,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2204 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:18:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2204_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:18:30,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2204_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2204 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2204 (KafkaRDD[2204] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2204.0 with 2 tasks
2017-07-26 18:18:30,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2204.0 (TID 4408, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:30,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2204.0 (TID 4409, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:30,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2204.0 (TID 4409)
2017-07-26 18:18:30,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2204.0 (TID 4408)
2017-07-26 18:18:30,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:30,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:30,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2204.0 (TID 4408). 635 bytes result sent to driver
2017-07-26 18:18:30,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2204.0 (TID 4409). 635 bytes result sent to driver
2017-07-26 18:18:30,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2204.0 (TID 4408) in 13 ms on localhost (1/2)
2017-07-26 18:18:30,078 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2204.0 (TID 4409) in 12 ms on localhost (2/2)
2017-07-26 18:18:30,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2204.0, whose tasks have all completed, from pool 
2017-07-26 18:18:30,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2204 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:18:30,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2204 finished: foreachPartition at streamingProcessTest.scala:48, took 0.030527 s
2017-07-26 18:18:30,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064310000 ms.0 from job set of time 1501064310000 ms
2017-07-26 18:18:30,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1501064310000 ms (execution: 0.062 s)
2017-07-26 18:18:30,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2203 from persistence list
2017-07-26 18:18:30,081 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2203
2017-07-26 18:18:30,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:30,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064306000 ms
2017-07-26 18:18:32,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064312000 ms
2017-07-26 18:18:32,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064312000 ms.0 from job set of time 1501064312000 ms
2017-07-26 18:18:32,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:32,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2205 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:32,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2205 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:32,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:32,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:32,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2205 (KafkaRDD[2205] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:32,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2205 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:18:32,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2205_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:18:32,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2205_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:32,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2205 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:32,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2205 (KafkaRDD[2205] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:32,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2205.0 with 2 tasks
2017-07-26 18:18:32,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2205.0 (TID 4410, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:32,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2205.0 (TID 4411, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:32,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2205.0 (TID 4411)
2017-07-26 18:18:32,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2205.0 (TID 4410)
2017-07-26 18:18:32,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:32,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:32,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2205.0 (TID 4410). 635 bytes result sent to driver
2017-07-26 18:18:32,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2205.0 (TID 4411). 714 bytes result sent to driver
2017-07-26 18:18:32,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2205.0 (TID 4410) in 10 ms on localhost (1/2)
2017-07-26 18:18:32,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2205.0 (TID 4411) in 8 ms on localhost (2/2)
2017-07-26 18:18:32,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2205.0, whose tasks have all completed, from pool 
2017-07-26 18:18:32,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2205 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:18:32,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2205 finished: foreachPartition at streamingProcessTest.scala:48, took 0.030456 s
2017-07-26 18:18:32,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064312000 ms.0 from job set of time 1501064312000 ms
2017-07-26 18:18:32,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1501064312000 ms (execution: 0.062 s)
2017-07-26 18:18:32,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2204 from persistence list
2017-07-26 18:18:32,081 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2204
2017-07-26 18:18:32,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:32,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064308000 ms
2017-07-26 18:18:34,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064314000 ms
2017-07-26 18:18:34,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064314000 ms.0 from job set of time 1501064314000 ms
2017-07-26 18:18:34,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:34,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2206 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2206 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2206 (KafkaRDD[2206] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:34,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2206 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:18:34,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2206_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:18:34,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2206_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:34,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2206 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:34,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2206 (KafkaRDD[2206] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:34,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2206.0 with 2 tasks
2017-07-26 18:18:34,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2206.0 (TID 4412, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:34,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2206.0 (TID 4413, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:34,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2206.0 (TID 4412)
2017-07-26 18:18:34,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2206.0 (TID 4413)
2017-07-26 18:18:34,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:34,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:34,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2206.0 (TID 4412). 635 bytes result sent to driver
2017-07-26 18:18:34,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2206.0 (TID 4413). 635 bytes result sent to driver
2017-07-26 18:18:34,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2206.0 (TID 4413) in 10 ms on localhost (1/2)
2017-07-26 18:18:34,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2206.0 (TID 4412) in 12 ms on localhost (2/2)
2017-07-26 18:18:34,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2206.0, whose tasks have all completed, from pool 
2017-07-26 18:18:34,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2206 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:18:34,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2206 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026316 s
2017-07-26 18:18:34,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064314000 ms.0 from job set of time 1501064314000 ms
2017-07-26 18:18:34,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501064314000 ms (execution: 0.057 s)
2017-07-26 18:18:34,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2205 from persistence list
2017-07-26 18:18:34,076 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2205
2017-07-26 18:18:34,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:34,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064310000 ms
2017-07-26 18:18:36,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064316000 ms
2017-07-26 18:18:36,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064316000 ms.0 from job set of time 1501064316000 ms
2017-07-26 18:18:36,063 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:36,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2207 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:36,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2207 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:36,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:36,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:36,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2206_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:36,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2207 (KafkaRDD[2207] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:36,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2193_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:36,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2207 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:18:36,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2194_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:36,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2195_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:36,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2207_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:18:36,073 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2207_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:36,074 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2196_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:36,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2207 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:36,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2207 (KafkaRDD[2207] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:36,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2207.0 with 2 tasks
2017-07-26 18:18:36,076 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2207.0 (TID 4414, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:36,076 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2197_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:36,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2207.0 (TID 4415, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:36,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2207.0 (TID 4414)
2017-07-26 18:18:36,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2207.0 (TID 4415)
2017-07-26 18:18:36,079 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2198_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:36,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:36,080 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:36,081 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2199_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:36,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2207.0 (TID 4415). 635 bytes result sent to driver
2017-07-26 18:18:36,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2207.0 (TID 4414). 635 bytes result sent to driver
2017-07-26 18:18:36,084 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2200_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:36,084 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2207.0 (TID 4415) in 8 ms on localhost (1/2)
2017-07-26 18:18:36,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2207.0 (TID 4414) in 10 ms on localhost (2/2)
2017-07-26 18:18:36,085 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2207.0, whose tasks have all completed, from pool 
2017-07-26 18:18:36,085 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2207 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:18:36,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2207 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021844 s
2017-07-26 18:18:36,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064316000 ms.0 from job set of time 1501064316000 ms
2017-07-26 18:18:36,086 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2201_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:36,086 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.086 s for time 1501064316000 ms (execution: 0.070 s)
2017-07-26 18:18:36,086 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2206 from persistence list
2017-07-26 18:18:36,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:36,087 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2206
2017-07-26 18:18:36,087 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064312000 ms
2017-07-26 18:18:36,088 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2202_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:36,089 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2203_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:36,091 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2204_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:36,092 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2205_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:38,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064318000 ms
2017-07-26 18:18:38,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064318000 ms.0 from job set of time 1501064318000 ms
2017-07-26 18:18:38,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:38,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2208 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:38,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2208 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:38,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:38,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:38,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2208 (KafkaRDD[2208] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:38,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2208 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:18:38,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2208_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:18:38,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2208_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:38,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2208 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:38,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2208 (KafkaRDD[2208] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:38,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2208.0 with 2 tasks
2017-07-26 18:18:38,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2208.0 (TID 4416, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:38,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2208.0 (TID 4417, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:38,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2208.0 (TID 4417)
2017-07-26 18:18:38,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2208.0 (TID 4416)
2017-07-26 18:18:38,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:38,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:38,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2208.0 (TID 4417). 635 bytes result sent to driver
2017-07-26 18:18:38,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2208.0 (TID 4416). 635 bytes result sent to driver
2017-07-26 18:18:38,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2208.0 (TID 4417) in 8 ms on localhost (1/2)
2017-07-26 18:18:38,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2208.0 (TID 4416) in 10 ms on localhost (2/2)
2017-07-26 18:18:38,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2208.0, whose tasks have all completed, from pool 
2017-07-26 18:18:38,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2208 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:18:38,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2208 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021798 s
2017-07-26 18:18:38,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064318000 ms.0 from job set of time 1501064318000 ms
2017-07-26 18:18:38,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501064318000 ms (execution: 0.050 s)
2017-07-26 18:18:38,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2207 from persistence list
2017-07-26 18:18:38,067 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2207
2017-07-26 18:18:38,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:38,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064314000 ms
2017-07-26 18:18:40,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064320000 ms
2017-07-26 18:18:40,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064320000 ms.0 from job set of time 1501064320000 ms
2017-07-26 18:18:40,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2209 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2209 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:40,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2209 (KafkaRDD[2209] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:40,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2209 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:18:40,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2209_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:18:40,050 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2209_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:40,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2209 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2209 (KafkaRDD[2209] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2209.0 with 2 tasks
2017-07-26 18:18:40,053 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2209.0 (TID 4418, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:40,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2209.0 (TID 4419, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:40,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2209.0 (TID 4418)
2017-07-26 18:18:40,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2209.0 (TID 4419)
2017-07-26 18:18:40,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:40,058 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:40,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2209.0 (TID 4418). 714 bytes result sent to driver
2017-07-26 18:18:40,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2209.0 (TID 4419). 635 bytes result sent to driver
2017-07-26 18:18:40,063 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2209.0 (TID 4418) in 11 ms on localhost (1/2)
2017-07-26 18:18:40,064 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2209.0 (TID 4419) in 11 ms on localhost (2/2)
2017-07-26 18:18:40,064 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2209.0, whose tasks have all completed, from pool 
2017-07-26 18:18:40,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2209 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:18:40,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2209 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026383 s
2017-07-26 18:18:40,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064320000 ms.0 from job set of time 1501064320000 ms
2017-07-26 18:18:40,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1501064320000 ms (execution: 0.048 s)
2017-07-26 18:18:40,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2208 from persistence list
2017-07-26 18:18:40,066 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2208
2017-07-26 18:18:40,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:40,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064316000 ms
2017-07-26 18:18:42,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064322000 ms
2017-07-26 18:18:42,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064322000 ms.0 from job set of time 1501064322000 ms
2017-07-26 18:18:42,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2210 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:42,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2210 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:42,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:42,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:42,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2210 (KafkaRDD[2210] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:42,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2210 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:18:42,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2210_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:18:42,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2210_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:18:42,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2210 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:42,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2210 (KafkaRDD[2210] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:42,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2210.0 with 2 tasks
2017-07-26 18:18:42,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2210.0 (TID 4420, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:42,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2210.0 (TID 4421, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:42,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2210.0 (TID 4420)
2017-07-26 18:18:42,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2210.0 (TID 4421)
2017-07-26 18:18:42,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:42,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:42,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2210.0 (TID 4420). 714 bytes result sent to driver
2017-07-26 18:18:42,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2210.0 (TID 4421). 714 bytes result sent to driver
2017-07-26 18:18:42,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2210.0 (TID 4420) in 11 ms on localhost (1/2)
2017-07-26 18:18:42,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2210.0 (TID 4421) in 9 ms on localhost (2/2)
2017-07-26 18:18:42,078 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2210.0, whose tasks have all completed, from pool 
2017-07-26 18:18:42,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2210 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:18:42,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2210 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028578 s
2017-07-26 18:18:42,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064322000 ms.0 from job set of time 1501064322000 ms
2017-07-26 18:18:42,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1501064322000 ms (execution: 0.062 s)
2017-07-26 18:18:42,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2209 from persistence list
2017-07-26 18:18:42,080 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2209
2017-07-26 18:18:42,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:42,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064318000 ms
2017-07-26 18:18:44,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064324000 ms
2017-07-26 18:18:44,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064324000 ms.0 from job set of time 1501064324000 ms
2017-07-26 18:18:44,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:44,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2211 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:44,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2211 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:44,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:44,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:44,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2211 (KafkaRDD[2211] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:44,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2211 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:18:44,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2211_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:18:44,048 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2211_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:44,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2211 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:44,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2211 (KafkaRDD[2211] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:44,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2211.0 with 2 tasks
2017-07-26 18:18:44,050 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2211.0 (TID 4422, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:44,050 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2211.0 (TID 4423, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:44,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2211.0 (TID 4423)
2017-07-26 18:18:44,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2211.0 (TID 4422)
2017-07-26 18:18:44,052 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:44,052 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:44,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2211.0 (TID 4423). 714 bytes result sent to driver
2017-07-26 18:18:44,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2211.0 (TID 4422). 714 bytes result sent to driver
2017-07-26 18:18:44,056 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2211.0 (TID 4423) in 6 ms on localhost (1/2)
2017-07-26 18:18:44,056 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2211.0 (TID 4422) in 7 ms on localhost (2/2)
2017-07-26 18:18:44,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2211.0, whose tasks have all completed, from pool 
2017-07-26 18:18:44,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2211 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:18:44,057 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2211 finished: foreachPartition at streamingProcessTest.scala:48, took 0.017019 s
2017-07-26 18:18:44,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064324000 ms.0 from job set of time 1501064324000 ms
2017-07-26 18:18:44,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.057 s for time 1501064324000 ms (execution: 0.040 s)
2017-07-26 18:18:44,058 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2210 from persistence list
2017-07-26 18:18:44,058 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2210
2017-07-26 18:18:44,058 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:44,058 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064320000 ms
2017-07-26 18:18:46,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064326000 ms
2017-07-26 18:18:46,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064326000 ms.0 from job set of time 1501064326000 ms
2017-07-26 18:18:46,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:46,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2212 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:46,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2212 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:46,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:46,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:46,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2212 (KafkaRDD[2212] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2212 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:18:46,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2212_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:18:46,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2212_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:46,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2212 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:46,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2212 (KafkaRDD[2212] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:46,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2212.0 with 2 tasks
2017-07-26 18:18:46,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2212.0 (TID 4424, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:46,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2212.0 (TID 4425, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:46,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2212.0 (TID 4424)
2017-07-26 18:18:46,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2212.0 (TID 4425)
2017-07-26 18:18:46,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:46,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:46,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2212.0 (TID 4424). 635 bytes result sent to driver
2017-07-26 18:18:46,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2212.0 (TID 4425). 635 bytes result sent to driver
2017-07-26 18:18:46,063 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2212.0 (TID 4424) in 8 ms on localhost (1/2)
2017-07-26 18:18:46,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2212.0 (TID 4425) in 7 ms on localhost (2/2)
2017-07-26 18:18:46,064 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2212.0, whose tasks have all completed, from pool 
2017-07-26 18:18:46,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2212 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:18:46,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2212 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019453 s
2017-07-26 18:18:46,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064326000 ms.0 from job set of time 1501064326000 ms
2017-07-26 18:18:46,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1501064326000 ms (execution: 0.048 s)
2017-07-26 18:18:46,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2211 from persistence list
2017-07-26 18:18:46,065 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2211
2017-07-26 18:18:46,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:46,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064322000 ms
2017-07-26 18:18:48,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064328000 ms
2017-07-26 18:18:48,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064328000 ms.0 from job set of time 1501064328000 ms
2017-07-26 18:18:48,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:48,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2213 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:48,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2213 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:48,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:48,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:48,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2213 (KafkaRDD[2213] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:48,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2213 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:18:48,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2213_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:18:48,031 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2213_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:48,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2213 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:48,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2213 (KafkaRDD[2213] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:48,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2213.0 with 2 tasks
2017-07-26 18:18:48,034 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2213.0 (TID 4426, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:48,034 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2213.0 (TID 4427, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:48,034 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2213.0 (TID 4426)
2017-07-26 18:18:48,034 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2213.0 (TID 4427)
2017-07-26 18:18:48,036 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:48,036 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:48,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2213.0 (TID 4427). 635 bytes result sent to driver
2017-07-26 18:18:48,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2213.0 (TID 4426). 635 bytes result sent to driver
2017-07-26 18:18:48,039 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2213.0 (TID 4426) in 6 ms on localhost (1/2)
2017-07-26 18:18:48,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2213.0 (TID 4427) in 6 ms on localhost (2/2)
2017-07-26 18:18:48,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2213 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:18:48,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2213.0, whose tasks have all completed, from pool 
2017-07-26 18:18:48,040 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2213 finished: foreachPartition at streamingProcessTest.scala:48, took 0.016918 s
2017-07-26 18:18:48,040 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064328000 ms.0 from job set of time 1501064328000 ms
2017-07-26 18:18:48,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.040 s for time 1501064328000 ms (execution: 0.027 s)
2017-07-26 18:18:48,041 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2212 from persistence list
2017-07-26 18:18:48,041 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2212
2017-07-26 18:18:48,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:48,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064324000 ms
2017-07-26 18:18:50,011 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064330000 ms
2017-07-26 18:18:50,011 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064330000 ms.0 from job set of time 1501064330000 ms
2017-07-26 18:18:50,021 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:50,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2214 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:50,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2214 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:50,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:50,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:50,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2214 (KafkaRDD[2214] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:50,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2214 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:18:50,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2214_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:18:50,027 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2214_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:50,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2214 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:50,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2214 (KafkaRDD[2214] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:50,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2214.0 with 2 tasks
2017-07-26 18:18:50,029 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2214.0 (TID 4428, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:50,030 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2214.0 (TID 4429, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:50,030 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2214.0 (TID 4428)
2017-07-26 18:18:50,030 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2214.0 (TID 4429)
2017-07-26 18:18:50,032 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:50,032 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:50,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2214.0 (TID 4428). 635 bytes result sent to driver
2017-07-26 18:18:50,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2214.0 (TID 4429). 635 bytes result sent to driver
2017-07-26 18:18:50,034 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2214.0 (TID 4428) in 6 ms on localhost (1/2)
2017-07-26 18:18:50,035 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2214.0 (TID 4429) in 6 ms on localhost (2/2)
2017-07-26 18:18:50,035 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2214.0, whose tasks have all completed, from pool 
2017-07-26 18:18:50,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2214 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:18:50,035 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2214 finished: foreachPartition at streamingProcessTest.scala:48, took 0.013675 s
2017-07-26 18:18:50,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064330000 ms.0 from job set of time 1501064330000 ms
2017-07-26 18:18:50,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.036 s for time 1501064330000 ms (execution: 0.025 s)
2017-07-26 18:18:50,036 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2213 from persistence list
2017-07-26 18:18:50,036 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2213
2017-07-26 18:18:50,036 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:50,036 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064326000 ms
2017-07-26 18:18:52,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064332000 ms
2017-07-26 18:18:52,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064332000 ms.0 from job set of time 1501064332000 ms
2017-07-26 18:18:52,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:52,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2215 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:52,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2215 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:52,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:52,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2215 (KafkaRDD[2215] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2215 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:18:52,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2215_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:18:52,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2215_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:52,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2215 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:52,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2215 (KafkaRDD[2215] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:52,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2215.0 with 2 tasks
2017-07-26 18:18:52,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2215.0 (TID 4430, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:52,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2215.0 (TID 4431, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:52,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2215.0 (TID 4431)
2017-07-26 18:18:52,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2215.0 (TID 4430)
2017-07-26 18:18:52,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:52,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:52,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2215.0 (TID 4430). 714 bytes result sent to driver
2017-07-26 18:18:52,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2215.0 (TID 4431). 714 bytes result sent to driver
2017-07-26 18:18:52,064 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2215.0 (TID 4430) in 8 ms on localhost (1/2)
2017-07-26 18:18:52,064 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2215.0 (TID 4431) in 7 ms on localhost (2/2)
2017-07-26 18:18:52,065 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2215.0, whose tasks have all completed, from pool 
2017-07-26 18:18:52,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2215 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:18:52,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2215 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018574 s
2017-07-26 18:18:52,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064332000 ms.0 from job set of time 1501064332000 ms
2017-07-26 18:18:52,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1501064332000 ms (execution: 0.048 s)
2017-07-26 18:18:52,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2214 from persistence list
2017-07-26 18:18:52,066 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2214
2017-07-26 18:18:52,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:52,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064328000 ms
2017-07-26 18:18:54,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064334000 ms
2017-07-26 18:18:54,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064334000 ms.0 from job set of time 1501064334000 ms
2017-07-26 18:18:54,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:54,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2216 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:54,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2216 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:54,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:54,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:54,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2216 (KafkaRDD[2216] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:54,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2216 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:18:54,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2216_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:18:54,052 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2216_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:18:54,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2216 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:54,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2216 (KafkaRDD[2216] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:54,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2216.0 with 2 tasks
2017-07-26 18:18:54,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2216.0 (TID 4432, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:54,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2216.0 (TID 4433, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:54,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2216.0 (TID 4432)
2017-07-26 18:18:54,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2216.0 (TID 4433)
2017-07-26 18:18:54,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:54,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:54,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2216.0 (TID 4432). 714 bytes result sent to driver
2017-07-26 18:18:54,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2216.0 (TID 4433). 714 bytes result sent to driver
2017-07-26 18:18:54,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2216.0 (TID 4433) in 13 ms on localhost (1/2)
2017-07-26 18:18:54,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2216.0 (TID 4432) in 15 ms on localhost (2/2)
2017-07-26 18:18:54,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2216.0, whose tasks have all completed, from pool 
2017-07-26 18:18:54,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2216 (foreachPartition at streamingProcessTest.scala:48) finished in 0.016 s
2017-07-26 18:18:54,071 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2216 finished: foreachPartition at streamingProcessTest.scala:48, took 0.032457 s
2017-07-26 18:18:54,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064334000 ms.0 from job set of time 1501064334000 ms
2017-07-26 18:18:54,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1501064334000 ms (execution: 0.056 s)
2017-07-26 18:18:54,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2215 from persistence list
2017-07-26 18:18:54,072 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2215
2017-07-26 18:18:54,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:54,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064330000 ms
2017-07-26 18:18:56,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064336000 ms
2017-07-26 18:18:56,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064336000 ms.0 from job set of time 1501064336000 ms
2017-07-26 18:18:56,023 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:56,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2217 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:56,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2217 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:56,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:56,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:56,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2217 (KafkaRDD[2217] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:56,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2217 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:18:56,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2217_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:18:56,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2217_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:56,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2217 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:56,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2217 (KafkaRDD[2217] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:56,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2217.0 with 2 tasks
2017-07-26 18:18:56,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2217.0 (TID 4434, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:56,031 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2217.0 (TID 4435, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:56,032 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2217.0 (TID 4434)
2017-07-26 18:18:56,032 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2217.0 (TID 4435)
2017-07-26 18:18:56,033 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:56,033 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:56,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2217.0 (TID 4435). 635 bytes result sent to driver
2017-07-26 18:18:56,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2217.0 (TID 4434). 635 bytes result sent to driver
2017-07-26 18:18:56,036 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2217.0 (TID 4435) in 5 ms on localhost (1/2)
2017-07-26 18:18:56,036 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2217.0 (TID 4434) in 6 ms on localhost (2/2)
2017-07-26 18:18:56,037 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2217.0, whose tasks have all completed, from pool 
2017-07-26 18:18:56,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2217 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:18:56,037 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2217 finished: foreachPartition at streamingProcessTest.scala:48, took 0.013495 s
2017-07-26 18:18:56,037 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064336000 ms.0 from job set of time 1501064336000 ms
2017-07-26 18:18:56,038 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2216 from persistence list
2017-07-26 18:18:56,038 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.037 s for time 1501064336000 ms (execution: 0.025 s)
2017-07-26 18:18:56,038 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2216
2017-07-26 18:18:56,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:56,038 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064332000 ms
2017-07-26 18:18:58,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064338000 ms
2017-07-26 18:18:58,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064338000 ms.0 from job set of time 1501064338000 ms
2017-07-26 18:18:58,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:18:58,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2218 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:18:58,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2218 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:18:58,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:18:58,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:18:58,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2218 (KafkaRDD[2218] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:18:58,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2218 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:18:58,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2218_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:18:58,048 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2218_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:18:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2218 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:18:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2218 (KafkaRDD[2218] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:18:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2218.0 with 2 tasks
2017-07-26 18:18:58,050 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2218.0 (TID 4436, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:18:58,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2218.0 (TID 4437, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:18:58,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2218.0 (TID 4436)
2017-07-26 18:18:58,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2218.0 (TID 4437)
2017-07-26 18:18:58,052 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:18:58,052 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:18:58,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2218.0 (TID 4437). 714 bytes result sent to driver
2017-07-26 18:18:58,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2218.0 (TID 4436). 714 bytes result sent to driver
2017-07-26 18:18:58,056 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2218.0 (TID 4437) in 6 ms on localhost (1/2)
2017-07-26 18:18:58,056 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2218.0 (TID 4436) in 6 ms on localhost (2/2)
2017-07-26 18:18:58,056 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2218.0, whose tasks have all completed, from pool 
2017-07-26 18:18:58,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2218 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:18:58,056 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2218 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018207 s
2017-07-26 18:18:58,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064338000 ms.0 from job set of time 1501064338000 ms
2017-07-26 18:18:58,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.057 s for time 1501064338000 ms (execution: 0.040 s)
2017-07-26 18:18:58,057 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2217 from persistence list
2017-07-26 18:18:58,057 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2217
2017-07-26 18:18:58,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:18:58,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064334000 ms
2017-07-26 18:19:00,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064340000 ms
2017-07-26 18:19:00,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064340000 ms.0 from job set of time 1501064340000 ms
2017-07-26 18:19:00,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:00,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2219 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:00,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2219 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:00,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:00,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:00,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2219 (KafkaRDD[2219] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:00,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2219 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:19:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2219_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:19:00,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2219_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2219 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2219 (KafkaRDD[2219] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2219.0 with 2 tasks
2017-07-26 18:19:00,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2219.0 (TID 4438, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:00,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2219.0 (TID 4439, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:00,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2219.0 (TID 4439)
2017-07-26 18:19:00,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2219.0 (TID 4438)
2017-07-26 18:19:00,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:00,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:00,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2219.0 (TID 4439). 714 bytes result sent to driver
2017-07-26 18:19:00,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2219.0 (TID 4438). 714 bytes result sent to driver
2017-07-26 18:19:00,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2219.0 (TID 4439) in 13 ms on localhost (1/2)
2017-07-26 18:19:00,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2219.0 (TID 4438) in 14 ms on localhost (2/2)
2017-07-26 18:19:00,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2219.0, whose tasks have all completed, from pool 
2017-07-26 18:19:00,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2219 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:19:00,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2219 finished: foreachPartition at streamingProcessTest.scala:48, took 0.030461 s
2017-07-26 18:19:00,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064340000 ms.0 from job set of time 1501064340000 ms
2017-07-26 18:19:00,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501064340000 ms (execution: 0.053 s)
2017-07-26 18:19:00,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2218 from persistence list
2017-07-26 18:19:00,070 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2218
2017-07-26 18:19:00,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:00,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064336000 ms
2017-07-26 18:19:02,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064342000 ms
2017-07-26 18:19:02,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064342000 ms.0 from job set of time 1501064342000 ms
2017-07-26 18:19:02,035 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:02,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2220 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:02,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2220 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:02,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:02,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:02,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2220 (KafkaRDD[2220] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:02,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2220 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:19:02,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2220_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:19:02,043 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2220_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:02,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2220 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:02,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2220 (KafkaRDD[2220] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:02,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2220.0 with 2 tasks
2017-07-26 18:19:02,045 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2220.0 (TID 4440, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:02,045 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2220.0 (TID 4441, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:02,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2220.0 (TID 4441)
2017-07-26 18:19:02,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2220.0 (TID 4440)
2017-07-26 18:19:02,048 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:02,048 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:02,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2220.0 (TID 4441). 635 bytes result sent to driver
2017-07-26 18:19:02,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2220.0 (TID 4440). 635 bytes result sent to driver
2017-07-26 18:19:02,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2220.0 (TID 4440) in 7 ms on localhost (1/2)
2017-07-26 18:19:02,052 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2220.0 (TID 4441) in 7 ms on localhost (2/2)
2017-07-26 18:19:02,052 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2220.0, whose tasks have all completed, from pool 
2017-07-26 18:19:02,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2220 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:19:02,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2220 finished: foreachPartition at streamingProcessTest.scala:48, took 0.017229 s
2017-07-26 18:19:02,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064342000 ms.0 from job set of time 1501064342000 ms
2017-07-26 18:19:02,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.052 s for time 1501064342000 ms (execution: 0.037 s)
2017-07-26 18:19:02,053 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2219 from persistence list
2017-07-26 18:19:02,053 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2219
2017-07-26 18:19:02,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:02,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064338000 ms
2017-07-26 18:19:04,010 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064344000 ms
2017-07-26 18:19:04,010 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064344000 ms.0 from job set of time 1501064344000 ms
2017-07-26 18:19:04,022 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2220_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:04,024 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2207_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:04,026 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2208_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:04,027 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2209_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:04,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:04,029 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2210_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:04,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2221 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:04,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2221 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:04,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:04,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:04,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2221 (KafkaRDD[2221] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:04,030 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2211_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:04,032 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2212_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:04,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2221 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:19:04,033 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2213_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:04,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2221_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:19:04,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2214_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:04,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2221_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:04,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2221 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:04,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2221 (KafkaRDD[2221] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:04,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2221.0 with 2 tasks
2017-07-26 18:19:04,037 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2221.0 (TID 4442, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:04,037 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2215_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:04,038 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2221.0 (TID 4443, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:04,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2221.0 (TID 4443)
2017-07-26 18:19:04,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2221.0 (TID 4442)
2017-07-26 18:19:04,039 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2216_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:04,040 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2217_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:04,041 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:04,041 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:04,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2221.0 (TID 4443). 635 bytes result sent to driver
2017-07-26 18:19:04,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2221.0 (TID 4442). 635 bytes result sent to driver
2017-07-26 18:19:04,043 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2218_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:04,044 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2221.0 (TID 4443) in 7 ms on localhost (1/2)
2017-07-26 18:19:04,044 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2219_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:04,045 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2221.0 (TID 4442) in 9 ms on localhost (2/2)
2017-07-26 18:19:04,045 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2221.0, whose tasks have all completed, from pool 
2017-07-26 18:19:04,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2221 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:19:04,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2221 finished: foreachPartition at streamingProcessTest.scala:48, took 0.015945 s
2017-07-26 18:19:04,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064344000 ms.0 from job set of time 1501064344000 ms
2017-07-26 18:19:04,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.045 s for time 1501064344000 ms (execution: 0.035 s)
2017-07-26 18:19:04,045 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2220 from persistence list
2017-07-26 18:19:04,046 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2220
2017-07-26 18:19:04,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:04,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064340000 ms
2017-07-26 18:19:06,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064346000 ms
2017-07-26 18:19:06,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064346000 ms.0 from job set of time 1501064346000 ms
2017-07-26 18:19:06,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:06,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2222 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:06,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2222 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:06,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:06,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:06,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2222 (KafkaRDD[2222] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:06,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2222 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:19:06,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2222_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:19:06,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2222_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:06,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2222 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:06,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2222 (KafkaRDD[2222] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:06,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2222.0 with 2 tasks
2017-07-26 18:19:06,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2222.0 (TID 4444, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:06,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2222.0 (TID 4445, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:06,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2222.0 (TID 4445)
2017-07-26 18:19:06,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2222.0 (TID 4444)
2017-07-26 18:19:06,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:06,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2222.0 (TID 4444). 635 bytes result sent to driver
2017-07-26 18:19:06,078 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2222.0 (TID 4444) in 12 ms on localhost (1/2)
2017-07-26 18:19:06,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:06,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2222.0 (TID 4445). 635 bytes result sent to driver
2017-07-26 18:19:06,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2222.0 (TID 4445) in 20 ms on localhost (2/2)
2017-07-26 18:19:06,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2222 (foreachPartition at streamingProcessTest.scala:48) finished in 0.021 s
2017-07-26 18:19:06,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2222.0, whose tasks have all completed, from pool 
2017-07-26 18:19:06,087 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2222 finished: foreachPartition at streamingProcessTest.scala:48, took 0.039195 s
2017-07-26 18:19:06,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064346000 ms.0 from job set of time 1501064346000 ms
2017-07-26 18:19:06,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1501064346000 ms (execution: 0.071 s)
2017-07-26 18:19:06,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2221 from persistence list
2017-07-26 18:19:06,088 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2221
2017-07-26 18:19:06,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:06,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064342000 ms
2017-07-26 18:19:08,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064348000 ms
2017-07-26 18:19:08,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064348000 ms.0 from job set of time 1501064348000 ms
2017-07-26 18:19:08,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:08,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2223 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:08,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2223 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:08,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2223 (KafkaRDD[2223] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:08,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2223 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:19:08,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2223_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:19:08,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2223_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:08,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2223 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:08,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2223 (KafkaRDD[2223] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:08,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2223.0 with 2 tasks
2017-07-26 18:19:08,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2223.0 (TID 4446, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:08,070 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2223.0 (TID 4447, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:08,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2223.0 (TID 4446)
2017-07-26 18:19:08,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2223.0 (TID 4447)
2017-07-26 18:19:08,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:08,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:08,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2223.0 (TID 4446). 714 bytes result sent to driver
2017-07-26 18:19:08,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2223.0 (TID 4447). 801 bytes result sent to driver
2017-07-26 18:19:08,086 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2223.0 (TID 4446) in 17 ms on localhost (1/2)
2017-07-26 18:19:08,087 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2223.0 (TID 4447) in 17 ms on localhost (2/2)
2017-07-26 18:19:08,087 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2223.0, whose tasks have all completed, from pool 
2017-07-26 18:19:08,087 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2223 (foreachPartition at streamingProcessTest.scala:48) finished in 0.020 s
2017-07-26 18:19:08,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2223 finished: foreachPartition at streamingProcessTest.scala:48, took 0.041185 s
2017-07-26 18:19:08,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064348000 ms.0 from job set of time 1501064348000 ms
2017-07-26 18:19:08,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1501064348000 ms (execution: 0.072 s)
2017-07-26 18:19:08,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2222 from persistence list
2017-07-26 18:19:08,091 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2222
2017-07-26 18:19:08,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:08,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064344000 ms
2017-07-26 18:19:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064350000 ms
2017-07-26 18:19:10,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064350000 ms.0 from job set of time 1501064350000 ms
2017-07-26 18:19:10,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2224 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2224 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2224 (KafkaRDD[2224] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:10,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2224 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:19:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2224_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:19:10,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2224_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:10,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2224 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:10,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2224 (KafkaRDD[2224] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:10,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2224.0 with 2 tasks
2017-07-26 18:19:10,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2224.0 (TID 4448, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:10,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2224.0 (TID 4449, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:10,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2224.0 (TID 4449)
2017-07-26 18:19:10,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2224.0 (TID 4448)
2017-07-26 18:19:10,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:10,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:10,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2224.0 (TID 4449). 635 bytes result sent to driver
2017-07-26 18:19:10,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2224.0 (TID 4448). 714 bytes result sent to driver
2017-07-26 18:19:10,079 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2224.0 (TID 4449) in 11 ms on localhost (1/2)
2017-07-26 18:19:10,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2224.0 (TID 4448) in 14 ms on localhost (2/2)
2017-07-26 18:19:10,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2224.0, whose tasks have all completed, from pool 
2017-07-26 18:19:10,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2224 (foreachPartition at streamingProcessTest.scala:48) finished in 0.015 s
2017-07-26 18:19:10,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2224 finished: foreachPartition at streamingProcessTest.scala:48, took 0.033383 s
2017-07-26 18:19:10,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064350000 ms.0 from job set of time 1501064350000 ms
2017-07-26 18:19:10,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501064350000 ms (execution: 0.065 s)
2017-07-26 18:19:10,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2223 from persistence list
2017-07-26 18:19:10,081 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2223
2017-07-26 18:19:10,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:10,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064346000 ms
2017-07-26 18:19:12,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064352000 ms
2017-07-26 18:19:12,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064352000 ms.0 from job set of time 1501064352000 ms
2017-07-26 18:19:12,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:12,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2225 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:12,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2225 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:12,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:12,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:12,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2225 (KafkaRDD[2225] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:12,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2225 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:19:12,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2225_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:19:12,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2225_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:12,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2225 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:12,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2225 (KafkaRDD[2225] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:12,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2225.0 with 2 tasks
2017-07-26 18:19:12,058 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2225.0 (TID 4450, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:12,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2225.0 (TID 4451, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:12,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2225.0 (TID 4450)
2017-07-26 18:19:12,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2225.0 (TID 4451)
2017-07-26 18:19:12,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:12,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:12,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2225.0 (TID 4450). 714 bytes result sent to driver
2017-07-26 18:19:12,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2225.0 (TID 4451). 714 bytes result sent to driver
2017-07-26 18:19:12,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2225.0 (TID 4450) in 9 ms on localhost (1/2)
2017-07-26 18:19:12,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2225.0 (TID 4451) in 9 ms on localhost (2/2)
2017-07-26 18:19:12,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2225.0, whose tasks have all completed, from pool 
2017-07-26 18:19:12,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2225 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:19:12,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2225 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025024 s
2017-07-26 18:19:12,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064352000 ms.0 from job set of time 1501064352000 ms
2017-07-26 18:19:12,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1501064352000 ms (execution: 0.052 s)
2017-07-26 18:19:12,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2224 from persistence list
2017-07-26 18:19:12,069 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2224
2017-07-26 18:19:12,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:12,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064348000 ms
2017-07-26 18:19:14,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064354000 ms
2017-07-26 18:19:14,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064354000 ms.0 from job set of time 1501064354000 ms
2017-07-26 18:19:14,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:14,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2226 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:14,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2226 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:14,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:14,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:14,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2226 (KafkaRDD[2226] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:14,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2226 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:19:14,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2226_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:19:14,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2226_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:14,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2226 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:14,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2226 (KafkaRDD[2226] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:14,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2226.0 with 2 tasks
2017-07-26 18:19:14,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2226.0 (TID 4452, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:14,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2226.0 (TID 4453, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:14,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2226.0 (TID 4452)
2017-07-26 18:19:14,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2226.0 (TID 4453)
2017-07-26 18:19:14,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:14,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:14,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2226.0 (TID 4453). 635 bytes result sent to driver
2017-07-26 18:19:14,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2226.0 (TID 4452). 714 bytes result sent to driver
2017-07-26 18:19:14,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2226.0 (TID 4452) in 10 ms on localhost (1/2)
2017-07-26 18:19:14,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2226.0 (TID 4453) in 9 ms on localhost (2/2)
2017-07-26 18:19:14,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2226.0, whose tasks have all completed, from pool 
2017-07-26 18:19:14,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2226 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:19:14,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2226 finished: foreachPartition at streamingProcessTest.scala:48, took 0.029919 s
2017-07-26 18:19:14,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064354000 ms.0 from job set of time 1501064354000 ms
2017-07-26 18:19:14,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1501064354000 ms (execution: 0.060 s)
2017-07-26 18:19:14,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2225 from persistence list
2017-07-26 18:19:14,079 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2225
2017-07-26 18:19:14,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:14,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064350000 ms
2017-07-26 18:19:16,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064356000 ms
2017-07-26 18:19:16,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064356000 ms.0 from job set of time 1501064356000 ms
2017-07-26 18:19:16,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2227 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2227 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:16,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2227 (KafkaRDD[2227] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:16,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2227 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:19:16,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2227_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:19:16,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2227_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2227 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2227 (KafkaRDD[2227] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2227.0 with 2 tasks
2017-07-26 18:19:16,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2227.0 (TID 4454, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:16,060 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2227.0 (TID 4455, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:16,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2227.0 (TID 4454)
2017-07-26 18:19:16,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2227.0 (TID 4455)
2017-07-26 18:19:16,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:16,063 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:16,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2227.0 (TID 4455). 635 bytes result sent to driver
2017-07-26 18:19:16,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2227.0 (TID 4454). 635 bytes result sent to driver
2017-07-26 18:19:16,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2227.0 (TID 4455) in 8 ms on localhost (1/2)
2017-07-26 18:19:16,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2227.0 (TID 4454) in 10 ms on localhost (2/2)
2017-07-26 18:19:16,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2227.0, whose tasks have all completed, from pool 
2017-07-26 18:19:16,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2227 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:19:16,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2227 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022837 s
2017-07-26 18:19:16,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064356000 ms.0 from job set of time 1501064356000 ms
2017-07-26 18:19:16,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501064356000 ms (execution: 0.053 s)
2017-07-26 18:19:16,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2226 from persistence list
2017-07-26 18:19:16,070 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2226
2017-07-26 18:19:16,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:16,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064352000 ms
2017-07-26 18:19:18,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064358000 ms
2017-07-26 18:19:18,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064358000 ms.0 from job set of time 1501064358000 ms
2017-07-26 18:19:18,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:18,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2228 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:18,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2228 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:18,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2228 (KafkaRDD[2228] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:18,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2228 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:19:18,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2228_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:19:18,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2228_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:18,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2228 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:18,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2228 (KafkaRDD[2228] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:18,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2228.0 with 2 tasks
2017-07-26 18:19:18,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2228.0 (TID 4456, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:18,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2228.0 (TID 4457, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:18,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2228.0 (TID 4456)
2017-07-26 18:19:18,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2228.0 (TID 4457)
2017-07-26 18:19:18,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:18,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:18,081 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2228.0 (TID 4456). 714 bytes result sent to driver
2017-07-26 18:19:18,081 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2228.0 (TID 4457). 714 bytes result sent to driver
2017-07-26 18:19:18,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2228.0 (TID 4456) in 19 ms on localhost (1/2)
2017-07-26 18:19:18,087 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2228.0 (TID 4457) in 17 ms on localhost (2/2)
2017-07-26 18:19:18,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2228.0, whose tasks have all completed, from pool 
2017-07-26 18:19:18,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2228 (foreachPartition at streamingProcessTest.scala:48) finished in 0.021 s
2017-07-26 18:19:18,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2228 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040988 s
2017-07-26 18:19:18,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064358000 ms.0 from job set of time 1501064358000 ms
2017-07-26 18:19:18,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1501064358000 ms (execution: 0.072 s)
2017-07-26 18:19:18,089 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2227 from persistence list
2017-07-26 18:19:18,090 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2227
2017-07-26 18:19:18,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:18,090 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064354000 ms
2017-07-26 18:19:20,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064360000 ms
2017-07-26 18:19:20,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064360000 ms.0 from job set of time 1501064360000 ms
2017-07-26 18:19:20,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:20,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2229 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:20,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2229 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:20,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:20,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:20,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2229 (KafkaRDD[2229] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:20,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2229 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:19:20,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2229_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:19:20,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2229_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:20,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2229 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:20,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2229 (KafkaRDD[2229] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:20,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2229.0 with 2 tasks
2017-07-26 18:19:20,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2229.0 (TID 4458, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:20,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2229.0 (TID 4459, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:20,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2229.0 (TID 4458)
2017-07-26 18:19:20,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2229.0 (TID 4459)
2017-07-26 18:19:20,080 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:20,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:20,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2229.0 (TID 4458). 714 bytes result sent to driver
2017-07-26 18:19:20,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2229.0 (TID 4459). 714 bytes result sent to driver
2017-07-26 18:19:20,089 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2229.0 (TID 4458) in 19 ms on localhost (1/2)
2017-07-26 18:19:20,090 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2229.0 (TID 4459) in 18 ms on localhost (2/2)
2017-07-26 18:19:20,090 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2229.0, whose tasks have all completed, from pool 
2017-07-26 18:19:20,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2229 (foreachPartition at streamingProcessTest.scala:48) finished in 0.020 s
2017-07-26 18:19:20,091 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2229 finished: foreachPartition at streamingProcessTest.scala:48, took 0.042400 s
2017-07-26 18:19:20,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064360000 ms.0 from job set of time 1501064360000 ms
2017-07-26 18:19:20,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1501064360000 ms (execution: 0.074 s)
2017-07-26 18:19:20,093 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2228 from persistence list
2017-07-26 18:19:20,094 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2228
2017-07-26 18:19:20,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:20,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064356000 ms
2017-07-26 18:19:22,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064362000 ms
2017-07-26 18:19:22,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064362000 ms.0 from job set of time 1501064362000 ms
2017-07-26 18:19:22,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2230 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2230 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2230 (KafkaRDD[2230] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:22,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2230 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:19:22,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2230_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:19:22,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2230_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:22,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2230 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:22,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2230 (KafkaRDD[2230] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:22,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2230.0 with 2 tasks
2017-07-26 18:19:22,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2230.0 (TID 4460, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:22,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2230.0 (TID 4461, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:22,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2230.0 (TID 4461)
2017-07-26 18:19:22,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2230.0 (TID 4460)
2017-07-26 18:19:22,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:22,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:22,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2230.0 (TID 4460). 714 bytes result sent to driver
2017-07-26 18:19:22,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2230.0 (TID 4461). 714 bytes result sent to driver
2017-07-26 18:19:22,066 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2230.0 (TID 4461) in 9 ms on localhost (1/2)
2017-07-26 18:19:22,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2230.0 (TID 4460) in 10 ms on localhost (2/2)
2017-07-26 18:19:22,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2230.0, whose tasks have all completed, from pool 
2017-07-26 18:19:22,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2230 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:19:22,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2230 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020684 s
2017-07-26 18:19:22,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064362000 ms.0 from job set of time 1501064362000 ms
2017-07-26 18:19:22,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1501064362000 ms (execution: 0.051 s)
2017-07-26 18:19:22,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2229 from persistence list
2017-07-26 18:19:22,068 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2229
2017-07-26 18:19:22,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:22,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064358000 ms
2017-07-26 18:19:24,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064364000 ms
2017-07-26 18:19:24,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064364000 ms.0 from job set of time 1501064364000 ms
2017-07-26 18:19:24,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:24,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2231 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:24,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2231 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:24,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:24,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:24,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2231 (KafkaRDD[2231] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:24,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2231 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:19:24,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2231_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:19:24,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2231_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:24,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2231 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:24,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2231 (KafkaRDD[2231] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:24,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2231.0 with 2 tasks
2017-07-26 18:19:24,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2231.0 (TID 4462, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:24,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2231.0 (TID 4463, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:24,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2231.0 (TID 4462)
2017-07-26 18:19:24,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2231.0 (TID 4463)
2017-07-26 18:19:24,083 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:24,083 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:24,087 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2231.0 (TID 4463). 714 bytes result sent to driver
2017-07-26 18:19:24,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2231.0 (TID 4462). 714 bytes result sent to driver
2017-07-26 18:19:24,090 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2231.0 (TID 4463) in 16 ms on localhost (1/2)
2017-07-26 18:19:24,090 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2231.0 (TID 4462) in 19 ms on localhost (2/2)
2017-07-26 18:19:24,090 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2231.0, whose tasks have all completed, from pool 
2017-07-26 18:19:24,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2231 (foreachPartition at streamingProcessTest.scala:48) finished in 0.019 s
2017-07-26 18:19:24,091 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2231 finished: foreachPartition at streamingProcessTest.scala:48, took 0.042379 s
2017-07-26 18:19:24,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064364000 ms.0 from job set of time 1501064364000 ms
2017-07-26 18:19:24,091 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.091 s for time 1501064364000 ms (execution: 0.074 s)
2017-07-26 18:19:24,091 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2230 from persistence list
2017-07-26 18:19:24,092 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2230
2017-07-26 18:19:24,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:24,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064360000 ms
2017-07-26 18:19:26,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064366000 ms
2017-07-26 18:19:26,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064366000 ms.0 from job set of time 1501064366000 ms
2017-07-26 18:19:26,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:26,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2232 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:26,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2232 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:26,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:26,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:26,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2232 (KafkaRDD[2232] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:26,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2232 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:19:26,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2232_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:19:26,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2232_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:26,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2232 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:26,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2232 (KafkaRDD[2232] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:26,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2232.0 with 2 tasks
2017-07-26 18:19:26,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2232.0 (TID 4464, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:26,073 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2232.0 (TID 4465, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:26,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2232.0 (TID 4465)
2017-07-26 18:19:26,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2232.0 (TID 4464)
2017-07-26 18:19:26,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:26,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:26,084 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2232.0 (TID 4464). 635 bytes result sent to driver
2017-07-26 18:19:26,084 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2232.0 (TID 4465). 635 bytes result sent to driver
2017-07-26 18:19:26,089 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2232.0 (TID 4465) in 17 ms on localhost (1/2)
2017-07-26 18:19:26,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2232.0 (TID 4464) in 20 ms on localhost (2/2)
2017-07-26 18:19:26,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2232.0, whose tasks have all completed, from pool 
2017-07-26 18:19:26,090 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2232 (foreachPartition at streamingProcessTest.scala:48) finished in 0.021 s
2017-07-26 18:19:26,091 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2232 finished: foreachPartition at streamingProcessTest.scala:48, took 0.042588 s
2017-07-26 18:19:26,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064366000 ms.0 from job set of time 1501064366000 ms
2017-07-26 18:19:26,092 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2231 from persistence list
2017-07-26 18:19:26,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1501064366000 ms (execution: 0.074 s)
2017-07-26 18:19:26,093 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2231
2017-07-26 18:19:26,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:26,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064362000 ms
2017-07-26 18:19:28,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064368000 ms
2017-07-26 18:19:28,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064368000 ms.0 from job set of time 1501064368000 ms
2017-07-26 18:19:28,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:28,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2233 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:28,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2233 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:28,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:28,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:28,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2233 (KafkaRDD[2233] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:28,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2233 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:19:28,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2233_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:19:28,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2233_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:28,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2233 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:28,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2233 (KafkaRDD[2233] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:28,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2233.0 with 2 tasks
2017-07-26 18:19:28,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2233.0 (TID 4466, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:28,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2233.0 (TID 4467, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:28,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2233.0 (TID 4466)
2017-07-26 18:19:28,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2233.0 (TID 4467)
2017-07-26 18:19:28,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:28,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:28,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2233.0 (TID 4467). 635 bytes result sent to driver
2017-07-26 18:19:28,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2233.0 (TID 4466). 635 bytes result sent to driver
2017-07-26 18:19:28,070 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2233.0 (TID 4467) in 9 ms on localhost (1/2)
2017-07-26 18:19:28,071 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2233.0 (TID 4466) in 10 ms on localhost (2/2)
2017-07-26 18:19:28,071 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2233.0, whose tasks have all completed, from pool 
2017-07-26 18:19:28,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2233 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:19:28,071 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2233 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022805 s
2017-07-26 18:19:28,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064368000 ms.0 from job set of time 1501064368000 ms
2017-07-26 18:19:28,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501064368000 ms (execution: 0.054 s)
2017-07-26 18:19:28,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2232 from persistence list
2017-07-26 18:19:28,072 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2232
2017-07-26 18:19:28,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:28,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064364000 ms
2017-07-26 18:19:30,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064370000 ms
2017-07-26 18:19:30,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064370000 ms.0 from job set of time 1501064370000 ms
2017-07-26 18:19:30,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2234 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2234 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:30,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:30,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:30,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2234 (KafkaRDD[2234] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:30,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2234 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:19:30,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2234_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:19:30,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2234_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2234 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2234 (KafkaRDD[2234] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2234.0 with 2 tasks
2017-07-26 18:19:30,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2234.0 (TID 4468, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:30,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2234.0 (TID 4469, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:30,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2234.0 (TID 4469)
2017-07-26 18:19:30,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2234.0 (TID 4468)
2017-07-26 18:19:30,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:30,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:30,079 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2221_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:30,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2234.0 (TID 4469). 708 bytes result sent to driver
2017-07-26 18:19:30,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2234.0 (TID 4468). 708 bytes result sent to driver
2017-07-26 18:19:30,081 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2222_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:30,082 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2234.0 (TID 4469) in 19 ms on localhost (1/2)
2017-07-26 18:19:30,082 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2234.0 (TID 4468) in 20 ms on localhost (2/2)
2017-07-26 18:19:30,082 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2234.0, whose tasks have all completed, from pool 
2017-07-26 18:19:30,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2234 (foreachPartition at streamingProcessTest.scala:48) finished in 0.020 s
2017-07-26 18:19:30,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2234 finished: foreachPartition at streamingProcessTest.scala:48, took 0.037231 s
2017-07-26 18:19:30,083 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2223_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:30,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064370000 ms.0 from job set of time 1501064370000 ms
2017-07-26 18:19:30,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1501064370000 ms (execution: 0.067 s)
2017-07-26 18:19:30,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2233 from persistence list
2017-07-26 18:19:30,084 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2233
2017-07-26 18:19:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:30,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064366000 ms
2017-07-26 18:19:30,085 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2224_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:30,087 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2225_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:30,088 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2226_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:30,090 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2227_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:30,092 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2228_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:30,093 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2229_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:30,095 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2230_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:30,097 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2231_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:30,098 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2232_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:30,100 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2233_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:32,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064372000 ms
2017-07-26 18:19:32,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064372000 ms.0 from job set of time 1501064372000 ms
2017-07-26 18:19:32,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:32,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2235 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:32,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2235 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:32,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:32,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:32,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2235 (KafkaRDD[2235] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:32,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2235 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:19:32,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2235_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:19:32,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2235_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:32,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2235 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:32,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2235 (KafkaRDD[2235] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:32,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2235.0 with 2 tasks
2017-07-26 18:19:32,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2235.0 (TID 4470, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:32,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2235.0 (TID 4471, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:32,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2235.0 (TID 4471)
2017-07-26 18:19:32,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2235.0 (TID 4470)
2017-07-26 18:19:32,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:32,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:32,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2235.0 (TID 4470). 714 bytes result sent to driver
2017-07-26 18:19:32,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2235.0 (TID 4471). 714 bytes result sent to driver
2017-07-26 18:19:32,086 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2235.0 (TID 4470) in 17 ms on localhost (1/2)
2017-07-26 18:19:32,087 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2235.0 (TID 4471) in 16 ms on localhost (2/2)
2017-07-26 18:19:32,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2235.0, whose tasks have all completed, from pool 
2017-07-26 18:19:32,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2235 (foreachPartition at streamingProcessTest.scala:48) finished in 0.019 s
2017-07-26 18:19:32,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2235 finished: foreachPartition at streamingProcessTest.scala:48, took 0.041011 s
2017-07-26 18:19:32,089 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064372000 ms.0 from job set of time 1501064372000 ms
2017-07-26 18:19:32,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.089 s for time 1501064372000 ms (execution: 0.072 s)
2017-07-26 18:19:32,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2234 from persistence list
2017-07-26 18:19:32,091 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2234
2017-07-26 18:19:32,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:32,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064368000 ms
2017-07-26 18:19:34,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064374000 ms
2017-07-26 18:19:34,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064374000 ms.0 from job set of time 1501064374000 ms
2017-07-26 18:19:34,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:34,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2236 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:34,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2236 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:34,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:34,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:34,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2236 (KafkaRDD[2236] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:34,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2236 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:19:34,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2236_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:19:34,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2236_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:34,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2236 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:34,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2236 (KafkaRDD[2236] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:34,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2236.0 with 2 tasks
2017-07-26 18:19:34,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2236.0 (TID 4472, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:34,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2236.0 (TID 4473, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:34,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2236.0 (TID 4473)
2017-07-26 18:19:34,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2236.0 (TID 4472)
2017-07-26 18:19:34,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:34,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:34,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2236.0 (TID 4472). 635 bytes result sent to driver
2017-07-26 18:19:34,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2236.0 (TID 4473). 635 bytes result sent to driver
2017-07-26 18:19:34,064 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2236.0 (TID 4472) in 8 ms on localhost (1/2)
2017-07-26 18:19:34,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2236.0 (TID 4473) in 7 ms on localhost (2/2)
2017-07-26 18:19:34,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2236.0, whose tasks have all completed, from pool 
2017-07-26 18:19:34,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2236 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:19:34,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2236 finished: foreachPartition at streamingProcessTest.scala:48, took 0.017752 s
2017-07-26 18:19:34,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064374000 ms.0 from job set of time 1501064374000 ms
2017-07-26 18:19:34,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1501064374000 ms (execution: 0.048 s)
2017-07-26 18:19:34,065 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2235 from persistence list
2017-07-26 18:19:34,066 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2235
2017-07-26 18:19:34,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:34,066 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064370000 ms
2017-07-26 18:19:36,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064376000 ms
2017-07-26 18:19:36,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064376000 ms.0 from job set of time 1501064376000 ms
2017-07-26 18:19:36,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:36,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2237 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:36,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2237 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:36,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:36,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:36,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2237 (KafkaRDD[2237] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:36,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2237 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:19:36,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2237_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:19:36,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2237_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:36,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2237 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:36,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2237 (KafkaRDD[2237] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:36,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2237.0 with 2 tasks
2017-07-26 18:19:36,064 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2237.0 (TID 4474, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:36,064 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2237.0 (TID 4475, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:36,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2237.0 (TID 4474)
2017-07-26 18:19:36,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2237.0 (TID 4475)
2017-07-26 18:19:36,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:36,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:36,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2237.0 (TID 4474). 635 bytes result sent to driver
2017-07-26 18:19:36,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2237.0 (TID 4475). 635 bytes result sent to driver
2017-07-26 18:19:36,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2237.0 (TID 4474) in 9 ms on localhost (1/2)
2017-07-26 18:19:36,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2237.0 (TID 4475) in 8 ms on localhost (2/2)
2017-07-26 18:19:36,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2237.0, whose tasks have all completed, from pool 
2017-07-26 18:19:36,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2237 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:19:36,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2237 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026829 s
2017-07-26 18:19:36,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064376000 ms.0 from job set of time 1501064376000 ms
2017-07-26 18:19:36,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1501064376000 ms (execution: 0.057 s)
2017-07-26 18:19:36,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2236 from persistence list
2017-07-26 18:19:36,075 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2236
2017-07-26 18:19:36,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:36,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064372000 ms
2017-07-26 18:19:38,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064378000 ms
2017-07-26 18:19:38,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064378000 ms.0 from job set of time 1501064378000 ms
2017-07-26 18:19:38,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:38,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2238 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:38,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2238 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:38,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:38,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:38,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2238 (KafkaRDD[2238] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:38,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2238 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:19:38,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2238_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:19:38,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2238_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:38,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2238 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:38,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2238 (KafkaRDD[2238] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:38,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2238.0 with 2 tasks
2017-07-26 18:19:38,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2238.0 (TID 4476, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:38,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2238.0 (TID 4477, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:38,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2238.0 (TID 4477)
2017-07-26 18:19:38,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2238.0 (TID 4476)
2017-07-26 18:19:38,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:38,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:38,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2238.0 (TID 4476). 635 bytes result sent to driver
2017-07-26 18:19:38,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2238.0 (TID 4477). 714 bytes result sent to driver
2017-07-26 18:19:38,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2238.0 (TID 4476) in 8 ms on localhost (1/2)
2017-07-26 18:19:38,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2238.0 (TID 4477) in 8 ms on localhost (2/2)
2017-07-26 18:19:38,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2238.0, whose tasks have all completed, from pool 
2017-07-26 18:19:38,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2238 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:19:38,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2238 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020969 s
2017-07-26 18:19:38,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064378000 ms.0 from job set of time 1501064378000 ms
2017-07-26 18:19:38,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501064378000 ms (execution: 0.048 s)
2017-07-26 18:19:38,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2237 from persistence list
2017-07-26 18:19:38,067 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2237
2017-07-26 18:19:38,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:38,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064374000 ms
2017-07-26 18:19:40,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064380000 ms
2017-07-26 18:19:40,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064380000 ms.0 from job set of time 1501064380000 ms
2017-07-26 18:19:40,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2239 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2239 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:40,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:40,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2239 (KafkaRDD[2239] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:40,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2239 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:19:40,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2239_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:19:40,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2239_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:40,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2239 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:40,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2239 (KafkaRDD[2239] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:40,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2239.0 with 2 tasks
2017-07-26 18:19:40,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2239.0 (TID 4478, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:40,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2239.0 (TID 4479, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:40,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2239.0 (TID 4478)
2017-07-26 18:19:40,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2239.0 (TID 4479)
2017-07-26 18:19:40,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:40,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:40,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2239.0 (TID 4479). 635 bytes result sent to driver
2017-07-26 18:19:40,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2239.0 (TID 4478). 635 bytes result sent to driver
2017-07-26 18:19:40,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2239.0 (TID 4479) in 9 ms on localhost (1/2)
2017-07-26 18:19:40,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2239.0 (TID 4478) in 10 ms on localhost (2/2)
2017-07-26 18:19:40,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2239.0, whose tasks have all completed, from pool 
2017-07-26 18:19:40,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2239 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:19:40,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2239 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022173 s
2017-07-26 18:19:40,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064380000 ms.0 from job set of time 1501064380000 ms
2017-07-26 18:19:40,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501064380000 ms (execution: 0.054 s)
2017-07-26 18:19:40,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2238 from persistence list
2017-07-26 18:19:40,071 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2238
2017-07-26 18:19:40,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:40,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064376000 ms
2017-07-26 18:19:42,196 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064382000 ms
2017-07-26 18:19:42,197 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064382000 ms.0 from job set of time 1501064382000 ms
2017-07-26 18:19:42,228 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:42,229 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2240 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:42,229 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2240 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:42,229 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:42,229 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:42,230 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2240 (KafkaRDD[2240] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:42,237 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2240 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:19:42,240 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2240_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:19:42,241 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2240_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:42,241 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2240 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:42,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2240 (KafkaRDD[2240] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:42,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2240.0 with 2 tasks
2017-07-26 18:19:42,243 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2240.0 (TID 4480, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:42,243 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2240.0 (TID 4481, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:42,244 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2240.0 (TID 4480)
2017-07-26 18:19:42,244 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2240.0 (TID 4481)
2017-07-26 18:19:42,246 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:42,246 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:42,247 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2240.0 (TID 4481). 635 bytes result sent to driver
2017-07-26 18:19:42,247 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2240.0 (TID 4480). 635 bytes result sent to driver
2017-07-26 18:19:42,249 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2240.0 (TID 4481) in 6 ms on localhost (1/2)
2017-07-26 18:19:42,250 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2240.0 (TID 4480) in 8 ms on localhost (2/2)
2017-07-26 18:19:42,250 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2240.0, whose tasks have all completed, from pool 
2017-07-26 18:19:42,250 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2240 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:19:42,250 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2240 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022244 s
2017-07-26 18:19:42,251 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064382000 ms.0 from job set of time 1501064382000 ms
2017-07-26 18:19:42,251 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.251 s for time 1501064382000 ms (execution: 0.054 s)
2017-07-26 18:19:42,251 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2239 from persistence list
2017-07-26 18:19:42,251 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2239
2017-07-26 18:19:42,251 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:42,251 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064378000 ms
2017-07-26 18:19:44,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064384000 ms
2017-07-26 18:19:44,025 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064384000 ms.0 from job set of time 1501064384000 ms
2017-07-26 18:19:44,057 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:44,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2241 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:44,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2241 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:44,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:44,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:44,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2241 (KafkaRDD[2241] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:44,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2241 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:19:44,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2241_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:19:44,080 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2241_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:44,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2241 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:44,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2241 (KafkaRDD[2241] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:44,084 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2241.0 with 2 tasks
2017-07-26 18:19:44,090 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2241.0 (TID 4482, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:44,091 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2241.0 (TID 4483, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:44,092 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2241.0 (TID 4482)
2017-07-26 18:19:44,092 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2241.0 (TID 4483)
2017-07-26 18:19:44,095 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:44,095 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:44,100 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2241.0 (TID 4483). 714 bytes result sent to driver
2017-07-26 18:19:44,102 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2241.0 (TID 4482). 714 bytes result sent to driver
2017-07-26 18:19:44,106 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2241.0 (TID 4483) in 15 ms on localhost (1/2)
2017-07-26 18:19:44,107 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2241.0 (TID 4482) in 21 ms on localhost (2/2)
2017-07-26 18:19:44,107 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2241.0, whose tasks have all completed, from pool 
2017-07-26 18:19:44,107 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2241 (foreachPartition at streamingProcessTest.scala:48) finished in 0.021 s
2017-07-26 18:19:44,108 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2241 finished: foreachPartition at streamingProcessTest.scala:48, took 0.050602 s
2017-07-26 18:19:44,108 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064384000 ms.0 from job set of time 1501064384000 ms
2017-07-26 18:19:44,109 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2240 from persistence list
2017-07-26 18:19:44,110 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.108 s for time 1501064384000 ms (execution: 0.083 s)
2017-07-26 18:19:44,110 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2240
2017-07-26 18:19:44,111 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:44,111 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064380000 ms
2017-07-26 18:19:46,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064386000 ms
2017-07-26 18:19:46,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064386000 ms.0 from job set of time 1501064386000 ms
2017-07-26 18:19:46,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2242 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2242 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:46,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:46,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2242 (KafkaRDD[2242] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:46,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2242 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:19:46,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2242_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:19:46,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2242_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:46,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2242 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:46,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2242 (KafkaRDD[2242] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:46,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2242.0 with 2 tasks
2017-07-26 18:19:46,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2242.0 (TID 4484, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:46,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2242.0 (TID 4485, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:46,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2242.0 (TID 4484)
2017-07-26 18:19:46,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2242.0 (TID 4485)
2017-07-26 18:19:46,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:46,063 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:46,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2242.0 (TID 4484). 714 bytes result sent to driver
2017-07-26 18:19:46,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2242.0 (TID 4485). 714 bytes result sent to driver
2017-07-26 18:19:46,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2242.0 (TID 4484) in 7 ms on localhost (1/2)
2017-07-26 18:19:46,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2242.0 (TID 4485) in 7 ms on localhost (2/2)
2017-07-26 18:19:46,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2242.0, whose tasks have all completed, from pool 
2017-07-26 18:19:46,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2242 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:19:46,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2242 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018404 s
2017-07-26 18:19:46,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064386000 ms.0 from job set of time 1501064386000 ms
2017-07-26 18:19:46,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501064386000 ms (execution: 0.052 s)
2017-07-26 18:19:46,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2241 from persistence list
2017-07-26 18:19:46,069 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2241
2017-07-26 18:19:46,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:46,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064382000 ms
2017-07-26 18:19:48,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064388000 ms
2017-07-26 18:19:48,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064388000 ms.0 from job set of time 1501064388000 ms
2017-07-26 18:19:48,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:48,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2243 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:48,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2243 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:48,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:48,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:48,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2243 (KafkaRDD[2243] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:48,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2243 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:19:48,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2243_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:19:48,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2243_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:48,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2243 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:48,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2243 (KafkaRDD[2243] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:48,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2243.0 with 2 tasks
2017-07-26 18:19:48,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2243.0 (TID 4486, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:48,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2243.0 (TID 4487, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:48,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2243.0 (TID 4486)
2017-07-26 18:19:48,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2243.0 (TID 4487)
2017-07-26 18:19:48,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:48,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:48,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2243.0 (TID 4487). 714 bytes result sent to driver
2017-07-26 18:19:48,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2243.0 (TID 4486). 714 bytes result sent to driver
2017-07-26 18:19:48,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2243.0 (TID 4487) in 11 ms on localhost (1/2)
2017-07-26 18:19:48,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2243.0 (TID 4486) in 12 ms on localhost (2/2)
2017-07-26 18:19:48,079 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2243.0, whose tasks have all completed, from pool 
2017-07-26 18:19:48,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2243 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:19:48,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2243 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028231 s
2017-07-26 18:19:48,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064388000 ms.0 from job set of time 1501064388000 ms
2017-07-26 18:19:48,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.080 s for time 1501064388000 ms (execution: 0.064 s)
2017-07-26 18:19:48,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2242 from persistence list
2017-07-26 18:19:48,080 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2242
2017-07-26 18:19:48,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:48,081 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064384000 ms
2017-07-26 18:19:50,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064390000 ms
2017-07-26 18:19:50,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064390000 ms.0 from job set of time 1501064390000 ms
2017-07-26 18:19:50,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:50,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2244 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:50,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2244 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:50,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:50,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2244 (KafkaRDD[2244] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:50,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2244 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:19:50,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2244_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:19:50,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2244_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:50,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2244 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:50,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2244 (KafkaRDD[2244] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:50,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2244.0 with 2 tasks
2017-07-26 18:19:50,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2244.0 (TID 4488, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:50,075 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2244.0 (TID 4489, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:50,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2244.0 (TID 4489)
2017-07-26 18:19:50,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2244.0 (TID 4488)
2017-07-26 18:19:50,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:50,081 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:50,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2244.0 (TID 4488). 635 bytes result sent to driver
2017-07-26 18:19:50,086 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2244.0 (TID 4489). 714 bytes result sent to driver
2017-07-26 18:19:50,090 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2244.0 (TID 4488) in 17 ms on localhost (1/2)
2017-07-26 18:19:50,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2244.0 (TID 4489) in 16 ms on localhost (2/2)
2017-07-26 18:19:50,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2244.0, whose tasks have all completed, from pool 
2017-07-26 18:19:50,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2244 (foreachPartition at streamingProcessTest.scala:48) finished in 0.019 s
2017-07-26 18:19:50,092 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2244 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040648 s
2017-07-26 18:19:50,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064390000 ms.0 from job set of time 1501064390000 ms
2017-07-26 18:19:50,093 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2243 from persistence list
2017-07-26 18:19:50,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1501064390000 ms (execution: 0.075 s)
2017-07-26 18:19:50,094 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2243
2017-07-26 18:19:50,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:50,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064386000 ms
2017-07-26 18:19:52,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064392000 ms
2017-07-26 18:19:52,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064392000 ms.0 from job set of time 1501064392000 ms
2017-07-26 18:19:52,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:52,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2245 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:52,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2245 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:52,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:52,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:52,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2245 (KafkaRDD[2245] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:52,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2245 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:19:52,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2245_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:19:52,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2245_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:52,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2245 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:52,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2245 (KafkaRDD[2245] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:52,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2245.0 with 2 tasks
2017-07-26 18:19:52,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2245.0 (TID 4490, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:52,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2245.0 (TID 4491, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:52,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2245.0 (TID 4491)
2017-07-26 18:19:52,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2245.0 (TID 4490)
2017-07-26 18:19:52,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:52,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:52,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2245.0 (TID 4490). 635 bytes result sent to driver
2017-07-26 18:19:52,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2245.0 (TID 4491). 635 bytes result sent to driver
2017-07-26 18:19:52,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2245.0 (TID 4490) in 13 ms on localhost (1/2)
2017-07-26 18:19:52,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2245.0 (TID 4491) in 12 ms on localhost (2/2)
2017-07-26 18:19:52,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2245.0, whose tasks have all completed, from pool 
2017-07-26 18:19:52,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2245 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:19:52,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2245 finished: foreachPartition at streamingProcessTest.scala:48, took 0.033793 s
2017-07-26 18:19:52,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064392000 ms.0 from job set of time 1501064392000 ms
2017-07-26 18:19:52,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1501064392000 ms (execution: 0.065 s)
2017-07-26 18:19:52,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2244 from persistence list
2017-07-26 18:19:52,084 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2244
2017-07-26 18:19:52,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:52,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064388000 ms
2017-07-26 18:19:54,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064394000 ms
2017-07-26 18:19:54,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064394000 ms.0 from job set of time 1501064394000 ms
2017-07-26 18:19:54,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:54,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2246 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:54,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2246 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:54,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:54,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:54,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2246 (KafkaRDD[2246] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:54,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2246 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:19:54,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2246_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:19:54,060 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2246_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:54,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2246 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:54,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2246 (KafkaRDD[2246] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:54,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2246.0 with 2 tasks
2017-07-26 18:19:54,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2246.0 (TID 4492, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:54,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2246.0 (TID 4493, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:54,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2246.0 (TID 4492)
2017-07-26 18:19:54,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2246.0 (TID 4493)
2017-07-26 18:19:54,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:54,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:54,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2246.0 (TID 4492). 635 bytes result sent to driver
2017-07-26 18:19:54,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2246.0 (TID 4493). 635 bytes result sent to driver
2017-07-26 18:19:54,069 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2246.0 (TID 4492) in 8 ms on localhost (1/2)
2017-07-26 18:19:54,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2246.0 (TID 4493) in 7 ms on localhost (2/2)
2017-07-26 18:19:54,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2246 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:19:54,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2246.0, whose tasks have all completed, from pool 
2017-07-26 18:19:54,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2246 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023916 s
2017-07-26 18:19:54,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064394000 ms.0 from job set of time 1501064394000 ms
2017-07-26 18:19:54,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1501064394000 ms (execution: 0.049 s)
2017-07-26 18:19:54,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2245 from persistence list
2017-07-26 18:19:54,071 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2245
2017-07-26 18:19:54,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:54,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064390000 ms
2017-07-26 18:19:56,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064396000 ms
2017-07-26 18:19:56,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064396000 ms.0 from job set of time 1501064396000 ms
2017-07-26 18:19:56,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:56,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2247 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:56,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2247 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:56,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:56,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:56,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2247 (KafkaRDD[2247] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:56,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2247 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:19:56,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2247_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:19:56,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2247_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:56,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2247 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:56,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2247 (KafkaRDD[2247] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:56,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2247.0 with 2 tasks
2017-07-26 18:19:56,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2247.0 (TID 4494, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:56,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2247.0 (TID 4495, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:56,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2247.0 (TID 4494)
2017-07-26 18:19:56,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2247.0 (TID 4495)
2017-07-26 18:19:56,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:56,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:56,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2247.0 (TID 4495). 714 bytes result sent to driver
2017-07-26 18:19:56,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2247.0 (TID 4494). 714 bytes result sent to driver
2017-07-26 18:19:56,067 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2247.0 (TID 4495) in 9 ms on localhost (1/2)
2017-07-26 18:19:56,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2247.0 (TID 4494) in 10 ms on localhost (2/2)
2017-07-26 18:19:56,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2247.0, whose tasks have all completed, from pool 
2017-07-26 18:19:56,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2247 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:19:56,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2247 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023681 s
2017-07-26 18:19:56,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064396000 ms.0 from job set of time 1501064396000 ms
2017-07-26 18:19:56,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501064396000 ms (execution: 0.052 s)
2017-07-26 18:19:56,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2246 from persistence list
2017-07-26 18:19:56,069 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2246
2017-07-26 18:19:56,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:56,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064392000 ms
2017-07-26 18:19:58,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064398000 ms
2017-07-26 18:19:58,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064398000 ms.0 from job set of time 1501064398000 ms
2017-07-26 18:19:58,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:19:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2248 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:19:58,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2248 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:19:58,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:19:58,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:19:58,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2248 (KafkaRDD[2248] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:19:58,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2248 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:19:58,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2248_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.3 MB)
2017-07-26 18:19:58,091 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2248_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:58,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2248 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:19:58,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2248 (KafkaRDD[2248] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:19:58,093 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2242_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:58,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2248.0 with 2 tasks
2017-07-26 18:19:58,096 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2248.0 (TID 4496, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:19:58,097 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2248.0 (TID 4497, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:19:58,098 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2248.0 (TID 4496)
2017-07-26 18:19:58,098 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2248.0 (TID 4497)
2017-07-26 18:19:58,100 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2234_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:58,107 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:19:58,107 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:19:58,108 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2235_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:58,109 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2248.0 (TID 4497). 714 bytes result sent to driver
2017-07-26 18:19:58,109 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2248.0 (TID 4496). 635 bytes result sent to driver
2017-07-26 18:19:58,111 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2236_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:19:58,111 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2248.0 (TID 4496) in 17 ms on localhost (1/2)
2017-07-26 18:19:58,112 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2248.0 (TID 4497) in 15 ms on localhost (2/2)
2017-07-26 18:19:58,112 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2248.0, whose tasks have all completed, from pool 
2017-07-26 18:19:58,112 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2248 (foreachPartition at streamingProcessTest.scala:48) finished in 0.018 s
2017-07-26 18:19:58,112 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2248 finished: foreachPartition at streamingProcessTest.scala:48, took 0.064141 s
2017-07-26 18:19:58,113 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064398000 ms.0 from job set of time 1501064398000 ms
2017-07-26 18:19:58,113 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.113 s for time 1501064398000 ms (execution: 0.097 s)
2017-07-26 18:19:58,113 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2247 from persistence list
2017-07-26 18:19:58,113 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2237_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:58,114 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2247
2017-07-26 18:19:58,114 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:19:58,114 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064394000 ms
2017-07-26 18:19:58,115 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2238_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:58,117 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2239_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:58,119 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2240_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:58,121 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2241_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:58,123 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2243_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:19:58,124 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2244_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:58,126 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2245_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:58,128 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2246_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:19:58,130 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2247_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:00,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064400000 ms
2017-07-26 18:20:00,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064400000 ms.0 from job set of time 1501064400000 ms
2017-07-26 18:20:00,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2249 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2249 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2249 (KafkaRDD[2249] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:00,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2249 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:20:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2249_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:20:00,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2249_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2249 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2249 (KafkaRDD[2249] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2249.0 with 2 tasks
2017-07-26 18:20:00,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2249.0 (TID 4498, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:00,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2249.0 (TID 4499, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:00,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2249.0 (TID 4498)
2017-07-26 18:20:00,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2249.0 (TID 4499)
2017-07-26 18:20:00,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:00,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:00,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2249.0 (TID 4499). 714 bytes result sent to driver
2017-07-26 18:20:00,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2249.0 (TID 4498). 714 bytes result sent to driver
2017-07-26 18:20:00,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2249.0 (TID 4499) in 10 ms on localhost (1/2)
2017-07-26 18:20:00,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2249.0 (TID 4498) in 11 ms on localhost (2/2)
2017-07-26 18:20:00,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2249.0, whose tasks have all completed, from pool 
2017-07-26 18:20:00,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2249 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:20:00,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2249 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026792 s
2017-07-26 18:20:00,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064400000 ms.0 from job set of time 1501064400000 ms
2017-07-26 18:20:00,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2248 from persistence list
2017-07-26 18:20:00,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501064400000 ms (execution: 0.058 s)
2017-07-26 18:20:00,077 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2248
2017-07-26 18:20:00,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:00,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064396000 ms
2017-07-26 18:20:02,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064402000 ms
2017-07-26 18:20:02,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064402000 ms.0 from job set of time 1501064402000 ms
2017-07-26 18:20:02,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:02,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2250 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2250 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:02,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:02,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2250 (KafkaRDD[2250] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:02,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2250 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:20:02,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2250_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:20:02,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2250_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:02,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2250 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:02,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2250 (KafkaRDD[2250] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:02,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2250.0 with 2 tasks
2017-07-26 18:20:02,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2250.0 (TID 4500, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:02,071 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2250.0 (TID 4501, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:02,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2250.0 (TID 4501)
2017-07-26 18:20:02,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2250.0 (TID 4500)
2017-07-26 18:20:02,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:02,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:02,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2250.0 (TID 4501). 635 bytes result sent to driver
2017-07-26 18:20:02,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2250.0 (TID 4500). 635 bytes result sent to driver
2017-07-26 18:20:02,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2250.0 (TID 4501) in 10 ms on localhost (1/2)
2017-07-26 18:20:02,081 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2250.0 (TID 4500) in 12 ms on localhost (2/2)
2017-07-26 18:20:02,081 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2250.0, whose tasks have all completed, from pool 
2017-07-26 18:20:02,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2250 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:20:02,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2250 finished: foreachPartition at streamingProcessTest.scala:48, took 0.033214 s
2017-07-26 18:20:02,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064402000 ms.0 from job set of time 1501064402000 ms
2017-07-26 18:20:02,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1501064402000 ms (execution: 0.065 s)
2017-07-26 18:20:02,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2249 from persistence list
2017-07-26 18:20:02,083 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2249
2017-07-26 18:20:02,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:02,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064398000 ms
2017-07-26 18:20:04,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064404000 ms
2017-07-26 18:20:04,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064404000 ms.0 from job set of time 1501064404000 ms
2017-07-26 18:20:04,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2251 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2251 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2251 (KafkaRDD[2251] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:04,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2251 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:20:04,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2251_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:20:04,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2251_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:04,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2251 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:04,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2251 (KafkaRDD[2251] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:04,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2251.0 with 2 tasks
2017-07-26 18:20:04,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2251.0 (TID 4502, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:04,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2251.0 (TID 4503, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:04,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2251.0 (TID 4503)
2017-07-26 18:20:04,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2251.0 (TID 4502)
2017-07-26 18:20:04,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:04,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:04,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2251.0 (TID 4503). 714 bytes result sent to driver
2017-07-26 18:20:04,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2251.0 (TID 4502). 714 bytes result sent to driver
2017-07-26 18:20:04,071 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2251.0 (TID 4502) in 10 ms on localhost (1/2)
2017-07-26 18:20:04,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2251.0 (TID 4503) in 9 ms on localhost (2/2)
2017-07-26 18:20:04,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2251.0, whose tasks have all completed, from pool 
2017-07-26 18:20:04,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2251 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:20:04,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2251 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023726 s
2017-07-26 18:20:04,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064404000 ms.0 from job set of time 1501064404000 ms
2017-07-26 18:20:04,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501064404000 ms (execution: 0.054 s)
2017-07-26 18:20:04,073 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2250 from persistence list
2017-07-26 18:20:04,073 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2250
2017-07-26 18:20:04,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:04,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064400000 ms
2017-07-26 18:20:06,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064406000 ms
2017-07-26 18:20:06,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064406000 ms.0 from job set of time 1501064406000 ms
2017-07-26 18:20:06,035 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:06,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2252 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:06,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2252 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:06,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:06,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:06,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2252 (KafkaRDD[2252] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:06,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2252 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:20:06,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2252_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:20:06,043 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2252_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:06,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2252 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:06,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2252 (KafkaRDD[2252] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:06,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2252.0 with 2 tasks
2017-07-26 18:20:06,046 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2252.0 (TID 4504, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:06,046 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2252.0 (TID 4505, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:06,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2252.0 (TID 4505)
2017-07-26 18:20:06,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2252.0 (TID 4504)
2017-07-26 18:20:06,049 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:06,049 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:06,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2252.0 (TID 4505). 635 bytes result sent to driver
2017-07-26 18:20:06,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2252.0 (TID 4504). 635 bytes result sent to driver
2017-07-26 18:20:06,052 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2252.0 (TID 4505) in 6 ms on localhost (1/2)
2017-07-26 18:20:06,053 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2252.0 (TID 4504) in 7 ms on localhost (2/2)
2017-07-26 18:20:06,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2252 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:20:06,053 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2252.0, whose tasks have all completed, from pool 
2017-07-26 18:20:06,053 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2252 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018158 s
2017-07-26 18:20:06,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064406000 ms.0 from job set of time 1501064406000 ms
2017-07-26 18:20:06,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.054 s for time 1501064406000 ms (execution: 0.036 s)
2017-07-26 18:20:06,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2251 from persistence list
2017-07-26 18:20:06,054 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2251
2017-07-26 18:20:06,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:06,055 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064402000 ms
2017-07-26 18:20:08,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064408000 ms
2017-07-26 18:20:08,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064408000 ms.0 from job set of time 1501064408000 ms
2017-07-26 18:20:08,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2253 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2253 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:08,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2253 (KafkaRDD[2253] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:08,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2253 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:20:08,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2253_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:20:08,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2253_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:08,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2253 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:08,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2253 (KafkaRDD[2253] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:08,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2253.0 with 2 tasks
2017-07-26 18:20:08,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2253.0 (TID 4506, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:08,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2253.0 (TID 4507, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:08,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2253.0 (TID 4507)
2017-07-26 18:20:08,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2253.0 (TID 4506)
2017-07-26 18:20:08,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:08,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:08,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2253.0 (TID 4506). 714 bytes result sent to driver
2017-07-26 18:20:08,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2253.0 (TID 4507). 714 bytes result sent to driver
2017-07-26 18:20:08,078 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2253.0 (TID 4507) in 9 ms on localhost (1/2)
2017-07-26 18:20:08,078 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2253.0 (TID 4506) in 10 ms on localhost (2/2)
2017-07-26 18:20:08,079 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2253.0, whose tasks have all completed, from pool 
2017-07-26 18:20:08,079 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2253 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:20:08,079 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2253 finished: foreachPartition at streamingProcessTest.scala:48, took 0.031376 s
2017-07-26 18:20:08,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064408000 ms.0 from job set of time 1501064408000 ms
2017-07-26 18:20:08,080 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.079 s for time 1501064408000 ms (execution: 0.063 s)
2017-07-26 18:20:08,080 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2252 from persistence list
2017-07-26 18:20:08,080 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2252
2017-07-26 18:20:08,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:08,080 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064404000 ms
2017-07-26 18:20:10,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064410000 ms
2017-07-26 18:20:10,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064410000 ms.0 from job set of time 1501064410000 ms
2017-07-26 18:20:10,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2254 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2254 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:10,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:10,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:10,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2254 (KafkaRDD[2254] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:10,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2254 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:20:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2254_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:20:10,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2254_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:10,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2254 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2254 (KafkaRDD[2254] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2254.0 with 2 tasks
2017-07-26 18:20:10,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2254.0 (TID 4508, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:10,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2254.0 (TID 4509, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:10,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2254.0 (TID 4508)
2017-07-26 18:20:10,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2254.0 (TID 4509)
2017-07-26 18:20:10,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:10,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:10,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2254.0 (TID 4509). 714 bytes result sent to driver
2017-07-26 18:20:10,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2254.0 (TID 4508). 714 bytes result sent to driver
2017-07-26 18:20:10,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2254.0 (TID 4508) in 9 ms on localhost (1/2)
2017-07-26 18:20:10,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2254.0 (TID 4509) in 9 ms on localhost (2/2)
2017-07-26 18:20:10,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2254.0, whose tasks have all completed, from pool 
2017-07-26 18:20:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2254 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:20:10,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2254 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022605 s
2017-07-26 18:20:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064410000 ms.0 from job set of time 1501064410000 ms
2017-07-26 18:20:10,075 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2253 from persistence list
2017-07-26 18:20:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501064410000 ms (execution: 0.058 s)
2017-07-26 18:20:10,076 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2253
2017-07-26 18:20:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064406000 ms
2017-07-26 18:20:12,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064412000 ms
2017-07-26 18:20:12,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064412000 ms.0 from job set of time 1501064412000 ms
2017-07-26 18:20:12,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:12,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2255 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:12,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2255 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:12,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:12,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:12,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2255 (KafkaRDD[2255] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:12,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2255 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:20:12,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2255_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:20:12,036 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2255_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:12,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2255 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:12,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2255 (KafkaRDD[2255] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:12,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2255.0 with 2 tasks
2017-07-26 18:20:12,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2255.0 (TID 4510, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:12,039 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2255.0 (TID 4511, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:12,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2255.0 (TID 4511)
2017-07-26 18:20:12,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2255.0 (TID 4510)
2017-07-26 18:20:12,041 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:12,041 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:12,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2255.0 (TID 4511). 635 bytes result sent to driver
2017-07-26 18:20:12,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2255.0 (TID 4510). 635 bytes result sent to driver
2017-07-26 18:20:12,044 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2255.0 (TID 4511) in 6 ms on localhost (1/2)
2017-07-26 18:20:12,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2255.0 (TID 4510) in 6 ms on localhost (2/2)
2017-07-26 18:20:12,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2255.0, whose tasks have all completed, from pool 
2017-07-26 18:20:12,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2255 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:20:12,045 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2255 finished: foreachPartition at streamingProcessTest.scala:48, took 0.015173 s
2017-07-26 18:20:12,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064412000 ms.0 from job set of time 1501064412000 ms
2017-07-26 18:20:12,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.045 s for time 1501064412000 ms (execution: 0.032 s)
2017-07-26 18:20:12,045 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2254 from persistence list
2017-07-26 18:20:12,046 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2254
2017-07-26 18:20:12,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:12,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064408000 ms
2017-07-26 18:20:14,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064414000 ms
2017-07-26 18:20:14,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064414000 ms.0 from job set of time 1501064414000 ms
2017-07-26 18:20:14,021 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:14,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2256 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:14,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2256 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:14,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:14,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:14,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2256 (KafkaRDD[2256] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:14,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2256 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:20:14,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2256_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:20:14,027 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2256_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:14,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2256 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:14,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2256 (KafkaRDD[2256] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:14,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2256.0 with 2 tasks
2017-07-26 18:20:14,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2256.0 (TID 4512, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:14,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2256.0 (TID 4513, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:14,030 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2256.0 (TID 4513)
2017-07-26 18:20:14,030 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2256.0 (TID 4512)
2017-07-26 18:20:14,032 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:14,032 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:14,034 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2256.0 (TID 4513). 635 bytes result sent to driver
2017-07-26 18:20:14,034 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2256.0 (TID 4512). 635 bytes result sent to driver
2017-07-26 18:20:14,035 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2256.0 (TID 4512) in 6 ms on localhost (1/2)
2017-07-26 18:20:14,035 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2256.0 (TID 4513) in 5 ms on localhost (2/2)
2017-07-26 18:20:14,035 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2256.0, whose tasks have all completed, from pool 
2017-07-26 18:20:14,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2256 (foreachPartition at streamingProcessTest.scala:48) finished in 0.006 s
2017-07-26 18:20:14,036 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2256 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014530 s
2017-07-26 18:20:14,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064414000 ms.0 from job set of time 1501064414000 ms
2017-07-26 18:20:14,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.036 s for time 1501064414000 ms (execution: 0.024 s)
2017-07-26 18:20:14,036 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2255 from persistence list
2017-07-26 18:20:14,037 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2255
2017-07-26 18:20:14,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:14,037 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064410000 ms
2017-07-26 18:20:16,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064416000 ms
2017-07-26 18:20:16,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064416000 ms.0 from job set of time 1501064416000 ms
2017-07-26 18:20:16,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2257 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2257 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:16,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2257 (KafkaRDD[2257] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:16,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2257 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:20:16,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2257_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:20:16,061 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2257_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:16,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2257 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:16,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2257 (KafkaRDD[2257] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:16,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2257.0 with 2 tasks
2017-07-26 18:20:16,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2257.0 (TID 4514, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:16,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2257.0 (TID 4515, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:16,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2257.0 (TID 4515)
2017-07-26 18:20:16,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2257.0 (TID 4514)
2017-07-26 18:20:16,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:16,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:16,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2257.0 (TID 4514). 714 bytes result sent to driver
2017-07-26 18:20:16,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2257.0 (TID 4515). 714 bytes result sent to driver
2017-07-26 18:20:16,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2257.0 (TID 4514) in 11 ms on localhost (1/2)
2017-07-26 18:20:16,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2257.0 (TID 4515) in 10 ms on localhost (2/2)
2017-07-26 18:20:16,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2257.0, whose tasks have all completed, from pool 
2017-07-26 18:20:16,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2257 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:20:16,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2257 finished: foreachPartition at streamingProcessTest.scala:48, took 0.026784 s
2017-07-26 18:20:16,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064416000 ms.0 from job set of time 1501064416000 ms
2017-07-26 18:20:16,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501064416000 ms (execution: 0.059 s)
2017-07-26 18:20:16,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2256 from persistence list
2017-07-26 18:20:16,077 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2256
2017-07-26 18:20:16,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:16,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064412000 ms
2017-07-26 18:20:18,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064418000 ms
2017-07-26 18:20:18,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064418000 ms.0 from job set of time 1501064418000 ms
2017-07-26 18:20:18,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:18,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2258 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:18,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2258 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:18,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:18,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:18,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2258 (KafkaRDD[2258] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:18,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2258 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:20:18,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2258_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:20:18,061 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2258_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:18,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2258 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:18,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2258 (KafkaRDD[2258] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:18,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2258.0 with 2 tasks
2017-07-26 18:20:18,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2258.0 (TID 4516, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:18,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2258.0 (TID 4517, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:18,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2258.0 (TID 4517)
2017-07-26 18:20:18,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2258.0 (TID 4516)
2017-07-26 18:20:18,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:18,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:18,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2258.0 (TID 4517). 714 bytes result sent to driver
2017-07-26 18:20:18,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2258.0 (TID 4516). 635 bytes result sent to driver
2017-07-26 18:20:18,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2258.0 (TID 4517) in 14 ms on localhost (1/2)
2017-07-26 18:20:18,081 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2258.0 (TID 4516) in 17 ms on localhost (2/2)
2017-07-26 18:20:18,081 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2258.0, whose tasks have all completed, from pool 
2017-07-26 18:20:18,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2258 (foreachPartition at streamingProcessTest.scala:48) finished in 0.017 s
2017-07-26 18:20:18,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2258 finished: foreachPartition at streamingProcessTest.scala:48, took 0.036198 s
2017-07-26 18:20:18,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064418000 ms.0 from job set of time 1501064418000 ms
2017-07-26 18:20:18,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1501064418000 ms (execution: 0.066 s)
2017-07-26 18:20:18,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2257 from persistence list
2017-07-26 18:20:18,084 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2257
2017-07-26 18:20:18,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:18,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064414000 ms
2017-07-26 18:20:20,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064420000 ms
2017-07-26 18:20:20,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064420000 ms.0 from job set of time 1501064420000 ms
2017-07-26 18:20:20,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2259 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2259 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2259 (KafkaRDD[2259] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:20,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2259 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:20:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2259_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:20:20,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2259_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:20,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2259 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:20,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2259 (KafkaRDD[2259] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:20,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2259.0 with 2 tasks
2017-07-26 18:20:20,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2259.0 (TID 4518, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:20,068 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2259.0 (TID 4519, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:20,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2259.0 (TID 4519)
2017-07-26 18:20:20,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2259.0 (TID 4518)
2017-07-26 18:20:20,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:20,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:20,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2259.0 (TID 4519). 714 bytes result sent to driver
2017-07-26 18:20:20,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2259.0 (TID 4518). 635 bytes result sent to driver
2017-07-26 18:20:20,077 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2259.0 (TID 4519) in 8 ms on localhost (1/2)
2017-07-26 18:20:20,077 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2259.0 (TID 4518) in 10 ms on localhost (2/2)
2017-07-26 18:20:20,077 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2259.0, whose tasks have all completed, from pool 
2017-07-26 18:20:20,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2259 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:20:20,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2259 finished: foreachPartition at streamingProcessTest.scala:48, took 0.030963 s
2017-07-26 18:20:20,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064420000 ms.0 from job set of time 1501064420000 ms
2017-07-26 18:20:20,079 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1501064420000 ms (execution: 0.061 s)
2017-07-26 18:20:20,079 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2258 from persistence list
2017-07-26 18:20:20,079 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2258
2017-07-26 18:20:20,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:20,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064416000 ms
2017-07-26 18:20:22,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064422000 ms
2017-07-26 18:20:22,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064422000 ms.0 from job set of time 1501064422000 ms
2017-07-26 18:20:22,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:22,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2260 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:22,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2260 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:22,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:22,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:22,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2260 (KafkaRDD[2260] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:22,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2260 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:20:22,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2260_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:20:22,040 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2260_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:22,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2260 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:22,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2260 (KafkaRDD[2260] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:22,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2260.0 with 2 tasks
2017-07-26 18:20:22,042 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2260.0 (TID 4520, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:22,043 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2260.0 (TID 4521, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:22,044 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2260.0 (TID 4521)
2017-07-26 18:20:22,044 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2260.0 (TID 4520)
2017-07-26 18:20:22,046 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:22,047 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:22,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2260.0 (TID 4521). 635 bytes result sent to driver
2017-07-26 18:20:22,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2260.0 (TID 4520). 714 bytes result sent to driver
2017-07-26 18:20:22,050 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2260.0 (TID 4521) in 7 ms on localhost (1/2)
2017-07-26 18:20:22,050 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2260.0 (TID 4520) in 9 ms on localhost (2/2)
2017-07-26 18:20:22,050 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2260.0, whose tasks have all completed, from pool 
2017-07-26 18:20:22,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2260 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:20:22,050 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2260 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020436 s
2017-07-26 18:20:22,051 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064422000 ms.0 from job set of time 1501064422000 ms
2017-07-26 18:20:22,051 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.051 s for time 1501064422000 ms (execution: 0.038 s)
2017-07-26 18:20:22,051 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2259 from persistence list
2017-07-26 18:20:22,051 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2259
2017-07-26 18:20:22,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:22,052 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064418000 ms
2017-07-26 18:20:24,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064424000 ms
2017-07-26 18:20:24,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064424000 ms.0 from job set of time 1501064424000 ms
2017-07-26 18:20:24,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:24,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2261 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:24,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2261 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:24,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:24,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:24,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2261 (KafkaRDD[2261] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:24,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2261 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:20:24,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2261_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:20:24,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2261_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:24,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2261 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:24,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2261 (KafkaRDD[2261] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:24,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2261.0 with 2 tasks
2017-07-26 18:20:24,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2261.0 (TID 4522, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:24,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2261.0 (TID 4523, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:24,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2261.0 (TID 4522)
2017-07-26 18:20:24,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2261.0 (TID 4523)
2017-07-26 18:20:24,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:24,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:24,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2261.0 (TID 4522). 635 bytes result sent to driver
2017-07-26 18:20:24,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2261.0 (TID 4523). 635 bytes result sent to driver
2017-07-26 18:20:24,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2261.0 (TID 4523) in 11 ms on localhost (1/2)
2017-07-26 18:20:24,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2261.0 (TID 4522) in 12 ms on localhost (2/2)
2017-07-26 18:20:24,083 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2261.0, whose tasks have all completed, from pool 
2017-07-26 18:20:24,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2261 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:20:24,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2261 finished: foreachPartition at streamingProcessTest.scala:48, took 0.030876 s
2017-07-26 18:20:24,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064424000 ms.0 from job set of time 1501064424000 ms
2017-07-26 18:20:24,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1501064424000 ms (execution: 0.065 s)
2017-07-26 18:20:24,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2260 from persistence list
2017-07-26 18:20:24,084 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2260
2017-07-26 18:20:24,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:24,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064420000 ms
2017-07-26 18:20:26,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064426000 ms
2017-07-26 18:20:26,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064426000 ms.0 from job set of time 1501064426000 ms
2017-07-26 18:20:26,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:26,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2262 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:26,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2262 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:26,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:26,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:26,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2262 (KafkaRDD[2262] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:26,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2261_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:26,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2262 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:20:26,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2248_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:26,077 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2249_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:26,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2262_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:20:26,079 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2262_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:26,080 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2250_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:26,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2262 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:26,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2262 (KafkaRDD[2262] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:26,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2262.0 with 2 tasks
2017-07-26 18:20:26,082 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2262.0 (TID 4524, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:26,082 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2251_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:26,082 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2262.0 (TID 4525, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:26,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2262.0 (TID 4525)
2017-07-26 18:20:26,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2262.0 (TID 4524)
2017-07-26 18:20:26,086 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2252_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:26,086 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:26,086 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:26,088 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2253_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:26,089 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2262.0 (TID 4524). 635 bytes result sent to driver
2017-07-26 18:20:26,089 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2262.0 (TID 4525). 635 bytes result sent to driver
2017-07-26 18:20:26,090 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2254_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:26,091 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2262.0 (TID 4524) in 10 ms on localhost (1/2)
2017-07-26 18:20:26,092 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2262.0 (TID 4525) in 9 ms on localhost (2/2)
2017-07-26 18:20:26,092 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2262.0, whose tasks have all completed, from pool 
2017-07-26 18:20:26,092 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2262 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:20:26,093 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2262 finished: foreachPartition at streamingProcessTest.scala:48, took 0.043915 s
2017-07-26 18:20:26,093 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2255_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:26,093 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064426000 ms.0 from job set of time 1501064426000 ms
2017-07-26 18:20:26,093 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2261 from persistence list
2017-07-26 18:20:26,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.093 s for time 1501064426000 ms (execution: 0.077 s)
2017-07-26 18:20:26,094 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2261
2017-07-26 18:20:26,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:26,094 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064422000 ms
2017-07-26 18:20:26,095 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2256_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:26,097 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2257_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:26,099 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2258_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:26,100 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2259_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:26,102 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2260_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:28,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064428000 ms
2017-07-26 18:20:28,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064428000 ms.0 from job set of time 1501064428000 ms
2017-07-26 18:20:28,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2263 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2263 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:28,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2263 (KafkaRDD[2263] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:28,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2263 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:20:28,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2263_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:20:28,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2263_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:28,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2263 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:28,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2263 (KafkaRDD[2263] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:28,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2263.0 with 2 tasks
2017-07-26 18:20:28,058 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2263.0 (TID 4526, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:28,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2263.0 (TID 4527, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:28,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2263.0 (TID 4527)
2017-07-26 18:20:28,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2263.0 (TID 4526)
2017-07-26 18:20:28,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:28,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:28,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2263.0 (TID 4526). 714 bytes result sent to driver
2017-07-26 18:20:28,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2263.0 (TID 4527). 635 bytes result sent to driver
2017-07-26 18:20:28,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2263.0 (TID 4526) in 9 ms on localhost (1/2)
2017-07-26 18:20:28,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2263.0 (TID 4527) in 8 ms on localhost (2/2)
2017-07-26 18:20:28,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2263.0, whose tasks have all completed, from pool 
2017-07-26 18:20:28,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2263 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:20:28,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2263 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019908 s
2017-07-26 18:20:28,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064428000 ms.0 from job set of time 1501064428000 ms
2017-07-26 18:20:28,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2262 from persistence list
2017-07-26 18:20:28,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1501064428000 ms (execution: 0.051 s)
2017-07-26 18:20:28,068 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2262
2017-07-26 18:20:28,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:28,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064424000 ms
2017-07-26 18:20:30,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064430000 ms
2017-07-26 18:20:30,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064430000 ms.0 from job set of time 1501064430000 ms
2017-07-26 18:20:30,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2264 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2264 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:30,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:30,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2264 (KafkaRDD[2264] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:30,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2264 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:20:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2264_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:20:30,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2264_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2264 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2264 (KafkaRDD[2264] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:30,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2264.0 with 2 tasks
2017-07-26 18:20:30,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2264.0 (TID 4528, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:30,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2264.0 (TID 4529, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:30,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2264.0 (TID 4528)
2017-07-26 18:20:30,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2264.0 (TID 4529)
2017-07-26 18:20:30,070 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:30,070 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:30,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2264.0 (TID 4529). 714 bytes result sent to driver
2017-07-26 18:20:30,073 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2264.0 (TID 4528). 714 bytes result sent to driver
2017-07-26 18:20:30,076 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2264.0 (TID 4529) in 10 ms on localhost (1/2)
2017-07-26 18:20:30,076 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2264.0 (TID 4528) in 11 ms on localhost (2/2)
2017-07-26 18:20:30,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2264.0, whose tasks have all completed, from pool 
2017-07-26 18:20:30,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2264 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:20:30,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2264 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028400 s
2017-07-26 18:20:30,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064430000 ms.0 from job set of time 1501064430000 ms
2017-07-26 18:20:30,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1501064430000 ms (execution: 0.061 s)
2017-07-26 18:20:30,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2263 from persistence list
2017-07-26 18:20:30,079 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2263
2017-07-26 18:20:30,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:30,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064426000 ms
2017-07-26 18:20:32,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064432000 ms
2017-07-26 18:20:32,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064432000 ms.0 from job set of time 1501064432000 ms
2017-07-26 18:20:32,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:32,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2265 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:32,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2265 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:32,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:32,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:32,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2265 (KafkaRDD[2265] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:32,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2265 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:20:32,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2265_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:20:32,052 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2265_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:32,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2265 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:32,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2265 (KafkaRDD[2265] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:32,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2265.0 with 2 tasks
2017-07-26 18:20:32,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2265.0 (TID 4530, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:32,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2265.0 (TID 4531, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:32,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2265.0 (TID 4531)
2017-07-26 18:20:32,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2265.0 (TID 4530)
2017-07-26 18:20:32,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:32,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:32,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2265.0 (TID 4531). 635 bytes result sent to driver
2017-07-26 18:20:32,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2265.0 (TID 4530). 635 bytes result sent to driver
2017-07-26 18:20:32,064 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2265.0 (TID 4530) in 10 ms on localhost (1/2)
2017-07-26 18:20:32,065 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2265.0 (TID 4531) in 10 ms on localhost (2/2)
2017-07-26 18:20:32,065 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2265.0, whose tasks have all completed, from pool 
2017-07-26 18:20:32,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2265 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:20:32,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2265 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022877 s
2017-07-26 18:20:32,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064432000 ms.0 from job set of time 1501064432000 ms
2017-07-26 18:20:32,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501064432000 ms (execution: 0.049 s)
2017-07-26 18:20:32,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2264 from persistence list
2017-07-26 18:20:32,067 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2264
2017-07-26 18:20:32,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:32,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064428000 ms
2017-07-26 18:20:34,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064434000 ms
2017-07-26 18:20:34,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064434000 ms.0 from job set of time 1501064434000 ms
2017-07-26 18:20:34,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:34,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2266 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:34,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2266 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:34,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:34,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:34,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2266 (KafkaRDD[2266] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:34,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2266 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:20:34,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2266_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:20:34,058 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2266_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:34,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2266 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:34,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2266 (KafkaRDD[2266] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:34,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2266.0 with 2 tasks
2017-07-26 18:20:34,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2266.0 (TID 4532, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:34,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2266.0 (TID 4533, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:34,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2266.0 (TID 4533)
2017-07-26 18:20:34,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2266.0 (TID 4532)
2017-07-26 18:20:34,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:34,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:34,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2266.0 (TID 4533). 635 bytes result sent to driver
2017-07-26 18:20:34,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2266.0 (TID 4532). 714 bytes result sent to driver
2017-07-26 18:20:34,070 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2266.0 (TID 4533) in 9 ms on localhost (1/2)
2017-07-26 18:20:34,071 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2266.0 (TID 4532) in 11 ms on localhost (2/2)
2017-07-26 18:20:34,071 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2266.0, whose tasks have all completed, from pool 
2017-07-26 18:20:34,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2266 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:20:34,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2266 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025651 s
2017-07-26 18:20:34,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064434000 ms.0 from job set of time 1501064434000 ms
2017-07-26 18:20:34,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1501064434000 ms (execution: 0.056 s)
2017-07-26 18:20:34,073 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2265 from persistence list
2017-07-26 18:20:34,074 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2265
2017-07-26 18:20:34,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:34,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064430000 ms
2017-07-26 18:20:36,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064436000 ms
2017-07-26 18:20:36,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064436000 ms.0 from job set of time 1501064436000 ms
2017-07-26 18:20:36,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:36,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2267 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:36,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2267 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:36,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:36,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:36,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2267 (KafkaRDD[2267] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:36,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2267 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:20:36,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2267_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:20:36,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2267_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:36,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2267 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:36,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2267 (KafkaRDD[2267] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:36,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2267.0 with 2 tasks
2017-07-26 18:20:36,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2267.0 (TID 4534, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:36,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2267.0 (TID 4535, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:36,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2267.0 (TID 4534)
2017-07-26 18:20:36,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2267.0 (TID 4535)
2017-07-26 18:20:36,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:36,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:36,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2267.0 (TID 4535). 714 bytes result sent to driver
2017-07-26 18:20:36,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2267.0 (TID 4534). 714 bytes result sent to driver
2017-07-26 18:20:36,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2267.0 (TID 4535) in 9 ms on localhost (1/2)
2017-07-26 18:20:36,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2267.0 (TID 4534) in 10 ms on localhost (2/2)
2017-07-26 18:20:36,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2267.0, whose tasks have all completed, from pool 
2017-07-26 18:20:36,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2267 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:20:36,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2267 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027943 s
2017-07-26 18:20:36,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064436000 ms.0 from job set of time 1501064436000 ms
2017-07-26 18:20:36,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501064436000 ms (execution: 0.059 s)
2017-07-26 18:20:36,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2266 from persistence list
2017-07-26 18:20:36,076 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2266
2017-07-26 18:20:36,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:36,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064432000 ms
2017-07-26 18:20:38,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064438000 ms
2017-07-26 18:20:38,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064438000 ms.0 from job set of time 1501064438000 ms
2017-07-26 18:20:38,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:38,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2268 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:38,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2268 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:38,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:38,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:38,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2268 (KafkaRDD[2268] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:38,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2268 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:20:38,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2268_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:20:38,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2268_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:38,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2268 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:38,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2268 (KafkaRDD[2268] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:38,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2268.0 with 2 tasks
2017-07-26 18:20:38,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2268.0 (TID 4536, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:38,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2268.0 (TID 4537, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:38,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2268.0 (TID 4537)
2017-07-26 18:20:38,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2268.0 (TID 4536)
2017-07-26 18:20:38,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:38,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:38,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2268.0 (TID 4537). 635 bytes result sent to driver
2017-07-26 18:20:38,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2268.0 (TID 4536). 635 bytes result sent to driver
2017-07-26 18:20:38,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2268.0 (TID 4537) in 9 ms on localhost (1/2)
2017-07-26 18:20:38,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2268.0 (TID 4536) in 11 ms on localhost (2/2)
2017-07-26 18:20:38,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2268.0, whose tasks have all completed, from pool 
2017-07-26 18:20:38,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2268 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:20:38,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2268 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025346 s
2017-07-26 18:20:38,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064438000 ms.0 from job set of time 1501064438000 ms
2017-07-26 18:20:38,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501064438000 ms (execution: 0.052 s)
2017-07-26 18:20:38,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2267 from persistence list
2017-07-26 18:20:38,070 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2267
2017-07-26 18:20:38,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:38,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064434000 ms
2017-07-26 18:20:40,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064440000 ms
2017-07-26 18:20:40,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064440000 ms.0 from job set of time 1501064440000 ms
2017-07-26 18:20:40,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:40,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2269 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:40,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2269 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:40,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:40,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:40,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2269 (KafkaRDD[2269] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:40,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2269 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:20:40,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2269_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:20:40,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2269_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:40,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2269 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:40,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2269 (KafkaRDD[2269] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:40,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2269.0 with 2 tasks
2017-07-26 18:20:40,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2269.0 (TID 4538, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:40,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2269.0 (TID 4539, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:40,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2269.0 (TID 4539)
2017-07-26 18:20:40,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2269.0 (TID 4538)
2017-07-26 18:20:40,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:40,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:40,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2269.0 (TID 4538). 635 bytes result sent to driver
2017-07-26 18:20:40,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2269.0 (TID 4539). 635 bytes result sent to driver
2017-07-26 18:20:40,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2269.0 (TID 4538) in 11 ms on localhost (1/2)
2017-07-26 18:20:40,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2269.0 (TID 4539) in 9 ms on localhost (2/2)
2017-07-26 18:20:40,072 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2269.0, whose tasks have all completed, from pool 
2017-07-26 18:20:40,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2269 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:20:40,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2269 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024834 s
2017-07-26 18:20:40,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064440000 ms.0 from job set of time 1501064440000 ms
2017-07-26 18:20:40,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1501064440000 ms (execution: 0.057 s)
2017-07-26 18:20:40,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2268 from persistence list
2017-07-26 18:20:40,074 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2268
2017-07-26 18:20:40,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:40,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064436000 ms
2017-07-26 18:20:42,117 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064442000 ms
2017-07-26 18:20:42,118 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064442000 ms.0 from job set of time 1501064442000 ms
2017-07-26 18:20:42,147 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:42,148 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2270 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:42,148 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2270 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:42,148 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:42,148 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:42,149 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2270 (KafkaRDD[2270] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:42,152 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2270 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:20:42,157 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2270_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:20:42,158 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2270_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:42,158 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2270 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:42,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2270 (KafkaRDD[2270] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:42,159 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2270.0 with 2 tasks
2017-07-26 18:20:42,160 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2270.0 (TID 4540, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:42,161 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2270.0 (TID 4541, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:42,161 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2270.0 (TID 4541)
2017-07-26 18:20:42,161 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2270.0 (TID 4540)
2017-07-26 18:20:42,164 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:42,164 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:42,166 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2270.0 (TID 4540). 635 bytes result sent to driver
2017-07-26 18:20:42,166 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2270.0 (TID 4541). 635 bytes result sent to driver
2017-07-26 18:20:42,169 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2270.0 (TID 4540) in 10 ms on localhost (1/2)
2017-07-26 18:20:42,169 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2270.0 (TID 4541) in 9 ms on localhost (2/2)
2017-07-26 18:20:42,170 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2270 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:20:42,170 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2270.0, whose tasks have all completed, from pool 
2017-07-26 18:20:42,170 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2270 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022418 s
2017-07-26 18:20:42,170 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064442000 ms.0 from job set of time 1501064442000 ms
2017-07-26 18:20:42,171 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.170 s for time 1501064442000 ms (execution: 0.052 s)
2017-07-26 18:20:42,171 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2269 from persistence list
2017-07-26 18:20:42,171 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2269
2017-07-26 18:20:42,171 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:42,171 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064438000 ms
2017-07-26 18:20:44,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064444000 ms
2017-07-26 18:20:44,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064444000 ms.0 from job set of time 1501064444000 ms
2017-07-26 18:20:44,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:44,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2271 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:44,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2271 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:44,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:44,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:44,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2271 (KafkaRDD[2271] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:44,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2271 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:20:44,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2271_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:20:44,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2271_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:44,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2271 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:44,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2271 (KafkaRDD[2271] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:44,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2271.0 with 2 tasks
2017-07-26 18:20:44,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2271.0 (TID 4542, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:44,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2271.0 (TID 4543, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:44,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2271.0 (TID 4542)
2017-07-26 18:20:44,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2271.0 (TID 4543)
2017-07-26 18:20:44,079 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:44,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:44,083 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2271.0 (TID 4543). 635 bytes result sent to driver
2017-07-26 18:20:44,083 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2271.0 (TID 4542). 714 bytes result sent to driver
2017-07-26 18:20:44,087 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2271.0 (TID 4543) in 16 ms on localhost (1/2)
2017-07-26 18:20:44,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2271.0 (TID 4542) in 19 ms on localhost (2/2)
2017-07-26 18:20:44,088 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2271.0, whose tasks have all completed, from pool 
2017-07-26 18:20:44,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2271 (foreachPartition at streamingProcessTest.scala:48) finished in 0.020 s
2017-07-26 18:20:44,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2271 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040543 s
2017-07-26 18:20:44,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064444000 ms.0 from job set of time 1501064444000 ms
2017-07-26 18:20:44,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1501064444000 ms (execution: 0.073 s)
2017-07-26 18:20:44,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2270 from persistence list
2017-07-26 18:20:44,091 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2270
2017-07-26 18:20:44,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:44,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064440000 ms
2017-07-26 18:20:46,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064446000 ms
2017-07-26 18:20:46,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064446000 ms.0 from job set of time 1501064446000 ms
2017-07-26 18:20:46,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:46,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2272 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:46,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2272 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:46,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:46,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:46,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2272 (KafkaRDD[2272] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:46,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2272 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:20:46,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2272_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:20:46,043 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2272_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:46,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2272 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:46,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2272 (KafkaRDD[2272] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:46,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2272.0 with 2 tasks
2017-07-26 18:20:46,046 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2272.0 (TID 4544, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:46,046 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2272.0 (TID 4545, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:46,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2272.0 (TID 4545)
2017-07-26 18:20:46,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2272.0 (TID 4544)
2017-07-26 18:20:46,048 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:46,048 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:46,050 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2272.0 (TID 4545). 635 bytes result sent to driver
2017-07-26 18:20:46,050 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2272.0 (TID 4544). 714 bytes result sent to driver
2017-07-26 18:20:46,051 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2272.0 (TID 4545) in 5 ms on localhost (1/2)
2017-07-26 18:20:46,052 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2272.0 (TID 4544) in 6 ms on localhost (2/2)
2017-07-26 18:20:46,052 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2272.0, whose tasks have all completed, from pool 
2017-07-26 18:20:46,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2272 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:20:46,052 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2272 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019228 s
2017-07-26 18:20:46,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064446000 ms.0 from job set of time 1501064446000 ms
2017-07-26 18:20:46,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.052 s for time 1501064446000 ms (execution: 0.036 s)
2017-07-26 18:20:46,053 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2271 from persistence list
2017-07-26 18:20:46,053 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2271
2017-07-26 18:20:46,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:46,053 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064442000 ms
2017-07-26 18:20:48,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064448000 ms
2017-07-26 18:20:48,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064448000 ms.0 from job set of time 1501064448000 ms
2017-07-26 18:20:48,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:48,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2273 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:48,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2273 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:48,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:48,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:48,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2273 (KafkaRDD[2273] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:48,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2273 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:20:48,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2273_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:20:48,048 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2273_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2273 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2273 (KafkaRDD[2273] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2273.0 with 2 tasks
2017-07-26 18:20:48,050 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2273.0 (TID 4546, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:48,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2273.0 (TID 4547, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:48,052 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2273.0 (TID 4547)
2017-07-26 18:20:48,052 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2273.0 (TID 4546)
2017-07-26 18:20:48,055 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:48,056 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:48,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2273.0 (TID 4546). 635 bytes result sent to driver
2017-07-26 18:20:48,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2273.0 (TID 4547). 714 bytes result sent to driver
2017-07-26 18:20:48,060 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2273.0 (TID 4546) in 11 ms on localhost (1/2)
2017-07-26 18:20:48,060 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2273.0 (TID 4547) in 9 ms on localhost (2/2)
2017-07-26 18:20:48,060 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2273.0, whose tasks have all completed, from pool 
2017-07-26 18:20:48,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2273 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:20:48,061 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2273 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022550 s
2017-07-26 18:20:48,061 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064448000 ms.0 from job set of time 1501064448000 ms
2017-07-26 18:20:48,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.061 s for time 1501064448000 ms (execution: 0.043 s)
2017-07-26 18:20:48,062 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2272 from persistence list
2017-07-26 18:20:48,062 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2272
2017-07-26 18:20:48,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:48,062 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064444000 ms
2017-07-26 18:20:50,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064450000 ms
2017-07-26 18:20:50,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064450000 ms.0 from job set of time 1501064450000 ms
2017-07-26 18:20:50,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:50,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2274 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:50,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2274 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:50,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:50,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:50,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2274 (KafkaRDD[2274] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:50,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2274 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:20:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2274_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:20:50,040 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2274_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2274 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2274 (KafkaRDD[2274] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:50,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2274.0 with 2 tasks
2017-07-26 18:20:50,041 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2274.0 (TID 4548, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:50,042 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2274.0 (TID 4549, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:50,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2274.0 (TID 4549)
2017-07-26 18:20:50,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2274.0 (TID 4548)
2017-07-26 18:20:50,043 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:50,044 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:50,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2274.0 (TID 4549). 714 bytes result sent to driver
2017-07-26 18:20:50,045 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2274.0 (TID 4548). 635 bytes result sent to driver
2017-07-26 18:20:50,048 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2274.0 (TID 4548) in 7 ms on localhost (1/2)
2017-07-26 18:20:50,048 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2274.0 (TID 4549) in 6 ms on localhost (2/2)
2017-07-26 18:20:50,048 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2274.0, whose tasks have all completed, from pool 
2017-07-26 18:20:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2274 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:20:50,049 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2274 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018085 s
2017-07-26 18:20:50,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064450000 ms.0 from job set of time 1501064450000 ms
2017-07-26 18:20:50,049 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.049 s for time 1501064450000 ms (execution: 0.036 s)
2017-07-26 18:20:50,049 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2273 from persistence list
2017-07-26 18:20:50,050 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2273
2017-07-26 18:20:50,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:50,050 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064446000 ms
2017-07-26 18:20:52,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064452000 ms
2017-07-26 18:20:52,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064452000 ms.0 from job set of time 1501064452000 ms
2017-07-26 18:20:52,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2275 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2275 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:52,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2275 (KafkaRDD[2275] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:52,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2275 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:20:52,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2275_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:20:52,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2275_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:52,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2275 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:52,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2275 (KafkaRDD[2275] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:52,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2275.0 with 2 tasks
2017-07-26 18:20:52,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2275.0 (TID 4550, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:52,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2275.0 (TID 4551, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:52,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2275.0 (TID 4550)
2017-07-26 18:20:52,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2275.0 (TID 4551)
2017-07-26 18:20:52,076 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:52,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:52,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2275.0 (TID 4550). 714 bytes result sent to driver
2017-07-26 18:20:52,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2275.0 (TID 4551). 714 bytes result sent to driver
2017-07-26 18:20:52,081 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2275.0 (TID 4550) in 13 ms on localhost (1/2)
2017-07-26 18:20:52,081 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2275.0 (TID 4551) in 11 ms on localhost (2/2)
2017-07-26 18:20:52,081 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2275.0, whose tasks have all completed, from pool 
2017-07-26 18:20:52,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2275 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:20:52,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2275 finished: foreachPartition at streamingProcessTest.scala:48, took 0.034803 s
2017-07-26 18:20:52,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064452000 ms.0 from job set of time 1501064452000 ms
2017-07-26 18:20:52,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1501064452000 ms (execution: 0.066 s)
2017-07-26 18:20:52,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2274 from persistence list
2017-07-26 18:20:52,083 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2274
2017-07-26 18:20:52,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:52,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064448000 ms
2017-07-26 18:20:54,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064454000 ms
2017-07-26 18:20:54,012 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064454000 ms.0 from job set of time 1501064454000 ms
2017-07-26 18:20:54,028 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2275_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:54,030 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2262_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:54,032 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2263_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:20:54,034 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2264_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:54,036 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2265_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:54,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:54,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2266_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:54,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2276 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:54,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2276 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:54,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:54,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:54,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2276 (KafkaRDD[2276] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:54,039 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2267_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:54,041 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2268_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:54,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2276 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:20:54,043 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2269_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:54,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2276_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:20:54,045 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2270_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:54,045 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2276_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:20:54,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2276 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:54,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2276 (KafkaRDD[2276] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:54,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2276.0 with 2 tasks
2017-07-26 18:20:54,047 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2271_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:54,048 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2276.0 (TID 4552, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:54,048 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2276.0 (TID 4553, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:54,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2276.0 (TID 4552)
2017-07-26 18:20:54,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2276.0 (TID 4553)
2017-07-26 18:20:54,049 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2272_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:54,051 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:54,051 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:54,052 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2273_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:54,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2276.0 (TID 4553). 635 bytes result sent to driver
2017-07-26 18:20:54,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2276.0 (TID 4552). 714 bytes result sent to driver
2017-07-26 18:20:54,053 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2274_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:54,055 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2276.0 (TID 4553) in 7 ms on localhost (1/2)
2017-07-26 18:20:54,055 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2276.0 (TID 4552) in 8 ms on localhost (2/2)
2017-07-26 18:20:54,056 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2276.0, whose tasks have all completed, from pool 
2017-07-26 18:20:54,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2276 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:20:54,056 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2276 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018918 s
2017-07-26 18:20:54,056 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064454000 ms.0 from job set of time 1501064454000 ms
2017-07-26 18:20:54,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.056 s for time 1501064454000 ms (execution: 0.044 s)
2017-07-26 18:20:54,057 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2275 from persistence list
2017-07-26 18:20:54,057 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2275
2017-07-26 18:20:54,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:54,057 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064450000 ms
2017-07-26 18:20:56,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064456000 ms
2017-07-26 18:20:56,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064456000 ms.0 from job set of time 1501064456000 ms
2017-07-26 18:20:56,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:56,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2277 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:56,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2277 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:56,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:56,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:56,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2277 (KafkaRDD[2277] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:56,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2277 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:20:56,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2277_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:20:56,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2277_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:56,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2277 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:56,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2277 (KafkaRDD[2277] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:56,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2277.0 with 2 tasks
2017-07-26 18:20:56,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2277.0 (TID 4554, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:56,061 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2277.0 (TID 4555, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:56,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2277.0 (TID 4554)
2017-07-26 18:20:56,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2277.0 (TID 4555)
2017-07-26 18:20:56,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:56,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:56,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2277.0 (TID 4554). 714 bytes result sent to driver
2017-07-26 18:20:56,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2277.0 (TID 4555). 714 bytes result sent to driver
2017-07-26 18:20:56,071 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2277.0 (TID 4555) in 11 ms on localhost (1/2)
2017-07-26 18:20:56,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2277.0 (TID 4554) in 13 ms on localhost (2/2)
2017-07-26 18:20:56,072 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2277.0, whose tasks have all completed, from pool 
2017-07-26 18:20:56,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2277 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:20:56,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2277 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027406 s
2017-07-26 18:20:56,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064456000 ms.0 from job set of time 1501064456000 ms
2017-07-26 18:20:56,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1501064456000 ms (execution: 0.054 s)
2017-07-26 18:20:56,073 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2276 from persistence list
2017-07-26 18:20:56,074 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2276
2017-07-26 18:20:56,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:56,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064452000 ms
2017-07-26 18:20:58,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064458000 ms
2017-07-26 18:20:58,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064458000 ms.0 from job set of time 1501064458000 ms
2017-07-26 18:20:58,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:20:58,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2278 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:20:58,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2278 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:20:58,026 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:20:58,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:20:58,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2278 (KafkaRDD[2278] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:20:58,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2278 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:20:58,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2278_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:20:58,034 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2278_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:20:58,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2278 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:20:58,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2278 (KafkaRDD[2278] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:20:58,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2278.0 with 2 tasks
2017-07-26 18:20:58,035 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2278.0 (TID 4556, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:20:58,035 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2278.0 (TID 4557, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:20:58,036 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2278.0 (TID 4556)
2017-07-26 18:20:58,036 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2278.0 (TID 4557)
2017-07-26 18:20:58,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:20:58,037 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:20:58,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2278.0 (TID 4556). 635 bytes result sent to driver
2017-07-26 18:20:58,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2278.0 (TID 4557). 635 bytes result sent to driver
2017-07-26 18:20:58,040 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2278.0 (TID 4556) in 6 ms on localhost (1/2)
2017-07-26 18:20:58,040 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2278.0 (TID 4557) in 5 ms on localhost (2/2)
2017-07-26 18:20:58,040 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2278.0, whose tasks have all completed, from pool 
2017-07-26 18:20:58,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2278 (foreachPartition at streamingProcessTest.scala:48) finished in 0.006 s
2017-07-26 18:20:58,040 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2278 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014389 s
2017-07-26 18:20:58,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064458000 ms.0 from job set of time 1501064458000 ms
2017-07-26 18:20:58,041 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2277 from persistence list
2017-07-26 18:20:58,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1501064458000 ms (execution: 0.023 s)
2017-07-26 18:20:58,041 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2277
2017-07-26 18:20:58,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:20:58,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064454000 ms
2017-07-26 18:21:00,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064460000 ms
2017-07-26 18:21:00,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064460000 ms.0 from job set of time 1501064460000 ms
2017-07-26 18:21:00,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2279 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2279 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:00,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2279 (KafkaRDD[2279] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:00,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2279 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:21:00,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2279_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:21:00,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2279_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:00,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2279 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:00,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2279 (KafkaRDD[2279] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:00,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2279.0 with 2 tasks
2017-07-26 18:21:00,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2279.0 (TID 4558, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:00,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2279.0 (TID 4559, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:00,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2279.0 (TID 4558)
2017-07-26 18:21:00,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2279.0 (TID 4559)
2017-07-26 18:21:00,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:00,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:00,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2279.0 (TID 4559). 635 bytes result sent to driver
2017-07-26 18:21:00,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2279.0 (TID 4558). 635 bytes result sent to driver
2017-07-26 18:21:00,074 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2279.0 (TID 4559) in 9 ms on localhost (1/2)
2017-07-26 18:21:00,074 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2279.0 (TID 4558) in 10 ms on localhost (2/2)
2017-07-26 18:21:00,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2279.0, whose tasks have all completed, from pool 
2017-07-26 18:21:00,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2279 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:21:00,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2279 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025603 s
2017-07-26 18:21:00,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064460000 ms.0 from job set of time 1501064460000 ms
2017-07-26 18:21:00,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501064460000 ms (execution: 0.058 s)
2017-07-26 18:21:00,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2278 from persistence list
2017-07-26 18:21:00,077 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2278
2017-07-26 18:21:00,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:00,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064456000 ms
2017-07-26 18:21:02,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064462000 ms
2017-07-26 18:21:02,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064462000 ms.0 from job set of time 1501064462000 ms
2017-07-26 18:21:02,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:02,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2280 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:02,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2280 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:02,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:02,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:02,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2280 (KafkaRDD[2280] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:02,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2280 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:21:02,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2280_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:21:02,051 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2280_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:02,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2280 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:02,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2280 (KafkaRDD[2280] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:02,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2280.0 with 2 tasks
2017-07-26 18:21:02,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2280.0 (TID 4560, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:02,055 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2280.0 (TID 4561, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:02,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2280.0 (TID 4560)
2017-07-26 18:21:02,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2280.0 (TID 4561)
2017-07-26 18:21:02,058 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:02,058 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:02,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2280.0 (TID 4561). 635 bytes result sent to driver
2017-07-26 18:21:02,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2280.0 (TID 4560). 635 bytes result sent to driver
2017-07-26 18:21:02,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2280.0 (TID 4561) in 8 ms on localhost (1/2)
2017-07-26 18:21:02,063 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2280.0 (TID 4560) in 10 ms on localhost (2/2)
2017-07-26 18:21:02,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2280 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:21:02,063 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2280.0, whose tasks have all completed, from pool 
2017-07-26 18:21:02,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2280 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021376 s
2017-07-26 18:21:02,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064462000 ms.0 from job set of time 1501064462000 ms
2017-07-26 18:21:02,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1501064462000 ms (execution: 0.048 s)
2017-07-26 18:21:02,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2279 from persistence list
2017-07-26 18:21:02,065 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2279
2017-07-26 18:21:02,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:02,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064458000 ms
2017-07-26 18:21:04,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064464000 ms
2017-07-26 18:21:04,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064464000 ms.0 from job set of time 1501064464000 ms
2017-07-26 18:21:04,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:04,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2281 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:04,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2281 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:04,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:04,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:04,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2281 (KafkaRDD[2281] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:04,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2281 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:21:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2281_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:21:04,050 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2281_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:04,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2281 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:04,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2281 (KafkaRDD[2281] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:04,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2281.0 with 2 tasks
2017-07-26 18:21:04,052 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2281.0 (TID 4562, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:04,052 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2281.0 (TID 4563, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:04,052 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2281.0 (TID 4562)
2017-07-26 18:21:04,052 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2281.0 (TID 4563)
2017-07-26 18:21:04,054 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:04,055 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:04,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2281.0 (TID 4562). 635 bytes result sent to driver
2017-07-26 18:21:04,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2281.0 (TID 4563). 714 bytes result sent to driver
2017-07-26 18:21:04,058 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2281.0 (TID 4562) in 7 ms on localhost (1/2)
2017-07-26 18:21:04,059 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2281.0 (TID 4563) in 6 ms on localhost (2/2)
2017-07-26 18:21:04,059 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2281.0, whose tasks have all completed, from pool 
2017-07-26 18:21:04,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2281 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:21:04,059 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2281 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019972 s
2017-07-26 18:21:04,059 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064464000 ms.0 from job set of time 1501064464000 ms
2017-07-26 18:21:04,060 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.059 s for time 1501064464000 ms (execution: 0.043 s)
2017-07-26 18:21:04,060 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2280 from persistence list
2017-07-26 18:21:04,060 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2280
2017-07-26 18:21:04,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:04,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064460000 ms
2017-07-26 18:21:06,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064466000 ms
2017-07-26 18:21:06,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064466000 ms.0 from job set of time 1501064466000 ms
2017-07-26 18:21:06,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:06,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2282 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:06,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2282 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:06,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:06,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:06,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2282 (KafkaRDD[2282] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:06,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2282 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:21:06,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2282_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:21:06,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2282_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:06,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2282 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:06,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2282 (KafkaRDD[2282] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:06,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2282.0 with 2 tasks
2017-07-26 18:21:06,065 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2282.0 (TID 4564, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:06,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2282.0 (TID 4565, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:06,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2282.0 (TID 4565)
2017-07-26 18:21:06,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2282.0 (TID 4564)
2017-07-26 18:21:06,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:06,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:06,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2282.0 (TID 4564). 714 bytes result sent to driver
2017-07-26 18:21:06,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2282.0 (TID 4565). 714 bytes result sent to driver
2017-07-26 18:21:06,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2282.0 (TID 4564) in 9 ms on localhost (1/2)
2017-07-26 18:21:06,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2282.0 (TID 4565) in 9 ms on localhost (2/2)
2017-07-26 18:21:06,075 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2282.0, whose tasks have all completed, from pool 
2017-07-26 18:21:06,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2282 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:21:06,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2282 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025594 s
2017-07-26 18:21:06,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064466000 ms.0 from job set of time 1501064466000 ms
2017-07-26 18:21:06,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.076 s for time 1501064466000 ms (execution: 0.058 s)
2017-07-26 18:21:06,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2281 from persistence list
2017-07-26 18:21:06,077 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2281
2017-07-26 18:21:06,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:06,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064462000 ms
2017-07-26 18:21:08,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064468000 ms
2017-07-26 18:21:08,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064468000 ms.0 from job set of time 1501064468000 ms
2017-07-26 18:21:08,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:08,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2283 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:08,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2283 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:08,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:08,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:08,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2283 (KafkaRDD[2283] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:08,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2283 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:21:08,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2283_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:21:08,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2283_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2283 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2283 (KafkaRDD[2283] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:08,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2283.0 with 2 tasks
2017-07-26 18:21:08,057 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2283.0 (TID 4566, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:08,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2283.0 (TID 4567, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:08,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2283.0 (TID 4567)
2017-07-26 18:21:08,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2283.0 (TID 4566)
2017-07-26 18:21:08,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:08,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:08,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2283.0 (TID 4566). 635 bytes result sent to driver
2017-07-26 18:21:08,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2283.0 (TID 4567). 635 bytes result sent to driver
2017-07-26 18:21:08,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2283.0 (TID 4567) in 9 ms on localhost (1/2)
2017-07-26 18:21:08,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2283.0 (TID 4566) in 10 ms on localhost (2/2)
2017-07-26 18:21:08,067 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2283.0, whose tasks have all completed, from pool 
2017-07-26 18:21:08,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2283 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:21:08,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2283 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022193 s
2017-07-26 18:21:08,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064468000 ms.0 from job set of time 1501064468000 ms
2017-07-26 18:21:08,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1501064468000 ms (execution: 0.048 s)
2017-07-26 18:21:08,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2282 from persistence list
2017-07-26 18:21:08,068 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2282
2017-07-26 18:21:08,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:08,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064464000 ms
2017-07-26 18:21:10,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064470000 ms
2017-07-26 18:21:10,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064470000 ms.0 from job set of time 1501064470000 ms
2017-07-26 18:21:10,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2284 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2284 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:10,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2284 (KafkaRDD[2284] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:10,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2284 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:21:10,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2284_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:21:10,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2284_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:10,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2284 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:10,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2284 (KafkaRDD[2284] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:10,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2284.0 with 2 tasks
2017-07-26 18:21:10,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2284.0 (TID 4568, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:10,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2284.0 (TID 4569, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:10,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2284.0 (TID 4569)
2017-07-26 18:21:10,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2284.0 (TID 4568)
2017-07-26 18:21:10,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:10,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:10,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2284.0 (TID 4568). 635 bytes result sent to driver
2017-07-26 18:21:10,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2284.0 (TID 4569). 714 bytes result sent to driver
2017-07-26 18:21:10,087 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2284.0 (TID 4568) in 18 ms on localhost (1/2)
2017-07-26 18:21:10,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2284.0 (TID 4569) in 16 ms on localhost (2/2)
2017-07-26 18:21:10,088 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2284.0, whose tasks have all completed, from pool 
2017-07-26 18:21:10,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2284 (foreachPartition at streamingProcessTest.scala:48) finished in 0.019 s
2017-07-26 18:21:10,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2284 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040379 s
2017-07-26 18:21:10,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064470000 ms.0 from job set of time 1501064470000 ms
2017-07-26 18:21:10,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.090 s for time 1501064470000 ms (execution: 0.073 s)
2017-07-26 18:21:10,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2283 from persistence list
2017-07-26 18:21:10,091 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2283
2017-07-26 18:21:10,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:10,092 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064466000 ms
2017-07-26 18:21:12,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064472000 ms
2017-07-26 18:21:12,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064472000 ms.0 from job set of time 1501064472000 ms
2017-07-26 18:21:12,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:12,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2285 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:12,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2285 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:12,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:12,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:12,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2285 (KafkaRDD[2285] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:12,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2285 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:21:12,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2285_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:21:12,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2285_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:12,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2285 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:12,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2285 (KafkaRDD[2285] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:12,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2285.0 with 2 tasks
2017-07-26 18:21:12,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2285.0 (TID 4570, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:12,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2285.0 (TID 4571, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:12,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2285.0 (TID 4570)
2017-07-26 18:21:12,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2285.0 (TID 4571)
2017-07-26 18:21:12,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:12,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:12,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2285.0 (TID 4571). 714 bytes result sent to driver
2017-07-26 18:21:12,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2285.0 (TID 4570). 714 bytes result sent to driver
2017-07-26 18:21:12,075 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2285.0 (TID 4570) in 10 ms on localhost (1/2)
2017-07-26 18:21:12,075 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2285.0 (TID 4571) in 10 ms on localhost (2/2)
2017-07-26 18:21:12,076 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2285.0, whose tasks have all completed, from pool 
2017-07-26 18:21:12,076 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2285 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:21:12,076 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2285 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028506 s
2017-07-26 18:21:12,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064472000 ms.0 from job set of time 1501064472000 ms
2017-07-26 18:21:12,077 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.077 s for time 1501064472000 ms (execution: 0.061 s)
2017-07-26 18:21:12,077 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2284 from persistence list
2017-07-26 18:21:12,077 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2284
2017-07-26 18:21:12,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:12,078 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064468000 ms
2017-07-26 18:21:14,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064474000 ms
2017-07-26 18:21:14,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064474000 ms.0 from job set of time 1501064474000 ms
2017-07-26 18:21:14,031 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:14,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2286 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:14,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2286 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:14,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:14,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:14,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2286 (KafkaRDD[2286] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:14,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2286 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:21:14,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2286_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:21:14,039 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2286_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:14,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2286 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:14,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2286 (KafkaRDD[2286] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:14,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2286.0 with 2 tasks
2017-07-26 18:21:14,041 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2286.0 (TID 4572, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:14,041 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2286.0 (TID 4573, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:14,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2286.0 (TID 4573)
2017-07-26 18:21:14,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2286.0 (TID 4572)
2017-07-26 18:21:14,043 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:14,043 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:14,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2286.0 (TID 4572). 635 bytes result sent to driver
2017-07-26 18:21:14,045 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2286.0 (TID 4573). 635 bytes result sent to driver
2017-07-26 18:21:14,047 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2286.0 (TID 4572) in 7 ms on localhost (1/2)
2017-07-26 18:21:14,047 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2286.0 (TID 4573) in 6 ms on localhost (2/2)
2017-07-26 18:21:14,047 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2286.0, whose tasks have all completed, from pool 
2017-07-26 18:21:14,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2286 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:21:14,047 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2286 finished: foreachPartition at streamingProcessTest.scala:48, took 0.016451 s
2017-07-26 18:21:14,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064474000 ms.0 from job set of time 1501064474000 ms
2017-07-26 18:21:14,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1501064474000 ms (execution: 0.033 s)
2017-07-26 18:21:14,048 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2285 from persistence list
2017-07-26 18:21:14,048 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2285
2017-07-26 18:21:14,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:14,048 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064470000 ms
2017-07-26 18:21:16,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064476000 ms
2017-07-26 18:21:16,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064476000 ms.0 from job set of time 1501064476000 ms
2017-07-26 18:21:16,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2287 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2287 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2287 (KafkaRDD[2287] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:16,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2287 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:21:16,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2287_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:21:16,058 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2287_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:16,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2287 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:16,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2287 (KafkaRDD[2287] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:16,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2287.0 with 2 tasks
2017-07-26 18:21:16,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2287.0 (TID 4574, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:16,061 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2287.0 (TID 4575, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:16,061 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2287.0 (TID 4574)
2017-07-26 18:21:16,061 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2287.0 (TID 4575)
2017-07-26 18:21:16,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:16,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:16,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2287.0 (TID 4575). 714 bytes result sent to driver
2017-07-26 18:21:16,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2287.0 (TID 4574). 714 bytes result sent to driver
2017-07-26 18:21:16,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2287.0 (TID 4575) in 9 ms on localhost (1/2)
2017-07-26 18:21:16,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2287.0 (TID 4574) in 10 ms on localhost (2/2)
2017-07-26 18:21:16,071 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2287.0, whose tasks have all completed, from pool 
2017-07-26 18:21:16,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2287 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:21:16,071 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2287 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023146 s
2017-07-26 18:21:16,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064476000 ms.0 from job set of time 1501064476000 ms
2017-07-26 18:21:16,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501064476000 ms (execution: 0.056 s)
2017-07-26 18:21:16,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2286 from persistence list
2017-07-26 18:21:16,073 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2286
2017-07-26 18:21:16,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:16,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064472000 ms
2017-07-26 18:21:18,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064478000 ms
2017-07-26 18:21:18,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064478000 ms.0 from job set of time 1501064478000 ms
2017-07-26 18:21:18,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:18,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2288 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:18,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2288 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:18,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:18,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:18,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2288 (KafkaRDD[2288] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:18,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2288 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:21:18,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2288_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:21:18,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2288_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:18,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2288 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:18,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2288 (KafkaRDD[2288] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:18,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2288.0 with 2 tasks
2017-07-26 18:21:18,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2288.0 (TID 4576, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:18,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2288.0 (TID 4577, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:18,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2288.0 (TID 4576)
2017-07-26 18:21:18,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2288.0 (TID 4577)
2017-07-26 18:21:18,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:18,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:18,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2288.0 (TID 4576). 635 bytes result sent to driver
2017-07-26 18:21:18,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2288.0 (TID 4577). 635 bytes result sent to driver
2017-07-26 18:21:18,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2288.0 (TID 4576) in 12 ms on localhost (1/2)
2017-07-26 18:21:18,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2288.0 (TID 4577) in 12 ms on localhost (2/2)
2017-07-26 18:21:18,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2288.0, whose tasks have all completed, from pool 
2017-07-26 18:21:18,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2288 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:21:18,077 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2288 finished: foreachPartition at streamingProcessTest.scala:48, took 0.030461 s
2017-07-26 18:21:18,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064478000 ms.0 from job set of time 1501064478000 ms
2017-07-26 18:21:18,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1501064478000 ms (execution: 0.062 s)
2017-07-26 18:21:18,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2287 from persistence list
2017-07-26 18:21:18,078 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2287
2017-07-26 18:21:18,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:18,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064474000 ms
2017-07-26 18:21:20,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064480000 ms
2017-07-26 18:21:20,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064480000 ms.0 from job set of time 1501064480000 ms
2017-07-26 18:21:20,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2289 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2289 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:20,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:20,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2289 (KafkaRDD[2289] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:20,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2289 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:21:20,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2289_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:21:20,064 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2289_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:20,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2289 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:20,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2289 (KafkaRDD[2289] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:20,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2289.0 with 2 tasks
2017-07-26 18:21:20,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2289.0 (TID 4578, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:20,069 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2289.0 (TID 4579, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:20,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2289.0 (TID 4579)
2017-07-26 18:21:20,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2289.0 (TID 4578)
2017-07-26 18:21:20,088 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:20,088 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:20,090 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2276_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:20,091 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2289.0 (TID 4579). 795 bytes result sent to driver
2017-07-26 18:21:20,091 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2289.0 (TID 4578). 874 bytes result sent to driver
2017-07-26 18:21:20,093 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2277_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:20,094 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2289.0 (TID 4578) in 26 ms on localhost (1/2)
2017-07-26 18:21:20,095 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2289.0 (TID 4579) in 26 ms on localhost (2/2)
2017-07-26 18:21:20,096 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2289.0, whose tasks have all completed, from pool 
2017-07-26 18:21:20,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2289 (foreachPartition at streamingProcessTest.scala:48) finished in 0.029 s
2017-07-26 18:21:20,096 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2289 finished: foreachPartition at streamingProcessTest.scala:48, took 0.049486 s
2017-07-26 18:21:20,096 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2278_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:20,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064480000 ms.0 from job set of time 1501064480000 ms
2017-07-26 18:21:20,097 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.097 s for time 1501064480000 ms (execution: 0.081 s)
2017-07-26 18:21:20,097 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2288 from persistence list
2017-07-26 18:21:20,097 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2288
2017-07-26 18:21:20,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:20,098 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064476000 ms
2017-07-26 18:21:20,098 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2279_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:20,099 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2280_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:20,101 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2281_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:20,103 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2282_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:20,105 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2283_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:20,106 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2284_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:20,108 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2285_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:20,109 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2286_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:20,110 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2287_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:20,111 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2288_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:22,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064482000 ms
2017-07-26 18:21:22,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064482000 ms.0 from job set of time 1501064482000 ms
2017-07-26 18:21:22,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2290 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2290 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:22,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2290 (KafkaRDD[2290] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:22,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2290 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:21:22,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2290_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:21:22,055 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2290_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:22,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2290 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:22,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2290 (KafkaRDD[2290] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:22,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2290.0 with 2 tasks
2017-07-26 18:21:22,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2290.0 (TID 4580, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:22,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2290.0 (TID 4581, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:22,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2290.0 (TID 4580)
2017-07-26 18:21:22,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2290.0 (TID 4581)
2017-07-26 18:21:22,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:22,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:22,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2290.0 (TID 4580). 635 bytes result sent to driver
2017-07-26 18:21:22,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2290.0 (TID 4581). 714 bytes result sent to driver
2017-07-26 18:21:22,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2290.0 (TID 4580) in 9 ms on localhost (1/2)
2017-07-26 18:21:22,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2290.0 (TID 4581) in 8 ms on localhost (2/2)
2017-07-26 18:21:22,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2290.0, whose tasks have all completed, from pool 
2017-07-26 18:21:22,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2290 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:21:22,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2290 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021124 s
2017-07-26 18:21:22,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064482000 ms.0 from job set of time 1501064482000 ms
2017-07-26 18:21:22,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.068 s for time 1501064482000 ms (execution: 0.052 s)
2017-07-26 18:21:22,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2289 from persistence list
2017-07-26 18:21:22,068 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2289
2017-07-26 18:21:22,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:22,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064478000 ms
2017-07-26 18:21:24,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064484000 ms
2017-07-26 18:21:24,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064484000 ms.0 from job set of time 1501064484000 ms
2017-07-26 18:21:24,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:24,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2291 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:24,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2291 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:24,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:24,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:24,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2291 (KafkaRDD[2291] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:24,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2291 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:21:24,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2291_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:21:24,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2291_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:24,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2291 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:24,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2291 (KafkaRDD[2291] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:24,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2291.0 with 2 tasks
2017-07-26 18:21:24,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2291.0 (TID 4582, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:24,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2291.0 (TID 4583, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:24,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2291.0 (TID 4582)
2017-07-26 18:21:24,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2291.0 (TID 4583)
2017-07-26 18:21:24,072 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:24,072 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:24,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2291.0 (TID 4582). 714 bytes result sent to driver
2017-07-26 18:21:24,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2291.0 (TID 4583). 714 bytes result sent to driver
2017-07-26 18:21:24,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2291.0 (TID 4582) in 16 ms on localhost (1/2)
2017-07-26 18:21:24,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2291.0 (TID 4583) in 16 ms on localhost (2/2)
2017-07-26 18:21:24,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2291.0, whose tasks have all completed, from pool 
2017-07-26 18:21:24,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2291 (foreachPartition at streamingProcessTest.scala:48) finished in 0.018 s
2017-07-26 18:21:24,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2291 finished: foreachPartition at streamingProcessTest.scala:48, took 0.037953 s
2017-07-26 18:21:24,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064484000 ms.0 from job set of time 1501064484000 ms
2017-07-26 18:21:24,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1501064484000 ms (execution: 0.067 s)
2017-07-26 18:21:24,085 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2290 from persistence list
2017-07-26 18:21:24,085 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2290
2017-07-26 18:21:24,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:24,086 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064480000 ms
2017-07-26 18:21:26,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064486000 ms
2017-07-26 18:21:26,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064486000 ms.0 from job set of time 1501064486000 ms
2017-07-26 18:21:26,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:26,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2292 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:26,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2292 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:26,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:26,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:26,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2292 (KafkaRDD[2292] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:26,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2292 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:21:26,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2292_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:21:26,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2292_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:26,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2292 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:26,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2292 (KafkaRDD[2292] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:26,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2292.0 with 2 tasks
2017-07-26 18:21:26,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2292.0 (TID 4584, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:26,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2292.0 (TID 4585, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:26,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2292.0 (TID 4585)
2017-07-26 18:21:26,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2292.0 (TID 4584)
2017-07-26 18:21:26,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:26,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:26,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2292.0 (TID 4585). 635 bytes result sent to driver
2017-07-26 18:21:26,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2292.0 (TID 4584). 635 bytes result sent to driver
2017-07-26 18:21:26,071 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2292.0 (TID 4585) in 9 ms on localhost (1/2)
2017-07-26 18:21:26,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2292.0 (TID 4584) in 10 ms on localhost (2/2)
2017-07-26 18:21:26,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2292.0, whose tasks have all completed, from pool 
2017-07-26 18:21:26,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2292 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:21:26,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2292 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025605 s
2017-07-26 18:21:26,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064486000 ms.0 from job set of time 1501064486000 ms
2017-07-26 18:21:26,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501064486000 ms (execution: 0.056 s)
2017-07-26 18:21:26,073 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2291 from persistence list
2017-07-26 18:21:26,073 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2291
2017-07-26 18:21:26,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:26,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064482000 ms
2017-07-26 18:21:28,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064488000 ms
2017-07-26 18:21:28,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064488000 ms.0 from job set of time 1501064488000 ms
2017-07-26 18:21:28,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:28,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2293 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:28,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2293 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:28,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:28,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:28,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2293 (KafkaRDD[2293] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:28,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2293 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:21:28,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2293_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:21:28,070 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2293_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:28,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2293 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:28,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2293 (KafkaRDD[2293] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:28,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2293.0 with 2 tasks
2017-07-26 18:21:28,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2293.0 (TID 4586, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:28,076 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2293.0 (TID 4587, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:28,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2293.0 (TID 4586)
2017-07-26 18:21:28,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2293.0 (TID 4587)
2017-07-26 18:21:28,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:28,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:28,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2293.0 (TID 4587). 714 bytes result sent to driver
2017-07-26 18:21:28,087 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2293.0 (TID 4586). 714 bytes result sent to driver
2017-07-26 18:21:28,092 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2293.0 (TID 4586) in 19 ms on localhost (1/2)
2017-07-26 18:21:28,092 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2293.0 (TID 4587) in 17 ms on localhost (2/2)
2017-07-26 18:21:28,093 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2293.0, whose tasks have all completed, from pool 
2017-07-26 18:21:28,093 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2293 (foreachPartition at streamingProcessTest.scala:48) finished in 0.020 s
2017-07-26 18:21:28,094 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2293 finished: foreachPartition at streamingProcessTest.scala:48, took 0.042887 s
2017-07-26 18:21:28,094 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064488000 ms.0 from job set of time 1501064488000 ms
2017-07-26 18:21:28,095 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.094 s for time 1501064488000 ms (execution: 0.078 s)
2017-07-26 18:21:28,095 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2292 from persistence list
2017-07-26 18:21:28,096 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2292
2017-07-26 18:21:28,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:28,097 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064484000 ms
2017-07-26 18:21:30,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064490000 ms
2017-07-26 18:21:30,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064490000 ms.0 from job set of time 1501064490000 ms
2017-07-26 18:21:30,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2294 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2294 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:30,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:30,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2294 (KafkaRDD[2294] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2294 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:21:30,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2294_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:21:30,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2294_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:30,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2294 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:30,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2294 (KafkaRDD[2294] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:30,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2294.0 with 2 tasks
2017-07-26 18:21:30,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2294.0 (TID 4588, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:30,076 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2294.0 (TID 4589, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:30,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2294.0 (TID 4589)
2017-07-26 18:21:30,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2294.0 (TID 4588)
2017-07-26 18:21:30,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:30,083 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:30,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2294.0 (TID 4589). 714 bytes result sent to driver
2017-07-26 18:21:30,087 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2294.0 (TID 4588). 714 bytes result sent to driver
2017-07-26 18:21:30,090 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2294.0 (TID 4589) in 16 ms on localhost (1/2)
2017-07-26 18:21:30,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2294.0 (TID 4588) in 18 ms on localhost (2/2)
2017-07-26 18:21:30,090 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2294.0, whose tasks have all completed, from pool 
2017-07-26 18:21:30,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2294 (foreachPartition at streamingProcessTest.scala:48) finished in 0.019 s
2017-07-26 18:21:30,091 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2294 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040082 s
2017-07-26 18:21:30,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064490000 ms.0 from job set of time 1501064490000 ms
2017-07-26 18:21:30,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1501064490000 ms (execution: 0.073 s)
2017-07-26 18:21:30,092 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2293 from persistence list
2017-07-26 18:21:30,093 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2293
2017-07-26 18:21:30,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:30,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064486000 ms
2017-07-26 18:21:32,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064492000 ms
2017-07-26 18:21:32,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064492000 ms.0 from job set of time 1501064492000 ms
2017-07-26 18:21:32,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:32,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2295 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:32,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2295 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:32,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:32,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:32,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2295 (KafkaRDD[2295] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:32,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2295 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:21:32,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2295_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:21:32,053 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2295_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:32,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2295 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:32,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2295 (KafkaRDD[2295] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:32,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2295.0 with 2 tasks
2017-07-26 18:21:32,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2295.0 (TID 4590, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:32,057 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2295.0 (TID 4591, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:32,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2295.0 (TID 4591)
2017-07-26 18:21:32,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2295.0 (TID 4590)
2017-07-26 18:21:32,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:32,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:32,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2295.0 (TID 4591). 714 bytes result sent to driver
2017-07-26 18:21:32,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2295.0 (TID 4590). 635 bytes result sent to driver
2017-07-26 18:21:32,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2295.0 (TID 4590) in 14 ms on localhost (1/2)
2017-07-26 18:21:32,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2295.0 (TID 4591) in 12 ms on localhost (2/2)
2017-07-26 18:21:32,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2295.0, whose tasks have all completed, from pool 
2017-07-26 18:21:32,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2295 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:21:32,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2295 finished: foreachPartition at streamingProcessTest.scala:48, took 0.030878 s
2017-07-26 18:21:32,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064492000 ms.0 from job set of time 1501064492000 ms
2017-07-26 18:21:32,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501064492000 ms (execution: 0.055 s)
2017-07-26 18:21:32,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2294 from persistence list
2017-07-26 18:21:32,071 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2294
2017-07-26 18:21:32,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:32,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064488000 ms
2017-07-26 18:21:34,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064494000 ms
2017-07-26 18:21:34,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064494000 ms.0 from job set of time 1501064494000 ms
2017-07-26 18:21:34,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:34,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2296 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:34,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2296 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:34,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:34,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:34,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2296 (KafkaRDD[2296] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:34,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2296 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:21:34,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2296_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:21:34,052 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2296_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:34,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2296 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:34,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2296 (KafkaRDD[2296] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:34,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2296.0 with 2 tasks
2017-07-26 18:21:34,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2296.0 (TID 4592, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:34,054 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2296.0 (TID 4593, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:34,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2296.0 (TID 4593)
2017-07-26 18:21:34,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2296.0 (TID 4592)
2017-07-26 18:21:34,056 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:34,056 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:34,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2296.0 (TID 4593). 635 bytes result sent to driver
2017-07-26 18:21:34,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2296.0 (TID 4592). 635 bytes result sent to driver
2017-07-26 18:21:34,060 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2296.0 (TID 4593) in 5 ms on localhost (1/2)
2017-07-26 18:21:34,060 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2296.0 (TID 4592) in 7 ms on localhost (2/2)
2017-07-26 18:21:34,060 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2296.0, whose tasks have all completed, from pool 
2017-07-26 18:21:34,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2296 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:21:34,060 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2296 finished: foreachPartition at streamingProcessTest.scala:48, took 0.018426 s
2017-07-26 18:21:34,061 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064494000 ms.0 from job set of time 1501064494000 ms
2017-07-26 18:21:34,061 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.061 s for time 1501064494000 ms (execution: 0.047 s)
2017-07-26 18:21:34,061 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2295 from persistence list
2017-07-26 18:21:34,061 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2295
2017-07-26 18:21:34,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:34,061 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064490000 ms
2017-07-26 18:21:36,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064496000 ms
2017-07-26 18:21:36,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064496000 ms.0 from job set of time 1501064496000 ms
2017-07-26 18:21:36,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:36,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2297 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:36,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2297 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:36,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:36,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:36,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2297 (KafkaRDD[2297] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:36,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2297 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:21:36,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2297_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:21:36,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2297_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:36,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2297 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:36,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2297 (KafkaRDD[2297] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:36,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2297.0 with 2 tasks
2017-07-26 18:21:36,067 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2297.0 (TID 4594, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:36,068 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2297.0 (TID 4595, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:36,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2297.0 (TID 4595)
2017-07-26 18:21:36,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2297.0 (TID 4594)
2017-07-26 18:21:36,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:36,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:36,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2297.0 (TID 4595). 635 bytes result sent to driver
2017-07-26 18:21:36,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2297.0 (TID 4594). 635 bytes result sent to driver
2017-07-26 18:21:36,081 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2297.0 (TID 4595) in 14 ms on localhost (1/2)
2017-07-26 18:21:36,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2297.0 (TID 4594) in 16 ms on localhost (2/2)
2017-07-26 18:21:36,082 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2297.0, whose tasks have all completed, from pool 
2017-07-26 18:21:36,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2297 (foreachPartition at streamingProcessTest.scala:48) finished in 0.016 s
2017-07-26 18:21:36,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2297 finished: foreachPartition at streamingProcessTest.scala:48, took 0.033934 s
2017-07-26 18:21:36,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064496000 ms.0 from job set of time 1501064496000 ms
2017-07-26 18:21:36,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1501064496000 ms (execution: 0.068 s)
2017-07-26 18:21:36,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2296 from persistence list
2017-07-26 18:21:36,085 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2296
2017-07-26 18:21:36,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:36,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064492000 ms
2017-07-26 18:21:38,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064498000 ms
2017-07-26 18:21:38,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064498000 ms.0 from job set of time 1501064498000 ms
2017-07-26 18:21:38,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2298 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2298 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2298 (KafkaRDD[2298] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:38,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2298 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:21:38,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2298_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:21:38,051 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2298_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:38,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2298 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:38,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2298 (KafkaRDD[2298] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:38,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2298.0 with 2 tasks
2017-07-26 18:21:38,053 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2298.0 (TID 4596, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:38,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2298.0 (TID 4597, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:38,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2298.0 (TID 4597)
2017-07-26 18:21:38,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2298.0 (TID 4596)
2017-07-26 18:21:38,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:38,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:38,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2298.0 (TID 4596). 635 bytes result sent to driver
2017-07-26 18:21:38,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2298.0 (TID 4597). 635 bytes result sent to driver
2017-07-26 18:21:38,062 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2298.0 (TID 4596) in 9 ms on localhost (1/2)
2017-07-26 18:21:38,063 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2298.0 (TID 4597) in 8 ms on localhost (2/2)
2017-07-26 18:21:38,063 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2298.0, whose tasks have all completed, from pool 
2017-07-26 18:21:38,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2298 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:21:38,063 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2298 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021292 s
2017-07-26 18:21:38,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064498000 ms.0 from job set of time 1501064498000 ms
2017-07-26 18:21:38,064 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.064 s for time 1501064498000 ms (execution: 0.048 s)
2017-07-26 18:21:38,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2297 from persistence list
2017-07-26 18:21:38,065 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2297
2017-07-26 18:21:38,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:38,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064494000 ms
2017-07-26 18:21:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064500000 ms
2017-07-26 18:21:40,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064500000 ms.0 from job set of time 1501064500000 ms
2017-07-26 18:21:40,033 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:40,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2299 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:40,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2299 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:40,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:40,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:40,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2299 (KafkaRDD[2299] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:40,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2299 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:21:40,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2299_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:21:40,045 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2299_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2299 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2299 (KafkaRDD[2299] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:40,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2299.0 with 2 tasks
2017-07-26 18:21:40,047 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2299.0 (TID 4598, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:40,048 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2299.0 (TID 4599, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:40,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2299.0 (TID 4599)
2017-07-26 18:21:40,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2299.0 (TID 4598)
2017-07-26 18:21:40,050 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:40,050 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:40,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2299.0 (TID 4598). 635 bytes result sent to driver
2017-07-26 18:21:40,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2299.0 (TID 4599). 635 bytes result sent to driver
2017-07-26 18:21:40,053 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2299.0 (TID 4598) in 6 ms on localhost (1/2)
2017-07-26 18:21:40,053 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2299.0 (TID 4599) in 6 ms on localhost (2/2)
2017-07-26 18:21:40,054 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2299.0, whose tasks have all completed, from pool 
2017-07-26 18:21:40,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2299 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:21:40,054 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2299 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020702 s
2017-07-26 18:21:40,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064500000 ms.0 from job set of time 1501064500000 ms
2017-07-26 18:21:40,054 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.054 s for time 1501064500000 ms (execution: 0.040 s)
2017-07-26 18:21:40,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2298 from persistence list
2017-07-26 18:21:40,055 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2298
2017-07-26 18:21:40,055 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:40,055 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064496000 ms
2017-07-26 18:21:42,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064502000 ms
2017-07-26 18:21:42,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064502000 ms.0 from job set of time 1501064502000 ms
2017-07-26 18:21:42,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2300 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2300 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:42,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2300 (KafkaRDD[2300] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:42,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2300 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:21:42,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2300_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:21:42,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2300_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:42,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2300 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:42,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2300 (KafkaRDD[2300] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:42,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2300.0 with 2 tasks
2017-07-26 18:21:42,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2300.0 (TID 4600, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:42,072 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2300.0 (TID 4601, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:42,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2300.0 (TID 4600)
2017-07-26 18:21:42,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2300.0 (TID 4601)
2017-07-26 18:21:42,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:42,076 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:42,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2300.0 (TID 4601). 714 bytes result sent to driver
2017-07-26 18:21:42,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2300.0 (TID 4600). 714 bytes result sent to driver
2017-07-26 18:21:42,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2300.0 (TID 4600) in 11 ms on localhost (1/2)
2017-07-26 18:21:42,082 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2300.0 (TID 4601) in 9 ms on localhost (2/2)
2017-07-26 18:21:42,082 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2300.0, whose tasks have all completed, from pool 
2017-07-26 18:21:42,082 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2300 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:21:42,082 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2300 finished: foreachPartition at streamingProcessTest.scala:48, took 0.032453 s
2017-07-26 18:21:42,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064502000 ms.0 from job set of time 1501064502000 ms
2017-07-26 18:21:42,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1501064502000 ms (execution: 0.065 s)
2017-07-26 18:21:42,083 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2299 from persistence list
2017-07-26 18:21:42,083 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2299
2017-07-26 18:21:42,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:42,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064498000 ms
2017-07-26 18:21:44,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064504000 ms
2017-07-26 18:21:44,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064504000 ms.0 from job set of time 1501064504000 ms
2017-07-26 18:21:44,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:44,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2301 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:44,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2301 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:44,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:44,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:44,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2301 (KafkaRDD[2301] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:44,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2301 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:21:44,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2301_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:21:44,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2301_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:44,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2301 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:44,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2301 (KafkaRDD[2301] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:44,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2301.0 with 2 tasks
2017-07-26 18:21:44,070 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2301.0 (TID 4602, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:44,071 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2301.0 (TID 4603, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:44,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2301.0 (TID 4603)
2017-07-26 18:21:44,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2301.0 (TID 4602)
2017-07-26 18:21:44,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:44,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2301.0 (TID 4603). 635 bytes result sent to driver
2017-07-26 18:21:44,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:44,082 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2301.0 (TID 4603) in 10 ms on localhost (1/2)
2017-07-26 18:21:44,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2301.0 (TID 4602). 714 bytes result sent to driver
2017-07-26 18:21:44,086 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2301.0 (TID 4602) in 16 ms on localhost (2/2)
2017-07-26 18:21:44,086 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2301 (foreachPartition at streamingProcessTest.scala:48) finished in 0.017 s
2017-07-26 18:21:44,086 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2301.0, whose tasks have all completed, from pool 
2017-07-26 18:21:44,086 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2301 finished: foreachPartition at streamingProcessTest.scala:48, took 0.037430 s
2017-07-26 18:21:44,087 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064504000 ms.0 from job set of time 1501064504000 ms
2017-07-26 18:21:44,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2300 from persistence list
2017-07-26 18:21:44,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.087 s for time 1501064504000 ms (execution: 0.071 s)
2017-07-26 18:21:44,088 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2300
2017-07-26 18:21:44,088 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:44,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064500000 ms
2017-07-26 18:21:46,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064506000 ms
2017-07-26 18:21:46,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064506000 ms.0 from job set of time 1501064506000 ms
2017-07-26 18:21:46,051 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:46,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2302 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:46,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2302 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:46,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:46,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:46,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2302 (KafkaRDD[2302] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:46,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2302 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:21:46,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2302_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:21:46,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2302_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:46,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2302 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:46,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2302 (KafkaRDD[2302] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:46,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2302.0 with 2 tasks
2017-07-26 18:21:46,067 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2302.0 (TID 4604, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:46,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2302.0 (TID 4605, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:46,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2302.0 (TID 4604)
2017-07-26 18:21:46,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2302.0 (TID 4605)
2017-07-26 18:21:46,071 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:46,071 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:46,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2302.0 (TID 4605). 635 bytes result sent to driver
2017-07-26 18:21:46,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2302.0 (TID 4604). 635 bytes result sent to driver
2017-07-26 18:21:46,077 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2302.0 (TID 4604) in 11 ms on localhost (1/2)
2017-07-26 18:21:46,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2302.0 (TID 4605) in 10 ms on localhost (2/2)
2017-07-26 18:21:46,077 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2302.0, whose tasks have all completed, from pool 
2017-07-26 18:21:46,077 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2302 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:21:46,078 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2302 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025820 s
2017-07-26 18:21:46,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064506000 ms.0 from job set of time 1501064506000 ms
2017-07-26 18:21:46,078 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2301 from persistence list
2017-07-26 18:21:46,078 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.078 s for time 1501064506000 ms (execution: 0.057 s)
2017-07-26 18:21:46,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:46,079 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2301
2017-07-26 18:21:46,079 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064502000 ms
2017-07-26 18:21:48,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064508000 ms
2017-07-26 18:21:48,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064508000 ms.0 from job set of time 1501064508000 ms
2017-07-26 18:21:48,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2303 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2303 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:48,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:48,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2303 (KafkaRDD[2303] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:48,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2303 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:21:48,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2303_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.3 MB)
2017-07-26 18:21:48,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2303_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:48,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2303 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:48,075 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2289_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:48,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2303 (KafkaRDD[2303] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:48,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2303.0 with 2 tasks
2017-07-26 18:21:48,077 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2303.0 (TID 4606, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:48,078 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2303.0 (TID 4607, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:48,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2303.0 (TID 4607)
2017-07-26 18:21:48,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2303.0 (TID 4606)
2017-07-26 18:21:48,078 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2290_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:48,081 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2291_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:48,082 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:48,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:48,084 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2292_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:21:48,085 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2303.0 (TID 4606). 714 bytes result sent to driver
2017-07-26 18:21:48,085 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2303.0 (TID 4607). 714 bytes result sent to driver
2017-07-26 18:21:48,086 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2293_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:48,087 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2303.0 (TID 4607) in 10 ms on localhost (1/2)
2017-07-26 18:21:48,087 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2303.0 (TID 4606) in 11 ms on localhost (2/2)
2017-07-26 18:21:48,087 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2303.0, whose tasks have all completed, from pool 
2017-07-26 18:21:48,088 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2303 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:21:48,088 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2303 finished: foreachPartition at streamingProcessTest.scala:48, took 0.040605 s
2017-07-26 18:21:48,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064508000 ms.0 from job set of time 1501064508000 ms
2017-07-26 18:21:48,088 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2294_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:48,088 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.088 s for time 1501064508000 ms (execution: 0.072 s)
2017-07-26 18:21:48,088 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2302 from persistence list
2017-07-26 18:21:48,089 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2302
2017-07-26 18:21:48,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:48,089 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064504000 ms
2017-07-26 18:21:48,090 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2295_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:48,091 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2296_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:48,093 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2297_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:48,094 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2298_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:48,097 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2299_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:48,098 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2300_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:48,100 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2301_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:48,101 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2302_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:50,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064510000 ms
2017-07-26 18:21:50,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064510000 ms.0 from job set of time 1501064510000 ms
2017-07-26 18:21:50,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2304 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2304 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:50,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:50,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2304 (KafkaRDD[2304] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:50,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2304 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:21:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2304_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:21:50,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2304_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2304 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:50,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2304 (KafkaRDD[2304] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:50,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2304.0 with 2 tasks
2017-07-26 18:21:50,056 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2304.0 (TID 4608, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:50,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2304.0 (TID 4609, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:50,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2304.0 (TID 4609)
2017-07-26 18:21:50,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2304.0 (TID 4608)
2017-07-26 18:21:50,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:50,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:50,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2304.0 (TID 4608). 635 bytes result sent to driver
2017-07-26 18:21:50,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2304.0 (TID 4609). 635 bytes result sent to driver
2017-07-26 18:21:50,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2304.0 (TID 4608) in 10 ms on localhost (1/2)
2017-07-26 18:21:50,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2304.0 (TID 4609) in 9 ms on localhost (2/2)
2017-07-26 18:21:50,066 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2304.0, whose tasks have all completed, from pool 
2017-07-26 18:21:50,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2304 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:21:50,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2304 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022385 s
2017-07-26 18:21:50,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064510000 ms.0 from job set of time 1501064510000 ms
2017-07-26 18:21:50,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501064510000 ms (execution: 0.051 s)
2017-07-26 18:21:50,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2303 from persistence list
2017-07-26 18:21:50,067 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2303
2017-07-26 18:21:50,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:50,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064506000 ms
2017-07-26 18:21:52,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064512000 ms
2017-07-26 18:21:52,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064512000 ms.0 from job set of time 1501064512000 ms
2017-07-26 18:21:52,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2305 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2305 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:52,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:52,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2305 (KafkaRDD[2305] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:52,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2305 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:21:52,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2305_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:21:52,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2305_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:52,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2305 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:52,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2305 (KafkaRDD[2305] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:52,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2305.0 with 2 tasks
2017-07-26 18:21:52,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2305.0 (TID 4610, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:52,060 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2305.0 (TID 4611, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:52,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2305.0 (TID 4610)
2017-07-26 18:21:52,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2305.0 (TID 4611)
2017-07-26 18:21:52,063 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:52,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:52,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2305.0 (TID 4611). 714 bytes result sent to driver
2017-07-26 18:21:52,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2305.0 (TID 4610). 714 bytes result sent to driver
2017-07-26 18:21:52,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2305.0 (TID 4610) in 8 ms on localhost (1/2)
2017-07-26 18:21:52,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2305.0 (TID 4611) in 8 ms on localhost (2/2)
2017-07-26 18:21:52,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2305.0, whose tasks have all completed, from pool 
2017-07-26 18:21:52,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2305 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:21:52,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2305 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020748 s
2017-07-26 18:21:52,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064512000 ms.0 from job set of time 1501064512000 ms
2017-07-26 18:21:52,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501064512000 ms (execution: 0.051 s)
2017-07-26 18:21:52,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2304 from persistence list
2017-07-26 18:21:52,070 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2304
2017-07-26 18:21:52,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:52,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064508000 ms
2017-07-26 18:21:54,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064514000 ms
2017-07-26 18:21:54,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064514000 ms.0 from job set of time 1501064514000 ms
2017-07-26 18:21:54,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:54,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2306 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:54,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2306 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:54,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:54,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:54,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2306 (KafkaRDD[2306] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:54,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2306 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:21:54,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2306_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:21:54,062 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2306_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:21:54,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2306 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:54,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2306 (KafkaRDD[2306] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:54,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2306.0 with 2 tasks
2017-07-26 18:21:54,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2306.0 (TID 4612, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:54,065 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2306.0 (TID 4613, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:54,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2306.0 (TID 4613)
2017-07-26 18:21:54,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2306.0 (TID 4612)
2017-07-26 18:21:54,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:54,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:54,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2306.0 (TID 4613). 635 bytes result sent to driver
2017-07-26 18:21:54,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2306.0 (TID 4612). 635 bytes result sent to driver
2017-07-26 18:21:54,072 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2306.0 (TID 4613) in 8 ms on localhost (1/2)
2017-07-26 18:21:54,072 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2306.0 (TID 4612) in 9 ms on localhost (2/2)
2017-07-26 18:21:54,072 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2306.0, whose tasks have all completed, from pool 
2017-07-26 18:21:54,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2306 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:21:54,073 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2306 finished: foreachPartition at streamingProcessTest.scala:48, took 0.023872 s
2017-07-26 18:21:54,073 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064514000 ms.0 from job set of time 1501064514000 ms
2017-07-26 18:21:54,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.073 s for time 1501064514000 ms (execution: 0.057 s)
2017-07-26 18:21:54,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2305 from persistence list
2017-07-26 18:21:54,074 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2305
2017-07-26 18:21:54,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:54,074 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064510000 ms
2017-07-26 18:21:56,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064516000 ms
2017-07-26 18:21:56,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064516000 ms.0 from job set of time 1501064516000 ms
2017-07-26 18:21:56,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:56,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2307 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:56,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2307 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:56,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:56,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:56,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2307 (KafkaRDD[2307] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:56,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2307 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:21:56,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2307_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:21:56,066 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2307_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:56,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2307 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:56,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2307 (KafkaRDD[2307] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:56,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2307.0 with 2 tasks
2017-07-26 18:21:56,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2307.0 (TID 4614, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:56,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2307.0 (TID 4615, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:56,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2307.0 (TID 4615)
2017-07-26 18:21:56,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2307.0 (TID 4614)
2017-07-26 18:21:56,073 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:56,073 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:56,076 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2307.0 (TID 4614). 635 bytes result sent to driver
2017-07-26 18:21:56,076 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2307.0 (TID 4615). 635 bytes result sent to driver
2017-07-26 18:21:56,079 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2307.0 (TID 4614) in 11 ms on localhost (1/2)
2017-07-26 18:21:56,079 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2307.0 (TID 4615) in 10 ms on localhost (2/2)
2017-07-26 18:21:56,079 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2307.0, whose tasks have all completed, from pool 
2017-07-26 18:21:56,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2307 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:21:56,080 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2307 finished: foreachPartition at streamingProcessTest.scala:48, took 0.032058 s
2017-07-26 18:21:56,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064516000 ms.0 from job set of time 1501064516000 ms
2017-07-26 18:21:56,081 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.081 s for time 1501064516000 ms (execution: 0.065 s)
2017-07-26 18:21:56,081 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2306 from persistence list
2017-07-26 18:21:56,082 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2306
2017-07-26 18:21:56,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:56,082 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064512000 ms
2017-07-26 18:21:58,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064518000 ms
2017-07-26 18:21:58,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064518000 ms.0 from job set of time 1501064518000 ms
2017-07-26 18:21:58,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:21:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2308 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:21:58,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2308 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:21:58,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:21:58,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:21:58,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2308 (KafkaRDD[2308] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:21:58,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2308 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:21:58,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2308_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:21:58,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2308_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:21:58,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2308 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:21:58,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2308 (KafkaRDD[2308] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:21:58,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2308.0 with 2 tasks
2017-07-26 18:21:58,063 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2308.0 (TID 4616, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:21:58,064 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2308.0 (TID 4617, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:21:58,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2308.0 (TID 4616)
2017-07-26 18:21:58,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2308.0 (TID 4617)
2017-07-26 18:21:58,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:21:58,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:21:58,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2308.0 (TID 4616). 635 bytes result sent to driver
2017-07-26 18:21:58,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2308.0 (TID 4617). 635 bytes result sent to driver
2017-07-26 18:21:58,072 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2308.0 (TID 4616) in 11 ms on localhost (1/2)
2017-07-26 18:21:58,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2308.0 (TID 4617) in 10 ms on localhost (2/2)
2017-07-26 18:21:58,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2308.0, whose tasks have all completed, from pool 
2017-07-26 18:21:58,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2308 (foreachPartition at streamingProcessTest.scala:48) finished in 0.012 s
2017-07-26 18:21:58,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2308 finished: foreachPartition at streamingProcessTest.scala:48, took 0.025136 s
2017-07-26 18:21:58,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064518000 ms.0 from job set of time 1501064518000 ms
2017-07-26 18:21:58,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1501064518000 ms (execution: 0.057 s)
2017-07-26 18:21:58,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2307 from persistence list
2017-07-26 18:21:58,075 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2307
2017-07-26 18:21:58,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:21:58,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064514000 ms
2017-07-26 18:22:00,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064520000 ms
2017-07-26 18:22:00,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064520000 ms.0 from job set of time 1501064520000 ms
2017-07-26 18:22:00,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2309 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2309 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2309 (KafkaRDD[2309] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:00,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2309 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:22:00,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2309_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:22:00,060 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2309_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2309 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2309 (KafkaRDD[2309] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2309.0 with 2 tasks
2017-07-26 18:22:00,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2309.0 (TID 4618, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:00,063 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2309.0 (TID 4619, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:00,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2309.0 (TID 4619)
2017-07-26 18:22:00,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2309.0 (TID 4618)
2017-07-26 18:22:00,066 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:00,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:00,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2309.0 (TID 4618). 635 bytes result sent to driver
2017-07-26 18:22:00,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2309.0 (TID 4619). 635 bytes result sent to driver
2017-07-26 18:22:00,069 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2309.0 (TID 4618) in 7 ms on localhost (1/2)
2017-07-26 18:22:00,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2309.0 (TID 4619) in 6 ms on localhost (2/2)
2017-07-26 18:22:00,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2309 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:22:00,070 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2309.0, whose tasks have all completed, from pool 
2017-07-26 18:22:00,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2309 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022496 s
2017-07-26 18:22:00,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064520000 ms.0 from job set of time 1501064520000 ms
2017-07-26 18:22:00,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501064520000 ms (execution: 0.054 s)
2017-07-26 18:22:00,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2308 from persistence list
2017-07-26 18:22:00,071 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2308
2017-07-26 18:22:00,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:00,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064516000 ms
2017-07-26 18:22:02,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064522000 ms
2017-07-26 18:22:02,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064522000 ms.0 from job set of time 1501064522000 ms
2017-07-26 18:22:02,046 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:02,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2310 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:02,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2310 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:02,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:02,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:02,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2310 (KafkaRDD[2310] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:02,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2310 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:22:02,059 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2310_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:22:02,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2310_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:02,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2310 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:02,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2310 (KafkaRDD[2310] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:02,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2310.0 with 2 tasks
2017-07-26 18:22:02,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2310.0 (TID 4620, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:02,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2310.0 (TID 4621, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:02,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2310.0 (TID 4621)
2017-07-26 18:22:02,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2310.0 (TID 4620)
2017-07-26 18:22:02,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:02,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:02,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2310.0 (TID 4621). 635 bytes result sent to driver
2017-07-26 18:22:02,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2310.0 (TID 4620). 635 bytes result sent to driver
2017-07-26 18:22:02,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2310.0 (TID 4621) in 9 ms on localhost (1/2)
2017-07-26 18:22:02,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2310.0 (TID 4620) in 13 ms on localhost (2/2)
2017-07-26 18:22:02,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2310.0, whose tasks have all completed, from pool 
2017-07-26 18:22:02,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2310 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:22:02,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2310 finished: foreachPartition at streamingProcessTest.scala:48, took 0.028729 s
2017-07-26 18:22:02,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064522000 ms.0 from job set of time 1501064522000 ms
2017-07-26 18:22:02,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2309 from persistence list
2017-07-26 18:22:02,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501064522000 ms (execution: 0.057 s)
2017-07-26 18:22:02,076 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2309
2017-07-26 18:22:02,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:02,077 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064518000 ms
2017-07-26 18:22:04,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064524000 ms
2017-07-26 18:22:04,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064524000 ms.0 from job set of time 1501064524000 ms
2017-07-26 18:22:04,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:04,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2311 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:04,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2311 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:04,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:04,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:04,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2311 (KafkaRDD[2311] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:04,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2311 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:22:04,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2311_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:22:04,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2311_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:04,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2311 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:04,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2311 (KafkaRDD[2311] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:04,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2311.0 with 2 tasks
2017-07-26 18:22:04,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2311.0 (TID 4622, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:04,059 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2311.0 (TID 4623, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:04,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2311.0 (TID 4623)
2017-07-26 18:22:04,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2311.0 (TID 4622)
2017-07-26 18:22:04,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:04,062 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:04,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2311.0 (TID 4623). 635 bytes result sent to driver
2017-07-26 18:22:04,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2311.0 (TID 4622). 635 bytes result sent to driver
2017-07-26 18:22:04,066 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2311.0 (TID 4623) in 7 ms on localhost (1/2)
2017-07-26 18:22:04,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2311.0 (TID 4622) in 8 ms on localhost (2/2)
2017-07-26 18:22:04,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2311.0, whose tasks have all completed, from pool 
2017-07-26 18:22:04,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2311 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:22:04,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2311 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019506 s
2017-07-26 18:22:04,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064524000 ms.0 from job set of time 1501064524000 ms
2017-07-26 18:22:04,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2310 from persistence list
2017-07-26 18:22:04,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1501064524000 ms (execution: 0.051 s)
2017-07-26 18:22:04,068 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2310
2017-07-26 18:22:04,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:04,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064520000 ms
2017-07-26 18:22:06,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064526000 ms
2017-07-26 18:22:06,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064526000 ms.0 from job set of time 1501064526000 ms
2017-07-26 18:22:06,050 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:06,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2312 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:06,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2312 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:06,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:06,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:06,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2312 (KafkaRDD[2312] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:06,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2312 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:22:06,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2312_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:22:06,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2312_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:06,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2312 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:06,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2312 (KafkaRDD[2312] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:06,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2312.0 with 2 tasks
2017-07-26 18:22:06,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2312.0 (TID 4624, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:06,070 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2312.0 (TID 4625, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:06,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2312.0 (TID 4624)
2017-07-26 18:22:06,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2312.0 (TID 4625)
2017-07-26 18:22:06,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:06,074 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:06,077 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2312.0 (TID 4625). 635 bytes result sent to driver
2017-07-26 18:22:06,077 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2312.0 (TID 4624). 635 bytes result sent to driver
2017-07-26 18:22:06,080 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2312.0 (TID 4625) in 10 ms on localhost (1/2)
2017-07-26 18:22:06,080 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2312.0 (TID 4624) in 12 ms on localhost (2/2)
2017-07-26 18:22:06,081 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2312.0, whose tasks have all completed, from pool 
2017-07-26 18:22:06,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2312 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:22:06,081 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2312 finished: foreachPartition at streamingProcessTest.scala:48, took 0.031008 s
2017-07-26 18:22:06,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064526000 ms.0 from job set of time 1501064526000 ms
2017-07-26 18:22:06,082 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.082 s for time 1501064526000 ms (execution: 0.065 s)
2017-07-26 18:22:06,082 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2311 from persistence list
2017-07-26 18:22:06,083 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2311
2017-07-26 18:22:06,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:06,083 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064522000 ms
2017-07-26 18:22:08,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064528000 ms
2017-07-26 18:22:08,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064528000 ms.0 from job set of time 1501064528000 ms
2017-07-26 18:22:08,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:08,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2313 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:08,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2313 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:08,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:08,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:08,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2313 (KafkaRDD[2313] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:08,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2313 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:22:08,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2313_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:22:08,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2313_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:22:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2313 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2313 (KafkaRDD[2313] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:08,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2313.0 with 2 tasks
2017-07-26 18:22:08,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2313.0 (TID 4626, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:08,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2313.0 (TID 4627, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:08,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2313.0 (TID 4627)
2017-07-26 18:22:08,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2313.0 (TID 4626)
2017-07-26 18:22:08,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:08,060 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:08,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2313.0 (TID 4626). 635 bytes result sent to driver
2017-07-26 18:22:08,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2313.0 (TID 4627). 714 bytes result sent to driver
2017-07-26 18:22:08,065 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2313.0 (TID 4626) in 9 ms on localhost (1/2)
2017-07-26 18:22:08,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2313.0 (TID 4627) in 8 ms on localhost (2/2)
2017-07-26 18:22:08,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2313.0, whose tasks have all completed, from pool 
2017-07-26 18:22:08,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2313 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:22:08,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2313 finished: foreachPartition at streamingProcessTest.scala:48, took 0.021965 s
2017-07-26 18:22:08,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064528000 ms.0 from job set of time 1501064528000 ms
2017-07-26 18:22:08,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1501064528000 ms (execution: 0.048 s)
2017-07-26 18:22:08,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2312 from persistence list
2017-07-26 18:22:08,067 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2312
2017-07-26 18:22:08,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:08,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064524000 ms
2017-07-26 18:22:10,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064530000 ms
2017-07-26 18:22:10,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064530000 ms.0 from job set of time 1501064530000 ms
2017-07-26 18:22:10,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2314 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2314 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:10,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2314 (KafkaRDD[2314] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:10,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2314 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:22:10,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2314_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:22:10,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2314_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:22:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2314 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2314 (KafkaRDD[2314] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2314.0 with 2 tasks
2017-07-26 18:22:10,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2314.0 (TID 4628, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:10,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2314.0 (TID 4629, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:10,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2314.0 (TID 4629)
2017-07-26 18:22:10,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2314.0 (TID 4628)
2017-07-26 18:22:10,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:10,067 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:10,070 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2314.0 (TID 4628). 722 bytes result sent to driver
2017-07-26 18:22:10,070 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2314.0 (TID 4629). 722 bytes result sent to driver
2017-07-26 18:22:10,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2314.0 (TID 4628) in 11 ms on localhost (1/2)
2017-07-26 18:22:10,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2314.0 (TID 4629) in 10 ms on localhost (2/2)
2017-07-26 18:22:10,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2314.0, whose tasks have all completed, from pool 
2017-07-26 18:22:10,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2314 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:22:10,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2314 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027508 s
2017-07-26 18:22:10,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064530000 ms.0 from job set of time 1501064530000 ms
2017-07-26 18:22:10,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501064530000 ms (execution: 0.059 s)
2017-07-26 18:22:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2313 from persistence list
2017-07-26 18:22:10,076 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2313
2017-07-26 18:22:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:10,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064526000 ms
2017-07-26 18:22:12,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064532000 ms
2017-07-26 18:22:12,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064532000 ms.0 from job set of time 1501064532000 ms
2017-07-26 18:22:12,052 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:12,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2315 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:12,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2315 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:12,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:12,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:12,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2315 (KafkaRDD[2315] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:12,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2315 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:22:12,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2315_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:22:12,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2315_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:22:12,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2315 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:12,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2315 (KafkaRDD[2315] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:12,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2315.0 with 2 tasks
2017-07-26 18:22:12,076 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2315.0 (TID 4630, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:12,077 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2315.0 (TID 4631, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:12,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2315.0 (TID 4631)
2017-07-26 18:22:12,078 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2315.0 (TID 4630)
2017-07-26 18:22:12,084 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:12,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:12,088 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2315.0 (TID 4630). 714 bytes result sent to driver
2017-07-26 18:22:12,088 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2315.0 (TID 4631). 714 bytes result sent to driver
2017-07-26 18:22:12,090 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2315.0 (TID 4630) in 15 ms on localhost (1/2)
2017-07-26 18:22:12,091 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2315.0 (TID 4631) in 15 ms on localhost (2/2)
2017-07-26 18:22:12,091 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2315.0, whose tasks have all completed, from pool 
2017-07-26 18:22:12,091 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2315 (foreachPartition at streamingProcessTest.scala:48) finished in 0.017 s
2017-07-26 18:22:12,092 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2315 finished: foreachPartition at streamingProcessTest.scala:48, took 0.039601 s
2017-07-26 18:22:12,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064532000 ms.0 from job set of time 1501064532000 ms
2017-07-26 18:22:12,092 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.092 s for time 1501064532000 ms (execution: 0.073 s)
2017-07-26 18:22:12,092 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2314 from persistence list
2017-07-26 18:22:12,093 [block-manager-slave-async-thread-pool-21] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2314
2017-07-26 18:22:12,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:12,093 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064528000 ms
2017-07-26 18:22:14,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064534000 ms
2017-07-26 18:22:14,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064534000 ms.0 from job set of time 1501064534000 ms
2017-07-26 18:22:14,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:14,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2316 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:14,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2316 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:14,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:14,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:14,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2316 (KafkaRDD[2316] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:14,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2316 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:22:14,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2316_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.4 MB)
2017-07-26 18:22:14,056 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2316_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:22:14,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2316 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:14,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2316 (KafkaRDD[2316] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:14,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2316.0 with 2 tasks
2017-07-26 18:22:14,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2316.0 (TID 4632, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:14,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2316.0 (TID 4633, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:14,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2316.0 (TID 4633)
2017-07-26 18:22:14,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2316.0 (TID 4632)
2017-07-26 18:22:14,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:14,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:14,066 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2316.0 (TID 4632). 635 bytes result sent to driver
2017-07-26 18:22:14,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2316.0 (TID 4633). 635 bytes result sent to driver
2017-07-26 18:22:14,068 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2316.0 (TID 4632) in 10 ms on localhost (1/2)
2017-07-26 18:22:14,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2316.0 (TID 4633) in 9 ms on localhost (2/2)
2017-07-26 18:22:14,069 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2316.0, whose tasks have all completed, from pool 
2017-07-26 18:22:14,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2316 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:22:14,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2316 finished: foreachPartition at streamingProcessTest.scala:48, took 0.024149 s
2017-07-26 18:22:14,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064534000 ms.0 from job set of time 1501064534000 ms
2017-07-26 18:22:14,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501064534000 ms (execution: 0.050 s)
2017-07-26 18:22:14,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2315 from persistence list
2017-07-26 18:22:14,070 [block-manager-slave-async-thread-pool-24] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2315
2017-07-26 18:22:14,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:14,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064530000 ms
2017-07-26 18:22:16,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064536000 ms
2017-07-26 18:22:16,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064536000 ms.0 from job set of time 1501064536000 ms
2017-07-26 18:22:16,048 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:16,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2317 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2317 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:16,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2317 (KafkaRDD[2317] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:16,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2316_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:22:16,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2317 stored as values in memory (estimated size 23.6 KB, free 413.4 MB)
2017-07-26 18:22:16,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2303_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:22:16,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2304_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:22:16,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2317_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:22:16,071 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2305_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:16,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2317_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.7 MB)
2017-07-26 18:22:16,072 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2317 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:16,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2317 (KafkaRDD[2317] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:16,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2317.0 with 2 tasks
2017-07-26 18:22:16,074 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2306_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:16,074 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2317.0 (TID 4634, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:16,075 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2317.0 (TID 4635, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:16,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2317.0 (TID 4635)
2017-07-26 18:22:16,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2317.0 (TID 4634)
2017-07-26 18:22:16,076 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2307_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:16,077 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:16,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:16,078 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2308_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:16,079 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2317.0 (TID 4635). 714 bytes result sent to driver
2017-07-26 18:22:16,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2317.0 (TID 4634). 714 bytes result sent to driver
2017-07-26 18:22:16,081 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2309_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:16,081 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2317.0 (TID 4635) in 6 ms on localhost (1/2)
2017-07-26 18:22:16,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2317.0 (TID 4634) in 9 ms on localhost (2/2)
2017-07-26 18:22:16,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2317 (foreachPartition at streamingProcessTest.scala:48) finished in 0.010 s
2017-07-26 18:22:16,083 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2317.0, whose tasks have all completed, from pool 
2017-07-26 18:22:16,083 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2310_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:16,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2317 finished: foreachPartition at streamingProcessTest.scala:48, took 0.035191 s
2017-07-26 18:22:16,083 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064536000 ms.0 from job set of time 1501064536000 ms
2017-07-26 18:22:16,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.083 s for time 1501064536000 ms (execution: 0.067 s)
2017-07-26 18:22:16,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2316 from persistence list
2017-07-26 18:22:16,084 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2316
2017-07-26 18:22:16,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:16,084 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064532000 ms
2017-07-26 18:22:16,084 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2311_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:16,086 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2312_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:22:16,087 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2313_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:22:16,089 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2314_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:22:16,090 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2315_piece0 on 192.168.31.105:53944 in memory (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:22:18,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064538000 ms
2017-07-26 18:22:18,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064538000 ms.0 from job set of time 1501064538000 ms
2017-07-26 18:22:18,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:18,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2318 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:18,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2318 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:18,029 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:18,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:18,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2318 (KafkaRDD[2318] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:18,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2318 stored as values in memory (estimated size 23.6 KB, free 413.9 MB)
2017-07-26 18:22:18,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2318_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:22:18,035 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2318_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:22:18,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2318 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:18,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2318 (KafkaRDD[2318] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:18,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2318.0 with 2 tasks
2017-07-26 18:22:18,037 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2318.0 (TID 4636, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:18,038 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2318.0 (TID 4637, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:18,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2318.0 (TID 4637)
2017-07-26 18:22:18,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2318.0 (TID 4636)
2017-07-26 18:22:18,039 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:18,039 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:18,041 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2318.0 (TID 4637). 714 bytes result sent to driver
2017-07-26 18:22:18,041 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2318.0 (TID 4636). 714 bytes result sent to driver
2017-07-26 18:22:18,042 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2318.0 (TID 4637) in 5 ms on localhost (1/2)
2017-07-26 18:22:18,043 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2318.0 (TID 4636) in 5 ms on localhost (2/2)
2017-07-26 18:22:18,043 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2318.0, whose tasks have all completed, from pool 
2017-07-26 18:22:18,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2318 (foreachPartition at streamingProcessTest.scala:48) finished in 0.007 s
2017-07-26 18:22:18,043 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2318 finished: foreachPartition at streamingProcessTest.scala:48, took 0.014042 s
2017-07-26 18:22:18,043 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064538000 ms.0 from job set of time 1501064538000 ms
2017-07-26 18:22:18,044 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.043 s for time 1501064538000 ms (execution: 0.030 s)
2017-07-26 18:22:18,044 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2317 from persistence list
2017-07-26 18:22:18,044 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2317
2017-07-26 18:22:18,044 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:18,044 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064534000 ms
2017-07-26 18:22:20,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064540000 ms
2017-07-26 18:22:20,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064540000 ms.0 from job set of time 1501064540000 ms
2017-07-26 18:22:20,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2319 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2319 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2319 (KafkaRDD[2319] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2319 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:22:20,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2319_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:22:20,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2319_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:22:20,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2319 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:20,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2319 (KafkaRDD[2319] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:20,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2319.0 with 2 tasks
2017-07-26 18:22:20,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2319.0 (TID 4638, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:20,054 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2319.0 (TID 4639, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:20,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2319.0 (TID 4638)
2017-07-26 18:22:20,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2319.0 (TID 4639)
2017-07-26 18:22:20,057 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:20,057 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:20,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2319.0 (TID 4639). 635 bytes result sent to driver
2017-07-26 18:22:20,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2319.0 (TID 4638). 635 bytes result sent to driver
2017-07-26 18:22:20,061 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2319.0 (TID 4639) in 6 ms on localhost (1/2)
2017-07-26 18:22:20,061 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2319.0 (TID 4638) in 8 ms on localhost (2/2)
2017-07-26 18:22:20,061 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2319.0, whose tasks have all completed, from pool 
2017-07-26 18:22:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2319 (foreachPartition at streamingProcessTest.scala:48) finished in 0.008 s
2017-07-26 18:22:20,062 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2319 finished: foreachPartition at streamingProcessTest.scala:48, took 0.019032 s
2017-07-26 18:22:20,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064540000 ms.0 from job set of time 1501064540000 ms
2017-07-26 18:22:20,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.062 s for time 1501064540000 ms (execution: 0.045 s)
2017-07-26 18:22:20,062 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2318 from persistence list
2017-07-26 18:22:20,063 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2318
2017-07-26 18:22:20,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:20,063 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064536000 ms
2017-07-26 18:22:22,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064542000 ms
2017-07-26 18:22:22,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064542000 ms.0 from job set of time 1501064542000 ms
2017-07-26 18:22:22,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:22,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2320 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:22,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2320 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:22,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:22,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:22,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2320 (KafkaRDD[2320] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:22,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2320 stored as values in memory (estimated size 23.6 KB, free 413.8 MB)
2017-07-26 18:22:22,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2320_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.8 MB)
2017-07-26 18:22:22,066 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2320_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.9 MB)
2017-07-26 18:22:22,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2320 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:22,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2320 (KafkaRDD[2320] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:22,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2320.0 with 2 tasks
2017-07-26 18:22:22,070 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2320.0 (TID 4640, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:22,071 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2320.0 (TID 4641, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:22,072 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2320.0 (TID 4640)
2017-07-26 18:22:22,072 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2320.0 (TID 4641)
2017-07-26 18:22:22,077 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:22,078 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:22,080 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2320.0 (TID 4640). 635 bytes result sent to driver
2017-07-26 18:22:22,080 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2320.0 (TID 4641). 635 bytes result sent to driver
2017-07-26 18:22:22,082 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2320.0 (TID 4640) in 13 ms on localhost (1/2)
2017-07-26 18:22:22,083 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2320.0 (TID 4641) in 11 ms on localhost (2/2)
2017-07-26 18:22:22,083 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2320.0, whose tasks have all completed, from pool 
2017-07-26 18:22:22,083 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2320 (foreachPartition at streamingProcessTest.scala:48) finished in 0.014 s
2017-07-26 18:22:22,083 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2320 finished: foreachPartition at streamingProcessTest.scala:48, took 0.035760 s
2017-07-26 18:22:22,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064542000 ms.0 from job set of time 1501064542000 ms
2017-07-26 18:22:22,084 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.084 s for time 1501064542000 ms (execution: 0.068 s)
2017-07-26 18:22:22,084 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2319 from persistence list
2017-07-26 18:22:22,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:22,085 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2319
2017-07-26 18:22:22,085 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064538000 ms
2017-07-26 18:22:24,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064544000 ms
2017-07-26 18:22:24,014 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064544000 ms.0 from job set of time 1501064544000 ms
2017-07-26 18:22:24,037 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:24,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2321 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:24,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2321 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:24,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:24,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:24,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2321 (KafkaRDD[2321] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:24,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2321 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:22:24,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2321_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:22:24,050 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2321_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:24,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2321 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:24,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2321 (KafkaRDD[2321] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:24,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2321.0 with 2 tasks
2017-07-26 18:22:24,054 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2321.0 (TID 4642, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:24,055 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2321.0 (TID 4643, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:24,055 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2321.0 (TID 4642)
2017-07-26 18:22:24,055 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2321.0 (TID 4643)
2017-07-26 18:22:24,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:24,059 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:24,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2321.0 (TID 4642). 635 bytes result sent to driver
2017-07-26 18:22:24,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2321.0 (TID 4643). 635 bytes result sent to driver
2017-07-26 18:22:24,065 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2321.0 (TID 4642) in 12 ms on localhost (1/2)
2017-07-26 18:22:24,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2321.0 (TID 4643) in 12 ms on localhost (2/2)
2017-07-26 18:22:24,066 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2321.0, whose tasks have all completed, from pool 
2017-07-26 18:22:24,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2321 (foreachPartition at streamingProcessTest.scala:48) finished in 0.013 s
2017-07-26 18:22:24,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2321 finished: foreachPartition at streamingProcessTest.scala:48, took 0.029702 s
2017-07-26 18:22:24,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064544000 ms.0 from job set of time 1501064544000 ms
2017-07-26 18:22:24,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2320 from persistence list
2017-07-26 18:22:24,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.067 s for time 1501064544000 ms (execution: 0.053 s)
2017-07-26 18:22:24,068 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2320
2017-07-26 18:22:24,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:24,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064540000 ms
2017-07-26 18:22:26,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064546000 ms
2017-07-26 18:22:26,013 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064546000 ms.0 from job set of time 1501064546000 ms
2017-07-26 18:22:26,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:26,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2322 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:26,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2322 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:26,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:26,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:26,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2322 (KafkaRDD[2322] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:26,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2322 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:22:26,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2322_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.7 MB)
2017-07-26 18:22:26,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2322_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:26,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2322 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:26,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2322 (KafkaRDD[2322] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:26,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2322.0 with 2 tasks
2017-07-26 18:22:26,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2322.0 (TID 4644, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:26,067 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2322.0 (TID 4645, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:26,067 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2322.0 (TID 4644)
2017-07-26 18:22:26,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2322.0 (TID 4645)
2017-07-26 18:22:26,069 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:26,069 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:26,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2322.0 (TID 4645). 635 bytes result sent to driver
2017-07-26 18:22:26,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2322.0 (TID 4644). 635 bytes result sent to driver
2017-07-26 18:22:26,073 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2322.0 (TID 4645) in 7 ms on localhost (1/2)
2017-07-26 18:22:26,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2322.0 (TID 4644) in 9 ms on localhost (2/2)
2017-07-26 18:22:26,073 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2322.0, whose tasks have all completed, from pool 
2017-07-26 18:22:26,073 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2322 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:22:26,074 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2322 finished: foreachPartition at streamingProcessTest.scala:48, took 0.029473 s
2017-07-26 18:22:26,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064546000 ms.0 from job set of time 1501064546000 ms
2017-07-26 18:22:26,074 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.074 s for time 1501064546000 ms (execution: 0.061 s)
2017-07-26 18:22:26,074 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2321 from persistence list
2017-07-26 18:22:26,075 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2321
2017-07-26 18:22:26,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:26,075 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064542000 ms
2017-07-26 18:22:28,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064548000 ms
2017-07-26 18:22:28,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064548000 ms.0 from job set of time 1501064548000 ms
2017-07-26 18:22:28,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:28,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2323 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:28,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2323 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:28,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:28,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:28,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2323 (KafkaRDD[2323] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:28,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2323 stored as values in memory (estimated size 23.6 KB, free 413.7 MB)
2017-07-26 18:22:28,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2323_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:22:28,054 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2323_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:28,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2323 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:28,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2323 (KafkaRDD[2323] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:28,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2323.0 with 2 tasks
2017-07-26 18:22:28,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2323.0 (TID 4646, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:28,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2323.0 (TID 4647, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:28,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2323.0 (TID 4647)
2017-07-26 18:22:28,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2323.0 (TID 4646)
2017-07-26 18:22:28,059 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:28,060 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:28,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2323.0 (TID 4647). 714 bytes result sent to driver
2017-07-26 18:22:28,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2323.0 (TID 4646). 635 bytes result sent to driver
2017-07-26 18:22:28,064 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2323.0 (TID 4647) in 7 ms on localhost (1/2)
2017-07-26 18:22:28,065 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2323.0 (TID 4646) in 9 ms on localhost (2/2)
2017-07-26 18:22:28,065 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2323.0, whose tasks have all completed, from pool 
2017-07-26 18:22:28,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2323 (foreachPartition at streamingProcessTest.scala:48) finished in 0.009 s
2017-07-26 18:22:28,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2323 finished: foreachPartition at streamingProcessTest.scala:48, took 0.020527 s
2017-07-26 18:22:28,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064548000 ms.0 from job set of time 1501064548000 ms
2017-07-26 18:22:28,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501064548000 ms (execution: 0.050 s)
2017-07-26 18:22:28,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2322 from persistence list
2017-07-26 18:22:28,067 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2322
2017-07-26 18:22:28,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:28,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064544000 ms
2017-07-26 18:22:30,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064550000 ms
2017-07-26 18:22:30,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064550000 ms.0 from job set of time 1501064550000 ms
2017-07-26 18:22:30,047 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:30,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2324 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:30,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2324 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:30,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:30,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:30,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2324 (KafkaRDD[2324] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:30,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2324 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:22:30,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2324_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:22:30,062 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2324_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:30,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2324 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2324 (KafkaRDD[2324] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:30,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2324.0 with 2 tasks
2017-07-26 18:22:30,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2324.0 (TID 4648, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:30,065 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2324.0 (TID 4649, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:30,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2324.0 (TID 4649)
2017-07-26 18:22:30,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2324.0 (TID 4648)
2017-07-26 18:22:30,068 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:30,068 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:30,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2324.0 (TID 4648). 714 bytes result sent to driver
2017-07-26 18:22:30,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2324.0 (TID 4649). 714 bytes result sent to driver
2017-07-26 18:22:30,074 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2324.0 (TID 4648) in 10 ms on localhost (1/2)
2017-07-26 18:22:30,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2324.0 (TID 4649) in 9 ms on localhost (2/2)
2017-07-26 18:22:30,074 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2324.0, whose tasks have all completed, from pool 
2017-07-26 18:22:30,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2324 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:22:30,075 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2324 finished: foreachPartition at streamingProcessTest.scala:48, took 0.027207 s
2017-07-26 18:22:30,075 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064550000 ms.0 from job set of time 1501064550000 ms
2017-07-26 18:22:30,076 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2323 from persistence list
2017-07-26 18:22:30,076 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.075 s for time 1501064550000 ms (execution: 0.058 s)
2017-07-26 18:22:30,076 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2323
2017-07-26 18:22:30,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:30,076 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064546000 ms
2017-07-26 18:22:32,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064552000 ms
2017-07-26 18:22:32,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064552000 ms.0 from job set of time 1501064552000 ms
2017-07-26 18:22:32,024 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:32,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2325 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:32,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2325 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:32,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:32,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:32,025 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2325 (KafkaRDD[2325] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:32,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2325 stored as values in memory (estimated size 23.6 KB, free 413.6 MB)
2017-07-26 18:22:32,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2325_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.6 MB)
2017-07-26 18:22:32,033 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2325_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:32,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2325 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:32,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2325 (KafkaRDD[2325] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:32,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2325.0 with 2 tasks
2017-07-26 18:22:32,035 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2325.0 (TID 4650, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:32,035 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2325.0 (TID 4651, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:32,035 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2325.0 (TID 4651)
2017-07-26 18:22:32,035 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2325.0 (TID 4650)
2017-07-26 18:22:32,037 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:32,037 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:32,038 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2325.0 (TID 4651). 635 bytes result sent to driver
2017-07-26 18:22:32,038 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2325.0 (TID 4650). 635 bytes result sent to driver
2017-07-26 18:22:32,040 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2325.0 (TID 4651) in 5 ms on localhost (1/2)
2017-07-26 18:22:32,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2325.0 (TID 4650) in 6 ms on localhost (2/2)
2017-07-26 18:22:32,040 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2325.0, whose tasks have all completed, from pool 
2017-07-26 18:22:32,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2325 (foreachPartition at streamingProcessTest.scala:48) finished in 0.006 s
2017-07-26 18:22:32,040 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2325 finished: foreachPartition at streamingProcessTest.scala:48, took 0.016210 s
2017-07-26 18:22:32,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064552000 ms.0 from job set of time 1501064552000 ms
2017-07-26 18:22:32,041 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.041 s for time 1501064552000 ms (execution: 0.026 s)
2017-07-26 18:22:32,041 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2324 from persistence list
2017-07-26 18:22:32,041 [block-manager-slave-async-thread-pool-23] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2324
2017-07-26 18:22:32,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:32,041 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064548000 ms
2017-07-26 18:22:34,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501064554000 ms
2017-07-26 18:22:34,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501064554000 ms.0 from job set of time 1501064554000 ms
2017-07-26 18:22:34,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessTest.scala:48
2017-07-26 18:22:34,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2326 (foreachPartition at streamingProcessTest.scala:48) with 2 output partitions
2017-07-26 18:22:34,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2326 (foreachPartition at streamingProcessTest.scala:48)
2017-07-26 18:22:34,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-26 18:22:34,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-26 18:22:34,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2326 (KafkaRDD[2326] at createDirectStream at streamingProcessTest.scala:39), which has no missing parents
2017-07-26 18:22:34,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2326 stored as values in memory (estimated size 23.6 KB, free 413.5 MB)
2017-07-26 18:22:34,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2326_piece0 stored as bytes in memory (estimated size 16.8 KB, free 413.5 MB)
2017-07-26 18:22:34,059 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2326_piece0 in memory on 192.168.31.105:53944 (size: 16.8 KB, free: 413.8 MB)
2017-07-26 18:22:34,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2326 from broadcast at DAGScheduler.scala:1012
2017-07-26 18:22:34,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2326 (KafkaRDD[2326] at createDirectStream at streamingProcessTest.scala:39)
2017-07-26 18:22:34,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2326.0 with 2 tasks
2017-07-26 18:22:34,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2326.0 (TID 4652, localhost, partition 0, ANY, 5658 bytes)
2017-07-26 18:22:34,062 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2326.0 (TID 4653, localhost, partition 1, ANY, 5658 bytes)
2017-07-26 18:22:34,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2326.0 (TID 4653)
2017-07-26 18:22:34,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2326.0 (TID 4652)
2017-07-26 18:22:34,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 396 is the same as ending offset skipping test04 0
2017-07-26 18:22:34,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 415 is the same as ending offset skipping test04 1
2017-07-26 18:22:34,068 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2326.0 (TID 4653). 714 bytes result sent to driver
2017-07-26 18:22:34,068 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2326.0 (TID 4652). 714 bytes result sent to driver
2017-07-26 18:22:34,071 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2326.0 (TID 4653) in 8 ms on localhost (1/2)
2017-07-26 18:22:34,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2326.0 (TID 4652) in 11 ms on localhost (2/2)
2017-07-26 18:22:34,071 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2326.0, whose tasks have all completed, from pool 
2017-07-26 18:22:34,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2326 (foreachPartition at streamingProcessTest.scala:48) finished in 0.011 s
2017-07-26 18:22:34,072 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2326 finished: foreachPartition at streamingProcessTest.scala:48, took 0.022609 s
2017-07-26 18:22:34,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501064554000 ms.0 from job set of time 1501064554000 ms
2017-07-26 18:22:34,072 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.072 s for time 1501064554000 ms (execution: 0.053 s)
2017-07-26 18:22:34,072 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2325 from persistence list
2017-07-26 18:22:34,073 [block-manager-slave-async-thread-pool-25] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2325
2017-07-26 18:22:34,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-26 18:22:34,073 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501064550000 ms
2017-07-27 16:18:35,621 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-07-27 16:18:36,003 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-07-27 16:18:36,110 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-07-27 16:18:36,110 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-07-27 16:18:36,111 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-07-27 16:18:36,113 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-07-27 16:18:36,114 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-07-27 16:18:36,888 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 64680.
2017-07-27 16:18:36,911 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-07-27 16:18:36,924 [main] ERROR [org.apache.spark.SparkContext] - Error initializing SparkContext.
java.lang.IllegalArgumentException: System memory 259522560 must be at least 471859200. Please increase heap size using the --driver-memory option or spark.driver.memory in Spark configuration.
	at org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:212) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:194) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:308) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:165) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:259) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:423) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:836) [spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84) [spark-streaming_2.11-2.0.0.jar:2.0.0]
	at streamingExample$.main(streamingExample.scala:18) [test-classes/:na]
	at streamingExample.main(streamingExample.scala) [test-classes/:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_111]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_111]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_111]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_111]
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144) [idea_rt.jar:na]
2017-07-27 16:18:36,929 [main] INFO  [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2017-07-27 16:18:36,933 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2017-07-27 16:20:29,278 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-07-27 16:20:29,486 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-07-27 16:20:29,571 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-07-27 16:20:29,571 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-07-27 16:20:29,572 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-07-27 16:20:29,572 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-07-27 16:20:29,573 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-07-27 16:20:30,292 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 64739.
2017-07-27 16:20:30,312 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-07-27 16:20:30,331 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-07-27 16:20:30,345 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-21a154b1-9663-47ea-9be7-b0239546a853
2017-07-27 16:20:30,370 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-07-27 16:20:30,413 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-07-27 16:20:30,509 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2002ms
2017-07-27 16:20:30,616 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-07-27 16:20:30,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@11c5af9{/jobs,null,AVAILABLE}
2017-07-27 16:20:30,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@55f277{/jobs/json,null,AVAILABLE}
2017-07-27 16:20:30,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1dabb18{/jobs/job,null,AVAILABLE}
2017-07-27 16:20:30,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@8f7922{/jobs/job/json,null,AVAILABLE}
2017-07-27 16:20:30,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1192798{/stages,null,AVAILABLE}
2017-07-27 16:20:30,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@eca7c6{/stages/json,null,AVAILABLE}
2017-07-27 16:20:30,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e6d4b8{/stages/stage,null,AVAILABLE}
2017-07-27 16:20:30,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1e5f0ef{/stages/stage/json,null,AVAILABLE}
2017-07-27 16:20:30,633 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@16a312c{/stages/pool,null,AVAILABLE}
2017-07-27 16:20:30,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@8d6290{/stages/pool/json,null,AVAILABLE}
2017-07-27 16:20:30,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@a0256d{/storage,null,AVAILABLE}
2017-07-27 16:20:30,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0d29e{/storage/json,null,AVAILABLE}
2017-07-27 16:20:30,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f57b3d{/storage/rdd,null,AVAILABLE}
2017-07-27 16:20:30,634 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1850751{/storage/rdd/json,null,AVAILABLE}
2017-07-27 16:20:30,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e3db41{/environment,null,AVAILABLE}
2017-07-27 16:20:30,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@df9a61{/environment/json,null,AVAILABLE}
2017-07-27 16:20:30,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1602954{/executors,null,AVAILABLE}
2017-07-27 16:20:30,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@260766{/executors/json,null,AVAILABLE}
2017-07-27 16:20:30,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3dec30{/executors/threadDump,null,AVAILABLE}
2017-07-27 16:20:30,635 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@a97744{/executors/threadDump/json,null,AVAILABLE}
2017-07-27 16:20:30,640 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@75c20c{/static,null,AVAILABLE}
2017-07-27 16:20:30,640 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@56e013{/,null,AVAILABLE}
2017-07-27 16:20:30,641 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1c9dfc5{/api,null,AVAILABLE}
2017-07-27 16:20:30,642 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@258c2f{/stages/stage/kill,null,AVAILABLE}
2017-07-27 16:20:30,648 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@49d8b2{HTTP/1.1}{0.0.0.0:4040}
2017-07-27 16:20:30,648 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2142ms
2017-07-27 16:20:30,648 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-07-27 16:20:30,650 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.105:4040
2017-07-27 16:20:30,738 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-07-27 16:20:30,767 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64752.
2017-07-27 16:20:30,768 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.105:64752
2017-07-27 16:20:30,770 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.105, 64752)
2017-07-27 16:20:30,772 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.105:64752 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.105, 64752)
2017-07-27 16:20:30,775 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.105, 64752)
2017-07-27 16:20:30,960 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@15f8707{/metrics/json,null,AVAILABLE}
2017-07-27 16:20:31,601 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 2000 ms
2017-07-27 16:20:31,601 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-07-27 16:20:31,602 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-07-27 16:20:31,603 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 2000 ms
2017-07-27 16:20:31,604 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@17c41d2
2017-07-27 16:20:31,604 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 2000 ms
2017-07-27 16:20:31,604 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-07-27 16:20:31,604 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-07-27 16:20:31,604 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 2000 ms
2017-07-27 16:20:31,604 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@1690cd1
2017-07-27 16:20:31,656 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1501143632000
2017-07-27 16:20:31,657 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1501143632000 ms
2017-07-27 16:20:31,657 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-07-27 16:20:31,660 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13fcd59{/streaming,null,AVAILABLE}
2017-07-27 16:20:31,660 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1d743e6{/streaming/json,null,AVAILABLE}
2017-07-27 16:20:31,661 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e8dcd3{/streaming/batch,null,AVAILABLE}
2017-07-27 16:20:31,661 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@df39b2{/streaming/batch/json,null,AVAILABLE}
2017-07-27 16:20:31,662 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@73fa8{/static/streaming,null,AVAILABLE}
2017-07-27 16:20:31,663 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-07-27 16:20:32,125 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501143632000 ms
2017-07-27 16:20:32,129 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501143632000 ms.0 from job set of time 1501143632000 ms
2017-07-27 16:20:32,160 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:30
2017-07-27 16:20:32,175 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreach at streamingExample.scala:30) with 2 output partitions
2017-07-27 16:20:32,175 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreach at streamingExample.scala:30)
2017-07-27 16:20:32,176 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 16:20:32,177 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 16:20:32,187 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at streamingExample.scala:25), which has no missing parents
2017-07-27 16:20:32,268 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-07-27 16:20:32,280 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 2.5 KB, free 413.9 MB)
2017-07-27 16:20:32,326 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1575.0 B, free 413.9 MB)
2017-07-27 16:20:32,328 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.105:64752 (size: 1575.0 B, free: 413.9 MB)
2017-07-27 16:20:32,333 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-07-27 16:20:32,338 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at streamingExample.scala:25)
2017-07-27 16:20:32,341 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-07-27 16:20:32,395 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5646 bytes)
2017-07-27 16:20:32,399 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5646 bytes)
2017-07-27 16:20:32,406 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-07-27 16:20:32,406 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-07-27 16:20:32,430 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 267 -> 568
2017-07-27 16:20:32,430 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 271 -> 545
2017-07-27 16:20:32,933 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 756 bytes result sent to driver
2017-07-27 16:20:32,940 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 542 ms on localhost (1/2)
2017-07-27 16:20:33,105 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 756 bytes result sent to driver
2017-07-27 16:20:33,106 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 738 ms on localhost (2/2)
2017-07-27 16:20:33,107 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreach at streamingExample.scala:30) finished in 0.753 s
2017-07-27 16:20:33,108 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-07-27 16:20:33,113 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreach at streamingExample.scala:30, took 0.952154 s
2017-07-27 16:20:33,117 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501143632000 ms.0 from job set of time 1501143632000 ms
2017-07-27 16:20:33,118 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.116 s for time 1501143632000 ms (execution: 0.989 s)
2017-07-27 16:20:33,124 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 16:20:33,129 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-07-27 16:20:34,010 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501143634000 ms
2017-07-27 16:20:34,011 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501143634000 ms.0 from job set of time 1501143634000 ms
2017-07-27 16:20:34,016 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:30
2017-07-27 16:20:34,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreach at streamingExample.scala:30) with 2 output partitions
2017-07-27 16:20:34,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreach at streamingExample.scala:30)
2017-07-27 16:20:34,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 16:20:34,017 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 16:20:34,018 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (KafkaRDD[1] at createDirectStream at streamingExample.scala:25), which has no missing parents
2017-07-27 16:20:34,019 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 2.5 KB, free 413.9 MB)
2017-07-27 16:20:34,022 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1564.0 B, free 413.9 MB)
2017-07-27 16:20:34,023 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.105:64752 (size: 1564.0 B, free: 413.9 MB)
2017-07-27 16:20:34,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-07-27 16:20:34,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (KafkaRDD[1] at createDirectStream at streamingExample.scala:25)
2017-07-27 16:20:34,024 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-07-27 16:20:34,026 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5646 bytes)
2017-07-27 16:20:34,028 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5646 bytes)
2017-07-27 16:20:34,028 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-07-27 16:20:34,029 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-07-27 16:20:34,032 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 16:20:34,032 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 16:20:34,033 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 756 bytes result sent to driver
2017-07-27 16:20:34,033 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 764 bytes result sent to driver
2017-07-27 16:20:34,034 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 9 ms on localhost (1/2)
2017-07-27 16:20:34,034 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 7 ms on localhost (2/2)
2017-07-27 16:20:34,035 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-07-27 16:20:34,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreach at streamingExample.scala:30) finished in 0.010 s
2017-07-27 16:20:34,035 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: foreach at streamingExample.scala:30, took 0.018929 s
2017-07-27 16:20:34,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501143634000 ms.0 from job set of time 1501143634000 ms
2017-07-27 16:20:34,036 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.036 s for time 1501143634000 ms (execution: 0.025 s)
2017-07-27 16:20:34,037 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-07-27 16:20:34,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 16:20:34,045 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-07-27 16:20:34,046 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-07-27 16:20:34,064 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on 192.168.31.105:64752 in memory (size: 1564.0 B, free: 413.9 MB)
2017-07-27 16:20:34,066 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_0_piece0 on 192.168.31.105:64752 in memory (size: 1575.0 B, free: 413.9 MB)
2017-07-27 16:20:36,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501143636000 ms
2017-07-27 16:20:36,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501143636000 ms.0 from job set of time 1501143636000 ms
2017-07-27 16:20:36,026 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:30
2017-07-27 16:20:36,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (foreach at streamingExample.scala:30) with 2 output partitions
2017-07-27 16:20:36,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (foreach at streamingExample.scala:30)
2017-07-27 16:20:36,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 16:20:36,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 16:20:36,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (KafkaRDD[2] at createDirectStream at streamingExample.scala:25), which has no missing parents
2017-07-27 16:20:36,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 413.9 MB)
2017-07-27 16:20:36,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1564.0 B, free 413.9 MB)
2017-07-27 16:20:36,038 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on 192.168.31.105:64752 (size: 1564.0 B, free: 413.9 MB)
2017-07-27 16:20:36,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2017-07-27 16:20:36,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (KafkaRDD[2] at createDirectStream at streamingExample.scala:25)
2017-07-27 16:20:36,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2017-07-27 16:20:36,041 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5646 bytes)
2017-07-27 16:20:36,042 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5646 bytes)
2017-07-27 16:20:36,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2017-07-27 16:20:36,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2017-07-27 16:20:36,045 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 16:20:36,045 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 16:20:36,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2.0 (TID 5). 756 bytes result sent to driver
2017-07-27 16:20:36,046 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 756 bytes result sent to driver
2017-07-27 16:20:36,047 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2.0 (TID 5) in 6 ms on localhost (1/2)
2017-07-27 16:20:36,048 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 8 ms on localhost (2/2)
2017-07-27 16:20:36,048 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-07-27 16:20:36,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (foreach at streamingExample.scala:30) finished in 0.009 s
2017-07-27 16:20:36,050 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: foreach at streamingExample.scala:30, took 0.023871 s
2017-07-27 16:20:36,052 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501143636000 ms.0 from job set of time 1501143636000 ms
2017-07-27 16:20:36,053 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.052 s for time 1501143636000 ms (execution: 0.033 s)
2017-07-27 16:20:36,054 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 1 from persistence list
2017-07-27 16:20:36,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 16:20:36,054 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-07-27 16:20:36,054 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501143632000 ms
2017-07-27 16:20:38,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501143638000 ms
2017-07-27 16:20:38,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501143638000 ms.0 from job set of time 1501143638000 ms
2017-07-27 16:20:38,038 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:30
2017-07-27 16:20:38,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (foreach at streamingExample.scala:30) with 2 output partitions
2017-07-27 16:20:38,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 3 (foreach at streamingExample.scala:30)
2017-07-27 16:20:38,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 16:20:38,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 16:20:38,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 3 (KafkaRDD[3] at createDirectStream at streamingExample.scala:25), which has no missing parents
2017-07-27 16:20:38,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 2.5 KB, free 413.9 MB)
2017-07-27 16:20:38,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1565.0 B, free 413.9 MB)
2017-07-27 16:20:38,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on 192.168.31.105:64752 (size: 1565.0 B, free: 413.9 MB)
2017-07-27 16:20:38,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2017-07-27 16:20:38,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 3 (KafkaRDD[3] at createDirectStream at streamingExample.scala:25)
2017-07-27 16:20:38,055 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 3.0 with 2 tasks
2017-07-27 16:20:38,058 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 3.0 (TID 6, localhost, partition 0, ANY, 5646 bytes)
2017-07-27 16:20:38,060 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 3.0 (TID 7, localhost, partition 1, ANY, 5646 bytes)
2017-07-27 16:20:38,060 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 3.0 (TID 6)
2017-07-27 16:20:38,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 3.0 (TID 7)
2017-07-27 16:20:38,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 16:20:38,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 16:20:38,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 3.0 (TID 7). 756 bytes result sent to driver
2017-07-27 16:20:38,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 3.0 (TID 6). 764 bytes result sent to driver
2017-07-27 16:20:38,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 3.0 (TID 7) in 9 ms on localhost (1/2)
2017-07-27 16:20:38,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 3.0 (TID 6) in 12 ms on localhost (2/2)
2017-07-27 16:20:38,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-07-27 16:20:38,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 3 (foreach at streamingExample.scala:30) finished in 0.014 s
2017-07-27 16:20:38,070 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: foreach at streamingExample.scala:30, took 0.031426 s
2017-07-27 16:20:38,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501143638000 ms.0 from job set of time 1501143638000 ms
2017-07-27 16:20:38,071 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2 from persistence list
2017-07-27 16:20:38,071 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.071 s for time 1501143638000 ms (execution: 0.053 s)
2017-07-27 16:20:38,072 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2
2017-07-27 16:20:38,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 16:20:38,072 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501143634000 ms
2017-07-27 16:20:40,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501143640000 ms
2017-07-27 16:20:40,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501143640000 ms.0 from job set of time 1501143640000 ms
2017-07-27 16:20:40,032 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:30
2017-07-27 16:20:40,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 4 (foreach at streamingExample.scala:30) with 2 output partitions
2017-07-27 16:20:40,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (foreach at streamingExample.scala:30)
2017-07-27 16:20:40,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 16:20:40,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 16:20:40,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (KafkaRDD[4] at createDirectStream at streamingExample.scala:25), which has no missing parents
2017-07-27 16:20:40,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 2.5 KB, free 413.9 MB)
2017-07-27 16:20:40,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1565.0 B, free 413.9 MB)
2017-07-27 16:20:40,048 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on 192.168.31.105:64752 (size: 1565.0 B, free: 413.9 MB)
2017-07-27 16:20:40,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2017-07-27 16:20:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 4 (KafkaRDD[4] at createDirectStream at streamingExample.scala:25)
2017-07-27 16:20:40,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 2 tasks
2017-07-27 16:20:40,055 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 8, localhost, partition 0, ANY, 5647 bytes)
2017-07-27 16:20:40,057 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 4.0 (TID 9, localhost, partition 1, ANY, 5647 bytes)
2017-07-27 16:20:40,057 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 4.0 (TID 9)
2017-07-27 16:20:40,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 8)
2017-07-27 16:20:40,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 16:20:40,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 16:20:40,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 8). 756 bytes result sent to driver
2017-07-27 16:20:40,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 4.0 (TID 9). 756 bytes result sent to driver
2017-07-27 16:20:40,064 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 8) in 10 ms on localhost (1/2)
2017-07-27 16:20:40,064 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 4.0 (TID 9) in 8 ms on localhost (2/2)
2017-07-27 16:20:40,064 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-07-27 16:20:40,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (foreach at streamingExample.scala:30) finished in 0.012 s
2017-07-27 16:20:40,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 4 finished: foreach at streamingExample.scala:30, took 0.032956 s
2017-07-27 16:20:40,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501143640000 ms.0 from job set of time 1501143640000 ms
2017-07-27 16:20:40,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501143640000 ms (execution: 0.048 s)
2017-07-27 16:20:40,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 3 from persistence list
2017-07-27 16:20:40,067 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 3
2017-07-27 16:20:40,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 16:20:40,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501143636000 ms
2017-07-27 16:20:42,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501143642000 ms
2017-07-27 16:20:42,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501143642000 ms.0 from job set of time 1501143642000 ms
2017-07-27 16:20:42,031 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:30
2017-07-27 16:20:42,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 5 (foreach at streamingExample.scala:30) with 2 output partitions
2017-07-27 16:20:42,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 5 (foreach at streamingExample.scala:30)
2017-07-27 16:20:42,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 16:20:42,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 16:20:42,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 5 (KafkaRDD[5] at createDirectStream at streamingExample.scala:25), which has no missing parents
2017-07-27 16:20:42,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 2.5 KB, free 413.9 MB)
2017-07-27 16:20:42,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1565.0 B, free 413.9 MB)
2017-07-27 16:20:42,049 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on 192.168.31.105:64752 (size: 1565.0 B, free: 413.9 MB)
2017-07-27 16:20:42,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2017-07-27 16:20:42,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 5 (KafkaRDD[5] at createDirectStream at streamingExample.scala:25)
2017-07-27 16:20:42,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 5.0 with 2 tasks
2017-07-27 16:20:42,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 5.0 (TID 10, localhost, partition 0, ANY, 5647 bytes)
2017-07-27 16:20:42,058 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 5.0 (TID 11, localhost, partition 1, ANY, 5647 bytes)
2017-07-27 16:20:42,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 5.0 (TID 10)
2017-07-27 16:20:42,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 5.0 (TID 11)
2017-07-27 16:20:42,065 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 16:20:42,065 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 16:20:42,067 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 5.0 (TID 11). 756 bytes result sent to driver
2017-07-27 16:20:42,066 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 5.0 (TID 10). 843 bytes result sent to driver
2017-07-27 16:20:42,068 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 5.0 (TID 11) in 12 ms on localhost (1/2)
2017-07-27 16:20:42,068 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 5.0 (TID 10) in 15 ms on localhost (2/2)
2017-07-27 16:20:42,069 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-07-27 16:20:42,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 5 (foreach at streamingExample.scala:30) finished in 0.016 s
2017-07-27 16:20:42,069 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 5 finished: foreach at streamingExample.scala:30, took 0.037544 s
2017-07-27 16:20:42,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501143642000 ms.0 from job set of time 1501143642000 ms
2017-07-27 16:20:42,070 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.070 s for time 1501143642000 ms (execution: 0.052 s)
2017-07-27 16:20:42,070 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 4 from persistence list
2017-07-27 16:20:42,071 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 4
2017-07-27 16:20:42,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 16:20:42,071 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501143638000 ms
2017-07-27 16:20:44,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501143644000 ms
2017-07-27 16:20:44,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501143644000 ms.0 from job set of time 1501143644000 ms
2017-07-27 16:20:44,032 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:30
2017-07-27 16:20:44,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 6 (foreach at streamingExample.scala:30) with 2 output partitions
2017-07-27 16:20:44,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 6 (foreach at streamingExample.scala:30)
2017-07-27 16:20:44,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 16:20:44,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 16:20:44,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 6 (KafkaRDD[6] at createDirectStream at streamingExample.scala:25), which has no missing parents
2017-07-27 16:20:44,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6 stored as values in memory (estimated size 2.5 KB, free 413.9 MB)
2017-07-27 16:20:44,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 1565.0 B, free 413.9 MB)
2017-07-27 16:20:44,047 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_6_piece0 in memory on 192.168.31.105:64752 (size: 1565.0 B, free: 413.9 MB)
2017-07-27 16:20:44,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2017-07-27 16:20:44,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 6 (KafkaRDD[6] at createDirectStream at streamingExample.scala:25)
2017-07-27 16:20:44,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 6.0 with 2 tasks
2017-07-27 16:20:44,050 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 6.0 (TID 12, localhost, partition 0, ANY, 5647 bytes)
2017-07-27 16:20:44,051 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 6.0 (TID 13, localhost, partition 1, ANY, 5647 bytes)
2017-07-27 16:20:44,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 6.0 (TID 13)
2017-07-27 16:20:44,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 6.0 (TID 12)
2017-07-27 16:20:44,053 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 16:20:44,053 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 16:20:44,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 6.0 (TID 12). 764 bytes result sent to driver
2017-07-27 16:20:44,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 6.0 (TID 13). 756 bytes result sent to driver
2017-07-27 16:20:44,055 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 6.0 (TID 12) in 6 ms on localhost (1/2)
2017-07-27 16:20:44,056 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 6.0 (TID 13) in 5 ms on localhost (2/2)
2017-07-27 16:20:44,056 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2017-07-27 16:20:44,056 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 6 (foreach at streamingExample.scala:30) finished in 0.007 s
2017-07-27 16:20:44,056 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 6 finished: foreach at streamingExample.scala:30, took 0.023931 s
2017-07-27 16:20:44,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501143644000 ms.0 from job set of time 1501143644000 ms
2017-07-27 16:20:44,057 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 5 from persistence list
2017-07-27 16:20:44,057 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.057 s for time 1501143644000 ms (execution: 0.038 s)
2017-07-27 16:20:44,058 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 16:20:44,058 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 5
2017-07-27 16:20:44,058 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501143640000 ms
2017-07-27 16:20:46,018 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501143646000 ms
2017-07-27 16:20:46,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501143646000 ms.0 from job set of time 1501143646000 ms
2017-07-27 16:20:46,031 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:30
2017-07-27 16:20:46,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 7 (foreach at streamingExample.scala:30) with 2 output partitions
2017-07-27 16:20:46,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 7 (foreach at streamingExample.scala:30)
2017-07-27 16:20:46,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 16:20:46,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 16:20:46,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 7 (KafkaRDD[7] at createDirectStream at streamingExample.scala:25), which has no missing parents
2017-07-27 16:20:46,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7 stored as values in memory (estimated size 2.5 KB, free 413.9 MB)
2017-07-27 16:20:46,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 1565.0 B, free 413.9 MB)
2017-07-27 16:20:46,048 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_7_piece0 in memory on 192.168.31.105:64752 (size: 1565.0 B, free: 413.9 MB)
2017-07-27 16:20:46,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2017-07-27 16:20:46,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 7 (KafkaRDD[7] at createDirectStream at streamingExample.scala:25)
2017-07-27 16:20:46,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 7.0 with 2 tasks
2017-07-27 16:20:46,056 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 7.0 (TID 14, localhost, partition 0, ANY, 5647 bytes)
2017-07-27 16:20:46,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 7.0 (TID 15, localhost, partition 1, ANY, 5647 bytes)
2017-07-27 16:20:46,058 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 7.0 (TID 15)
2017-07-27 16:20:46,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 7.0 (TID 14)
2017-07-27 16:20:46,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 16:20:46,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 7.0 (TID 15). 756 bytes result sent to driver
2017-07-27 16:20:46,062 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 16:20:46,062 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 7.0 (TID 15) in 5 ms on localhost (1/2)
2017-07-27 16:20:46,063 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 7.0 (TID 14). 756 bytes result sent to driver
2017-07-27 16:20:46,065 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 7.0 (TID 14) in 10 ms on localhost (2/2)
2017-07-27 16:20:46,065 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2017-07-27 16:20:46,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 7 (foreach at streamingExample.scala:30) finished in 0.011 s
2017-07-27 16:20:46,066 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 7 finished: foreach at streamingExample.scala:30, took 0.033364 s
2017-07-27 16:20:46,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501143646000 ms.0 from job set of time 1501143646000 ms
2017-07-27 16:20:46,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 6 from persistence list
2017-07-27 16:20:46,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.066 s for time 1501143646000 ms (execution: 0.047 s)
2017-07-27 16:20:46,067 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 6
2017-07-27 16:20:46,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 16:20:46,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501143642000 ms
2017-07-27 16:20:48,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501143648000 ms
2017-07-27 16:20:48,017 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501143648000 ms.0 from job set of time 1501143648000 ms
2017-07-27 16:20:48,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:30
2017-07-27 16:20:48,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 8 (foreach at streamingExample.scala:30) with 2 output partitions
2017-07-27 16:20:48,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 8 (foreach at streamingExample.scala:30)
2017-07-27 16:20:48,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 16:20:48,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 16:20:48,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 8 (KafkaRDD[8] at createDirectStream at streamingExample.scala:25), which has no missing parents
2017-07-27 16:20:48,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8 stored as values in memory (estimated size 2.5 KB, free 413.9 MB)
2017-07-27 16:20:48,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 1565.0 B, free 413.9 MB)
2017-07-27 16:20:48,044 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_8_piece0 in memory on 192.168.31.105:64752 (size: 1565.0 B, free: 413.9 MB)
2017-07-27 16:20:48,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2017-07-27 16:20:48,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 8 (KafkaRDD[8] at createDirectStream at streamingExample.scala:25)
2017-07-27 16:20:48,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 8.0 with 2 tasks
2017-07-27 16:20:48,051 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 8.0 (TID 16, localhost, partition 0, ANY, 5647 bytes)
2017-07-27 16:20:48,052 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 8.0 (TID 17, localhost, partition 1, ANY, 5647 bytes)
2017-07-27 16:20:48,053 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 8.0 (TID 17)
2017-07-27 16:20:48,053 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 8.0 (TID 16)
2017-07-27 16:20:48,058 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 16:20:48,058 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 16:20:48,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 8.0 (TID 16). 677 bytes result sent to driver
2017-07-27 16:20:48,060 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 8.0 (TID 17). 756 bytes result sent to driver
2017-07-27 16:20:48,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 8.0 (TID 16) in 12 ms on localhost (1/2)
2017-07-27 16:20:48,062 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 8.0 (TID 17) in 11 ms on localhost (2/2)
2017-07-27 16:20:48,063 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2017-07-27 16:20:48,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 8 (foreach at streamingExample.scala:30) finished in 0.014 s
2017-07-27 16:20:48,064 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 8 finished: foreach at streamingExample.scala:30, took 0.033460 s
2017-07-27 16:20:48,065 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501143648000 ms.0 from job set of time 1501143648000 ms
2017-07-27 16:20:48,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.065 s for time 1501143648000 ms (execution: 0.048 s)
2017-07-27 16:20:48,066 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 7 from persistence list
2017-07-27 16:20:48,067 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 7
2017-07-27 16:20:48,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 16:20:48,067 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501143644000 ms
2017-07-27 16:20:53,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501143650000 ms
2017-07-27 16:20:53,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501143650000 ms.0 from job set of time 1501143650000 ms
2017-07-27 16:20:53,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:30
2017-07-27 16:20:53,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 9 (foreach at streamingExample.scala:30) with 2 output partitions
2017-07-27 16:20:53,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 9 (foreach at streamingExample.scala:30)
2017-07-27 16:20:53,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 16:20:53,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 16:20:53,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 9 (KafkaRDD[9] at createDirectStream at streamingExample.scala:25), which has no missing parents
2017-07-27 16:20:53,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9 stored as values in memory (estimated size 2.5 KB, free 413.9 MB)
2017-07-27 16:20:53,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 1565.0 B, free 413.9 MB)
2017-07-27 16:20:53,034 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_9_piece0 in memory on 192.168.31.105:64752 (size: 1565.0 B, free: 413.9 MB)
2017-07-27 16:20:53,036 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501143652000 ms
2017-07-27 16:20:53,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2017-07-27 16:20:53,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 9 (KafkaRDD[9] at createDirectStream at streamingExample.scala:25)
2017-07-27 16:20:53,037 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 9.0 with 2 tasks
2017-07-27 16:20:53,039 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 9.0 (TID 18, localhost, partition 0, ANY, 5647 bytes)
2017-07-27 16:20:53,039 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 9.0 (TID 19, localhost, partition 1, ANY, 5647 bytes)
2017-07-27 16:20:53,039 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 9.0 (TID 18)
2017-07-27 16:20:53,039 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 9.0 (TID 19)
2017-07-27 16:20:53,041 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 16:20:53,042 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 16:20:53,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 9.0 (TID 19). 756 bytes result sent to driver
2017-07-27 16:20:53,043 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 9.0 (TID 18). 677 bytes result sent to driver
2017-07-27 16:20:53,043 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 9.0 (TID 19) in 4 ms on localhost (1/2)
2017-07-27 16:20:53,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 9.0 (TID 18) in 6 ms on localhost (2/2)
2017-07-27 16:20:53,044 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2017-07-27 16:20:53,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 9 (foreach at streamingExample.scala:30) finished in 0.007 s
2017-07-27 16:20:53,044 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 9 finished: foreach at streamingExample.scala:30, took 0.019122 s
2017-07-27 16:20:53,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501143650000 ms.0 from job set of time 1501143650000 ms
2017-07-27 16:20:53,045 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 8 from persistence list
2017-07-27 16:20:53,045 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 3.045 s for time 1501143650000 ms (execution: 0.024 s)
2017-07-27 16:20:53,046 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 8
2017-07-27 16:20:53,046 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501143652000 ms.0 from job set of time 1501143652000 ms
2017-07-27 16:20:53,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 16:20:53,046 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501143646000 ms
2017-07-27 16:20:53,049 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:30
2017-07-27 16:20:53,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 10 (foreach at streamingExample.scala:30) with 2 output partitions
2017-07-27 16:20:53,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 10 (foreach at streamingExample.scala:30)
2017-07-27 16:20:53,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 16:20:53,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 16:20:53,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 10 (KafkaRDD[10] at createDirectStream at streamingExample.scala:25), which has no missing parents
2017-07-27 16:20:53,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10 stored as values in memory (estimated size 2.5 KB, free 413.9 MB)
2017-07-27 16:20:53,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10_piece0 stored as bytes in memory (estimated size 1565.0 B, free 413.9 MB)
2017-07-27 16:20:53,059 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_10_piece0 in memory on 192.168.31.105:64752 (size: 1565.0 B, free: 413.9 MB)
2017-07-27 16:20:53,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2017-07-27 16:20:53,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 10 (KafkaRDD[10] at createDirectStream at streamingExample.scala:25)
2017-07-27 16:20:53,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 10.0 with 2 tasks
2017-07-27 16:20:53,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 10.0 (TID 20, localhost, partition 0, ANY, 5647 bytes)
2017-07-27 16:20:53,062 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 10.0 (TID 21, localhost, partition 1, ANY, 5647 bytes)
2017-07-27 16:20:53,062 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 10.0 (TID 21)
2017-07-27 16:20:53,062 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 10.0 (TID 20)
2017-07-27 16:20:53,064 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 16:20:53,064 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 16:20:53,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 10.0 (TID 20). 677 bytes result sent to driver
2017-07-27 16:20:53,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 10.0 (TID 21). 677 bytes result sent to driver
2017-07-27 16:20:53,065 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 10.0 (TID 20) in 3 ms on localhost (1/2)
2017-07-27 16:20:53,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 10.0 (TID 21) in 3 ms on localhost (2/2)
2017-07-27 16:20:53,065 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2017-07-27 16:20:53,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 10 (foreach at streamingExample.scala:30) finished in 0.005 s
2017-07-27 16:20:53,065 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 10 finished: foreach at streamingExample.scala:30, took 0.015951 s
2017-07-27 16:20:53,066 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501143652000 ms.0 from job set of time 1501143652000 ms
2017-07-27 16:20:53,067 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 9 from persistence list
2017-07-27 16:20:53,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.066 s for time 1501143652000 ms (execution: 0.020 s)
2017-07-27 16:20:53,069 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 9
2017-07-27 16:20:53,069 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 16:20:53,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501143648000 ms
2017-07-27 16:20:54,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501143654000 ms
2017-07-27 16:20:54,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501143654000 ms.0 from job set of time 1501143654000 ms
2017-07-27 16:20:54,025 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:30
2017-07-27 16:20:54,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 11 (foreach at streamingExample.scala:30) with 2 output partitions
2017-07-27 16:20:54,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 11 (foreach at streamingExample.scala:30)
2017-07-27 16:20:54,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 16:20:54,027 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 16:20:54,028 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 11 (KafkaRDD[11] at createDirectStream at streamingExample.scala:25), which has no missing parents
2017-07-27 16:20:54,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11 stored as values in memory (estimated size 2.5 KB, free 413.9 MB)
2017-07-27 16:20:54,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11_piece0 stored as bytes in memory (estimated size 1565.0 B, free 413.9 MB)
2017-07-27 16:20:54,037 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_11_piece0 in memory on 192.168.31.105:64752 (size: 1565.0 B, free: 413.9 MB)
2017-07-27 16:20:54,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 11 from broadcast at DAGScheduler.scala:1012
2017-07-27 16:20:54,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 11 (KafkaRDD[11] at createDirectStream at streamingExample.scala:25)
2017-07-27 16:20:54,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 11.0 with 2 tasks
2017-07-27 16:20:54,040 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 11.0 (TID 22, localhost, partition 0, ANY, 5647 bytes)
2017-07-27 16:20:54,041 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 11.0 (TID 23, localhost, partition 1, ANY, 5647 bytes)
2017-07-27 16:20:54,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 11.0 (TID 23)
2017-07-27 16:20:54,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 11.0 (TID 22)
2017-07-27 16:20:54,044 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 16:20:54,044 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 16:20:54,045 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 11.0 (TID 23). 677 bytes result sent to driver
2017-07-27 16:20:54,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 11.0 (TID 22). 677 bytes result sent to driver
2017-07-27 16:20:54,046 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 11.0 (TID 23) in 5 ms on localhost (1/2)
2017-07-27 16:20:54,046 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 11.0 (TID 22) in 7 ms on localhost (2/2)
2017-07-27 16:20:54,047 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2017-07-27 16:20:54,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 11 (foreach at streamingExample.scala:30) finished in 0.007 s
2017-07-27 16:20:54,047 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 11 finished: foreach at streamingExample.scala:30, took 0.021798 s
2017-07-27 16:20:54,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501143654000 ms.0 from job set of time 1501143654000 ms
2017-07-27 16:20:54,048 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 10 from persistence list
2017-07-27 16:20:54,048 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.048 s for time 1501143654000 ms (execution: 0.033 s)
2017-07-27 16:20:54,049 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 10
2017-07-27 16:20:54,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 16:20:54,049 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501143650000 ms
2017-07-27 18:57:11,029 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-07-27 18:57:11,230 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-07-27 18:57:11,315 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-07-27 18:57:11,316 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-07-27 18:57:11,316 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-07-27 18:57:11,317 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-07-27 18:57:11,317 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-07-27 18:57:12,061 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 52698.
2017-07-27 18:57:12,081 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-07-27 18:57:12,100 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-07-27 18:57:12,114 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-02e6c151-9981-482a-b808-9b29c0b6941a
2017-07-27 18:57:12,129 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-07-27 18:57:12,174 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-07-27 18:57:12,264 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2031ms
2017-07-27 18:57:12,363 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-07-27 18:57:12,381 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1e5f0ef{/jobs,null,AVAILABLE}
2017-07-27 18:57:12,381 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@16a312c{/jobs/json,null,AVAILABLE}
2017-07-27 18:57:12,381 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@8d6290{/jobs/job,null,AVAILABLE}
2017-07-27 18:57:12,382 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@a0256d{/jobs/job/json,null,AVAILABLE}
2017-07-27 18:57:12,382 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0d29e{/stages,null,AVAILABLE}
2017-07-27 18:57:12,382 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f57b3d{/stages/json,null,AVAILABLE}
2017-07-27 18:57:12,382 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1850751{/stages/stage,null,AVAILABLE}
2017-07-27 18:57:12,382 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e3db41{/stages/stage/json,null,AVAILABLE}
2017-07-27 18:57:12,383 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@df9a61{/stages/pool,null,AVAILABLE}
2017-07-27 18:57:12,383 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1602954{/stages/pool/json,null,AVAILABLE}
2017-07-27 18:57:12,383 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@260766{/storage,null,AVAILABLE}
2017-07-27 18:57:12,383 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3dec30{/storage/json,null,AVAILABLE}
2017-07-27 18:57:12,383 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@a97744{/storage/rdd,null,AVAILABLE}
2017-07-27 18:57:12,384 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@75c20c{/storage/rdd/json,null,AVAILABLE}
2017-07-27 18:57:12,384 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@56e013{/environment,null,AVAILABLE}
2017-07-27 18:57:12,384 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1c9dfc5{/environment/json,null,AVAILABLE}
2017-07-27 18:57:12,385 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@258c2f{/executors,null,AVAILABLE}
2017-07-27 18:57:12,385 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@ac4385{/executors/json,null,AVAILABLE}
2017-07-27 18:57:12,385 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@131787b{/executors/threadDump,null,AVAILABLE}
2017-07-27 18:57:12,385 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f7b4af{/executors/threadDump/json,null,AVAILABLE}
2017-07-27 18:57:12,390 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@df16aa{/static,null,AVAILABLE}
2017-07-27 18:57:12,390 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@a1d21f{/,null,AVAILABLE}
2017-07-27 18:57:12,391 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@bf8650{/api,null,AVAILABLE}
2017-07-27 18:57:12,391 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1c60324{/stages/stage/kill,null,AVAILABLE}
2017-07-27 18:57:12,399 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@43d68f{HTTP/1.1}{0.0.0.0:4040}
2017-07-27 18:57:12,399 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2166ms
2017-07-27 18:57:12,399 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-07-27 18:57:12,401 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.105:4040
2017-07-27 18:57:12,487 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-07-27 18:57:12,517 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52711.
2017-07-27 18:57:12,518 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.105:52711
2017-07-27 18:57:12,519 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.105, 52711)
2017-07-27 18:57:12,522 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.105:52711 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.105, 52711)
2017-07-27 18:57:12,525 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.105, 52711)
2017-07-27 18:57:12,711 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1bde877{/metrics/json,null,AVAILABLE}
2017-07-27 18:57:13,254 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 2000 ms
2017-07-27 18:57:13,256 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-07-27 18:57:13,257 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-07-27 18:57:13,258 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 2000 ms
2017-07-27 18:57:13,259 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@141b8c9
2017-07-27 18:57:13,259 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 2000 ms
2017-07-27 18:57:13,259 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-07-27 18:57:13,259 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-07-27 18:57:13,259 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 2000 ms
2017-07-27 18:57:13,259 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@57281
2017-07-27 18:57:13,259 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 2000 ms
2017-07-27 18:57:13,259 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-07-27 18:57:13,259 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-07-27 18:57:13,259 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 2000 ms
2017-07-27 18:57:13,260 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@e1fb02
2017-07-27 18:57:13,315 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1501153034000
2017-07-27 18:57:13,316 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1501153034000 ms
2017-07-27 18:57:13,317 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-07-27 18:57:13,319 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@379273{/streaming,null,AVAILABLE}
2017-07-27 18:57:13,320 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1740dae{/streaming/json,null,AVAILABLE}
2017-07-27 18:57:13,321 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@57fadf{/streaming/batch,null,AVAILABLE}
2017-07-27 18:57:13,321 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@82cd4f{/streaming/batch/json,null,AVAILABLE}
2017-07-27 18:57:13,323 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@814f43{/static/streaming,null,AVAILABLE}
2017-07-27 18:57:13,324 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-07-27 18:57:14,141 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501153034000 ms
2017-07-27 18:57:14,144 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501153034000 ms.0 from job set of time 1501153034000 ms
2017-07-27 18:57:14,166 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreach at streamingExample.scala:31
2017-07-27 18:57:14,178 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreach at streamingExample.scala:31) with 2 output partitions
2017-07-27 18:57:14,179 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreach at streamingExample.scala:31)
2017-07-27 18:57:14,179 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 18:57:14,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 18:57:14,189 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingExample.scala:30), which has no missing parents
2017-07-27 18:57:14,271 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-07-27 18:57:14,284 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.0 KB, free 413.9 MB)
2017-07-27 18:57:14,335 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1834.0 B, free 413.9 MB)
2017-07-27 18:57:14,338 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.105:52711 (size: 1834.0 B, free: 413.9 MB)
2017-07-27 18:57:14,342 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-07-27 18:57:14,349 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingExample.scala:30)
2017-07-27 18:57:14,351 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-07-27 18:57:14,409 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5646 bytes)
2017-07-27 18:57:14,413 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5646 bytes)
2017-07-27 18:57:14,418 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-07-27 18:57:14,418 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-07-27 18:57:14,445 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 271 -> 545
2017-07-27 18:57:14,445 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 267 -> 568
2017-07-27 18:57:16,039 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501153036000 ms
2017-07-27 19:12:50,445 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-07-27 19:12:50,647 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-07-27 19:12:50,742 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-07-27 19:12:50,742 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-07-27 19:12:50,743 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-07-27 19:12:50,743 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-07-27 19:12:50,744 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-07-27 19:12:51,501 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 52975.
2017-07-27 19:12:51,521 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-07-27 19:12:51,539 [main] ERROR [org.apache.spark.SparkContext] - Error initializing SparkContext.
java.lang.IllegalArgumentException: System memory 259522560 must be at least 471859200. Please increase heap size using the --driver-memory option or spark.driver.memory in Spark configuration.
	at org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:212) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:194) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:308) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:165) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:259) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:423) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:836) [spark-streaming_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84) [spark-streaming_2.11-2.0.0.jar:2.0.0]
	at cn.datapark.process.education.process.streamingProcessNew$.main(streamingProcessNew.scala:34) [classes/:na]
	at cn.datapark.process.education.process.streamingProcessNew.main(streamingProcessNew.scala) [classes/:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_111]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_111]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_111]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_111]
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144) [idea_rt.jar:na]
2017-07-27 19:12:51,544 [main] INFO  [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2017-07-27 19:12:51,548 [Thread-0] INFO  [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2017-07-27 19:13:20,142 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-07-27 19:13:20,370 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-07-27 19:13:20,451 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-07-27 19:13:20,452 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-07-27 19:13:20,452 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-07-27 19:13:20,453 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-07-27 19:13:20,453 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-07-27 19:13:21,197 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 53000.
2017-07-27 19:13:21,218 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-07-27 19:13:21,236 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-07-27 19:13:21,250 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-509a355e-67f2-4dc0-bce3-c7fd1b71e870
2017-07-27 19:13:21,264 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-07-27 19:13:21,308 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-07-27 19:13:21,392 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2054ms
2017-07-27 19:13:21,498 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-07-27 19:13:21,517 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/jobs,null,AVAILABLE}
2017-07-27 19:13:21,518 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/jobs/json,null,AVAILABLE}
2017-07-27 19:13:21,518 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/jobs/job,null,AVAILABLE}
2017-07-27 19:13:21,518 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/jobs/job/json,null,AVAILABLE}
2017-07-27 19:13:21,518 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/stages,null,AVAILABLE}
2017-07-27 19:13:21,519 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/stages/json,null,AVAILABLE}
2017-07-27 19:13:21,519 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/stages/stage,null,AVAILABLE}
2017-07-27 19:13:21,519 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/stages/stage/json,null,AVAILABLE}
2017-07-27 19:13:21,519 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/stages/pool,null,AVAILABLE}
2017-07-27 19:13:21,519 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/stages/pool/json,null,AVAILABLE}
2017-07-27 19:13:21,520 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/storage,null,AVAILABLE}
2017-07-27 19:13:21,520 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/storage/json,null,AVAILABLE}
2017-07-27 19:13:21,520 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/storage/rdd,null,AVAILABLE}
2017-07-27 19:13:21,520 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/storage/rdd/json,null,AVAILABLE}
2017-07-27 19:13:21,520 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/environment,null,AVAILABLE}
2017-07-27 19:13:21,520 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0fba8{/environment/json,null,AVAILABLE}
2017-07-27 19:13:21,521 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5de5a4{/executors,null,AVAILABLE}
2017-07-27 19:13:21,521 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1ef6856{/executors/json,null,AVAILABLE}
2017-07-27 19:13:21,521 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@b7a938{/executors/threadDump,null,AVAILABLE}
2017-07-27 19:13:21,521 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1af2e7d{/executors/threadDump/json,null,AVAILABLE}
2017-07-27 19:13:21,527 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@648ce9{/static,null,AVAILABLE}
2017-07-27 19:13:21,527 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9bf9eb{/,null,AVAILABLE}
2017-07-27 19:13:21,528 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1526469{/api,null,AVAILABLE}
2017-07-27 19:13:21,528 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@14872f8{/stages/stage/kill,null,AVAILABLE}
2017-07-27 19:13:21,537 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@1a90fa8{HTTP/1.1}{0.0.0.0:4040}
2017-07-27 19:13:21,537 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2199ms
2017-07-27 19:13:21,538 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-07-27 19:13:21,540 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.105:4040
2017-07-27 19:13:21,617 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-07-27 19:13:21,654 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53013.
2017-07-27 19:13:21,655 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.105:53013
2017-07-27 19:13:21,657 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.105, 53013)
2017-07-27 19:13:21,660 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.105:53013 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.105, 53013)
2017-07-27 19:13:21,662 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.105, 53013)
2017-07-27 19:13:21,890 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13ddbd9{/metrics/json,null,AVAILABLE}
2017-07-27 19:13:22,420 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-07-27 19:13:22,421 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-07-27 19:13:22,421 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-07-27 19:13:22,422 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-07-27 19:13:22,423 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@1d33d36
2017-07-27 19:13:22,423 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-07-27 19:13:22,423 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-07-27 19:13:22,423 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-07-27 19:13:22,423 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-07-27 19:13:22,423 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@10caea9
2017-07-27 19:13:22,423 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-07-27 19:13:22,423 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-07-27 19:13:22,423 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-07-27 19:13:22,423 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-07-27 19:13:22,423 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@1dde2fa
2017-07-27 19:13:22,464 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1501154005000
2017-07-27 19:13:22,464 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1501154005000 ms
2017-07-27 19:13:22,465 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-07-27 19:13:22,466 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cf395{/streaming,null,AVAILABLE}
2017-07-27 19:13:22,467 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6aa04a{/streaming/json,null,AVAILABLE}
2017-07-27 19:13:22,468 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch,null,AVAILABLE}
2017-07-27 19:13:22,468 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@814f43{/streaming/batch/json,null,AVAILABLE}
2017-07-27 19:13:22,469 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1a00961{/static/streaming,null,AVAILABLE}
2017-07-27 19:13:22,470 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-07-27 19:13:25,185 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154005000 ms
2017-07-27 19:13:25,198 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154005000 ms.0 from job set of time 1501154005000 ms
2017-07-27 19:13:25,244 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:13:25,259 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:13:25,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:13:25,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:13:25,262 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:13:25,272 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:13:25,341 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-07-27 19:13:25,353 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:13:25,840 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:13:25,842 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:13:25,847 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:13:25,850 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:47)
2017-07-27 19:13:25,852 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-07-27 19:13:25,895 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-07-27 19:13:25,898 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-07-27 19:13:25,904 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-07-27 19:13:25,904 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-07-27 19:13:25,931 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 271 -> 296
2017-07-27 19:13:25,931 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 267 -> 292
2017-07-27 19:13:26,168 [Executor task launch worker-1] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-07-27 19:13:26,455 [Executor task launch worker-1] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-07-27 19:13:30,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154010000 ms
2017-07-27 19:13:32,837 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-07-27 19:13:32,869 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 6993 ms on localhost (1/2)
2017-07-27 19:13:35,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154015000 ms
2017-07-27 19:13:35,535 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-07-27 19:13:35,550 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 9652 ms on localhost (2/2)
2017-07-27 19:13:35,553 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcessNew.scala:49) finished in 9.687 s
2017-07-27 19:13:35,554 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-07-27 19:13:35,566 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcessNew.scala:49, took 10.320769 s
2017-07-27 19:13:35,585 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154005000 ms.0 from job set of time 1501154005000 ms
2017-07-27 19:13:35,587 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 10.582 s for time 1501154005000 ms (execution: 10.394 s)
2017-07-27 19:13:35,588 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154010000 ms.0 from job set of time 1501154010000 ms
2017-07-27 19:13:35,593 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:13:35,593 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:13:35,594 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:13:35,594 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:13:35,594 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:13:35,595 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:13:35,596 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:13:35,597 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:13:35,602 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-07-27 19:13:35,602 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:13:35,603 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:13:35,603 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:13:35,604 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:47)
2017-07-27 19:13:35,604 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-07-27 19:13:35,606 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-07-27 19:13:35,607 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-07-27 19:13:35,607 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-07-27 19:13:35,607 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-07-27 19:13:35,611 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 296 -> 321
2017-07-27 19:13:35,611 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 292 -> 317
2017-07-27 19:13:38,344 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 787 bytes result sent to driver
2017-07-27 19:13:38,352 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 2745 ms on localhost (1/2)
2017-07-27 19:13:38,771 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 787 bytes result sent to driver
2017-07-27 19:13:38,774 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 3170 ms on localhost (2/2)
2017-07-27 19:13:38,775 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-07-27 19:13:38,775 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcessNew.scala:49) finished in 3.170 s
2017-07-27 19:13:38,775 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: foreachPartition at streamingProcessNew.scala:49, took 3.182048 s
2017-07-27 19:13:38,775 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154010000 ms.0 from job set of time 1501154010000 ms
2017-07-27 19:13:38,776 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 8.775 s for time 1501154010000 ms (execution: 3.187 s)
2017-07-27 19:13:38,776 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154015000 ms.0 from job set of time 1501154015000 ms
2017-07-27 19:13:38,791 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-07-27 19:13:38,813 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:13:38,815 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:13:38,815 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:13:38,815 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:13:38,816 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:13:38,816 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:13:38,817 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:13:38,824 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:13:38,867 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:13:38,868 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:13:38,869 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:47)
2017-07-27 19:13:38,869 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2017-07-27 19:13:38,871 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-07-27 19:13:38,871 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5655 bytes)
2017-07-27 19:13:38,871 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:13:38,871 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-07-27 19:13:38,873 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5655 bytes)
2017-07-27 19:13:38,873 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2017-07-27 19:13:38,873 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2017-07-27 19:13:38,875 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-07-27 19:13:38,875 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-07-27 19:13:38,876 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 317 -> 342
2017-07-27 19:13:38,876 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 321 -> 346
2017-07-27 19:13:39,464 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:13:40,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154020000 ms
2017-07-27 19:13:40,020 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 787 bytes result sent to driver
2017-07-27 19:13:40,023 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 1154 ms on localhost (1/2)
2017-07-27 19:13:42,226 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2.0 (TID 5). 787 bytes result sent to driver
2017-07-27 19:13:42,229 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2.0 (TID 5) in 3358 ms on localhost (2/2)
2017-07-27 19:13:42,229 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-07-27 19:13:42,229 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (foreachPartition at streamingProcessNew.scala:49) finished in 3.360 s
2017-07-27 19:13:42,230 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: foreachPartition at streamingProcessNew.scala:49, took 3.415765 s
2017-07-27 19:13:42,230 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154015000 ms.0 from job set of time 1501154015000 ms
2017-07-27 19:13:42,231 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 7.230 s for time 1501154015000 ms (execution: 3.454 s)
2017-07-27 19:13:42,231 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 3 from persistence list
2017-07-27 19:13:42,231 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154020000 ms.0 from job set of time 1501154020000 ms
2017-07-27 19:13:42,231 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 3
2017-07-27 19:13:42,231 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2 from persistence list
2017-07-27 19:13:42,232 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2
2017-07-27 19:13:42,232 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:13:42,232 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154005000 ms
2017-07-27 19:13:42,237 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:13:42,238 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:13:42,238 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 3 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:13:42,238 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:13:42,238 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:13:42,238 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:13:42,240 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:13:42,245 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:13:42,246 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:13:42,246 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:13:42,246 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcessNew.scala:47)
2017-07-27 19:13:42,247 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 3.0 with 2 tasks
2017-07-27 19:13:42,248 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 3.0 (TID 6, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:13:42,249 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 3.0 (TID 7, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:13:42,249 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 3.0 (TID 6)
2017-07-27 19:13:42,249 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 3.0 (TID 7)
2017-07-27 19:13:42,252 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 342 -> 367
2017-07-27 19:13:42,252 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 346 -> 371
2017-07-27 19:13:43,243 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 3.0 (TID 7). 714 bytes result sent to driver
2017-07-27 19:13:43,246 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 3.0 (TID 7) in 997 ms on localhost (1/2)
2017-07-27 19:13:45,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154025000 ms
2017-07-27 19:13:45,368 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:13:46,935 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 3.0 (TID 6). 787 bytes result sent to driver
2017-07-27 19:13:46,937 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 3.0 (TID 6) in 4690 ms on localhost (2/2)
2017-07-27 19:13:46,938 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-07-27 19:13:46,938 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 3 (foreachPartition at streamingProcessNew.scala:49) finished in 4.691 s
2017-07-27 19:13:46,938 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: foreachPartition at streamingProcessNew.scala:49, took 4.700660 s
2017-07-27 19:13:46,938 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154020000 ms.0 from job set of time 1501154020000 ms
2017-07-27 19:13:46,939 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 6.938 s for time 1501154020000 ms (execution: 4.707 s)
2017-07-27 19:13:46,939 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 5 from persistence list
2017-07-27 19:13:46,939 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 4 from persistence list
2017-07-27 19:13:46,940 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154025000 ms.0 from job set of time 1501154025000 ms
2017-07-27 19:13:46,941 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:13:46,941 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154010000 ms
2017-07-27 19:13:46,942 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 4
2017-07-27 19:13:46,942 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 5
2017-07-27 19:13:46,949 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:13:46,950 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 4 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:13:46,950 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:13:46,950 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:13:46,950 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:13:46,950 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:13:46,952 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:13:46,959 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:13:46,960 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:13:46,960 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:13:46,961 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcessNew.scala:47)
2017-07-27 19:13:46,961 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 2 tasks
2017-07-27 19:13:46,963 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 8, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:13:46,965 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 4.0 (TID 9, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:13:46,965 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 4.0 (TID 9)
2017-07-27 19:13:46,965 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 8)
2017-07-27 19:13:46,967 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 371 -> 396
2017-07-27 19:13:46,967 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 367 -> 392
2017-07-27 19:13:47,486 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_3_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:13:47,894 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 8). 787 bytes result sent to driver
2017-07-27 19:13:47,896 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 8) in 935 ms on localhost (1/2)
2017-07-27 19:13:47,990 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 4.0 (TID 9). 787 bytes result sent to driver
2017-07-27 19:13:47,992 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 4.0 (TID 9) in 1028 ms on localhost (2/2)
2017-07-27 19:13:47,992 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (foreachPartition at streamingProcessNew.scala:49) finished in 1.031 s
2017-07-27 19:13:47,992 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-07-27 19:13:47,992 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 4 finished: foreachPartition at streamingProcessNew.scala:49, took 1.043027 s
2017-07-27 19:13:47,993 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154025000 ms.0 from job set of time 1501154025000 ms
2017-07-27 19:13:47,993 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.993 s for time 1501154025000 ms (execution: 1.053 s)
2017-07-27 19:13:47,993 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 7 from persistence list
2017-07-27 19:13:47,993 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 7
2017-07-27 19:13:47,994 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 6 from persistence list
2017-07-27 19:13:47,994 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 6
2017-07-27 19:13:47,994 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:13:47,994 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154015000 ms
2017-07-27 19:13:50,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154030000 ms
2017-07-27 19:13:50,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154030000 ms.0 from job set of time 1501154030000 ms
2017-07-27 19:13:50,044 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:13:50,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 5 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:13:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 5 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:13:50,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:13:50,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:13:50,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:13:50,053 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:13:50,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:13:50,067 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:13:50,069 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:13:50,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcessNew.scala:47)
2017-07-27 19:13:50,071 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 5.0 with 2 tasks
2017-07-27 19:13:50,076 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 5.0 (TID 10, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:13:50,078 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 5.0 (TID 11, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:13:50,078 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 5.0 (TID 10)
2017-07-27 19:13:50,079 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 5.0 (TID 11)
2017-07-27 19:13:50,081 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 396 -> 421
2017-07-27 19:13:50,082 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 392 -> 417
2017-07-27 19:13:51,030 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_4_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:13:51,680 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 5.0 (TID 10). 787 bytes result sent to driver
2017-07-27 19:13:51,686 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 5.0 (TID 10) in 1614 ms on localhost (1/2)
2017-07-27 19:13:52,979 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 5.0 (TID 11). 787 bytes result sent to driver
2017-07-27 19:13:52,981 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 5.0 (TID 11) in 2904 ms on localhost (2/2)
2017-07-27 19:13:52,981 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 5 (foreachPartition at streamingProcessNew.scala:49) finished in 2.909 s
2017-07-27 19:13:52,981 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-07-27 19:13:52,982 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 5 finished: foreachPartition at streamingProcessNew.scala:49, took 2.936574 s
2017-07-27 19:13:52,982 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154030000 ms.0 from job set of time 1501154030000 ms
2017-07-27 19:13:52,982 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 9 from persistence list
2017-07-27 19:13:52,982 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.982 s for time 1501154030000 ms (execution: 2.961 s)
2017-07-27 19:13:52,983 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 9
2017-07-27 19:13:52,983 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 8 from persistence list
2017-07-27 19:13:52,984 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 8
2017-07-27 19:13:52,984 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:13:52,984 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154020000 ms
2017-07-27 19:13:55,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154035000 ms
2017-07-27 19:13:55,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154035000 ms.0 from job set of time 1501154035000 ms
2017-07-27 19:13:55,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:13:55,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 6 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:13:55,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 6 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:13:55,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:13:55,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:13:55,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:13:55,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:13:55,058 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:13:55,060 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_6_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:13:55,062 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:13:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcessNew.scala:47)
2017-07-27 19:13:55,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 6.0 with 2 tasks
2017-07-27 19:13:55,066 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 6.0 (TID 12, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:13:55,068 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 6.0 (TID 13, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:13:55,069 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 6.0 (TID 13)
2017-07-27 19:13:55,073 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 6.0 (TID 12)
2017-07-27 19:13:55,075 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 421 -> 446
2017-07-27 19:13:55,078 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 417 -> 442
2017-07-27 19:13:55,400 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_5_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:13:56,877 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 6.0 (TID 13). 787 bytes result sent to driver
2017-07-27 19:13:56,879 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 6.0 (TID 13) in 1812 ms on localhost (1/2)
2017-07-27 19:13:57,710 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 6.0 (TID 12). 874 bytes result sent to driver
2017-07-27 19:13:57,712 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 6.0 (TID 12) in 2648 ms on localhost (2/2)
2017-07-27 19:13:57,712 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2017-07-27 19:13:57,712 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 6 (foreachPartition at streamingProcessNew.scala:49) finished in 2.648 s
2017-07-27 19:13:57,712 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 6 finished: foreachPartition at streamingProcessNew.scala:49, took 2.671473 s
2017-07-27 19:13:57,713 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154035000 ms.0 from job set of time 1501154035000 ms
2017-07-27 19:13:57,713 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.713 s for time 1501154035000 ms (execution: 2.693 s)
2017-07-27 19:13:57,713 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 11 from persistence list
2017-07-27 19:13:57,713 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 11
2017-07-27 19:13:57,714 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 10 from persistence list
2017-07-27 19:13:57,714 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 10
2017-07-27 19:13:57,714 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:13:57,714 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154025000 ms
2017-07-27 19:14:00,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154040000 ms
2017-07-27 19:14:00,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154040000 ms.0 from job set of time 1501154040000 ms
2017-07-27 19:14:00,041 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:14:00,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 7 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:14:00,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 7 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:14:00,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:14:00,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:14:00,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:14:00,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:14:00,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:14:00,063 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_7_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:00,064 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:14:00,065 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcessNew.scala:47)
2017-07-27 19:14:00,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 7.0 with 2 tasks
2017-07-27 19:14:00,069 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 7.0 (TID 14, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:14:00,070 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 7.0 (TID 15, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:14:00,071 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 7.0 (TID 14)
2017-07-27 19:14:00,071 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 7.0 (TID 15)
2017-07-27 19:14:00,074 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 446 -> 471
2017-07-27 19:14:00,075 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 442 -> 467
2017-07-27 19:14:00,645 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_6_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:01,600 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 7.0 (TID 14). 787 bytes result sent to driver
2017-07-27 19:14:01,602 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 7.0 (TID 14) in 1535 ms on localhost (1/2)
2017-07-27 19:14:05,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154045000 ms
2017-07-27 19:14:09,373 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 7.0 (TID 15). 787 bytes result sent to driver
2017-07-27 19:14:09,375 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 7.0 (TID 15) in 9306 ms on localhost (2/2)
2017-07-27 19:14:09,375 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2017-07-27 19:14:09,375 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 7 (foreachPartition at streamingProcessNew.scala:49) finished in 9.308 s
2017-07-27 19:14:09,376 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 7 finished: foreachPartition at streamingProcessNew.scala:49, took 9.333801 s
2017-07-27 19:14:09,376 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154040000 ms.0 from job set of time 1501154040000 ms
2017-07-27 19:14:09,377 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 9.376 s for time 1501154040000 ms (execution: 9.355 s)
2017-07-27 19:14:09,377 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 13 from persistence list
2017-07-27 19:14:09,377 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154045000 ms.0 from job set of time 1501154045000 ms
2017-07-27 19:14:09,377 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 13
2017-07-27 19:14:09,377 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 12 from persistence list
2017-07-27 19:14:09,377 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 12
2017-07-27 19:14:09,377 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:14:09,378 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154030000 ms
2017-07-27 19:14:09,384 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:14:09,385 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 8 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:14:09,385 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 8 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:14:09,385 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:14:09,385 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:14:09,385 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:14:09,387 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:14:09,390 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:14:09,391 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_8_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:09,392 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:14:09,392 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcessNew.scala:47)
2017-07-27 19:14:09,392 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 8.0 with 2 tasks
2017-07-27 19:14:09,393 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 8.0 (TID 16, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:14:09,394 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 8.0 (TID 17, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:14:09,394 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 8.0 (TID 17)
2017-07-27 19:14:09,394 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 8.0 (TID 16)
2017-07-27 19:14:09,396 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 467 -> 492
2017-07-27 19:14:09,396 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 471 -> 496
2017-07-27 19:14:09,842 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_7_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:10,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154050000 ms
2017-07-27 19:14:15,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154055000 ms
2017-07-27 19:14:15,284 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 8.0 (TID 16). 787 bytes result sent to driver
2017-07-27 19:14:15,286 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 8.0 (TID 16) in 5893 ms on localhost (1/2)
2017-07-27 19:14:20,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154060000 ms
2017-07-27 19:14:20,863 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 8.0 (TID 17). 787 bytes result sent to driver
2017-07-27 19:14:20,865 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 8.0 (TID 17) in 11472 ms on localhost (2/2)
2017-07-27 19:14:20,865 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2017-07-27 19:14:20,865 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 8 (foreachPartition at streamingProcessNew.scala:49) finished in 11.473 s
2017-07-27 19:14:20,866 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 8 finished: foreachPartition at streamingProcessNew.scala:49, took 11.481029 s
2017-07-27 19:14:20,866 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154045000 ms.0 from job set of time 1501154045000 ms
2017-07-27 19:14:20,866 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 15 from persistence list
2017-07-27 19:14:20,866 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 15.866 s for time 1501154045000 ms (execution: 11.489 s)
2017-07-27 19:14:20,867 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154050000 ms.0 from job set of time 1501154050000 ms
2017-07-27 19:14:20,867 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 14 from persistence list
2017-07-27 19:14:20,867 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 15
2017-07-27 19:14:20,867 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:14:20,867 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154035000 ms
2017-07-27 19:14:20,867 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 14
2017-07-27 19:14:20,874 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:14:20,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 9 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:14:20,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 9 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:14:20,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:14:20,875 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:14:20,876 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:14:20,877 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:14:20,880 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:14:20,881 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_9_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:20,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:14:20,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcessNew.scala:47)
2017-07-27 19:14:20,881 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 9.0 with 2 tasks
2017-07-27 19:14:20,882 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 9.0 (TID 18, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:14:20,883 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 9.0 (TID 19, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:14:20,883 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 9.0 (TID 18)
2017-07-27 19:14:20,883 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 9.0 (TID 19)
2017-07-27 19:14:20,885 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 492 -> 517
2017-07-27 19:14:20,885 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 496 -> 521
2017-07-27 19:14:21,986 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_8_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:25,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154065000 ms
2017-07-27 19:14:27,474 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 9.0 (TID 18). 787 bytes result sent to driver
2017-07-27 19:14:27,476 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 9.0 (TID 18) in 6594 ms on localhost (1/2)
2017-07-27 19:14:30,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154070000 ms
2017-07-27 19:14:30,269 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 9.0 (TID 19). 787 bytes result sent to driver
2017-07-27 19:14:30,271 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 9.0 (TID 19) in 9387 ms on localhost (2/2)
2017-07-27 19:14:30,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 9 (foreachPartition at streamingProcessNew.scala:49) finished in 9.389 s
2017-07-27 19:14:30,271 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2017-07-27 19:14:30,271 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 9 finished: foreachPartition at streamingProcessNew.scala:49, took 9.396650 s
2017-07-27 19:14:30,272 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154050000 ms.0 from job set of time 1501154050000 ms
2017-07-27 19:14:30,272 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 20.272 s for time 1501154050000 ms (execution: 9.405 s)
2017-07-27 19:14:30,272 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 17 from persistence list
2017-07-27 19:14:30,272 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154055000 ms.0 from job set of time 1501154055000 ms
2017-07-27 19:14:30,272 [block-manager-slave-async-thread-pool-2] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 17
2017-07-27 19:14:30,272 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 16 from persistence list
2017-07-27 19:14:30,273 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:14:30,273 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 16
2017-07-27 19:14:30,273 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154040000 ms
2017-07-27 19:14:30,279 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:14:30,280 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 10 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:14:30,280 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 10 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:14:30,280 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:14:30,280 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:14:30,280 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:14:30,281 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:14:30,284 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:14:30,285 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_10_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:30,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:14:30,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcessNew.scala:47)
2017-07-27 19:14:30,286 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 10.0 with 2 tasks
2017-07-27 19:14:30,288 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 10.0 (TID 20, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:14:30,288 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 10.0 (TID 21, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:14:30,288 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 10.0 (TID 20)
2017-07-27 19:14:30,288 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 10.0 (TID 21)
2017-07-27 19:14:30,290 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 517 -> 542
2017-07-27 19:14:30,290 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 521 -> 545
2017-07-27 19:14:30,739 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_9_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:35,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154075000 ms
2017-07-27 19:14:36,365 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 10.0 (TID 20). 787 bytes result sent to driver
2017-07-27 19:14:36,367 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 10.0 (TID 20) in 6081 ms on localhost (1/2)
2017-07-27 19:14:37,930 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 10.0 (TID 21). 787 bytes result sent to driver
2017-07-27 19:14:37,933 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 10.0 (TID 21) in 7645 ms on localhost (2/2)
2017-07-27 19:14:37,934 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 10 (foreachPartition at streamingProcessNew.scala:49) finished in 7.647 s
2017-07-27 19:14:37,934 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2017-07-27 19:14:37,935 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 10 finished: foreachPartition at streamingProcessNew.scala:49, took 7.654979 s
2017-07-27 19:14:37,935 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154055000 ms.0 from job set of time 1501154055000 ms
2017-07-27 19:14:37,936 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 19 from persistence list
2017-07-27 19:14:37,936 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 22.935 s for time 1501154055000 ms (execution: 7.663 s)
2017-07-27 19:14:37,936 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154060000 ms.0 from job set of time 1501154060000 ms
2017-07-27 19:14:37,936 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 19
2017-07-27 19:14:37,937 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 18 from persistence list
2017-07-27 19:14:37,937 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 18
2017-07-27 19:14:37,937 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:14:37,938 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154045000 ms
2017-07-27 19:14:37,946 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:14:37,947 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 11 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:14:37,948 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 11 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:14:37,948 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:14:37,948 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:14:37,949 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:14:37,951 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:14:37,956 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:14:37,957 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_11_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:37,958 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 11 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:14:37,958 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcessNew.scala:47)
2017-07-27 19:14:37,959 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 11.0 with 2 tasks
2017-07-27 19:14:37,961 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 11.0 (TID 22, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:14:37,962 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 11.0 (TID 23, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:14:37,962 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 11.0 (TID 23)
2017-07-27 19:14:37,962 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 11.0 (TID 22)
2017-07-27 19:14:37,965 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 542 -> 567
2017-07-27 19:14:37,965 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:14:37,969 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 11.0 (TID 23). 714 bytes result sent to driver
2017-07-27 19:14:37,972 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 11.0 (TID 23) in 11 ms on localhost (1/2)
2017-07-27 19:14:40,022 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154080000 ms
2017-07-27 19:14:42,153 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_10_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:44,252 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 11.0 (TID 22). 787 bytes result sent to driver
2017-07-27 19:14:44,254 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 11.0 (TID 22) in 6295 ms on localhost (2/2)
2017-07-27 19:14:44,254 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2017-07-27 19:14:44,254 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 11 (foreachPartition at streamingProcessNew.scala:49) finished in 6.295 s
2017-07-27 19:14:44,254 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 11 finished: foreachPartition at streamingProcessNew.scala:49, took 6.307544 s
2017-07-27 19:14:44,255 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154060000 ms.0 from job set of time 1501154060000 ms
2017-07-27 19:14:44,255 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 21 from persistence list
2017-07-27 19:14:44,256 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 21
2017-07-27 19:14:44,256 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 20 from persistence list
2017-07-27 19:14:44,256 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 24.255 s for time 1501154060000 ms (execution: 6.319 s)
2017-07-27 19:14:44,256 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154065000 ms.0 from job set of time 1501154065000 ms
2017-07-27 19:14:44,256 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 20
2017-07-27 19:14:44,256 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:14:44,256 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154050000 ms
2017-07-27 19:14:44,261 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:14:44,262 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 12 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:14:44,262 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 12 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:14:44,262 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:14:44,262 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:14:44,262 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:14:44,263 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:14:44,267 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:14:44,268 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_12_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:44,268 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:14:44,268 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcessNew.scala:47)
2017-07-27 19:14:44,269 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 12.0 with 2 tasks
2017-07-27 19:14:44,270 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 12.0 (TID 24, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:14:44,270 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 12.0 (TID 25, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:14:44,270 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 12.0 (TID 25)
2017-07-27 19:14:44,270 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 12.0 (TID 24)
2017-07-27 19:14:44,272 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:14:44,272 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 567 -> 568
2017-07-27 19:14:44,275 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 12.0 (TID 25). 714 bytes result sent to driver
2017-07-27 19:14:44,277 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 12.0 (TID 25) in 7 ms on localhost (1/2)
2017-07-27 19:14:44,834 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 12.0 (TID 24). 714 bytes result sent to driver
2017-07-27 19:14:44,835 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 12.0 (TID 24) in 566 ms on localhost (2/2)
2017-07-27 19:14:44,835 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2017-07-27 19:14:44,835 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 12 (foreachPartition at streamingProcessNew.scala:49) finished in 0.566 s
2017-07-27 19:14:44,836 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 12 finished: foreachPartition at streamingProcessNew.scala:49, took 0.574359 s
2017-07-27 19:14:44,836 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154065000 ms.0 from job set of time 1501154065000 ms
2017-07-27 19:14:44,836 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 19.836 s for time 1501154065000 ms (execution: 0.580 s)
2017-07-27 19:14:44,836 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 23 from persistence list
2017-07-27 19:14:44,837 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154070000 ms.0 from job set of time 1501154070000 ms
2017-07-27 19:14:44,837 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 23
2017-07-27 19:14:44,837 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 22 from persistence list
2017-07-27 19:14:44,837 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 22
2017-07-27 19:14:44,837 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:14:44,837 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154055000 ms
2017-07-27 19:14:44,842 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:14:44,843 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 13 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:14:44,843 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 13 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:14:44,843 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:14:44,843 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:14:44,844 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:14:44,845 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:14:44,848 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:14:44,849 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_13_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:44,850 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 13 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:14:44,850 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcessNew.scala:47)
2017-07-27 19:14:44,850 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 13.0 with 2 tasks
2017-07-27 19:14:44,851 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 13.0 (TID 26, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:14:44,852 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 13.0 (TID 27, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:14:44,852 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 13.0 (TID 27)
2017-07-27 19:14:44,852 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 13.0 (TID 26)
2017-07-27 19:14:44,853 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 19:14:44,853 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:14:44,856 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 13.0 (TID 26). 714 bytes result sent to driver
2017-07-27 19:14:44,856 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 13.0 (TID 27). 714 bytes result sent to driver
2017-07-27 19:14:44,858 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 13.0 (TID 26) in 7 ms on localhost (1/2)
2017-07-27 19:14:44,858 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 13.0 (TID 27) in 7 ms on localhost (2/2)
2017-07-27 19:14:44,858 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2017-07-27 19:14:44,858 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 13 (foreachPartition at streamingProcessNew.scala:49) finished in 0.008 s
2017-07-27 19:14:44,858 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 13 finished: foreachPartition at streamingProcessNew.scala:49, took 0.016100 s
2017-07-27 19:14:44,859 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154070000 ms.0 from job set of time 1501154070000 ms
2017-07-27 19:14:44,859 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 14.859 s for time 1501154070000 ms (execution: 0.023 s)
2017-07-27 19:14:44,859 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 25 from persistence list
2017-07-27 19:14:44,859 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154075000 ms.0 from job set of time 1501154075000 ms
2017-07-27 19:14:44,859 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 25
2017-07-27 19:14:44,859 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 24 from persistence list
2017-07-27 19:14:44,860 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 24
2017-07-27 19:14:44,860 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:14:44,860 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154060000 ms
2017-07-27 19:14:44,865 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:14:44,866 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 14 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:14:44,866 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 14 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:14:44,866 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:14:44,866 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:14:44,866 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:14:44,867 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:14:44,870 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:14:44,871 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_14_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:44,871 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:14:44,871 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcessNew.scala:47)
2017-07-27 19:14:44,871 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 14.0 with 2 tasks
2017-07-27 19:14:44,872 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 14.0 (TID 28, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:14:44,873 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 14.0 (TID 29, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:14:44,873 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 14.0 (TID 29)
2017-07-27 19:14:44,873 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 14.0 (TID 28)
2017-07-27 19:14:44,874 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:14:44,874 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 19:14:44,878 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 14.0 (TID 29). 801 bytes result sent to driver
2017-07-27 19:14:44,878 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 14.0 (TID 28). 801 bytes result sent to driver
2017-07-27 19:14:44,879 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 14.0 (TID 28) in 7 ms on localhost (1/2)
2017-07-27 19:14:44,880 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 14.0 (TID 29) in 8 ms on localhost (2/2)
2017-07-27 19:14:44,880 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2017-07-27 19:14:44,880 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 14 (foreachPartition at streamingProcessNew.scala:49) finished in 0.008 s
2017-07-27 19:14:44,880 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 14 finished: foreachPartition at streamingProcessNew.scala:49, took 0.014866 s
2017-07-27 19:14:44,881 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154075000 ms.0 from job set of time 1501154075000 ms
2017-07-27 19:14:44,881 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 9.881 s for time 1501154075000 ms (execution: 0.022 s)
2017-07-27 19:14:44,881 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 27 from persistence list
2017-07-27 19:14:44,881 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154080000 ms.0 from job set of time 1501154080000 ms
2017-07-27 19:14:44,881 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 27
2017-07-27 19:14:44,881 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 26 from persistence list
2017-07-27 19:14:44,882 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 26
2017-07-27 19:14:44,882 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:14:44,882 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154065000 ms
2017-07-27 19:14:44,887 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:14:44,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 15 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:14:44,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 15 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:14:44,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:14:44,887 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:14:44,888 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:14:44,889 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:14:44,891 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:14:44,892 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_15_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:44,892 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 15 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:14:44,892 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcessNew.scala:47)
2017-07-27 19:14:44,892 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 15.0 with 2 tasks
2017-07-27 19:14:44,893 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 15.0 (TID 30, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:14:44,894 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 15.0 (TID 31, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:14:44,894 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 15.0 (TID 30)
2017-07-27 19:14:44,894 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 15.0 (TID 31)
2017-07-27 19:14:44,895 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:14:44,895 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 19:14:44,898 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 15.0 (TID 31). 714 bytes result sent to driver
2017-07-27 19:14:44,898 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 15.0 (TID 30). 714 bytes result sent to driver
2017-07-27 19:14:44,900 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 15.0 (TID 30) in 6 ms on localhost (1/2)
2017-07-27 19:14:44,900 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 15.0 (TID 31) in 6 ms on localhost (2/2)
2017-07-27 19:14:44,900 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2017-07-27 19:14:44,900 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 15 (foreachPartition at streamingProcessNew.scala:49) finished in 0.007 s
2017-07-27 19:14:44,900 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 15 finished: foreachPartition at streamingProcessNew.scala:49, took 0.013308 s
2017-07-27 19:14:44,901 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154080000 ms.0 from job set of time 1501154080000 ms
2017-07-27 19:14:44,901 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 4.901 s for time 1501154080000 ms (execution: 0.020 s)
2017-07-27 19:14:44,901 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 29 from persistence list
2017-07-27 19:14:44,901 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 29
2017-07-27 19:14:44,901 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 28 from persistence list
2017-07-27 19:14:44,902 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 28
2017-07-27 19:14:44,902 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:14:44,902 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154070000 ms
2017-07-27 19:14:45,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154085000 ms
2017-07-27 19:14:45,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154085000 ms.0 from job set of time 1501154085000 ms
2017-07-27 19:14:45,029 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:14:45,030 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 16 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:14:45,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 16 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:14:45,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:14:45,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:14:45,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:14:45,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:14:45,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:14:45,040 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_16_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:45,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 16 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:14:45,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcessNew.scala:47)
2017-07-27 19:14:45,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 16.0 with 2 tasks
2017-07-27 19:14:45,044 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 16.0 (TID 32, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:14:45,045 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 16.0 (TID 33, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:14:45,045 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 16.0 (TID 32)
2017-07-27 19:14:45,046 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 16.0 (TID 33)
2017-07-27 19:14:45,048 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:14:45,050 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 19:14:45,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 16.0 (TID 33). 714 bytes result sent to driver
2017-07-27 19:14:45,057 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 16.0 (TID 32). 714 bytes result sent to driver
2017-07-27 19:14:45,057 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 16.0 (TID 33) in 13 ms on localhost (1/2)
2017-07-27 19:14:45,060 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 16.0 (TID 32) in 18 ms on localhost (2/2)
2017-07-27 19:14:45,061 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2017-07-27 19:14:45,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 16 (foreachPartition at streamingProcessNew.scala:49) finished in 0.018 s
2017-07-27 19:14:45,061 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 16 finished: foreachPartition at streamingProcessNew.scala:49, took 0.031543 s
2017-07-27 19:14:45,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154085000 ms.0 from job set of time 1501154085000 ms
2017-07-27 19:14:45,063 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 31 from persistence list
2017-07-27 19:14:45,063 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.063 s for time 1501154085000 ms (execution: 0.045 s)
2017-07-27 19:14:45,064 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 31
2017-07-27 19:14:45,064 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 30 from persistence list
2017-07-27 19:14:45,064 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 30
2017-07-27 19:14:45,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:14:45,065 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154075000 ms
2017-07-27 19:14:50,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154090000 ms
2017-07-27 19:14:50,015 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154090000 ms.0 from job set of time 1501154090000 ms
2017-07-27 19:14:50,034 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:14:50,034 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_16_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:50,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 17 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:14:50,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 17 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:14:50,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:14:50,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:14:50,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:14:50,036 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_11_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:50,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:14:50,039 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_12_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:50,041 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_13_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:50,043 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_14_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:50,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:14:50,044 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_17_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:50,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 17 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:14:50,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcessNew.scala:47)
2017-07-27 19:14:50,046 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_15_piece0 on 192.168.31.105:53013 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:50,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 17.0 with 2 tasks
2017-07-27 19:14:50,048 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 17.0 (TID 34, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:14:50,048 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 17.0 (TID 35, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:14:50,049 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 17.0 (TID 34)
2017-07-27 19:14:50,049 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 17.0 (TID 35)
2017-07-27 19:14:50,050 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 19:14:50,050 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:14:50,054 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 17.0 (TID 34). 714 bytes result sent to driver
2017-07-27 19:14:50,054 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 17.0 (TID 35). 714 bytes result sent to driver
2017-07-27 19:14:50,056 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 17.0 (TID 34) in 9 ms on localhost (1/2)
2017-07-27 19:14:50,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 17.0 (TID 35) in 8 ms on localhost (2/2)
2017-07-27 19:14:50,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 17 (foreachPartition at streamingProcessNew.scala:49) finished in 0.011 s
2017-07-27 19:14:50,057 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2017-07-27 19:14:50,058 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 17 finished: foreachPartition at streamingProcessNew.scala:49, took 0.023588 s
2017-07-27 19:14:50,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154090000 ms.0 from job set of time 1501154090000 ms
2017-07-27 19:14:50,058 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.058 s for time 1501154090000 ms (execution: 0.043 s)
2017-07-27 19:14:50,058 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 33 from persistence list
2017-07-27 19:14:50,059 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 33
2017-07-27 19:14:50,059 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 32 from persistence list
2017-07-27 19:14:50,059 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 32
2017-07-27 19:14:50,059 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:14:50,060 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154080000 ms
2017-07-27 19:14:55,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154095000 ms
2017-07-27 19:14:55,020 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154095000 ms.0 from job set of time 1501154095000 ms
2017-07-27 19:14:55,040 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:14:55,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 18 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:14:55,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 18 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:14:55,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:14:55,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:14:55,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:14:55,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:14:55,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:14:55,056 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_18_piece0 in memory on 192.168.31.105:53013 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:14:55,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 18 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:14:55,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcessNew.scala:47)
2017-07-27 19:14:55,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 18.0 with 2 tasks
2017-07-27 19:14:55,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 18.0 (TID 36, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:14:55,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 18.0 (TID 37, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:14:55,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 18.0 (TID 36)
2017-07-27 19:14:55,059 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 18.0 (TID 37)
2017-07-27 19:14:55,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 19:14:55,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:14:55,065 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 18.0 (TID 37). 714 bytes result sent to driver
2017-07-27 19:14:55,065 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 18.0 (TID 36). 714 bytes result sent to driver
2017-07-27 19:14:55,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 18.0 (TID 37) in 8 ms on localhost (1/2)
2017-07-27 19:14:55,067 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 18.0 (TID 36) in 9 ms on localhost (2/2)
2017-07-27 19:14:55,068 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2017-07-27 19:14:55,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 18 (foreachPartition at streamingProcessNew.scala:49) finished in 0.010 s
2017-07-27 19:14:55,068 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 18 finished: foreachPartition at streamingProcessNew.scala:49, took 0.027773 s
2017-07-27 19:14:55,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154095000 ms.0 from job set of time 1501154095000 ms
2017-07-27 19:14:55,069 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.069 s for time 1501154095000 ms (execution: 0.049 s)
2017-07-27 19:14:55,069 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 35 from persistence list
2017-07-27 19:14:55,069 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 35
2017-07-27 19:14:55,069 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 34 from persistence list
2017-07-27 19:14:55,070 [block-manager-slave-async-thread-pool-3] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 34
2017-07-27 19:14:55,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:14:55,070 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154085000 ms
2017-07-27 19:19:40,198 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-07-27 19:19:40,450 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-07-27 19:19:40,556 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-07-27 19:19:40,557 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-07-27 19:19:40,559 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-07-27 19:19:40,560 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-07-27 19:19:40,562 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-07-27 19:19:41,395 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 53204.
2017-07-27 19:19:41,415 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-07-27 19:19:41,433 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-07-27 19:19:41,447 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-3323b27d-e151-4612-b1f7-4ecdcdb27835
2017-07-27 19:19:41,460 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-07-27 19:19:41,504 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-07-27 19:19:41,587 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2178ms
2017-07-27 19:19:41,694 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-07-27 19:19:41,716 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/jobs,null,AVAILABLE}
2017-07-27 19:19:41,716 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/jobs/json,null,AVAILABLE}
2017-07-27 19:19:41,716 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/jobs/job,null,AVAILABLE}
2017-07-27 19:19:41,717 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/jobs/job/json,null,AVAILABLE}
2017-07-27 19:19:41,717 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/stages,null,AVAILABLE}
2017-07-27 19:19:41,718 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/stages/json,null,AVAILABLE}
2017-07-27 19:19:41,718 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/stages/stage,null,AVAILABLE}
2017-07-27 19:19:41,718 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/stages/stage/json,null,AVAILABLE}
2017-07-27 19:19:41,719 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/stages/pool,null,AVAILABLE}
2017-07-27 19:19:41,719 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/stages/pool/json,null,AVAILABLE}
2017-07-27 19:19:41,719 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/storage,null,AVAILABLE}
2017-07-27 19:19:41,719 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/storage/json,null,AVAILABLE}
2017-07-27 19:19:41,719 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/storage/rdd,null,AVAILABLE}
2017-07-27 19:19:41,719 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/storage/rdd/json,null,AVAILABLE}
2017-07-27 19:19:41,720 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/environment,null,AVAILABLE}
2017-07-27 19:19:41,720 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0fba8{/environment/json,null,AVAILABLE}
2017-07-27 19:19:41,720 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5de5a4{/executors,null,AVAILABLE}
2017-07-27 19:19:41,720 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1ef6856{/executors/json,null,AVAILABLE}
2017-07-27 19:19:41,720 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@b7a938{/executors/threadDump,null,AVAILABLE}
2017-07-27 19:19:41,720 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1af2e7d{/executors/threadDump/json,null,AVAILABLE}
2017-07-27 19:19:41,725 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@648ce9{/static,null,AVAILABLE}
2017-07-27 19:19:41,726 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9bf9eb{/,null,AVAILABLE}
2017-07-27 19:19:41,726 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1526469{/api,null,AVAILABLE}
2017-07-27 19:19:41,726 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@14872f8{/stages/stage/kill,null,AVAILABLE}
2017-07-27 19:19:41,734 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@1a90fa8{HTTP/1.1}{0.0.0.0:4040}
2017-07-27 19:19:41,734 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2325ms
2017-07-27 19:19:41,734 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-07-27 19:19:41,736 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.105:4040
2017-07-27 19:19:41,822 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-07-27 19:19:41,857 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53217.
2017-07-27 19:19:41,858 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.105:53217
2017-07-27 19:19:41,860 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.105, 53217)
2017-07-27 19:19:41,862 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.105:53217 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.105, 53217)
2017-07-27 19:19:41,864 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.105, 53217)
2017-07-27 19:19:42,078 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13ddbd9{/metrics/json,null,AVAILABLE}
2017-07-27 19:19:42,578 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-07-27 19:19:42,578 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-07-27 19:19:42,579 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-07-27 19:19:42,580 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-07-27 19:19:42,581 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@2d559
2017-07-27 19:19:42,581 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-07-27 19:19:42,581 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-07-27 19:19:42,581 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-07-27 19:19:42,581 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-07-27 19:19:42,581 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@1a3b008
2017-07-27 19:19:42,582 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-07-27 19:19:42,582 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-07-27 19:19:42,582 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-07-27 19:19:42,582 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-07-27 19:19:42,582 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@16e6830
2017-07-27 19:19:42,627 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1501154385000
2017-07-27 19:19:42,628 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1501154385000 ms
2017-07-27 19:19:42,629 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-07-27 19:19:42,630 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cf395{/streaming,null,AVAILABLE}
2017-07-27 19:19:42,631 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6aa04a{/streaming/json,null,AVAILABLE}
2017-07-27 19:19:42,631 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch,null,AVAILABLE}
2017-07-27 19:19:42,631 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@814f43{/streaming/batch/json,null,AVAILABLE}
2017-07-27 19:19:42,632 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1a00961{/static/streaming,null,AVAILABLE}
2017-07-27 19:19:42,633 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-07-27 19:19:45,102 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154385000 ms
2017-07-27 19:19:45,106 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154385000 ms.0 from job set of time 1501154385000 ms
2017-07-27 19:19:45,132 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:19:45,145 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:19:45,146 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:19:45,146 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:19:45,147 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:19:45,156 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:19:45,217 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-07-27 19:19:45,225 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:19:45,435 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:19:45,438 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:19:45,440 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:19:45,444 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:47)
2017-07-27 19:19:45,446 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-07-27 19:19:45,487 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-07-27 19:19:45,490 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-07-27 19:19:45,496 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-07-27 19:19:45,496 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-07-27 19:19:45,520 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 267 -> 292
2017-07-27 19:19:45,520 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 271 -> 296
2017-07-27 19:19:45,588 [Executor task launch worker-1] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-07-27 19:19:45,829 [Executor task launch worker-1] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-07-27 19:19:50,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154390000 ms
2017-07-27 19:19:51,069 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-07-27 19:19:51,086 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 5617 ms on localhost (1/2)
2017-07-27 19:19:52,213 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 787 bytes result sent to driver
2017-07-27 19:19:52,223 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 6733 ms on localhost (2/2)
2017-07-27 19:19:52,225 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcessNew.scala:49) finished in 6.765 s
2017-07-27 19:19:52,226 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-07-27 19:19:52,238 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcessNew.scala:49, took 7.104959 s
2017-07-27 19:19:52,243 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154385000 ms.0 from job set of time 1501154385000 ms
2017-07-27 19:19:52,245 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 7.242 s for time 1501154385000 ms (execution: 7.138 s)
2017-07-27 19:19:52,246 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154390000 ms.0 from job set of time 1501154390000 ms
2017-07-27 19:19:52,252 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:19:52,253 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:19:52,253 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:19:52,253 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:19:52,253 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:19:52,253 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:19:52,254 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:19:52,256 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:19:52,258 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-07-27 19:19:52,262 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:19:52,263 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:19:52,263 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:19:52,264 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:47)
2017-07-27 19:19:52,264 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-07-27 19:19:52,266 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-07-27 19:19:52,267 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-07-27 19:19:52,268 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-07-27 19:19:52,268 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-07-27 19:19:52,271 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 292 -> 317
2017-07-27 19:19:52,271 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 296 -> 321
2017-07-27 19:19:55,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154395000 ms
2017-07-27 19:19:55,137 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 787 bytes result sent to driver
2017-07-27 19:19:55,141 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 2875 ms on localhost (1/2)
2017-07-27 19:19:55,484 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 787 bytes result sent to driver
2017-07-27 19:19:55,488 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 3224 ms on localhost (2/2)
2017-07-27 19:19:55,489 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-07-27 19:19:55,489 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreachPartition at streamingProcessNew.scala:49) finished in 3.225 s
2017-07-27 19:19:55,489 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: foreachPartition at streamingProcessNew.scala:49, took 3.236920 s
2017-07-27 19:19:55,489 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154390000 ms.0 from job set of time 1501154390000 ms
2017-07-27 19:19:55,490 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 5.489 s for time 1501154390000 ms (execution: 3.243 s)
2017-07-27 19:19:55,490 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154395000 ms.0 from job set of time 1501154395000 ms
2017-07-27 19:19:55,490 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 1 from persistence list
2017-07-27 19:19:55,498 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:19:55,499 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:19:55,499 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:19:55,499 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:19:55,499 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:19:55,499 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:19:55,500 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 0 from persistence list
2017-07-27 19:19:55,501 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:19:55,505 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 1
2017-07-27 19:19:55,507 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:19:55,507 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:19:55,508 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-07-27 19:19:55,508 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 0
2017-07-27 19:19:55,508 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:19:55,509 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:19:55,510 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at streamingProcessNew.scala:47)
2017-07-27 19:19:55,510 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2017-07-27 19:19:55,514 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5655 bytes)
2017-07-27 19:19:55,516 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5655 bytes)
2017-07-27 19:19:55,516 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2017-07-27 19:19:55,516 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2017-07-27 19:19:55,520 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 321 -> 346
2017-07-27 19:19:55,520 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 317 -> 342
2017-07-27 19:19:56,058 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:19:56,614 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 787 bytes result sent to driver
2017-07-27 19:19:56,618 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 1108 ms on localhost (1/2)
2017-07-27 19:19:58,876 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2.0 (TID 5). 787 bytes result sent to driver
2017-07-27 19:19:58,884 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2.0 (TID 5) in 3370 ms on localhost (2/2)
2017-07-27 19:19:58,885 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (foreachPartition at streamingProcessNew.scala:49) finished in 3.375 s
2017-07-27 19:19:58,885 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-07-27 19:19:58,886 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: foreachPartition at streamingProcessNew.scala:49, took 3.388251 s
2017-07-27 19:19:58,888 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154395000 ms.0 from job set of time 1501154395000 ms
2017-07-27 19:19:58,889 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 3 from persistence list
2017-07-27 19:19:58,889 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 3.888 s for time 1501154395000 ms (execution: 3.398 s)
2017-07-27 19:19:58,890 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 3
2017-07-27 19:19:58,890 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 2 from persistence list
2017-07-27 19:19:58,891 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 2
2017-07-27 19:19:58,892 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:19:58,893 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154385000 ms
2017-07-27 19:20:00,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154400000 ms
2017-07-27 19:20:00,022 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154400000 ms.0 from job set of time 1501154400000 ms
2017-07-27 19:20:00,045 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:20:00,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:20:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 3 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:20:00,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:20:00,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:20:00,050 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:20:00,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:20:00,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:20:00,069 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:00,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:20:00,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at streamingProcessNew.scala:47)
2017-07-27 19:20:00,070 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 3.0 with 2 tasks
2017-07-27 19:20:00,073 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 3.0 (TID 6, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:20:00,074 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 3.0 (TID 7, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:20:00,075 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 3.0 (TID 7)
2017-07-27 19:20:00,075 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 3.0 (TID 6)
2017-07-27 19:20:00,079 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 342 -> 367
2017-07-27 19:20:00,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 346 -> 371
2017-07-27 19:20:01,114 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 3.0 (TID 7). 714 bytes result sent to driver
2017-07-27 19:20:01,116 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 3.0 (TID 7) in 1043 ms on localhost (1/2)
2017-07-27 19:20:03,236 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:04,918 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 3.0 (TID 6). 787 bytes result sent to driver
2017-07-27 19:20:04,921 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 3.0 (TID 6) in 4850 ms on localhost (2/2)
2017-07-27 19:20:04,922 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 3 (foreachPartition at streamingProcessNew.scala:49) finished in 4.851 s
2017-07-27 19:20:04,922 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-07-27 19:20:04,922 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: foreachPartition at streamingProcessNew.scala:49, took 4.876646 s
2017-07-27 19:20:04,922 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154400000 ms.0 from job set of time 1501154400000 ms
2017-07-27 19:20:04,923 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 4.922 s for time 1501154400000 ms (execution: 4.900 s)
2017-07-27 19:20:04,923 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 5 from persistence list
2017-07-27 19:20:04,923 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 5
2017-07-27 19:20:04,923 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 4 from persistence list
2017-07-27 19:20:04,924 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 4
2017-07-27 19:20:04,924 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:20:04,924 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154390000 ms
2017-07-27 19:20:05,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154405000 ms
2017-07-27 19:20:05,018 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154405000 ms.0 from job set of time 1501154405000 ms
2017-07-27 19:20:05,030 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:20:05,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 4 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:20:05,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:20:05,031 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:20:05,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:20:05,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:20:05,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:20:05,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:20:05,044 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:05,045 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:20:05,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at streamingProcessNew.scala:47)
2017-07-27 19:20:05,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 2 tasks
2017-07-27 19:20:05,050 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 8, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:20:05,050 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 4.0 (TID 9, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:20:05,051 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 4.0 (TID 9)
2017-07-27 19:20:05,051 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 8)
2017-07-27 19:20:05,053 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 367 -> 392
2017-07-27 19:20:05,053 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 371 -> 396
2017-07-27 19:20:05,566 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_3_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:06,025 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 8). 787 bytes result sent to driver
2017-07-27 19:20:06,027 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 8) in 979 ms on localhost (1/2)
2017-07-27 19:20:06,423 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 4.0 (TID 9). 787 bytes result sent to driver
2017-07-27 19:20:06,426 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 4.0 (TID 9) in 1376 ms on localhost (2/2)
2017-07-27 19:20:06,426 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (foreachPartition at streamingProcessNew.scala:49) finished in 1.378 s
2017-07-27 19:20:06,426 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-07-27 19:20:06,427 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 4 finished: foreachPartition at streamingProcessNew.scala:49, took 1.396509 s
2017-07-27 19:20:06,427 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154405000 ms.0 from job set of time 1501154405000 ms
2017-07-27 19:20:06,427 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 1.427 s for time 1501154405000 ms (execution: 1.409 s)
2017-07-27 19:20:06,427 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 7 from persistence list
2017-07-27 19:20:06,428 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 7
2017-07-27 19:20:06,428 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 6 from persistence list
2017-07-27 19:20:06,428 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 6
2017-07-27 19:20:06,429 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:20:06,429 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154395000 ms
2017-07-27 19:20:10,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154410000 ms
2017-07-27 19:20:10,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154410000 ms.0 from job set of time 1501154410000 ms
2017-07-27 19:20:10,043 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:20:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 5 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:20:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 5 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:20:10,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:20:10,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:20:10,048 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:20:10,052 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:20:10,063 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:20:10,065 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:10,066 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:20:10,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at streamingProcessNew.scala:47)
2017-07-27 19:20:10,068 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 5.0 with 2 tasks
2017-07-27 19:20:10,072 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 5.0 (TID 10, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:20:10,073 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 5.0 (TID 11, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:20:10,074 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 5.0 (TID 10)
2017-07-27 19:20:10,074 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 5.0 (TID 11)
2017-07-27 19:20:10,080 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 396 -> 421
2017-07-27 19:20:10,080 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 392 -> 417
2017-07-27 19:20:10,973 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_4_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:11,571 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 5.0 (TID 10). 787 bytes result sent to driver
2017-07-27 19:20:11,574 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 5.0 (TID 10) in 1505 ms on localhost (1/2)
2017-07-27 19:20:12,898 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 5.0 (TID 11). 787 bytes result sent to driver
2017-07-27 19:20:12,900 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 5.0 (TID 11) in 2827 ms on localhost (2/2)
2017-07-27 19:20:12,900 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 5 (foreachPartition at streamingProcessNew.scala:49) finished in 2.831 s
2017-07-27 19:20:12,900 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-07-27 19:20:12,900 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 5 finished: foreachPartition at streamingProcessNew.scala:49, took 2.856106 s
2017-07-27 19:20:12,901 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154410000 ms.0 from job set of time 1501154410000 ms
2017-07-27 19:20:12,901 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.901 s for time 1501154410000 ms (execution: 2.880 s)
2017-07-27 19:20:12,901 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 9 from persistence list
2017-07-27 19:20:12,901 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 9
2017-07-27 19:20:12,902 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 8 from persistence list
2017-07-27 19:20:12,902 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 8
2017-07-27 19:20:12,902 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:20:12,902 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154400000 ms
2017-07-27 19:20:15,021 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154415000 ms
2017-07-27 19:20:15,024 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154415000 ms.0 from job set of time 1501154415000 ms
2017-07-27 19:20:15,042 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:20:15,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 6 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:20:15,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 6 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:20:15,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:20:15,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:20:15,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:20:15,046 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:20:15,051 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:20:15,053 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_6_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:20:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at streamingProcessNew.scala:47)
2017-07-27 19:20:15,054 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 6.0 with 2 tasks
2017-07-27 19:20:15,057 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 6.0 (TID 12, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:20:15,058 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 6.0 (TID 13, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:20:15,058 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 6.0 (TID 13)
2017-07-27 19:20:15,059 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 6.0 (TID 12)
2017-07-27 19:20:15,061 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 421 -> 446
2017-07-27 19:20:15,061 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 417 -> 442
2017-07-27 19:20:15,524 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_5_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:16,867 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 6.0 (TID 13). 787 bytes result sent to driver
2017-07-27 19:20:16,869 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 6.0 (TID 13) in 1812 ms on localhost (1/2)
2017-07-27 19:20:17,776 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 6.0 (TID 12). 787 bytes result sent to driver
2017-07-27 19:20:17,779 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 6.0 (TID 12) in 2723 ms on localhost (2/2)
2017-07-27 19:20:17,779 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 6 (foreachPartition at streamingProcessNew.scala:49) finished in 2.724 s
2017-07-27 19:20:17,779 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2017-07-27 19:20:17,779 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 6 finished: foreachPartition at streamingProcessNew.scala:49, took 2.736953 s
2017-07-27 19:20:17,779 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154415000 ms.0 from job set of time 1501154415000 ms
2017-07-27 19:20:17,780 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 11 from persistence list
2017-07-27 19:20:17,780 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 2.779 s for time 1501154415000 ms (execution: 2.757 s)
2017-07-27 19:20:17,780 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 11
2017-07-27 19:20:17,780 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 10 from persistence list
2017-07-27 19:20:17,781 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 10
2017-07-27 19:20:17,781 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:20:17,781 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154405000 ms
2017-07-27 19:20:20,020 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154420000 ms
2017-07-27 19:20:20,021 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154420000 ms.0 from job set of time 1501154420000 ms
2017-07-27 19:20:20,039 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:20:20,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 7 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:20:20,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 7 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:20:20,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:20:20,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:20:20,043 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:20:20,047 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:20:20,057 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:20:20,059 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_7_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:20:20,060 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at streamingProcessNew.scala:47)
2017-07-27 19:20:20,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 7.0 with 2 tasks
2017-07-27 19:20:20,063 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 7.0 (TID 14, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:20:20,064 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 7.0 (TID 15, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:20:20,064 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 7.0 (TID 14)
2017-07-27 19:20:20,064 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 7.0 (TID 15)
2017-07-27 19:20:20,066 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 446 -> 471
2017-07-27 19:20:20,067 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 442 -> 467
2017-07-27 19:20:20,798 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_6_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:21,790 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 7.0 (TID 14). 787 bytes result sent to driver
2017-07-27 19:20:21,792 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 7.0 (TID 14) in 1731 ms on localhost (1/2)
2017-07-27 19:20:25,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154425000 ms
2017-07-27 19:20:30,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154430000 ms
2017-07-27 19:20:30,403 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 7.0 (TID 15). 787 bytes result sent to driver
2017-07-27 19:20:30,406 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 7.0 (TID 15) in 10343 ms on localhost (2/2)
2017-07-27 19:20:30,406 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2017-07-27 19:20:30,406 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 7 (foreachPartition at streamingProcessNew.scala:49) finished in 10.345 s
2017-07-27 19:20:30,407 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 7 finished: foreachPartition at streamingProcessNew.scala:49, took 10.367209 s
2017-07-27 19:20:30,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154420000 ms.0 from job set of time 1501154420000 ms
2017-07-27 19:20:30,407 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 13 from persistence list
2017-07-27 19:20:30,407 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 10.407 s for time 1501154420000 ms (execution: 10.386 s)
2017-07-27 19:20:30,408 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154425000 ms.0 from job set of time 1501154425000 ms
2017-07-27 19:20:30,408 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 13
2017-07-27 19:20:30,408 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 12 from persistence list
2017-07-27 19:20:30,408 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 12
2017-07-27 19:20:30,408 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:20:30,408 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154410000 ms
2017-07-27 19:20:30,414 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:20:30,414 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 8 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:20:30,414 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 8 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:20:30,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:20:30,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:20:30,415 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:20:30,416 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:20:30,420 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:20:30,421 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_8_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:30,421 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:20:30,421 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at streamingProcessNew.scala:47)
2017-07-27 19:20:30,422 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 8.0 with 2 tasks
2017-07-27 19:20:30,423 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 8.0 (TID 16, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:20:30,424 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 8.0 (TID 17, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:20:30,424 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 8.0 (TID 17)
2017-07-27 19:20:30,424 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 8.0 (TID 16)
2017-07-27 19:20:30,426 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 467 -> 492
2017-07-27 19:20:30,426 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 471 -> 496
2017-07-27 19:20:31,559 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_7_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:35,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154435000 ms
2017-07-27 19:20:38,005 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 8.0 (TID 16). 787 bytes result sent to driver
2017-07-27 19:20:38,007 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 8.0 (TID 16) in 7585 ms on localhost (1/2)
2017-07-27 19:20:40,014 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154440000 ms
2017-07-27 19:20:42,653 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 8.0 (TID 17). 787 bytes result sent to driver
2017-07-27 19:20:42,655 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 8.0 (TID 17) in 12231 ms on localhost (2/2)
2017-07-27 19:20:42,655 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2017-07-27 19:20:42,655 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 8 (foreachPartition at streamingProcessNew.scala:49) finished in 12.233 s
2017-07-27 19:20:42,655 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 8 finished: foreachPartition at streamingProcessNew.scala:49, took 12.241103 s
2017-07-27 19:20:42,656 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154425000 ms.0 from job set of time 1501154425000 ms
2017-07-27 19:20:42,656 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 17.656 s for time 1501154425000 ms (execution: 12.249 s)
2017-07-27 19:20:42,656 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 15 from persistence list
2017-07-27 19:20:42,656 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154430000 ms.0 from job set of time 1501154430000 ms
2017-07-27 19:20:42,657 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 15
2017-07-27 19:20:42,657 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 14 from persistence list
2017-07-27 19:20:42,657 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 14
2017-07-27 19:20:42,657 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:20:42,657 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154415000 ms
2017-07-27 19:20:42,662 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:20:42,663 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 9 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:20:42,664 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 9 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:20:42,664 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:20:42,664 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:20:42,665 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:20:42,666 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:20:42,669 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:20:42,670 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_9_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:42,671 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:20:42,671 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at streamingProcessNew.scala:47)
2017-07-27 19:20:42,671 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 9.0 with 2 tasks
2017-07-27 19:20:42,672 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 9.0 (TID 18, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:20:42,673 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 9.0 (TID 19, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:20:42,673 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 9.0 (TID 19)
2017-07-27 19:20:42,673 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 9.0 (TID 18)
2017-07-27 19:20:42,675 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 492 -> 517
2017-07-27 19:20:42,675 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 496 -> 521
2017-07-27 19:20:43,224 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_8_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:45,013 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154445000 ms
2017-07-27 19:20:48,023 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 9.0 (TID 18). 787 bytes result sent to driver
2017-07-27 19:20:48,024 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 9.0 (TID 18) in 5352 ms on localhost (1/2)
2017-07-27 19:20:50,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154450000 ms
2017-07-27 19:20:51,268 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 9.0 (TID 19). 787 bytes result sent to driver
2017-07-27 19:20:51,269 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 9.0 (TID 19) in 8596 ms on localhost (2/2)
2017-07-27 19:20:51,269 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2017-07-27 19:20:51,269 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 9 (foreachPartition at streamingProcessNew.scala:49) finished in 8.598 s
2017-07-27 19:20:51,270 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 9 finished: foreachPartition at streamingProcessNew.scala:49, took 8.606985 s
2017-07-27 19:20:51,270 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154430000 ms.0 from job set of time 1501154430000 ms
2017-07-27 19:20:51,270 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 21.270 s for time 1501154430000 ms (execution: 8.614 s)
2017-07-27 19:20:51,270 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 17 from persistence list
2017-07-27 19:20:51,271 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154435000 ms.0 from job set of time 1501154435000 ms
2017-07-27 19:20:51,271 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 17
2017-07-27 19:20:51,271 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 16 from persistence list
2017-07-27 19:20:51,271 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 16
2017-07-27 19:20:51,271 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:20:51,271 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154420000 ms
2017-07-27 19:20:51,276 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:20:51,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 10 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:20:51,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 10 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:20:51,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:20:51,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:20:51,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:20:51,278 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:20:51,282 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:20:51,291 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_10_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:51,292 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:20:51,292 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at streamingProcessNew.scala:47)
2017-07-27 19:20:51,292 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 10.0 with 2 tasks
2017-07-27 19:20:51,293 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 10.0 (TID 20, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:20:51,294 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 10.0 (TID 21, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:20:51,294 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 10.0 (TID 21)
2017-07-27 19:20:51,294 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 10.0 (TID 20)
2017-07-27 19:20:51,296 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 521 -> 545
2017-07-27 19:20:51,296 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 517 -> 542
2017-07-27 19:20:51,477 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_9_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:55,015 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154455000 ms
2017-07-27 19:20:56,819 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 10.0 (TID 20). 787 bytes result sent to driver
2017-07-27 19:20:56,821 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 10.0 (TID 20) in 5528 ms on localhost (1/2)
2017-07-27 19:20:57,886 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 10.0 (TID 21). 787 bytes result sent to driver
2017-07-27 19:20:57,893 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 10.0 (TID 21) in 6598 ms on localhost (2/2)
2017-07-27 19:20:57,894 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2017-07-27 19:20:57,895 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 10 (foreachPartition at streamingProcessNew.scala:49) finished in 6.603 s
2017-07-27 19:20:57,896 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 10 finished: foreachPartition at streamingProcessNew.scala:49, took 6.619695 s
2017-07-27 19:20:57,898 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154435000 ms.0 from job set of time 1501154435000 ms
2017-07-27 19:20:57,899 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 19 from persistence list
2017-07-27 19:20:57,899 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 22.898 s for time 1501154435000 ms (execution: 6.628 s)
2017-07-27 19:20:57,899 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154440000 ms.0 from job set of time 1501154440000 ms
2017-07-27 19:20:57,900 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 19
2017-07-27 19:20:57,900 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 18 from persistence list
2017-07-27 19:20:57,901 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 18
2017-07-27 19:20:57,901 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:20:57,901 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154425000 ms
2017-07-27 19:20:57,913 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:20:57,914 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 11 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:20:57,914 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 11 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:20:57,914 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:20:57,914 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:20:57,914 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:20:57,916 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:20:58,000 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:20:58,002 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_11_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:20:58,003 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 11 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:20:58,004 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at streamingProcessNew.scala:47)
2017-07-27 19:20:58,004 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 11.0 with 2 tasks
2017-07-27 19:20:58,007 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 11.0 (TID 22, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:20:58,008 [dispatcher-event-loop-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 11.0 (TID 23, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:20:58,009 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 11.0 (TID 23)
2017-07-27 19:20:58,009 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 11.0 (TID 22)
2017-07-27 19:20:58,012 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 542 -> 567
2017-07-27 19:20:58,012 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:20:58,018 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 11.0 (TID 23). 714 bytes result sent to driver
2017-07-27 19:20:58,022 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 11.0 (TID 23) in 15 ms on localhost (1/2)
2017-07-27 19:21:00,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154460000 ms
2017-07-27 19:21:02,146 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_10_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:21:04,225 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 11.0 (TID 22). 787 bytes result sent to driver
2017-07-27 19:21:04,227 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 11.0 (TID 22) in 6222 ms on localhost (2/2)
2017-07-27 19:21:04,227 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2017-07-27 19:21:04,227 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 11 (foreachPartition at streamingProcessNew.scala:49) finished in 6.222 s
2017-07-27 19:21:04,227 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 11 finished: foreachPartition at streamingProcessNew.scala:49, took 6.314226 s
2017-07-27 19:21:04,228 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154440000 ms.0 from job set of time 1501154440000 ms
2017-07-27 19:21:04,228 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 24.228 s for time 1501154440000 ms (execution: 6.329 s)
2017-07-27 19:21:04,228 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 21 from persistence list
2017-07-27 19:21:04,228 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154445000 ms.0 from job set of time 1501154445000 ms
2017-07-27 19:21:04,228 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 21
2017-07-27 19:21:04,228 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 20 from persistence list
2017-07-27 19:21:04,229 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 20
2017-07-27 19:21:04,229 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:21:04,229 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154430000 ms
2017-07-27 19:21:04,234 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:21:04,234 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 12 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:21:04,234 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 12 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:21:04,234 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:21:04,235 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:21:04,235 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:21:04,236 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:21:04,239 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:21:04,240 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_12_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:21:04,241 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:21:04,241 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at streamingProcessNew.scala:47)
2017-07-27 19:21:04,242 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 12.0 with 2 tasks
2017-07-27 19:21:04,243 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 12.0 (TID 24, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:21:04,244 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 12.0 (TID 25, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:21:04,244 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 12.0 (TID 24)
2017-07-27 19:21:04,244 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 12.0 (TID 25)
2017-07-27 19:21:04,245 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 567 -> 568
2017-07-27 19:21:04,245 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:21:04,249 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 12.0 (TID 25). 714 bytes result sent to driver
2017-07-27 19:21:04,252 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 12.0 (TID 25) in 8 ms on localhost (1/2)
2017-07-27 19:21:05,012 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154465000 ms
2017-07-27 19:21:05,063 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 12.0 (TID 24). 714 bytes result sent to driver
2017-07-27 19:21:05,066 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 12.0 (TID 24) in 824 ms on localhost (2/2)
2017-07-27 19:21:05,067 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 12 (foreachPartition at streamingProcessNew.scala:49) finished in 0.824 s
2017-07-27 19:21:05,067 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2017-07-27 19:21:05,067 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 12 finished: foreachPartition at streamingProcessNew.scala:49, took 0.833030 s
2017-07-27 19:21:05,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154445000 ms.0 from job set of time 1501154445000 ms
2017-07-27 19:21:05,067 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 20.067 s for time 1501154445000 ms (execution: 0.839 s)
2017-07-27 19:21:05,067 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 23 from persistence list
2017-07-27 19:21:05,068 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154450000 ms.0 from job set of time 1501154450000 ms
2017-07-27 19:21:05,068 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 23
2017-07-27 19:21:05,068 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 22 from persistence list
2017-07-27 19:21:05,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:21:05,068 [block-manager-slave-async-thread-pool-4] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 22
2017-07-27 19:21:05,068 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154435000 ms
2017-07-27 19:21:05,073 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:21:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 13 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:21:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 13 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:21:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:21:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:21:05,074 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:21:05,075 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:21:05,078 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:21:05,080 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_13_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:21:05,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 13 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:21:05,080 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at streamingProcessNew.scala:47)
2017-07-27 19:21:05,081 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 13.0 with 2 tasks
2017-07-27 19:21:05,082 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 13.0 (TID 26, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:21:05,082 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 13.0 (TID 27, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:21:05,082 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 13.0 (TID 26)
2017-07-27 19:21:05,082 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 13.0 (TID 27)
2017-07-27 19:21:05,084 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 19:21:05,084 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:21:05,087 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 13.0 (TID 27). 714 bytes result sent to driver
2017-07-27 19:21:05,087 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 13.0 (TID 26). 714 bytes result sent to driver
2017-07-27 19:21:05,089 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 13.0 (TID 27) in 6 ms on localhost (1/2)
2017-07-27 19:21:05,089 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 13.0 (TID 26) in 8 ms on localhost (2/2)
2017-07-27 19:21:05,089 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2017-07-27 19:21:05,089 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 13 (foreachPartition at streamingProcessNew.scala:49) finished in 0.008 s
2017-07-27 19:21:05,089 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 13 finished: foreachPartition at streamingProcessNew.scala:49, took 0.015998 s
2017-07-27 19:21:05,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154450000 ms.0 from job set of time 1501154450000 ms
2017-07-27 19:21:05,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 15.090 s for time 1501154450000 ms (execution: 0.022 s)
2017-07-27 19:21:05,090 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 25 from persistence list
2017-07-27 19:21:05,090 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 25
2017-07-27 19:21:05,090 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 24 from persistence list
2017-07-27 19:21:05,090 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154455000 ms.0 from job set of time 1501154455000 ms
2017-07-27 19:21:05,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:21:05,091 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 24
2017-07-27 19:21:05,091 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154440000 ms
2017-07-27 19:21:05,095 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:21:05,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 14 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:21:05,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 14 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:21:05,096 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:21:05,097 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:21:05,097 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:21:05,098 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:21:05,102 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:21:05,102 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_14_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:21:05,102 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:21:05,102 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at streamingProcessNew.scala:47)
2017-07-27 19:21:05,103 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 14.0 with 2 tasks
2017-07-27 19:21:05,104 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 14.0 (TID 28, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:21:05,104 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 14.0 (TID 29, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:21:05,104 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 14.0 (TID 29)
2017-07-27 19:21:05,104 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 14.0 (TID 28)
2017-07-27 19:21:05,105 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:21:05,105 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 19:21:05,108 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 14.0 (TID 29). 714 bytes result sent to driver
2017-07-27 19:21:05,108 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 14.0 (TID 28). 714 bytes result sent to driver
2017-07-27 19:21:05,110 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 14.0 (TID 28) in 7 ms on localhost (1/2)
2017-07-27 19:21:05,111 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 14.0 (TID 29) in 7 ms on localhost (2/2)
2017-07-27 19:21:05,111 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2017-07-27 19:21:05,111 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 14 (foreachPartition at streamingProcessNew.scala:49) finished in 0.008 s
2017-07-27 19:21:05,111 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 14 finished: foreachPartition at streamingProcessNew.scala:49, took 0.015613 s
2017-07-27 19:21:05,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154455000 ms.0 from job set of time 1501154455000 ms
2017-07-27 19:21:05,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 10.111 s for time 1501154455000 ms (execution: 0.021 s)
2017-07-27 19:21:05,112 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154460000 ms.0 from job set of time 1501154460000 ms
2017-07-27 19:21:05,112 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 27 from persistence list
2017-07-27 19:21:05,112 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 27
2017-07-27 19:21:05,112 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 26 from persistence list
2017-07-27 19:21:05,113 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 26
2017-07-27 19:21:05,113 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:21:05,113 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154445000 ms
2017-07-27 19:21:05,118 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:21:05,118 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 15 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:21:05,119 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 15 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:21:05,119 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:21:05,119 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:21:05,119 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:21:05,120 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:21:05,122 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:21:05,123 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_15_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:21:05,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 15 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:21:05,123 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at streamingProcessNew.scala:47)
2017-07-27 19:21:05,124 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 15.0 with 2 tasks
2017-07-27 19:21:05,125 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 15.0 (TID 30, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:21:05,125 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 15.0 (TID 31, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:21:05,125 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 15.0 (TID 31)
2017-07-27 19:21:05,125 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 15.0 (TID 30)
2017-07-27 19:21:05,126 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:21:05,126 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 19:21:05,129 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 15.0 (TID 31). 714 bytes result sent to driver
2017-07-27 19:21:05,130 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 15.0 (TID 30). 714 bytes result sent to driver
2017-07-27 19:21:05,131 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 15.0 (TID 31) in 6 ms on localhost (1/2)
2017-07-27 19:21:05,132 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 15.0 (TID 30) in 8 ms on localhost (2/2)
2017-07-27 19:21:05,132 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2017-07-27 19:21:05,132 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 15 (foreachPartition at streamingProcessNew.scala:49) finished in 0.008 s
2017-07-27 19:21:05,132 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 15 finished: foreachPartition at streamingProcessNew.scala:49, took 0.014548 s
2017-07-27 19:21:05,133 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154460000 ms.0 from job set of time 1501154460000 ms
2017-07-27 19:21:05,133 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 5.133 s for time 1501154460000 ms (execution: 0.021 s)
2017-07-27 19:21:05,133 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 29 from persistence list
2017-07-27 19:21:05,133 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154465000 ms.0 from job set of time 1501154465000 ms
2017-07-27 19:21:05,133 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 29
2017-07-27 19:21:05,133 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 28 from persistence list
2017-07-27 19:21:05,134 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 28
2017-07-27 19:21:05,134 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:21:05,134 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154450000 ms
2017-07-27 19:21:05,138 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:21:05,139 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 16 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:21:05,139 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 16 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:21:05,139 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:21:05,139 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:21:05,139 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:21:05,140 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:21:05,149 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:21:05,149 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_11_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:21:05,149 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_16_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:21:05,150 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 16 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:21:05,150 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at streamingProcessNew.scala:47)
2017-07-27 19:21:05,151 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 16.0 with 2 tasks
2017-07-27 19:21:05,151 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_12_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:21:05,152 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 16.0 (TID 32, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:21:05,152 [dispatcher-event-loop-3] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_13_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:21:05,153 [dispatcher-event-loop-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 16.0 (TID 33, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:21:05,153 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 16.0 (TID 33)
2017-07-27 19:21:05,153 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 16.0 (TID 32)
2017-07-27 19:21:05,154 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_14_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:21:05,155 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 19:21:05,155 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:21:05,156 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_15_piece0 on 192.168.31.105:53217 in memory (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:21:05,158 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 16.0 (TID 32). 714 bytes result sent to driver
2017-07-27 19:21:05,158 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 16.0 (TID 33). 714 bytes result sent to driver
2017-07-27 19:21:05,160 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 16.0 (TID 33) in 7 ms on localhost (1/2)
2017-07-27 19:21:05,160 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 16.0 (TID 32) in 9 ms on localhost (2/2)
2017-07-27 19:21:05,160 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2017-07-27 19:21:05,160 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 16 (foreachPartition at streamingProcessNew.scala:49) finished in 0.009 s
2017-07-27 19:21:05,160 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 16 finished: foreachPartition at streamingProcessNew.scala:49, took 0.021685 s
2017-07-27 19:21:05,161 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154465000 ms.0 from job set of time 1501154465000 ms
2017-07-27 19:21:05,161 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.161 s for time 1501154465000 ms (execution: 0.028 s)
2017-07-27 19:21:05,161 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 31 from persistence list
2017-07-27 19:21:05,161 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 31
2017-07-27 19:21:05,161 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 30 from persistence list
2017-07-27 19:21:05,162 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 30
2017-07-27 19:21:05,162 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:21:05,162 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154455000 ms
2017-07-27 19:21:10,019 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154470000 ms
2017-07-27 19:21:10,019 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154470000 ms.0 from job set of time 1501154470000 ms
2017-07-27 19:21:10,034 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:21:10,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 17 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:21:10,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 17 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:21:10,035 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:21:10,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:21:10,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:21:10,038 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:21:10,042 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:21:10,043 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_17_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:21:10,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 17 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:21:10,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at streamingProcessNew.scala:47)
2017-07-27 19:21:10,044 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 17.0 with 2 tasks
2017-07-27 19:21:10,046 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 17.0 (TID 34, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:21:10,047 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 17.0 (TID 35, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:21:10,048 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 17.0 (TID 34)
2017-07-27 19:21:10,048 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 17.0 (TID 35)
2017-07-27 19:21:10,051 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:21:10,051 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 19:21:10,056 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 17.0 (TID 34). 714 bytes result sent to driver
2017-07-27 19:21:10,056 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 17.0 (TID 35). 714 bytes result sent to driver
2017-07-27 19:21:10,059 [task-result-getter-2] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 17.0 (TID 34) in 14 ms on localhost (1/2)
2017-07-27 19:21:10,060 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 17.0 (TID 35) in 13 ms on localhost (2/2)
2017-07-27 19:21:10,060 [task-result-getter-3] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2017-07-27 19:21:10,061 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 17 (foreachPartition at streamingProcessNew.scala:49) finished in 0.015 s
2017-07-27 19:21:10,061 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 17 finished: foreachPartition at streamingProcessNew.scala:49, took 0.026718 s
2017-07-27 19:21:10,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154470000 ms.0 from job set of time 1501154470000 ms
2017-07-27 19:21:10,062 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.062 s for time 1501154470000 ms (execution: 0.043 s)
2017-07-27 19:21:10,062 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 33 from persistence list
2017-07-27 19:21:10,063 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 33
2017-07-27 19:21:10,063 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 32 from persistence list
2017-07-27 19:21:10,063 [block-manager-slave-async-thread-pool-0] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 32
2017-07-27 19:21:10,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:21:10,064 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154460000 ms
2017-07-27 19:21:15,016 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154475000 ms
2017-07-27 19:21:15,016 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154475000 ms.0 from job set of time 1501154475000 ms
2017-07-27 19:21:15,031 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:21:15,032 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 18 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:21:15,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 18 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:21:15,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:21:15,033 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:21:15,034 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:21:15,036 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:21:15,039 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:21:15,040 [dispatcher-event-loop-1] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_18_piece0 in memory on 192.168.31.105:53217 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:21:15,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 18 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:21:15,040 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at map at streamingProcessNew.scala:47)
2017-07-27 19:21:15,041 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 18.0 with 2 tasks
2017-07-27 19:21:15,042 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 18.0 (TID 36, localhost, partition 0, ANY, 5656 bytes)
2017-07-27 19:21:15,042 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 18.0 (TID 37, localhost, partition 1, ANY, 5656 bytes)
2017-07-27 19:21:15,042 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 18.0 (TID 36)
2017-07-27 19:21:15,042 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 18.0 (TID 37)
2017-07-27 19:21:15,043 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 568 is the same as ending offset skipping test04 1
2017-07-27 19:21:15,043 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Beginning offset 545 is the same as ending offset skipping test04 0
2017-07-27 19:21:15,047 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 18.0 (TID 37). 714 bytes result sent to driver
2017-07-27 19:21:15,047 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 18.0 (TID 36). 714 bytes result sent to driver
2017-07-27 19:21:15,049 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 18.0 (TID 37) in 7 ms on localhost (1/2)
2017-07-27 19:21:15,049 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 18.0 (TID 36) in 8 ms on localhost (2/2)
2017-07-27 19:21:15,049 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2017-07-27 19:21:15,049 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 18 (foreachPartition at streamingProcessNew.scala:49) finished in 0.008 s
2017-07-27 19:21:15,049 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 18 finished: foreachPartition at streamingProcessNew.scala:49, took 0.017693 s
2017-07-27 19:21:15,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154475000 ms.0 from job set of time 1501154475000 ms
2017-07-27 19:21:15,050 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 0.050 s for time 1501154475000 ms (execution: 0.034 s)
2017-07-27 19:21:15,050 [JobGenerator] INFO  [org.apache.spark.rdd.MapPartitionsRDD] - Removing RDD 35 from persistence list
2017-07-27 19:21:15,051 [block-manager-slave-async-thread-pool-1] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 35
2017-07-27 19:21:15,051 [JobGenerator] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Removing RDD 34 from persistence list
2017-07-27 19:21:15,051 [block-manager-slave-async-thread-pool-5] INFO  [org.apache.spark.storage.BlockManager] - Removing RDD 34
2017-07-27 19:21:15,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:21:15,051 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 1501154465000 ms
2017-07-27 19:24:11,514 [main] INFO  [org.apache.spark.SparkContext] - Running Spark version 2.0.0
2017-07-27 19:24:11,713 [main] WARN  [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-07-27 19:24:11,791 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls to: cluster
2017-07-27 19:24:11,792 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls to: cluster
2017-07-27 19:24:11,792 [main] INFO  [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2017-07-27 19:24:11,793 [main] INFO  [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2017-07-27 19:24:11,793 [main] INFO  [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cluster); groups with view permissions: Set(); users  with modify permissions: Set(cluster); groups with modify permissions: Set()
2017-07-27 19:24:12,646 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 53386.
2017-07-27 19:24:12,666 [main] INFO  [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2017-07-27 19:24:12,684 [main] INFO  [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2017-07-27 19:24:12,698 [main] INFO  [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-74c2f731-cfc9-4881-96d1-e3627d75fa46
2017-07-27 19:24:12,712 [main] INFO  [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 413.9 MB
2017-07-27 19:24:12,755 [main] INFO  [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2017-07-27 19:24:12,842 [main] INFO  [org.spark_project.jetty.util.log] - Logging initialized @2120ms
2017-07-27 19:24:12,946 [main] INFO  [org.spark_project.jetty.server.Server] - jetty-9.2.z-SNAPSHOT
2017-07-27 19:24:12,968 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1b3bc3{/jobs,null,AVAILABLE}
2017-07-27 19:24:12,968 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d3b1f5{/jobs/json,null,AVAILABLE}
2017-07-27 19:24:12,969 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1648ee9{/jobs/job,null,AVAILABLE}
2017-07-27 19:24:12,969 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@d6972f{/jobs/job/json,null,AVAILABLE}
2017-07-27 19:24:12,969 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1be7cd5{/stages,null,AVAILABLE}
2017-07-27 19:24:12,970 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@51218e{/stages/json,null,AVAILABLE}
2017-07-27 19:24:12,970 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@185fa6b{/stages/stage,null,AVAILABLE}
2017-07-27 19:24:12,971 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1366c9b{/stages/stage/json,null,AVAILABLE}
2017-07-27 19:24:12,971 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@158ed3c{/stages/pool,null,AVAILABLE}
2017-07-27 19:24:12,971 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@faea88{/stages/pool/json,null,AVAILABLE}
2017-07-27 19:24:12,971 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@320ade{/storage,null,AVAILABLE}
2017-07-27 19:24:12,972 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@beebb7{/storage/json,null,AVAILABLE}
2017-07-27 19:24:12,972 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13cb11{/storage/rdd,null,AVAILABLE}
2017-07-27 19:24:12,972 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec031{/storage/rdd/json,null,AVAILABLE}
2017-07-27 19:24:12,972 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1005f6c{/environment,null,AVAILABLE}
2017-07-27 19:24:12,972 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f0fba8{/environment/json,null,AVAILABLE}
2017-07-27 19:24:12,972 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5de5a4{/executors,null,AVAILABLE}
2017-07-27 19:24:12,973 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1ef6856{/executors/json,null,AVAILABLE}
2017-07-27 19:24:12,973 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@b7a938{/executors/threadDump,null,AVAILABLE}
2017-07-27 19:24:12,973 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1af2e7d{/executors/threadDump/json,null,AVAILABLE}
2017-07-27 19:24:12,978 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@648ce9{/static,null,AVAILABLE}
2017-07-27 19:24:12,978 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9bf9eb{/,null,AVAILABLE}
2017-07-27 19:24:12,979 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1526469{/api,null,AVAILABLE}
2017-07-27 19:24:12,979 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@14872f8{/stages/stage/kill,null,AVAILABLE}
2017-07-27 19:24:12,989 [main] INFO  [org.spark_project.jetty.server.ServerConnector] - Started ServerConnector@1a90fa8{HTTP/1.1}{0.0.0.0:4040}
2017-07-27 19:24:12,989 [main] INFO  [org.spark_project.jetty.server.Server] - Started @2267ms
2017-07-27 19:24:12,989 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2017-07-27 19:24:12,991 [main] INFO  [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.31.105:4040
2017-07-27 19:24:13,068 [main] INFO  [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2017-07-27 19:24:13,103 [main] INFO  [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53400.
2017-07-27 19:24:13,104 [main] INFO  [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.31.105:53400
2017-07-27 19:24:13,105 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.31.105, 53400)
2017-07-27 19:24:13,109 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.31.105:53400 with 413.9 MB RAM, BlockManagerId(driver, 192.168.31.105, 53400)
2017-07-27 19:24:13,110 [main] INFO  [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.31.105, 53400)
2017-07-27 19:24:13,319 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@13ddbd9{/metrics/json,null,AVAILABLE}
2017-07-27 19:24:13,827 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Slide time = 5000 ms
2017-07-27 19:24:13,828 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Storage level = Serialized 1x Replicated
2017-07-27 19:24:13,828 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Checkpoint interval = null
2017-07-27 19:24:13,829 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Remember interval = 5000 ms
2017-07-27 19:24:13,829 [streaming-start] INFO  [org.apache.spark.streaming.kafka.DirectKafkaInputDStream] - Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@21f64a
2017-07-27 19:24:13,829 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Slide time = 5000 ms
2017-07-27 19:24:13,829 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Storage level = Serialized 1x Replicated
2017-07-27 19:24:13,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Checkpoint interval = null
2017-07-27 19:24:13,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Remember interval = 5000 ms
2017-07-27 19:24:13,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.MappedDStream] - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@80d347
2017-07-27 19:24:13,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Slide time = 5000 ms
2017-07-27 19:24:13,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Storage level = Serialized 1x Replicated
2017-07-27 19:24:13,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Checkpoint interval = null
2017-07-27 19:24:13,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Remember interval = 5000 ms
2017-07-27 19:24:13,830 [streaming-start] INFO  [org.apache.spark.streaming.dstream.ForEachDStream] - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@1868246
2017-07-27 19:24:13,869 [streaming-start] INFO  [org.apache.spark.streaming.util.RecurringTimer] - Started timer for JobGenerator at time 1501154655000
2017-07-27 19:24:13,869 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobGenerator] - Started JobGenerator at 1501154655000 ms
2017-07-27 19:24:13,870 [streaming-start] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Started JobScheduler
2017-07-27 19:24:13,871 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cf395{/streaming,null,AVAILABLE}
2017-07-27 19:24:13,872 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6aa04a{/streaming/json,null,AVAILABLE}
2017-07-27 19:24:13,873 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@9fdb08{/streaming/batch,null,AVAILABLE}
2017-07-27 19:24:13,873 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@814f43{/streaming/batch/json,null,AVAILABLE}
2017-07-27 19:24:13,875 [main] INFO  [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1a00961{/static/streaming,null,AVAILABLE}
2017-07-27 19:24:13,876 [main] INFO  [org.apache.spark.streaming.StreamingContext] - StreamingContext started
2017-07-27 19:24:15,133 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154655000 ms
2017-07-27 19:24:15,138 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154655000 ms.0 from job set of time 1501154655000 ms
2017-07-27 19:24:15,167 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:24:15,180 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:24:15,181 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:24:15,182 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:24:15,183 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:24:15,192 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:24:15,253 [dag-scheduler-event-loop] WARN  [org.apache.spark.util.SizeEstimator] - Failed to check whether UseCompressedOops is set; assuming yes
2017-07-27 19:24:15,260 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:24:15,461 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:24:15,464 [dispatcher-event-loop-0] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.31.105:53400 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:24:15,466 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:24:15,471 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at streamingProcessNew.scala:47)
2017-07-27 19:24:15,473 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2017-07-27 19:24:15,516 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, ANY, 5655 bytes)
2017-07-27 19:24:15,519 [dispatcher-event-loop-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, ANY, 5655 bytes)
2017-07-27 19:24:15,525 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2017-07-27 19:24:15,525 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2017-07-27 19:24:15,553 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 267 -> 292
2017-07-27 19:24:15,553 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 271 -> 296
2017-07-27 19:24:15,626 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCPConfig] - LogStatementsEnabled is set to true, but log4j level is not set at DEBUG. Disabling statement logging.
2017-07-27 19:24:15,857 [Executor task launch worker-0] WARN  [com.jolbox.bonecp.BoneCP] - Thread close connection monitoring has been enabled. This will negatively impact on your performance. Only enable this option for debugging purposes!
2017-07-27 19:24:19,838 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 874 bytes result sent to driver
2017-07-27 19:24:19,853 [task-result-getter-0] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 4333 ms on localhost (1/2)
2017-07-27 19:24:20,017 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Added jobs for time 1501154660000 ms
2017-07-27 19:24:22,233 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 787 bytes result sent to driver
2017-07-27 19:24:22,240 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 6745 ms on localhost (2/2)
2017-07-27 19:24:22,241 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreachPartition at streamingProcessNew.scala:49) finished in 6.754 s
2017-07-27 19:24:22,242 [task-result-getter-1] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-07-27 19:24:22,251 [streaming-job-executor-0] INFO  [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreachPartition at streamingProcessNew.scala:49, took 7.082862 s
2017-07-27 19:24:22,257 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Finished job streaming job 1501154655000 ms.0 from job set of time 1501154655000 ms
2017-07-27 19:24:22,259 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Total delay: 7.256 s for time 1501154655000 ms (execution: 7.120 s)
2017-07-27 19:24:22,259 [JobScheduler] INFO  [org.apache.spark.streaming.scheduler.JobScheduler] - Starting job streaming job 1501154660000 ms.0 from job set of time 1501154660000 ms
2017-07-27 19:24:22,267 [streaming-job-executor-0] INFO  [org.apache.spark.SparkContext] - Starting job: foreachPartition at streamingProcessNew.scala:49
2017-07-27 19:24:22,268 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.ReceivedBlockTracker] - Deleting batches: 
2017-07-27 19:24:22,268 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (foreachPartition at streamingProcessNew.scala:49) with 2 output partitions
2017-07-27 19:24:22,268 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreachPartition at streamingProcessNew.scala:49)
2017-07-27 19:24:22,268 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2017-07-27 19:24:22,269 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2017-07-27 19:24:22,269 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:47), which has no missing parents
2017-07-27 19:24:22,271 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 413.9 MB)
2017-07-27 19:24:22,273 [JobGenerator] INFO  [org.apache.spark.streaming.scheduler.InputInfoTracker] - remove old batch metadata: 
2017-07-27 19:24:22,276 [dag-scheduler-event-loop] INFO  [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 413.9 MB)
2017-07-27 19:24:22,276 [dispatcher-event-loop-2] INFO  [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.31.105:53400 (size: 2.1 KB, free: 413.9 MB)
2017-07-27 19:24:22,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2017-07-27 19:24:22,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at streamingProcessNew.scala:47)
2017-07-27 19:24:22,277 [dag-scheduler-event-loop] INFO  [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2017-07-27 19:24:22,281 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5655 bytes)
2017-07-27 19:24:22,282 [dispatcher-event-loop-3] INFO  [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5655 bytes)
2017-07-27 19:24:22,283 [Executor task launch worker-1] INFO  [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2017-07-27 19:24:22,283 [Executor task launch worker-0] INFO  [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2017-07-27 19:24:22,286 [Executor task launch worker-1] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 0 offsets 296 -> 321
2017-07-27 19:24:22,286 [Executor task launch worker-0] INFO  [org.apache.spark.streaming.kafka.KafkaRDD] - Computing topic test04, partition 1 offsets 292 -> 317
